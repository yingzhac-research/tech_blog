{
  "hash": "c80d9a9eff670b2455e3fe07e98164ba",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第4章：Tokenization与数据基础\"\nsubtitle: \"被低估的基础设施：从文本到模型输入的艺术\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-25\"\ncategories: [NLP, Tokenization, BPE, WordPiece, SentencePiece, Data]\ntags: [分词, 子词, Byte-level BPE, Unigram LM, 数据清洗, 多语言]\ndescription: \"Tokenizer不是预处理工具，它是模型架构的隐藏维度：从词级别到子词方法的演进，以及分词策略对模型能力的深远影响。\"\nimage: \"figures/chapter-4/fig-bpe-merge-process.png\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\n---\n\n> **核心论点**：Tokenizer不是预处理工具，它是模型架构的隐藏维度。\n>\n> **历史坐标**：2016 | Sennrich et al. (BPE for NMT) | 从词级别到子词的范式转变\n\n---\n\n## 从上一章说起\n\n上一章我们学习了词向量——如何用稠密向量表示词的语义。Word2Vec、GloVe、FastText让我们第一次能够在数学上捕获\"cat\"和\"dog\"的相似性。这是表示学习的巨大进步。\n\n但我们回避了一个看似简单却极其重要的问题：**这些\"词\"是从哪里来的？**\n\n回顾FastText的出现动机：Word2Vec无法处理训练时没见过的词（OOV问题）。FastText通过将词分解为子词（character n-grams）来解决这个问题——即使\"unfriendliness\"没出现过，只要它的子词（\"un\"、\"friend\"、\"ness\"）出现过，就能组合出一个合理的向量。\n\nFastText的成功暗示了一个深刻的洞察：**\"词\"并不是文本处理的最佳单位**。\n\n这引出了本章的核心问题：在把文本送入模型之前，我们应该如何切分它？按词切？按字符切？还是有更聪明的方法？这个看似是\"预处理\"的问题，实际上深刻地影响着模型的能力、效率和公平性。\n\n> 💡 **本章核心洞察**：Tokenizer决定了模型\"看到\"什么。它不是可以随意选择的预处理步骤，而是模型架构的隐藏维度——影响计算效率、多语言公平性、数学推理能力，甚至安全性。\n\n---\n\n## 问题的本质是什么？\n\n### 什么是一个\"词\"？\n\n这个问题比看起来复杂得多。\n\n对于英语，我们可能直觉地认为\"词就是空格分隔的单位\"。但仔细想想：\n\n- \"don't\"是一个词还是两个？\n- \"New York\"是一个词还是两个？\n- \"ice-cream\"呢？\n- \"it's\"和\"its\"在tokenization层面应该相同还是不同？\n\n中文更麻烦。中文没有空格分隔词，需要专门的**分词**（word segmentation）算法。考虑这个句子：\n\n> \"研究生命的起源\"\n\n这应该切分为\"研究/生命/的/起源\"还是\"研究生/命/的/起源\"？没有上下文，无法确定。中文分词本身就是一个复杂的NLP任务，需要模型来完成——但我们现在讨论的是如何把文本送入模型，这形成了一个先有鸡还是先有蛋的悖论。\n\n再看形态丰富的语言，如德语、芬兰语、土耳其语。德语允许复合词任意组合，\"Donaudampfschifffahrtsgesellschaftskapitän\"（多瑙河轮船公司船长）是一个合法的德语词。土耳其语是黏着语，一个词可以通过添加后缀表达复杂的语法关系。如果坚持用词作为基本单位，词汇表会爆炸式增长，而且每个词的训练样本会稀少到无法学习。\n\n### 问题的精确定义\n\n让我们精确地定义Tokenization要解决的问题。\n\n给定一个字符串$s$，我们需要找到一个函数$\\text{tokenize}: \\Sigma^* \\rightarrow \\mathcal{V}^*$，把任意字符串映射为一个token序列，其中$\\mathcal{V}$是预定义的词汇表（vocabulary）。\n\n这个函数需要满足几个关键性质：**完备性**——任何输入字符串都能被tokenize，不能有\"无法处理\"的情况；**确定性**——相同的输入总是产生相同的输出；以及理想情况下的**可逆性**——从token序列能够恢复原始字符串。\n\n除了这些基本约束，我们还希望函数在多个维度上表现良好。**词汇表大小**不能太大（否则embedding矩阵爆炸），也不能太小（否则序列太长）。产生的**序列长度**要合理（否则计算成本高，超出上下文窗口）。理想情况下，每个token应该有明确的**语义完整性**。此外，不同语言的相同内容，token数量应该大致相当——即**跨语言公平性**。\n\n这些目标之间存在张力。词汇表越小，序列越长；词汇表越大，稀有token越多。找到正确的平衡是tokenization的核心挑战。\n\n### 为什么这个问题重要？\n\n你可能会问：这真的那么重要吗？不就是切分文本吗？\n\n让我用几个具体的例子说明tokenizer选择对模型的深远影响。\n\n**计算效率**。同样的文本，不同tokenizer产生的序列长度可能相差2-3倍。如果你的模型有4096的上下文窗口，一个高效的tokenizer可以让你处理的实际文本量翻倍。Transformer的计算复杂度是$O(n^2)$（$n$是序列长度），序列长度减半意味着计算量减少75%。\n\n**多语言公平性**。同样的语义内容，英文可能只需要1个token，而中文可能需要2-3个token，日文可能需要更多。这意味着在相同的上下文窗口内，模型能处理的中文内容比英文少。这不是技术实现的细节，而是系统性的不公平。\n\n**数学和代码能力**。数字\"380\"可能被切分为单个token\"380\"，也可能被切分为\"3\"、\"8\"、\"0\"三个token。研究表明，后者会严重损害模型的算术推理能力——模型很难理解\"380\"和\"381\"只差1。类似地，代码中的缩进、变量名、运算符的切分方式直接影响模型理解代码的能力。\n\n**安全性**。Tokenizer的切分边界可能被攻击者利用。某些prompt injection攻击正是利用了特定token序列的特殊行为。理解tokenizer是理解模型安全边界的前提。\n\n---\n\n## 核心思想与直觉\n\n### 三种基本策略\n\n面对\"如何切分文本\"这个问题，有三种基本策略，它们代表了不同的权衡。\n\n**策略一：词级别（Word-level）**\n\n最直观的方法是按词切分。建立一个词汇表，包含训练语料中出现的所有词（或最常见的N个词），然后把每个词映射到一个token。\n\n这种方法的优点是每个token都有明确的语义——\"cat\"就是猫，\"running\"就是正在跑。但缺点也很明显：词汇表庞大（英语常用词就有几十万），OOV问题严重（任何不在词汇表中的词都无法处理），形态变化浪费空间（\"run\"、\"runs\"、\"running\"、\"ran\"是四个独立的token）。\n\n**策略二：字符级别（Character-level）**\n\n走向另一个极端：按字符切分。词汇表只需要包含所有可能的字符（英语只需要不到100个），OOV问题彻底消失。\n\n但代价是序列变得极长。\"tokenization\"这个词变成了13个token，计算成本大增。更糟糕的是，字符本身几乎没有语义——\"t\"、\"o\"、\"k\"...模型需要从这些原子构建词的含义，这是一个巨大的学习负担。\n\n**策略三：子词级别（Subword-level）**\n\n能不能找到一个中间地带？这正是现代NLP的选择：**子词分词**（subword tokenization）。\n\n核心思想是：常见的词保持完整，罕见的词分解为更小的、有意义的单元。比如：\n\n- \"running\" → \"running\"（常见，保持完整）\n- \"tokenization\" → \"token\" + \"ization\"（罕见，分解为常见子词）\n- \"unfriendliness\" → \"un\" + \"friend\" + \"li\" + \"ness\"\n\n这样，词汇表大小可控（通常32K-64K），序列长度合理，而且任何词都能被切分（最坏情况退化到字符级别）。子词还经常具有语义或语法含义——\"un-\"表示否定，\"-ing\"表示进行时，\"-ness\"表示名词化。\n\n![三种Tokenization策略对比：以\"tokenization\"为例，词级别产生1个token，字符级别产生12个token，而子词（BPE）只需要2个token——\"token\"和\"ization\"。子词方法在词汇表大小和序列长度之间找到了最佳平衡。](figures/chapter-4/fig-tokenization-strategies.png){#fig-tokenization-strategies}\n\n::: {.figure-caption}\n*Python生成，概念参考 Sennrich et al. (2016) \"Neural Machine Translation of Rare Words with Subword Units\". [arXiv:1508.07909](https://arxiv.org/abs/1508.07909)*\n:::\n\n### 子词分词的直觉：数据压缩\n\n理解子词分词的最佳方式是把它类比为**数据压缩**。\n\n想象你要设计一个编码方案，用最少的符号表示大量文本。直觉上，你会：\n\n1. 给最常见的模式分配短码\n2. 让不常见的模式用常见模式的组合表示\n\n这正是BPE（Byte Pair Encoding）的核心思想，而BPE最初就是一种数据压缩算法。\n\n考虑一个简单的例子。假设你的文本中经常出现\"th\"、\"the\"、\"ing\"这些模式。如果每次都用两个或三个字符表示它们，很浪费。不如给\"th\"一个专门的符号（比如\"θ\"），给\"the\"另一个符号（比如\"Θ\"）。这样，原本需要3个符号的\"the\"只需要1个符号。\n\n子词分词做的就是这件事：自动发现文本中的高频模式，给它们分配token。最终的词汇表是数据驱动的，反映了训练语料的统计规律。\n\n---\n\n## 技术细节\n\n### BPE算法：从数据压缩到NLP\n\nBPE（Byte Pair Encoding）是目前最广泛使用的子词分词算法之一，被GPT系列、RoBERTa等模型采用。\n\n**算法思想**：从字符开始，迭代地合并最频繁的相邻token对，直到达到目标词汇表大小。\n\n让我们用一个具体的例子来理解这个过程。\n\n#### 完整数值示例：BPE词汇表构建\n\n**设定**：训练语料包含以下词及其频率：\n\n| 词 | 频率 |\n|---|---|\n| low | 5 |\n| lower | 2 |\n| newest | 6 |\n| widest | 3 |\n\n**Step 0：初始化**\n\n首先，我们把每个词表示为字符序列，并添加一个特殊的词尾标记\"_\"（表示词边界）：\n\n```\nl o w _     : 5\nl o w e r _ : 2\nn e w e s t _ : 6\nw i d e s t _ : 3\n```\n\n初始词汇表是所有字符：`{l, o, w, e, r, n, s, t, i, d, _}`，共11个token。\n\n**Step 1：统计相邻token对频率**\n\n遍历所有词，统计相邻token对出现的总次数：\n\n| Token对 | 频率计算 | 总频率 |\n|---|---|---|\n| (l, o) | 5 + 2 = 7 | 7 |\n| (o, w) | 5 + 2 = 7 | 7 |\n| (w, _) | 5 | 5 |\n| (w, e) | 2 | 2 |\n| (e, r) | 2 | 2 |\n| (r, _) | 2 | 2 |\n| (n, e) | 6 | 6 |\n| (e, w) | 6 | 6 |\n| (e, s) | 6 + 3 = 9 | **9** |\n| (s, t) | 6 + 3 = 9 | **9** |\n| (t, _) | 6 + 3 = 9 | **9** |\n| (w, i) | 3 | 3 |\n| (i, d) | 3 | 3 |\n| (d, e) | 3 | 3 |\n\n最高频的对是 (e, s)、(s, t)、(t, _)，都是9次。我们选择第一个遇到的：**(e, s)**。\n\n**Step 2：合并最频繁的对**\n\n创建新token \"es\"，替换所有出现的 (e, s)：\n\n```\nl o w _     : 5\nl o w e r _ : 2\nn e w es t _ : 6\nw i d es t _ : 3\n```\n\n词汇表变为：`{l, o, w, e, r, n, s, t, i, d, _, es}`，12个token。\n\n**Step 3：重复统计和合并**\n\n继续统计相邻对频率：\n\n| Token对 | 总频率 |\n|---|---|\n| (es, t) | 6 + 3 = **9** |\n| (l, o) | 7 |\n| (o, w) | 7 |\n| ... | ... |\n\n最高频：**(es, t) = 9**，合并为 \"est\"：\n\n```\nl o w _     : 5\nl o w e r _ : 2\nn e w est _ : 6\nw i d est _ : 3\n```\n\n词汇表：`{l, o, w, e, r, n, s, t, i, d, _, es, est}`，13个token。\n\n**Step 4：继续合并**\n\n| Token对 | 总频率 |\n|---|---|\n| (est, _) | 6 + 3 = **9** |\n| (l, o) | 7 |\n| (o, w) | 7 |\n| ... | ... |\n\n合并 **(est, _)** 为 \"est_\"：\n\n```\nl o w _     : 5\nl o w e r _ : 2\nn e w est_ : 6\nw i d est_ : 3\n```\n\n**Step 5-7：继续迭代**\n\n| 迭代 | 合并的对 | 新token |\n|---|---|---|\n| 5 | (l, o) | lo |\n| 6 | (lo, w) | low |\n| 7 | (low, _) | low_ |\n\n经过7次合并后：\n\n```\nlow_     : 5\nlow e r _ : 2\nn e w est_ : 6\nw i d est_ : 3\n```\n\n**最终词汇表**（假设目标大小是18）：\n\n```\n{l, o, w, e, r, n, s, t, i, d, _, es, est, est_, lo, low, low_, ...}\n```\n\n**关键观察**：\n\n1. 高频词\"low\"被学习为完整token\n2. 常见后缀\"est_\"被学习为一个token\n3. 罕见词\"lower\"仍然需要多个token：low + e + r + _\n4. 词汇表从纯字符逐步构建，包含字符、常见子词、常见词\n\n![BPE合并过程可视化：以\"lowest\"为例，从字符序列出发，依次合并最高频的相邻对（e,s）→\"es\"、（es,t）→\"est\"、（l,o）→\"lo\"，词汇表逐步增长。每一步合并都由语料统计驱动。](figures/chapter-4/fig-bpe-merge-process.png){#fig-bpe-merge-process}\n\n::: {.figure-caption}\n*Python生成，数值示例基于 Sennrich et al. (2016) \"Neural Machine Translation of Rare Words with Subword Units\", Section 3. [arXiv:1508.07909](https://arxiv.org/abs/1508.07909)*\n:::\n\n#### BPE的形式化算法\n\n::: {.callout-note}\n## Algorithm 1: Learn BPE Operations (Sennrich et al., 2016)\n\n以下是论文原文中的Python实现。这段代码展示了BPE算法的核心逻辑：统计相邻符号对的频率，合并最频繁的对，重复直到达到目标词汇表大小。\n\n```python\nimport re, collections\n\ndef get_stats(vocab):\n    \"\"\"统计所有相邻符号对的频率\"\"\"\n    pairs = collections.defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols)-1):\n            pairs[symbols[i],symbols[i+1]] += freq\n    return pairs\n\ndef merge_vocab(pair, v_in):\n    \"\"\"将vocab中所有出现的pair合并为新符号\"\"\"\n    v_out = {}\n    bigram = re.escape(' '.join(pair))\n    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n    for word in v_in:\n        w_out = p.sub(''.join(pair), word)\n        v_out[w_out] = v_in[word]\n    return v_out\n\n# 示例词汇表（词用空格分隔的字符序列表示）\nvocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n         'n e w e s t </w>':6, 'w i d e s t </w>':3}\nnum_merges = 10\n\nfor i in range(num_merges):\n    pairs = get_stats(vocab)\n    best = max(pairs, key=pairs.get)\n    vocab = merge_vocab(best, vocab)\n    print(best)\n```\n\n**输出**（前4次合并）：\n```\n('e', 's')      →  es\n('es', 't')     →  est\n('est', '</w>') →  est</w>\n('l', 'o')      →  lo\n...\n```\n\n*Source: [Sennrich et al. (2016)](https://arxiv.org/abs/1508.07909), Algorithm 1*\n:::\n\n论文中使用`</w>`作为词尾标记（end-of-word），这与我们之前示例中的`_`作用相同。下面是一个更详细的实现版本，带有完整注释：\n\n```python\ndef learn_bpe(corpus, num_merges):\n    \"\"\"\n    学习BPE词汇表\n\n    Args:\n        corpus: 词频字典 {word: frequency}\n        num_merges: 合并次数（决定最终词汇表大小）\n\n    Returns:\n        merges: 合并规则列表\n        vocab: 最终词汇表\n    \"\"\"\n    # Step 1: 初始化——将词拆分为字符，添加词尾标记\n    vocab = {}\n    for word, freq in corpus.items():\n        # \"low\" -> ['l', 'o', 'w', '_']\n        chars = list(word) + ['_']\n        vocab[tuple(chars)] = freq\n\n    merges = []\n\n    for i in range(num_merges):\n        # Step 2: 统计所有相邻token对的频率\n        pairs = defaultdict(int)\n        for word, freq in vocab.items():\n            for j in range(len(word) - 1):\n                pairs[(word[j], word[j+1])] += freq\n\n        if not pairs:\n            break\n\n        # Step 3: 找到最频繁的对\n        best_pair = max(pairs, key=pairs.get)\n        merges.append(best_pair)\n\n        # Step 4: 合并这个对，更新vocab\n        new_vocab = {}\n        for word, freq in vocab.items():\n            new_word = merge_pair(word, best_pair)\n            new_vocab[new_word] = freq\n        vocab = new_vocab\n\n    return merges, vocab\n\ndef merge_pair(word, pair):\n    \"\"\"将word中所有出现的pair合并\"\"\"\n    new_word = []\n    i = 0\n    while i < len(word):\n        if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n            new_word.append(word[i] + word[i+1])\n            i += 2\n        else:\n            new_word.append(word[i])\n            i += 1\n    return tuple(new_word)\n```\n\n#### 使用BPE进行Tokenization\n\n训练好BPE词汇表后，如何tokenize一个新词？\n\n```python\ndef tokenize_bpe(word, merges):\n    \"\"\"\n    使用学习到的BPE规则tokenize一个词\n\n    核心思想：按照学习时的合并顺序，依次应用合并规则\n    \"\"\"\n    # 初始化为字符序列\n    word = list(word) + ['_']\n\n    # 按顺序应用每个合并规则\n    for pair in merges:\n        i = 0\n        while i < len(word) - 1:\n            if (word[i], word[i+1]) == pair:\n                word = word[:i] + [word[i] + word[i+1]] + word[i+2:]\n            else:\n                i += 1\n\n    return word\n```\n\n**例子**：tokenize \"lowest\"\n\n- 初始：['l', 'o', 'w', 'e', 's', 't', '_']\n- 应用 (e, s) → 'es'：['l', 'o', 'w', 'es', 't', '_']\n- 应用 (es, t) → 'est'：['l', 'o', 'w', 'est', '_']\n- 应用 (est, _) → 'est_'：['l', 'o', 'w', 'est_']\n- 应用 (l, o) → 'lo'：['lo', 'w', 'est_']\n- 应用 (lo, w) → 'low'：['low', 'est_']\n\n最终结果：**['low', 'est_']**\n\n即使\"lowest\"没有出现在训练语料中，BPE也能将它合理地分解为\"low\"和\"est_\"——这两个都是学习到的有意义的子词。\n\n#### BPE与其他分词方法的实证对比\n\nSennrich et al. (2016) 在德语语料上对比了不同分词方法的效果。下表展示了各方法的token数、词汇表大小和未知词数量：\n\n| 分词方法 | Token数量 | 词汇表大小 | 测试集未知词 |\n|----------|-----------|------------|--------------|\n| 不分词（词级别） | 100M | 1,750,000 | 1,079 |\n| 字符级别 | 550M | 3,000 | 0 |\n| Character bigrams | 306M | 20,000 | 34 |\n| Character trigrams | 214M | 120,000 | 59 |\n| 复合词拆分 | 102M | 1,100,000 | 643 |\n| Morfessor | 109M | 544,000 | 237 |\n| 连字符分割 | 186M | 404,000 | 230 |\n| **BPE** | 112M | 63,000 | **0** |\n| **BPE (joint)** | 111M | 82,000 | 32 |\n\n: 不同分词技术在德语训练语料上的统计对比。*Source: [Sennrich et al. (2016)](https://arxiv.org/abs/1508.07909), Table 1* {#tbl-segmentation-comparison}\n\n**关键观察**：\n\n- **字符级别**：完全消除未知词，但token数量爆炸（5.5倍），计算成本极高\n- **传统形态学方法**（复合词拆分、Morfessor）：仍有大量未知词，词汇表依然庞大\n- **BPE**：在保持合理token数量（仅比词级别多12%）的同时，实现了**零未知词**，词汇表紧凑（63K）\n\n这正是BPE成为现代NLP标准的原因：它在**词汇表大小**、**序列长度**和**OOV处理**之间找到了最佳平衡。\n\n### WordPiece：BERT的选择\n\nWordPiece是Google提出的子词算法，被BERT、DistilBERT等模型使用。它与BPE的思想相似，但在选择合并哪个pair时使用了不同的标准。\n\n**BPE vs WordPiece的关键区别**：\n\n- BPE选择**出现频率最高**的pair\n- WordPiece选择**合并后使语言模型困惑度下降最多**的pair\n\nWordPiece的选择标准可以写成：\n\n$$\n\\text{score}(x, y) = \\frac{\\text{freq}(xy)}{\\text{freq}(x) \\times \\text{freq}(y)}\n$$\n\n这个分数本质上是在问：$xy$一起出现的频率，相对于$x$和$y$各自出现频率的乘积，高多少？这类似于点互信息（PMI），衡量的是$x$和$y$的共现是否超出了随机期望。\n\n**直觉理解**：考虑\"th\"和\"qu\"。在英语中，\"th\"几乎总是一起出现（\"the\"、\"this\"、\"that\"），而\"qu\"后面几乎总是跟着\"u\"（\"question\"、\"queen\"）。WordPiece的标准会优先合并这种\"绑定程度\"高的pair，而不仅仅是出现次数多的pair。\n\n**WordPiece的特殊标记**：\n\nWordPiece使用\"##\"前缀标记非词首的子词。比如\"tokenization\"可能被切分为：\n\n```\n[\"token\", \"##ization\"]\n```\n\n\"##\"告诉我们\"ization\"不是一个独立的词，而是接在前一个token后面的后缀。这个设计使得从token序列恢复原始文本更容易。\n\n### Unigram Language Model：概率视角\n\nUnigram LM是另一种子词分词方法，由SentencePiece库实现，被T5、ALBERT等模型使用。它采用了一个根本不同的视角：不是从小到大构建词汇表，而是从大到小剪枝。\n\n**核心思想**：假设每个子词独立出现，用一个unigram语言模型给每种切分方式打分。\n\n给定词汇表$\\mathcal{V}$和每个子词$x$的概率$p(x)$，一个句子$S$的某种切分$\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$的概率是：\n\n$$\nP(\\mathbf{x}) = \\prod_{i=1}^{n} p(x_i)\n$$\n\n最优切分是使这个概率最大化的切分：\n\n$$\n\\mathbf{x}^* = \\arg\\max_{\\mathbf{x}} P(\\mathbf{x}) = \\arg\\max_{\\mathbf{x}} \\sum_{i=1}^{n} \\log p(x_i)\n$$\n\n**训练过程**（与BPE相反）：\n\n1. 从一个很大的初始词汇表开始（包含所有字符、常见子串等）\n2. 用EM算法估计每个子词的概率\n3. 计算每个子词的\"贡献\"——移除它会使整体likelihood下降多少\n4. 移除贡献最小的子词（保留一定比例）\n5. 重复2-4，直到词汇表达到目标大小\n\nUnigram相比BPE有几个独特的优势。首先，它理论上更优雅——每种切分都有明确的概率解释，而不是像BPE那样依赖贪心的频率统计。更重要的是，Unigram可以为同一个词给出多种切分，这使得subword regularization成为可能：训练时按概率采样不同的切分方式作为数据增强，从而增强模型的鲁棒性。\n\n**例子**：对于\"unigram\"，可能的切分包括：\n\n- [\"uni\", \"gram\"] 概率$p_1$\n- [\"un\", \"i\", \"gram\"] 概率$p_2$\n- [\"u\", \"n\", \"i\", \"g\", \"r\", \"a\", \"m\"] 概率$p_3$\n\n推理时选择概率最高的切分，训练时可以按概率采样不同切分。\n\n### SentencePiece：语言无关的统一方案\n\nSentencePiece是Google开发的一个tokenization库，它的关键创新是**直接在原始文本上操作**，不需要预先的词分割。\n\n**传统流程的问题**：\n\n```\n原始文本 → 词分割（按空格或分词器）→ 子词分割（BPE/WordPiece）\n```\n\n这个流程对英语工作良好（空格是自然的词边界），但对中文、日文等没有空格的语言不友好——需要先用一个分词器，引入了额外的复杂性和错误来源。\n\n**SentencePiece的方案**：\n\n```\n原始文本（包括空格）→ 子词分割\n```\n\nSentencePiece把空格也当作一个普通字符处理，用特殊符号\"▁\"（U+2581）表示词边界。比如\"Hello world\"变成\"▁Hello▁world\"，然后直接在这个字符序列上做BPE或Unigram。\n\n这样，不需要任何语言特定的预处理，同一个算法可以处理任何语言。空格信息被保留在token中（以\"▁\"的形式），不会丢失。\n\n```python\n# SentencePiece的tokenization示例\nimport sentencepiece as spm\n\n# 加载预训练模型\nsp = spm.SentencePieceProcessor()\nsp.Load(\"model.model\")\n\n# Tokenize\ntext = \"Hello world\"\ntokens = sp.EncodeAsPieces(text)\n# 输出: ['▁Hello', '▁world']\n\ntext_zh = \"你好世界\"\ntokens_zh = sp.EncodeAsPieces(text_zh)\n# 输出: ['▁', '你', '好', '世', '界']  # 中文字符通常各自成为token\n```\n\n### Byte-level BPE：GPT-2的创新\n\nGPT-2引入了一个重要的创新：**Byte-level BPE**。\n\n**传统方法的问题**：即使是SentencePiece，也需要定义一个字符集。如果遇到训练时没见过的字符（比如某种稀有语言的Unicode字符），仍然会变成未知token。\n\n**Byte-level的解决方案**：不在字符级别操作，而是在**字节级别**操作。任何文本都可以用UTF-8编码为字节序列（0-255），所以基础词汇表只需要256个token。\n\n```python\n# 任何字符串都可以变成字节序列\ntext = \"Hello 你好 🎉\"\nbytes_seq = text.encode('utf-8')\n# b'Hello \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd \\xf0\\x9f\\x8e\\x89'\n```\n\n然后在这个字节序列上做BPE，学习常见的字节组合。\n\nByte-level方法的关键优势在于它彻底解决了OOV问题——任何Unicode字符都能编码，无论是emoji、稀有语言、甚至乱码。同时它完全语言无关，不需要任何语言特定的处理，而且词汇表大小可控，从256个基础字节出发，通过合并操作构建到目标大小。\n\n不过这种方法也有潜在代价：多字节字符可能被切开。UTF-8编码中，中文字符通常占3个字节，一个中文字符可能被切分为多个token，导致中文的token效率比英文低。\n\n**为什么现代LLM都用Byte-level？**\n\n因为它是最\"安全\"的选择。对于要部署到全世界、处理任意输入的大型语言模型，不能有任何\"无法处理\"的情况。Byte-level BPE保证了完备性——任何输入都能被tokenize，最坏情况只是效率低一点（字节序列很长）。\n\n---\n\n## Tokenizer是模型的一部分\n\n这是本章最重要的观点：**Tokenizer不是可以随意选择的预处理步骤，它是模型架构的隐藏维度**。\n\n### 计算效率：序列长度的隐藏成本\n\n同样的文本，不同tokenizer产生的序列长度可能相差巨大。\n\n考虑这个英文句子：\n\n> \"The transformer architecture revolutionized natural language processing.\"\n\n| Tokenizer | Token数量 | Token序列 |\n|---|---|---|\n| 词级别 | 7 | [The, transformer, architecture, revolutionized, natural, language, processing] |\n| GPT-2 (BPE) | 7 | [The, Ġtransformer, Ġarchitecture, Ġrevolution, ized, Ġnatural, Ġlanguage, Ġprocessing] |\n| 字符级别 | 62 | [T, h, e, ␣, t, r, a, n, s, ...] |\n\n对于这个简单句子，差异不大。但看看代码：\n\n```python\ndef calculate_attention(query, key, value):\n    scores = torch.matmul(query, key.transpose(-2, -1))\n    return torch.softmax(scores, dim=-1) @ value\n```\n\n| Tokenizer | Token数量 |\n|---|---|\n| GPT-2 | ~40 |\n| 字符级别 | ~150 |\n\n差异接近4倍！在Transformer中，attention的计算复杂度是$O(n^2)$，这意味着字符级别的计算量是GPT-2的16倍。\n\n### 多语言公平性：隐藏的不平等\n\n这是一个经常被忽视但极其重要的问题。\n\n考虑\"人工智能\"这个概念：\n\n| 语言 | 文本 | GPT-2 Token数 |\n|---|---|---|\n| 英语 | artificial intelligence | 2 |\n| 中文 | 人工智能 | 4-6（取决于具体tokenizer）|\n| 日语 | 人工知能 | 4-6 |\n| 阿拉伯语 | الذكاء الاصطناعي | 8-12 |\n\n![不同语言表达\"Artificial Intelligence\"所需的token数量差异（GPT-2 Tokenizer）。英文仅需2个token，而阿拉伯语需要9个，中文和日文也远多于英文。这种不对称源于tokenizer训练语料中英文的主导地位。](figures/chapter-4/fig-multilingual-efficiency.png){#fig-multilingual-efficiency}\n\n::: {.figure-caption}\n*Python生成，数据参考 Petrov et al. (2023) \"Language Model Tokenizers Introduce Unfairness Between Languages\". [arXiv:2305.15425](https://arxiv.org/abs/2305.15425)*\n:::\n\n**这意味着什么？** 在相同的上下文窗口（比如4096 tokens）内，模型能处理的中文内容比英文少得多。由于API按token计费，中文用户的调用成本也更高。更深层地说，模型学习中文需要消耗更多的\"token带宽\"，这可能导致中文能力系统性地弱于英文。\n\n这不是技术实现的细节，而是系统性的不公平。解决这个问题需要在设计tokenizer时就考虑多语言平衡，或者使用专门为某种语言优化的tokenizer。\n\n### 数学和代码能力：数字的切分\n\n数字的tokenization对模型的算术能力有深远影响。\n\n考虑数字\"12345\"的几种切分方式：\n\n| 切分方式 | Tokens |\n|---|---|\n| 整体 | [\"12345\"] |\n| 按位 | [\"1\", \"2\", \"3\", \"4\", \"5\"] |\n| 混合 | [\"123\", \"45\"] |\n\n研究表明，当数字被切分为多个token时，模型很难进行算术运算。\n\n**为什么？**\n\n考虑加法\"12345 + 67890\"。如果模型看到的是：\n```\n[\"123\", \"45\", \"+\", \"678\", \"90\"]\n```\n\n它需要：\n1. 理解\"123\"和\"45\"组合成12345\n2. 理解\"678\"和\"90\"组合成67890\n3. 进行加法\n4. 把结果拆分回token\n\n这比直接处理完整数字复杂得多。而且，不同的数字可能有不同的切分方式，模型需要学习无数种组合模式。\n\n**现代方法的改进**：\n\n- 一些模型专门把数字处理为单独的token\n- 或者把数字按位切分，统一处理方式\n- 或者在训练数据中加入大量算术例子，让模型学习切分模式\n\n### 安全性：Tokenization边界的攻击\n\nTokenizer的切分边界可能被攻击者利用。\n\n**例子：Token边界绕过**\n\n假设某个安全过滤器阻止了\"dangerous\"这个词。但如果tokenizer把它切分为\"danger\" + \"ous\"，攻击者可能通过改变输入方式（比如添加空格或特殊字符）来改变切分，绕过过滤。\n\n**例子：特殊Token利用**\n\n某些tokenizer有特殊token（如`<|endoftext|>`），如果用户输入能够产生这些token，可能导致意外行为。这是一些prompt injection攻击的基础。\n\n**安全启示**：\n\n- 安全过滤应该同时在原始文本和token级别进行\n- 理解tokenizer的行为是理解模型安全边界的前提\n- 特殊token需要特别处理，防止用户输入产生它们\n\n---\n\n## 数据质量：清洗、去重与污染\n\nTokenization是数据进入模型的入口，但数据本身的质量同样关键。\"Garbage in, garbage out\"在大语言模型时代更加明显。\n\n### 数据清洗：从噪声中提取信号\n\n互联网上的原始文本充满噪声——HTML标签和JavaScript代码混杂其中，导航菜单、广告、版权声明占据大量篇幅，重复的模板文本（如页脚）反复出现，编码错误导致的乱码以及色情、暴力、仇恨言论也难以避免。\n\n清洗流程通常从**语言过滤**开始，识别文本语言并保留目标语言；然后进行**质量过滤**，基于困惑度、文本长度、特殊字符比例等指标去除低质量文本；接着是**内容过滤**，移除有害内容、广告和模板文本；最后是**格式清理**，统一编码，移除HTML标签，规范化空白字符。\n\n研究表明，在同等数据量下，高质量数据训练的模型显著优于在原始数据上训练的模型。这也是为什么数据清洗是大模型训练的核心竞争力之一。\n\n### 数据去重：重复的危害\n\n大规模语料中存在大量重复。有些是完全相同的文档被爬取多次，有些是近似重复（如同一新闻的不同来源）。\n\n重复数据的危害是多方面的。最直接的影响是**浪费计算**——模型在相同内容上反复训练。更隐蔽的是，某些被大量复制的内容会获得不成比例的影响力，**偏向特定来源**。模型也可能倾向于**记忆**这些重复内容，而不是学习通用模式。此外，如果测试数据在训练集中出现过，评估结果将不可信——这就是**评估失真**问题。\n\nDeepMind的Chinchilla研究进一步凸显了数据质量的重要性：更大的、更干净的数据集可以让更小的模型达到更大模型的效果。这直接促使了对数据去重的重视。\n\n在实践中，去重方法可以按精度分级：**精确去重**基于hash（如MD5、SHA）检测完全相同的文档；**近似去重**使用MinHash、SimHash等技术识别相似但不完全相同的文档；**N-gram去重**则更为激进，移除包含训练集中大段连续文本的样本。\n\n### 数据污染：训练-测试泄漏\n\n一个更微妙的问题是**数据污染**（data contamination）：测试集的内容出现在训练集中。\n\n**为什么这是问题？**\n\n如果模型在训练时\"见过\"测试题和答案，它可能只是在检索记忆，而不是真正的泛化。这会导致benchmark分数虚高，无法反映模型的真实能力。\n\n**污染来源**：\n\n- 互联网上有大量benchmark数据集的讨论和解答\n- 某些评测集本身就是从互联网收集的\n- 不同数据集可能有重叠的来源\n\n**检测方法**：\n\n- 检查测试样本是否在训练集中有高度重叠\n- 使用模型的困惑度——如果模型对测试样本的困惑度异常低，可能是记忆\n- 对测试样本做微小修改（如改变人名），观察模型表现变化\n\n**案例**：GPT-4的技术报告专门讨论了数据污染问题，并对可能被污染的benchmark结果做了标注。这种透明度是负责任的做法。\n\n---\n\n## 深入理解\n\n> **研究者必读**：这一节探讨tokenization的理论基础、边界条件和开放问题\n\n### 理论视角：最优分词的条件\n\n从信息论角度，最优的tokenization应该最小化描述数据的总比特数。这等价于最大化：\n\n$$\n\\mathcal{L} = \\sum_{s \\in \\mathcal{D}} \\log P(s)\n$$\n\n其中$P(s)$是分词方案下句子$s$的概率。\n\n对于Unigram模型，这有闭式解。但对于BPE这种贪心算法，只能保证局部最优。\n\n**有趣的问题**：BPE的贪心策略与全局最优差多少？实证研究表明，在大多数情况下差异不大，但理论上的保证仍然缺失。\n\n### 边界条件：子词分词的假设\n\n子词分词隐含了几个假设：\n\n**假设1：词可以有意义地分解**\n\nBPE假设常见的子词组合是有意义的。但这并不总是成立——\"ing\"作为后缀有语法意义，但\"qu\"只是拼写惯例，没有独立含义。\n\n**假设2：统计规律反映语言结构**\n\nBPE用频率来决定合并顺序，假设高频模式是语言中重要的单元。但频率受训练语料影响——如果语料偏向某个领域，分词结果也会偏向该领域。\n\n**假设3：分词策略语言无关**\n\nSentencePiece等工具用相同算法处理所有语言。但不同语言有不同的结构——中文是孤立语（morphology少），土耳其语是黏着语（morphology丰富）。同一算法可能对它们的效果不同。\n\n### 开放研究问题\n\n如果你要在tokenization方向做研究，可以考虑这些问题。\n\n**问题1：最优词汇表大小是多少？**\n\n词汇表大小是一个关键超参数。太小导致序列太长，太大导致稀有token训练不足。最优大小取决于数据量、模型大小、目标语言等因素。是否存在一个理论指导？\n\n**问题2：如何实现真正的多语言公平？**\n\n现有tokenizer对英文有系统性偏好。如何设计一个对所有语言公平的tokenizer？这可能需要权衡——让英文效率降低来提升其他语言，是否可接受？\n\n**问题3：Tokenization与模型能力的关系**\n\n数字切分影响算术能力，这已被证实。还有哪些模型能力受tokenization影响？推理、常识、代码理解...？能否设计专门的tokenization策略来增强特定能力？\n\n**问题4：端到端学习tokenization**\n\n目前tokenization与模型训练是分离的。能否让模型自己学习如何分词？这涉及到离散结构的可微分学习，是一个开放问题。\n\n---\n\n## 工程实践：Hugging Face Tokenizers库使用\n\n### 使用预训练Tokenizer\n\n::: {#4906ab28 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import AutoTokenizer\n\n# 加载GPT-2的tokenizer（Byte-level BPE）\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# 基本tokenization\ntext = \"Hello, how are you doing today?\"\ntokens = tokenizer.tokenize(text)\nprint(f\"Tokens: {tokens}\")\n# 输出: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', 'Ġdoing', 'Ġtoday', '?']\n# 注意：Ġ表示词前的空格（Byte-level BPE的特点）\n\n# 转换为ID\ninput_ids = tokenizer.encode(text)\nprint(f\"Input IDs: {input_ids}\")\n\n# 解码回文本\ndecoded = tokenizer.decode(input_ids)\nprint(f\"Decoded: {decoded}\")\n\n# 批量处理，带padding和truncation\ntexts = [\"Hello world\", \"This is a longer sentence for testing\"]\nencoded = tokenizer(texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\nprint(f\"Input IDs shape: {encoded['input_ids'].shape}\")\nprint(f\"Attention mask: {encoded['attention_mask']}\")\n```\n:::\n\n\n### 比较不同Tokenizer\n\n::: {#5e9a44ff .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import AutoTokenizer\n\n# 加载不同模型的tokenizer\ntokenizers = {\n    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n    \"T5\": AutoTokenizer.from_pretrained(\"t5-base\"),\n    \"LLaMA\": AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\"),\n}\n\n# 测试文本\ntest_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"人工智能正在改变世界。\",\n    \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\",\n    \"12345 + 67890 = 80235\",\n]\n\nprint(\"Token数量比较：\\n\")\nfor text in test_texts:\n    print(f\"Text: {text[:50]}...\")\n    for name, tok in tokenizers.items():\n        tokens = tok.tokenize(text)\n        print(f\"  {name}: {len(tokens)} tokens\")\n    print()\n```\n:::\n\n\n### 训练自定义Tokenizer\n\n::: {#f17db2f6 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n\n# 创建BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\n\n# 设置预分词器（按空格和标点分割）\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n\n# 设置解码器\ntokenizer.decoder = decoders.ByteLevel()\n\n# 准备训练器\ntrainer = trainers.BpeTrainer(\n    vocab_size=10000,\n    min_frequency=2,\n    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<mask>\"]\n)\n\n# 训练（从文件或迭代器）\nfiles = [\"train_data.txt\"]\ntokenizer.train(files, trainer)\n\n# 保存\ntokenizer.save(\"my_tokenizer.json\")\n\n# 加载并使用\nfrom tokenizers import Tokenizer\nloaded_tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\noutput = loaded_tokenizer.encode(\"Hello, world!\")\nprint(f\"Tokens: {output.tokens}\")\nprint(f\"IDs: {output.ids}\")\n```\n:::\n\n\n### 分析Tokenization效率\n\n::: {#edcac469 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom collections import Counter\n\ndef analyze_tokenizer(tokenizer, texts):\n    \"\"\"分析tokenizer在给定文本上的效率\"\"\"\n\n    total_chars = sum(len(t) for t in texts)\n    total_tokens = sum(len(tokenizer.tokenize(t)) for t in texts)\n\n    # 字符/token比率（越高越高效）\n    ratio = total_chars / total_tokens\n\n    # Token长度分布\n    all_tokens = []\n    for t in texts:\n        all_tokens.extend(tokenizer.tokenize(t))\n\n    token_lengths = [len(t.replace('Ġ', '').replace('▁', '')) for t in all_tokens]\n\n    print(f\"Total characters: {total_chars}\")\n    print(f\"Total tokens: {total_tokens}\")\n    print(f\"Characters per token: {ratio:.2f}\")\n    print(f\"Token length distribution:\")\n    print(f\"  Mean: {np.mean(token_lengths):.2f}\")\n    print(f\"  Median: {np.median(token_lengths):.0f}\")\n    print(f\"  Min: {min(token_lengths)}, Max: {max(token_lengths)}\")\n\n    # 最常见的token\n    token_counts = Counter(all_tokens)\n    print(f\"\\nTop 10 most common tokens:\")\n    for token, count in token_counts.most_common(10):\n        print(f\"  {repr(token)}: {count}\")\n\n# 使用示例\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nsample_texts = [\n    \"Machine learning is a subset of artificial intelligence.\",\n    \"Deep neural networks have revolutionized natural language processing.\",\n    \"Transformers use self-attention mechanisms to process sequential data.\",\n]\nanalyze_tokenizer(tokenizer, sample_texts)\n```\n:::\n\n\n---\n\n## 局限性与未解决的问题\n\n### Tokenization的固有局限\n\n即使是最先进的子词tokenizer，也有一些固有的局限性。\n\n**语言偏见无法完全消除**。Tokenizer是在特定语料上训练的，必然反映该语料的语言分布。如果训练语料以英文为主，tokenizer就会对英文优化。这不是算法的问题，而是数据的问题——真正的多语言公平需要在数据收集阶段就考虑。\n\n**分词边界可能破坏语义**。无论BPE还是Unigram，都是基于统计的方法，不理解语言的语义结构。\"unhappiness\"可能被切分为\"un\"+\"happiness\"（语义上合理），也可能被切分为\"unha\"+\"ppiness\"（纯粹基于频率）。后者对模型学习不利。\n\n**Tokenization与模型训练的割裂**。目前的做法是先确定tokenizer，然后固定它训练模型。但最优的tokenization可能依赖于下游任务和模型架构。这种割裂是否限制了模型的上限？\n\n### 下一章的铺垫\n\n我们已经知道如何把文本转换为token序列，也知道如何用词向量表示每个token。但一个句子不只是token的无序集合——**词序蕴含着关键的语义信息**。\n\n\"狗咬人\"和\"人咬狗\"有完全不同的含义，但如果我们只是把词向量加起来或取平均，就会丢失这种区分。我们需要一种方法来建模**序列结构**，捕获词与词之间的顺序关系和长距离依赖。\n\n这正是下一章要讨论的问题。循环神经网络（RNN）通过\"记忆\"之前看到的内容，让模型能够处理变长序列。LSTM和GRU通过门控机制解决了梯度消失问题。但它们也有自己的局限——无法并行、长距离依赖仍然困难——这些局限最终导向了Attention机制和Transformer的诞生。\n\n> 下一章预告：第5章将介绍循环神经网络，理解它如何通过\"时间上的权重共享\"来处理序列，以及门控机制如何让信息在长序列中存活。\n\n---\n\n## 本章小结\n\n### 核心要点回顾\n\n这一章我们深入探讨了一个常被忽视但极其重要的问题：**如何将文本切分为模型可处理的单元**。\n\n核心洞察是：**Tokenizer不是预处理工具，而是模型架构的隐藏维度**。它影响：\n\n- **计算效率**：序列长度直接决定计算成本\n- **多语言公平性**：不同语言的token效率差异导致系统性不平等\n- **模型能力**：数字、代码的切分方式影响相应的推理能力\n- **安全性**：tokenization边界可能被攻击者利用\n\n我们学习了三种基本的tokenization策略：\n\n- **词级别**：语义清晰但OOV严重、词汇表爆炸\n- **字符级别**：没有OOV但序列太长、语义碎片化\n- **子词级别**：在两者之间找到平衡，是现代NLP的标准选择\n\n子词分词的核心算法包括：\n\n- **BPE**：贪心地合并最频繁的相邻对，从字符构建词汇表\n- **WordPiece**：类似BPE但用likelihood改进选择合并对\n- **Unigram**：概率视角，从大词汇表剪枝到目标大小\n- **Byte-level BPE**：在字节级别操作，彻底解决未知字符问题\n\n最后，我们讨论了数据质量的重要性——清洗、去重、避免污染是大模型训练的关键环节。\n\n### 关键概念速查\n\n| 概念 | 含义 |\n|------|------|\n| BPE | Byte Pair Encoding，迭代合并最频繁token对 |\n| WordPiece | Google的子词算法，用于BERT |\n| Unigram LM | 概率视角的子词分词，支持多种切分 |\n| SentencePiece | 语言无关的tokenization库 |\n| Byte-level | 在字节级别操作，彻底解决OOV |\n| OOV | Out-of-Vocabulary，无法处理的未知词 |\n| 数据污染 | 测试集内容出现在训练集中 |\n\n### 思考题\n\n1. **[概念理解]** 为什么说\"Tokenizer是模型架构的隐藏维度\"？试举三个具体例子说明tokenizer选择如何影响模型行为。\n\n2. **[算法实践]** 手动执行BPE算法。给定语料 {\"ab\": 5, \"abc\": 3, \"abcd\": 2}，执行3次合并，写出每一步的词汇表和合并规则。\n\n3. **[工程实践]** 使用Hugging Face的tokenizers库，比较GPT-2、BERT、T5的tokenizer在中文文本上的效率。计算每个tokenizer的\"字符/token比率\"，并分析差异的原因。\n\n4. **[研究思考]** 设计一个实验来量化tokenization对模型算术能力的影响。你会如何控制变量？需要什么样的数据集？\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n- **[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) (Sennrich et al., 2016)**：BPE在NLP中的应用\n  - **arXiv**: 1508.07909\n  - **重点读**：\n    - Section 3.2：BPE算法描述和Algorithm 1伪代码\n    - Table 1：不同分词方法的统计对比\n    - Section 5.2：翻译示例分析\n  - **可跳过**：Section 4的实验细节（除非你要做机器翻译）\n  - **核心贡献**：将1994年的数据压缩算法引入NLP，解决OOV问题\n  - **官方代码**：[github.com/rsennrich/subword-nmt](https://github.com/rsennrich/subword-nmt)\n\n- **[SentencePiece: A simple and language independent subword tokenizer](https://arxiv.org/abs/1808.06226) (Kudo & Richardson, 2018)**\n  - **arXiv**: 1808.06226\n  - **重点读**：语言无关设计的动机——为什么不需要预分词\n\n### 理论分析\n\n- **[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959) (Kudo, 2018)**：Unigram LM的原始论文\n  - **arXiv**: 1804.10959\n  - 概率视角的子词分词，支持训练时采样不同切分\n  - **重点读**：Section 3的Unigram Language Model formulation\n\n### 实证研究\n\n- **How Good is Your Tokenizer? (Rust et al., 2021)**：多语言tokenizer的系统性评估\n  - 揭示了英语偏见的严重程度\n\n- **Tokenizer Choice Matters: Downstream Tasks Benefit from Task-Specific Tokenization (2023)**\n  - 不同任务可能需要不同的tokenization策略\n\n### 工具和资源\n\n- **Hugging Face Tokenizers库**：高效的tokenizer训练和使用\n- **SentencePiece官方库**：Google的tokenization工具\n- **tiktoken**：OpenAI的BPE实现，用于GPT系列\n\n---\n\n## 历史注脚\n\nBPE（Byte Pair Encoding）的历史颇为有趣。它最初是Philip Gage在1994年提出的**数据压缩算法**，与NLP毫无关系。算法思想很简单：找到数据中最频繁的字节对，用一个新字节替换它们，重复这个过程直到无法进一步压缩。\n\n2016年，Edinburgh大学的Rico Sennrich等人在研究机器翻译时遇到了一个问题：神经翻译模型无法处理罕见词。他们想到了BPE——既然BPE能够压缩数据，它应该也能产生一种\"压缩\"的文本表示，其中常见模式被合并为单一单元。\n\n这个跨领域的知识迁移非常成功。BPE不仅解决了OOV问题，还显著提升了翻译质量，特别是在形态丰富的语言上。论文发表后迅速被广泛采用，成为现代NLP的标准组件。\n\n一个有趣的细节是，Sennrich等人的论文最初投稿时被审稿人质疑\"太简单\"。但正如Word2Vec的故事，最有影响力的工作往往不是最复杂的，而是找到了正确的简化。BPE的成功说明，有时候领域外的老技术，在新问题上可能有意想不到的效果。\n\n今天，几乎所有大型语言模型都使用BPE或其变体。从GPT到LLaMA，从Claude到Gemini，BPE（或其byte-level版本）是它们的共同基础。一个1994年的数据压缩算法，成为了2020年代AI革命的基石——这本身就是技术史上的一个美丽故事。\n\n",
    "supporting": [
      "ch04-tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}