{
  "hash": "19e97e231d627ff07695e238423201a7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ç¬¬5ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„è¯ç”Ÿ\"\nsubtitle: \"æ‰“ç ´ä¿¡æ¯ç“¶é¢ˆï¼šè®©è§£ç å™¨å­¦ä¼š'å›å¤´çœ‹'\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-25\"\ncategories: [NLP, Attention, Seq2Seq, æœºå™¨ç¿»è¯‘, Bahdanau]\ntags: [æ³¨æ„åŠ›æœºåˆ¶, åŠ æ€§æ³¨æ„åŠ›, å¯¹é½æ¨¡å‹, ä¸Šä¸‹æ–‡å‘é‡, ç¥ç»æœºå™¨ç¿»è¯‘]\ndescription: \"Attentionæœºåˆ¶çš„è¯ç”Ÿï¼šBahdanauå¦‚ä½•è®©è§£ç å™¨å­¦ä¼š'å›å¤´çœ‹'ï¼Œæ‰“ç ´Seq2Seqçš„ä¿¡æ¯ç“¶é¢ˆï¼Œä»¥åŠæ³¨æ„åŠ›æƒé‡çš„å¯è§£é‡Šæ€§ã€‚\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\n---\n\n> **æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•è®©è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œèƒ½å¤Ÿè®¿é—®è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯åªä¾èµ–ä¸€ä¸ªå‹ç¼©åçš„å‘é‡ï¼Ÿ\n>\n> **å†å²åæ ‡**ï¼š2014-2015 | Bahdanau, Cho, Bengio | ç¥ç»æœºå™¨ç¿»è¯‘çš„çªç ´\n\n---\n\n## ä»ä¸Šä¸€ç« è¯´èµ·\n\nä¸Šä¸€ç« æˆ‘ä»¬è§è¯äº†RNNçš„è¾‰ç…Œä¸å›°å¢ƒã€‚LSTMå’ŒGRUé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒSeq2Seqæ¶æ„è®©ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤„ç†ç¿»è¯‘ã€æ‘˜è¦ç­‰åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ã€‚\n\nä½†Seq2Seqæœ‰ä¸€ä¸ªè‡´å‘½çš„è®¾è®¡ç¼ºé™·ï¼š**ä¿¡æ¯ç“¶é¢ˆ**ã€‚\n\nå›é¡¾Seq2Seqçš„å·¥ä½œæ–¹å¼ï¼šç¼–ç å™¨è¯»å–æ•´ä¸ªè¾“å…¥åºåˆ—ï¼Œå°†æ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ $\\mathbf{c}$ ä¸­ï¼›è§£ç å™¨ä»…å‡­è¿™ä¸ªå‘é‡ï¼Œé€è¯ç”Ÿæˆè¾“å‡ºã€‚è¿™æ„å‘³ç€ï¼Œæ— è®ºè¾“å…¥æ˜¯5ä¸ªè¯è¿˜æ˜¯50ä¸ªè¯ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½è¦å¡è¿›åŒä¸€ä¸ªç»´åº¦çš„å‘é‡ã€‚\n\nSutskeverç­‰äºº(2014)çš„å®éªŒæ¸…æ¥šåœ°å±•ç¤ºäº†è¿™ä¸ªé—®é¢˜ï¼šå½“è¾“å…¥å¥å­è¶…è¿‡20ä¸ªè¯æ—¶ï¼Œç¿»è¯‘è´¨é‡æ€¥å‰§ä¸‹é™ã€‚æ›´é•¿çš„å¥å­åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œè€Œå›ºå®šå¤§å°çš„å‘é‡æ— æ³•æ‰¿è½½ã€‚\n\nè®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ„Ÿå—è¿™ä¸ªé—®é¢˜ã€‚è€ƒè™‘ç¿»è¯‘ä»»åŠ¡ï¼š\n\n> **è‹±è¯­**ï¼šThe agreement on the European Economic Area was signed in August 1992.\n>\n> **æ³•è¯­**ï¼šL'accord sur la zone Ã©conomique europÃ©enne a Ã©tÃ© signÃ© en aoÃ»t 1992.\n\nå½“è§£ç å™¨ç”Ÿæˆ\"aoÃ»t\"ï¼ˆå…«æœˆï¼‰æ—¶ï¼Œå®ƒéœ€è¦çŸ¥é“åŸæ–‡ä¸­çš„\"August\"ã€‚ä½†åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œ\"August\"è¿™ä¸ªè¯é¦–å…ˆè¢«ç¼–ç è¿›éšè—çŠ¶æ€ï¼Œç„¶åä¸å…¶ä»–æ‰€æœ‰è¯çš„ä¿¡æ¯æ··åˆåœ¨ä¸€èµ·ï¼Œæœ€ç»ˆå‹ç¼©æˆä¸Šä¸‹æ–‡å‘é‡ $\\mathbf{c}$ã€‚è§£ç å™¨è¦ä»è¿™ä¸ªå‹ç¼©åçš„å‘é‡ä¸­\"æŒ–å‡º\"Augustçš„ä¿¡æ¯â€”â€”è¿™å°±åƒä»ä¸€é”…æ±¤é‡Œæ‰¾å›åŸæ¥çš„é£Ÿæã€‚\n\næ›´ç³Ÿç³•çš„æ˜¯ï¼Œå¥å­ä¸­çš„æŸäº›è¯å¯¹å½“å‰ç”Ÿæˆçš„è¯æ›´é‡è¦ã€‚ç¿»è¯‘\"August\"æ—¶ï¼Œæ¨¡å‹æœ€éœ€è¦å…³æ³¨çš„æ˜¯åŸæ–‡ä¸­çš„\"August\"ï¼Œè€Œä¸æ˜¯\"The\"æˆ–\"was\"ã€‚ä½†æ ‡å‡†Seq2Seqå¯¹æ‰€æœ‰è¾“å…¥ä½ç½®ä¸€è§†åŒä»â€”â€”å®ƒä»¬éƒ½è¢«åŒç­‰åœ°å‹ç¼©è¿›äº† $\\mathbf{c}$ã€‚\n\n> ğŸ’¡ **æœ¬ç« æ ¸å¿ƒæ´å¯Ÿ**ï¼šè§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œåº”è¯¥èƒ½å¤Ÿ**æœ‰é€‰æ‹©åœ°å…³æ³¨**è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®ã€‚ä¸åŒçš„è¾“å‡ºè¯éœ€è¦å…³æ³¨ä¸åŒçš„è¾“å…¥è¯â€”â€”è¿™å°±æ˜¯\"æ³¨æ„åŠ›\"çš„æœ¬è´¨ã€‚Attentionæœºåˆ¶è®©è§£ç å™¨åœ¨æ¯ä¸€æ­¥éƒ½èƒ½è®¿é—®ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼Œå¹¶æ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€è®¡ç®—å®ƒä»¬çš„é‡è¦æ€§æƒé‡ã€‚\n\n---\n\n## é—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ\n\n### é—®é¢˜çš„ç²¾ç¡®å®šä¹‰\n\nè®©æˆ‘ä»¬å½¢å¼åŒ–åœ°æè¿°Seq2Seqçš„ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ã€‚\n\nåœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œç¼–ç å™¨äº§ç”Ÿä¸€ç³»åˆ—éšè—çŠ¶æ€ $\\mathbf{h}_1^{enc}, \\mathbf{h}_2^{enc}, \\ldots, \\mathbf{h}_T^{enc}$ï¼Œä½†åªæœ‰æœ€åä¸€ä¸ªçŠ¶æ€ $\\mathbf{h}_T^{enc}$ è¢«ä¼ é€’ç»™è§£ç å™¨ä½œä¸ºä¸Šä¸‹æ–‡å‘é‡ï¼š\n\n$$\n\\mathbf{c} = \\mathbf{h}_T^{enc}\n$$\n\nè§£ç å™¨çš„æ¯ä¸€æ­¥éƒ½ä½¿ç”¨è¿™åŒä¸€ä¸ª $\\mathbf{c}$ï¼š\n\n$$\n\\mathbf{h}_t^{dec} = f(\\mathbf{h}_{t-1}^{dec}, y_{t-1}, \\mathbf{c})\n$$\n\né—®é¢˜åœ¨äºï¼š$\\mathbf{c}$ æ˜¯ä¸€ä¸ª**é™æ€**çš„ã€**å…¨å±€**çš„è¡¨ç¤ºã€‚å®ƒåœ¨è§£ç çš„æ¯ä¸€æ­¥éƒ½ä¿æŒä¸å˜ï¼Œæ— æ³•æ ¹æ®å½“å‰ç”Ÿæˆçš„è¯åŠ¨æ€è°ƒæ•´ã€‚\n\nä»ä¿¡æ¯è®ºçš„è§’åº¦çœ‹ï¼Œå¦‚æœè¾“å…¥åºåˆ— $\\mathbf{x}$ çš„ä¿¡æ¯ç†µæ˜¯ $H(\\mathbf{x})$ï¼Œè€Œ $\\mathbf{c}$ çš„ç»´åº¦æ˜¯ $d$ï¼Œé‚£ä¹ˆ $\\mathbf{c}$ æœ€å¤šèƒ½æºå¸¦ $O(d)$ çš„ä¿¡æ¯é‡ã€‚å½“ $H(\\mathbf{x}) > O(d)$ æ—¶ï¼Œä¿¡æ¯ä¸¢å¤±æ˜¯ä¸å¯é¿å…çš„ã€‚\n\n### ä¹‹å‰çš„å°è¯•ä¸ºä½•å¤±è´¥ï¼Ÿ\n\nåœ¨Attentionå‡ºç°ä¹‹å‰ï¼Œç ”ç©¶è€…å°è¯•è¿‡ä¸€äº›ç¼“è§£ä¿¡æ¯ç“¶é¢ˆçš„æ–¹æ³•ï¼š\n\n**å¢åŠ ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦**ï¼šç›´è§‰ä¸Šï¼Œæ›´å¤§çš„ $\\mathbf{c}$ å¯ä»¥æºå¸¦æ›´å¤šä¿¡æ¯ã€‚ä½†è¿™åªæ˜¯å»¶ç¼“é—®é¢˜ï¼Œè€Œéè§£å†³é—®é¢˜ã€‚è€Œä¸”æ›´å¤§çš„å‘é‡æ„å‘³ç€æ›´å¤šå‚æ•°ï¼Œæ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚\n\n**ä½¿ç”¨åŒå‘RNN**ï¼šè®©ç¼–ç å™¨åŒæ—¶ä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦è¯»å–è¾“å…¥ï¼Œç„¶åæ‹¼æ¥ä¸¤ä¸ªæ–¹å‘çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚è¿™ç¡®å®èƒ½æ•è·æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†ä»ç„¶æ˜¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå‘é‡â€”â€”åªæ˜¯è¿™ä¸ªå‘é‡ç¨å¾®å¤§äº†ä¸€ç‚¹ã€‚\n\n**è¾“å…¥åè½¬**ï¼šSutskeverç­‰äººå‘ç°ï¼Œå°†è¾“å…¥åºåˆ—åè½¬åå†è¾“å…¥ç¼–ç å™¨ï¼Œç¿»è¯‘æ•ˆæœæ›´å¥½ã€‚è¿™æ˜¯å› ä¸ºè¾“å…¥çš„æœ€åå‡ ä¸ªè¯ï¼ˆåè½¬åå˜æˆæœ€å…ˆè¾“å…¥çš„è¯ï¼‰ä¸è¾“å‡ºçš„æœ€å…ˆå‡ ä¸ªè¯å¾€å¾€æœ‰æ›´å¼ºçš„å¯¹åº”å…³ç³»ã€‚ä½†è¿™åªæ˜¯ä¸€ä¸ªå¯å‘å¼æŠ€å·§ï¼Œä¸èƒ½ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜ã€‚\n\nè¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è§¦åŠé—®é¢˜çš„æ ¸å¿ƒï¼š**è§£ç å™¨åªèƒ½çœ‹åˆ°ä¸€ä¸ªå›ºå®šçš„ã€å…¨å±€çš„è¡¨ç¤ºï¼Œæ— æ³•åŠ¨æ€åœ°è®¿é—®è¾“å…¥çš„ä¸åŒéƒ¨åˆ†**ã€‚\n\n### æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„è§£å†³æ–¹æ¡ˆï¼Ÿ\n\nç†æƒ³çš„è§£å†³æ–¹æ¡ˆåº”è¯¥å…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š\n\n1. **åŠ¨æ€æ€§**ï¼šè§£ç å™¨åœ¨ç”Ÿæˆä¸åŒè¯æ—¶ï¼Œåº”è¯¥èƒ½å¤Ÿå…³æ³¨è¾“å…¥çš„ä¸åŒä½ç½®\n2. **è½¯é€‰æ‹©**ï¼šä¸æ˜¯ç¡¬æ€§åœ°é€‰æ‹©æŸä¸€ä¸ªä½ç½®ï¼Œè€Œæ˜¯å¯¹æ‰€æœ‰ä½ç½®è®¡ç®—ä¸€ä¸ªé‡è¦æ€§åˆ†å¸ƒ\n3. **ç«¯åˆ°ç«¯å¯è®­ç»ƒ**ï¼šæ•´ä¸ªæœºåˆ¶åº”è¯¥å¯ä»¥é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–\n4. **å¯è§£é‡Šæ€§**ï¼šæ¨¡å‹å…³æ³¨å“ªäº›ä½ç½®åº”è¯¥æ˜¯å¯ä»¥è§‚å¯Ÿå’Œç†è§£çš„\n\nè¿™äº›ç‰¹æ€§æ­£æ˜¯Attentionæœºåˆ¶æ‰€æä¾›çš„ã€‚\n\n---\n\n## æ ¸å¿ƒæ€æƒ³ä¸ç›´è§‰\n\n### å…³é”®æ´å¯Ÿï¼šåŠ¨æ€çš„ã€åŸºäºå†…å®¹çš„å¯»å€\n\nAttentionçš„æ ¸å¿ƒæ´å¯Ÿå¯ä»¥ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼š\n\n> **è®©è§£ç å™¨åœ¨æ¯ä¸€æ­¥éƒ½èƒ½\"å›å¤´çœ‹\"ç¼–ç å™¨çš„æ‰€æœ‰ä½ç½®ï¼Œå¹¶æ ¹æ®å½“å‰éœ€è¦åŠ¨æ€å†³å®šå…³æ³¨å“ªäº›ä½ç½®ã€‚**\n\nè¿™ä¸ªæƒ³æ³•å¬èµ·æ¥ç®€å•ï¼Œä½†å®ƒå½»åº•æ”¹å˜äº†åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„èŒƒå¼ã€‚\n\n### ç›´è§‰è§£é‡Šï¼šèšå…‰ç¯ä¸å›¾ä¹¦é¦†\n\næƒ³è±¡ä½ åœ¨ä¸€ä¸ªé»‘æš—çš„å›¾ä¹¦é¦†é‡Œæ‰¾ä¹¦ã€‚ä¼ ç»ŸSeq2Seqå°±åƒæ˜¯ï¼šä½ å…ˆç”¨æ‰‹ç”µç­’å¿«é€Ÿæ‰«è¿‡æ‰€æœ‰ä¹¦æ¶ï¼Œç„¶åå…³æ‰æ‰‹ç”µç­’ï¼Œä»…å‡­è®°å¿†å»å–ä¹¦ã€‚ä½ å¯¹æ•´ä¸ªå›¾ä¹¦é¦†æœ‰ä¸€ä¸ªæ¨¡ç³Šçš„æ•´ä½“å°è±¡ï¼Œä½†ç»†èŠ‚å¾ˆå®¹æ˜“é—å¿˜ã€‚\n\nAttentionæœºåˆ¶åˆ™åƒæ˜¯ï¼šä½ æ‰‹é‡Œæœ‰ä¸€ä¸ª**å¯è°ƒèŠ‚çš„èšå…‰ç¯**ã€‚å½“ä½ éœ€è¦æ‰¾æŸæœ¬ä¹¦æ—¶ï¼Œä½ å¯ä»¥æŠŠèšå…‰ç¯ç…§å‘ç›¸å…³çš„ä¹¦æ¶ï¼Œä»”ç»†æŸ¥çœ‹é‚£é‡Œçš„ä¹¦åã€‚ä¸åŒçš„æŸ¥è¯¢éœ€æ±‚ä¼šè®©ä½ æŠŠå…‰ç…§å‘ä¸åŒçš„ä½ç½®ã€‚\n\næ›´å…·ä½“åœ°è¯´ï¼Œå½“è§£ç å™¨ç”Ÿæˆ\"aoÃ»t\"ï¼ˆå…«æœˆï¼‰è¿™ä¸ªè¯æ—¶ï¼ŒAttentionæœºåˆ¶ä¼šï¼š\n\n1. æŸ¥çœ‹ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼ˆå›¾ä¹¦é¦†çš„æ‰€æœ‰ä¹¦æ¶ï¼‰\n2. è®¡ç®—æ¯ä¸ªä½ç½®ä¸å½“å‰ä»»åŠ¡çš„ç›¸å…³æ€§ï¼ˆåˆ¤æ–­æ¯ä¸ªä¹¦æ¶æ˜¯å¦å¯èƒ½æœ‰ä½ è¦çš„ä¹¦ï¼‰\n3. æŠŠ\"èšå…‰ç¯\"ä¸»è¦ç…§å‘ç›¸å…³çš„ä½ç½®ï¼ˆ\"August\"å¯¹åº”çš„ç¼–ç å™¨çŠ¶æ€ï¼‰\n4. ä»è¿™äº›ä½ç½®æ±‡æ€»ä¿¡æ¯ï¼Œè¾…åŠ©ç”Ÿæˆå½“å‰è¯\n\n### å¦ä¸€ä¸ªç±»æ¯”ï¼šåŠ æƒæŠ•ç¥¨\n\nä½ ä¹Ÿå¯ä»¥æŠŠAttentionç†è§£ä¸ºä¸€ç§åŠ æƒæŠ•ç¥¨æœºåˆ¶ã€‚\n\næƒ³è±¡è§£ç å™¨æ˜¯ä¸€ä¸ªé¢†å¯¼ï¼Œéœ€è¦åšä¸€ä¸ªå†³å®šï¼ˆç”Ÿæˆä¸‹ä¸€ä¸ªè¯ï¼‰ã€‚å®ƒæœ‰ä¸€ä¸ªé¡¾é—®å›¢é˜Ÿï¼ˆç¼–ç å™¨çš„å„ä¸ªéšè—çŠ¶æ€ï¼‰ï¼Œæ¯ä¸ªé¡¾é—®æŒæ¡è¾“å…¥åºåˆ—ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯ã€‚\n\nä¼ ç»ŸSeq2Seqï¼šåªå¬ä¸€ä¸ª\"æ€»é¡¾é—®\"çš„æ„è§ï¼ˆä¸Šä¸‹æ–‡å‘é‡ $\\mathbf{c}$ï¼‰ï¼Œè¿™ä¸ªæ€»é¡¾é—®è¦ç»¼åˆæ‰€æœ‰äººçš„ä¿¡æ¯ã€‚\n\nAttentionæœºåˆ¶ï¼šç›´æ¥å¾è¯¢æ¯ä¸ªé¡¾é—®çš„æ„è§ï¼Œç„¶åæ ¹æ®è®®é¢˜ç›¸å…³æ€§ç»™ä¸åŒé¡¾é—®çš„æ„è§èµ‹äºˆä¸åŒæƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—å‡ºæœ€ç»ˆå†³å®šã€‚\n\n### è®¾è®¡åŠ¨æœºï¼šä¸ºä»€ä¹ˆé€‰æ‹©è½¯æ³¨æ„åŠ›ï¼Ÿ\n\nAttentionæœºåˆ¶æœ‰ä¸¤ç§å˜ä½“ï¼š\n\n- **è½¯æ³¨æ„åŠ›ï¼ˆSoft Attentionï¼‰**ï¼šå¯¹æ‰€æœ‰ä½ç½®è®¡ç®—æ¦‚ç‡åˆ†å¸ƒï¼ŒåŠ æƒæ±‚å’Œ\n- **ç¡¬æ³¨æ„åŠ›ï¼ˆHard Attentionï¼‰**ï¼šé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼Œåªçœ‹é‚£é‡Œçš„ä¿¡æ¯\n\nBahdanauç­‰äººé€‰æ‹©äº†è½¯æ³¨æ„åŠ›ï¼ŒåŸå› æ˜¯ï¼š\n\n1. **å¯å¾®åˆ†**ï¼šè½¯æ³¨æ„åŠ›çš„åŠ æƒæ±‚å’Œæ˜¯å¯å¾®çš„ï¼Œå¯ä»¥ç”¨æ ‡å‡†çš„åå‘ä¼ æ’­è®­ç»ƒ\n2. **ç¨³å®š**ï¼šç¡¬æ³¨æ„åŠ›éœ€è¦é‡‡æ ·æˆ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®­ç»ƒï¼Œæ–¹å·®å¤§ï¼Œä¸ç¨³å®š\n3. **ä¿¡æ¯æ›´ä¸°å¯Œ**ï¼šè½¯æ³¨æ„åŠ›å¯ä»¥åŒæ—¶åˆ©ç”¨å¤šä¸ªä½ç½®çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯éæ­¤å³å½¼\n\nç¡¬æ³¨æ„åŠ›ä¹Ÿæœ‰å…¶ä¼˜åŠ¿ï¼ˆè®¡ç®—æ›´é«˜æ•ˆï¼Œæ›´ç¨€ç–ï¼‰ï¼Œä½†åœ¨å®è·µä¸­ï¼Œè½¯æ³¨æ„åŠ›å› å…¶ç®€å•å’Œæœ‰æ•ˆæˆä¸ºäº†ä¸»æµã€‚\n\n---\n\n## æŠ€æœ¯ç»†èŠ‚\n\n### Bahdanau Attentionï¼šåŠ æ€§æ³¨æ„åŠ›\n\n2014å¹´ï¼ŒBahdanauã€Choå’ŒBengioæå‡ºäº†ç¬¬ä¸€ä¸ªæˆåŠŸçš„æ³¨æ„åŠ›æœºåˆ¶ç”¨äºæœºå™¨ç¿»è¯‘ã€‚è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚\n\né¦–å…ˆï¼Œç¼–ç å™¨ä½¿ç”¨**åŒå‘RNN**ï¼Œåœ¨æ¯ä¸ªä½ç½® $j$ äº§ç”Ÿä¸€ä¸ªéšè—çŠ¶æ€ï¼š\n\n$$\n\\mathbf{h}_j = [\\overrightarrow{\\mathbf{h}}_j; \\overleftarrow{\\mathbf{h}}_j]\n$$\n\nå…¶ä¸­ $\\overrightarrow{\\mathbf{h}}_j$ æ˜¯å‰å‘RNNçš„éšè—çŠ¶æ€ï¼Œ$\\overleftarrow{\\mathbf{h}}_j$ æ˜¯åå‘RNNçš„éšè—çŠ¶æ€ã€‚æ‹¼æ¥åï¼Œ$\\mathbf{h}_j$ åŒæ—¶åŒ…å«äº†ä½ç½® $j$ çš„å·¦ä¾§å’Œå³ä¾§ä¸Šä¸‹æ–‡ã€‚\n\nåœ¨è§£ç çš„ç¬¬ $i$ æ­¥ï¼Œæˆ‘ä»¬è®¡ç®—ä¸€ä¸ª**åŠ¨æ€çš„ä¸Šä¸‹æ–‡å‘é‡** $\\mathbf{c}_i$ï¼ˆæ³¨æ„ï¼šä¸å†æ˜¯å›ºå®šçš„ $\\mathbf{c}$ï¼Œè€Œæ˜¯æ¯ä¸€æ­¥éƒ½ä¸åŒçš„ $\\mathbf{c}_i$ï¼‰ï¼š\n\n$$\n\\mathbf{c}_i = \\sum_{j=1}^{T_x} \\alpha_{ij} \\mathbf{h}_j\n$$\n\nå…¶ä¸­ $\\alpha_{ij}$ æ˜¯ç¬¬ $i$ æ­¥è§£ç æ—¶ï¼Œå¯¹è¾“å…¥ä½ç½® $j$ çš„æ³¨æ„åŠ›æƒé‡ã€‚\n\n### æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—\n\né‚£ä¹ˆ $\\alpha_{ij}$ æ˜¯æ€ä¹ˆè®¡ç®—çš„å‘¢ï¼Ÿè¿™æ˜¯Attentionæœºåˆ¶çš„æ ¸å¿ƒã€‚\n\né¦–å…ˆï¼Œè®¡ç®—ä¸€ä¸ª**å¯¹é½åˆ†æ•°ï¼ˆalignment scoreï¼‰** $e_{ij}$ï¼Œè¡¡é‡è§£ç å™¨å½“å‰çŠ¶æ€ä¸ç¼–ç å™¨ä½ç½® $j$ çš„ç›¸å…³æ€§ï¼š\n\n$$\ne_{ij} = a(\\mathbf{s}_{i-1}, \\mathbf{h}_j)\n$$\n\nå…¶ä¸­ $\\mathbf{s}_{i-1}$ æ˜¯è§£ç å™¨åœ¨ç¬¬ $i-1$ æ­¥çš„éšè—çŠ¶æ€ï¼Œ$a$ æ˜¯ä¸€ä¸ª**å¯¹é½æ¨¡å‹ï¼ˆalignment modelï¼‰**ã€‚\n\nBahdanauä½¿ç”¨äº†ä¸€ä¸ªå•å±‚å‰é¦ˆç½‘ç»œä½œä¸ºå¯¹é½æ¨¡å‹ï¼š\n\n$$\ne_{ij} = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a \\mathbf{s}_{i-1} + \\mathbf{U}_a \\mathbf{h}_j)\n$$\n\nè¿™è¢«ç§°ä¸º**åŠ æ€§æ³¨æ„åŠ›ï¼ˆadditive attentionï¼‰**ï¼Œå› ä¸º $\\mathbf{s}_{i-1}$ å’Œ $\\mathbf{h}_j$ æ˜¯é€šè¿‡åŠ æ³•ç»“åˆçš„ã€‚\n\nç„¶åï¼Œå¯¹æ‰€æœ‰ä½ç½®çš„åˆ†æ•°åšsoftmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼š\n\n$$\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n$$\n\nsoftmaxç¡®ä¿äº†ï¼š\n\n- æ‰€æœ‰æƒé‡éƒ½æ˜¯æ­£æ•°ï¼š$\\alpha_{ij} > 0$\n- æƒé‡ä¹‹å’Œä¸º1ï¼š$\\sum_j \\alpha_{ij} = 1$\n\nè¿™æ ·ï¼Œ$\\alpha_{ij}$ å¯ä»¥è§£é‡Šä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒâ€”â€”è§£ç å™¨åœ¨ç¬¬ $i$ æ­¥\"å…³æ³¨\"è¾“å…¥ä½ç½® $j$ çš„æ¦‚ç‡ã€‚\n\n::: {.callout-note}\n## Algorithm: Bahdanau Attention (Bahdanau et al., 2015)\n\n```python\ndef bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):\n    \"\"\"\n    Bahdanau (åŠ æ€§) æ³¨æ„åŠ›æœºåˆ¶\n\n    å‚æ•°:\n        s_prev: è§£ç å™¨ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ [batch, dec_hidden]\n        encoder_outputs: ç¼–ç å™¨æ‰€æœ‰éšè—çŠ¶æ€ [batch, src_len, enc_hidden]\n        W_a, U_a, v_a: å¯å­¦ä¹ å‚æ•°\n\n    è¿”å›:\n        context: ä¸Šä¸‹æ–‡å‘é‡ [batch, enc_hidden]\n        attention_weights: æ³¨æ„åŠ›æƒé‡ [batch, src_len]\n    \"\"\"\n    # Step 1: è®¡ç®—å¯¹é½åˆ†æ•°\n    # s_prev å¹¿æ’­åˆ°æ‰€æœ‰æºä½ç½®\n    scores = v_a @ tanh(W_a @ s_prev + U_a @ encoder_outputs)  # [batch, src_len]\n\n    # Step 2: Softmax å½’ä¸€åŒ–\n    attention_weights = softmax(scores, dim=-1)  # [batch, src_len]\n\n    # Step 3: åŠ æƒæ±‚å’Œ\n    context = attention_weights @ encoder_outputs  # [batch, enc_hidden]\n\n    return context, attention_weights\n```\n\n*Source: Bahdanau, Cho, & Bengio (2015) \"Neural Machine Translation by Jointly Learning to Align and Translate\", ICLR 2015. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*\n:::\n\nä¸‹å›¾å±•ç¤ºäº†å¸¦Bahdanau Attentionçš„RNN Encoder-Decoderæ¶æ„ï¼š\n\n![å¸¦Bahdanau Attentionçš„RNN Encoder-Decoderæ¶æ„ã€‚ç¼–ç å™¨ï¼ˆåº•éƒ¨ï¼‰ä½¿ç”¨åŒå‘RNNå¤„ç†è¾“å…¥åºåˆ—ï¼Œäº§ç”Ÿéšè—çŠ¶æ€åºåˆ—ã€‚è§£ç å™¨ï¼ˆé¡¶éƒ¨ï¼‰åœ¨æ¯ä¸€æ­¥é€šè¿‡Attentionæœºåˆ¶åŠ¨æ€è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼šå°†å½“å‰è§£ç å™¨çŠ¶æ€ä¸æ‰€æœ‰ç¼–ç å™¨çŠ¶æ€æ¯”è¾ƒï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡$c_t$ï¼Œè¾…åŠ©ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ã€‚](figures/chapter-5/original/fig-bahdanau-attention-d2l.svg){#fig-attention-mechanism width=70%}\n\n::: {.figure-caption}\n*Source: Dive into Deep Learning, Figure 11.4.2. [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)*\n:::\n\n### å®Œæ•´æ•°å€¼ç¤ºä¾‹ï¼šAttentionè®¡ç®—\n\nè®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå°ä¾‹å­èµ°ä¸€éå®Œæ•´çš„Attentionè®¡ç®—è¿‡ç¨‹ã€‚\n\n**è®¾å®š**ï¼š\n\n- è¾“å…¥åºåˆ—ï¼š3ä¸ªè¯ï¼ˆ\"I love NLP\"ï¼‰ï¼Œç¼–ç åå¾—åˆ°3ä¸ªéšè—çŠ¶æ€\n- è§£ç å™¨éšè—çŠ¶æ€ç»´åº¦ï¼š$d_s = 4$\n- ç¼–ç å™¨éšè—çŠ¶æ€ç»´åº¦ï¼š$d_h = 4$\n- æ³¨æ„åŠ›ä¸­é—´ç»´åº¦ï¼š$d_a = 3$\n\n**ç¼–ç å™¨è¾“å‡º**ï¼ˆå‡è®¾å·²ç»è®¡ç®—å¥½ï¼‰ï¼š\n\n$$\n\\mathbf{h}_1 = [0.2, 0.5, -0.3, 0.8]^\\top \\quad \\text{(\"I\")}\n$$\n\n$$\n\\mathbf{h}_2 = [0.7, -0.2, 0.4, 0.1]^\\top \\quad \\text{(\"love\")}\n$$\n\n$$\n\\mathbf{h}_3 = [-0.1, 0.6, 0.5, -0.4]^\\top \\quad \\text{(\"NLP\")}\n$$\n\n**è§£ç å™¨å½“å‰çŠ¶æ€**ï¼ˆæ­£åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªç›®æ ‡è¯ï¼‰ï¼š\n\n$$\n\\mathbf{s}_0 = [0.1, -0.3, 0.4, 0.2]^\\top\n$$\n\n**å‚æ•°**ï¼ˆç®€åŒ–çš„éšæœºå€¼ï¼‰ï¼š\n\n$$\n\\mathbf{W}_a = \\begin{bmatrix} 0.1 & -0.2 & 0.3 & 0.1 \\\\ 0.2 & 0.1 & -0.1 & 0.2 \\\\ -0.1 & 0.3 & 0.2 & -0.2 \\end{bmatrix}, \\quad\n\\mathbf{U}_a = \\begin{bmatrix} 0.2 & 0.1 & -0.2 & 0.3 \\\\ -0.1 & 0.2 & 0.1 & 0.1 \\\\ 0.3 & -0.1 & 0.2 & -0.1 \\end{bmatrix}\n$$\n\n$$\n\\mathbf{v}_a = [0.5, -0.3, 0.4]^\\top\n$$\n\n**Step 1ï¼šè®¡ç®— $\\mathbf{W}_a \\mathbf{s}_0$**\n\n$$\n\\mathbf{W}_a \\mathbf{s}_0 = \\begin{bmatrix} 0.1 \\cdot 0.1 + (-0.2) \\cdot (-0.3) + 0.3 \\cdot 0.4 + 0.1 \\cdot 0.2 \\\\ \\vdots \\end{bmatrix} = \\begin{bmatrix} 0.21 \\\\ 0.03 \\\\ 0.04 \\end{bmatrix}\n$$\n\n**Step 2ï¼šå¯¹æ¯ä¸ªç¼–ç å™¨çŠ¶æ€è®¡ç®— $\\mathbf{U}_a \\mathbf{h}_j$**\n\n$$\n\\mathbf{U}_a \\mathbf{h}_1 = [0.33, 0.14, -0.05]^\\top\n$$\n\n$$\n\\mathbf{U}_a \\mathbf{h}_2 = [0.13, 0.06, 0.29]^\\top\n$$\n\n$$\n\\mathbf{U}_a \\mathbf{h}_3 = [-0.15, 0.17, 0.05]^\\top\n$$\n\n**Step 3ï¼šè®¡ç®—å¯¹é½åˆ†æ•° $e_{1j}$**\n\n$$\ne_{11} = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a \\mathbf{s}_0 + \\mathbf{U}_a \\mathbf{h}_1) = \\mathbf{v}_a^\\top \\tanh([0.54, 0.17, -0.01]^\\top)\n$$\n\n$$\n= [0.5, -0.3, 0.4] \\cdot [\\tanh(0.54), \\tanh(0.17), \\tanh(-0.01)]^\\top\n$$\n\n$$\n= [0.5, -0.3, 0.4] \\cdot [0.49, 0.17, -0.01]^\\top = 0.24 - 0.05 - 0.004 \\approx 0.19\n$$\n\nç±»ä¼¼åœ°è®¡ç®— $e_{12}$ å’Œ $e_{13}$ï¼š\n\n$$\ne_{12} \\approx 0.25, \\quad e_{13} \\approx 0.08\n$$\n\n**Step 4ï¼šSoftmaxå½’ä¸€åŒ–**\n\n$$\n\\alpha_{11} = \\frac{\\exp(0.19)}{\\exp(0.19) + \\exp(0.25) + \\exp(0.08)} = \\frac{1.21}{1.21 + 1.28 + 1.08} = \\frac{1.21}{3.57} \\approx 0.34\n$$\n\n$$\n\\alpha_{12} = \\frac{1.28}{3.57} \\approx 0.36, \\quad \\alpha_{13} = \\frac{1.08}{3.57} \\approx 0.30\n$$\n\n**Step 5ï¼šè®¡ç®—ä¸Šä¸‹æ–‡å‘é‡**\n\n$$\n\\mathbf{c}_1 = \\alpha_{11} \\mathbf{h}_1 + \\alpha_{12} \\mathbf{h}_2 + \\alpha_{13} \\mathbf{h}_3\n$$\n\n$$\n= 0.34 \\cdot [0.2, 0.5, -0.3, 0.8]^\\top + 0.36 \\cdot [0.7, -0.2, 0.4, 0.1]^\\top + 0.30 \\cdot [-0.1, 0.6, 0.5, -0.4]^\\top\n$$\n\n$$\n\\approx [0.29, 0.18, 0.09, 0.19]^\\top\n$$\n\n**è§£è¯»**ï¼šåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¨¡å‹å¯¹\"love\"çš„å…³æ³¨æœ€å¤šï¼ˆ0.36ï¼‰ï¼Œå…¶æ¬¡æ˜¯\"I\"ï¼ˆ0.34ï¼‰å’Œ\"NLP\"ï¼ˆ0.30ï¼‰ã€‚æ³¨æ„åŠ›æƒé‡ç›¸å¯¹å‡åŒ€ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯éšæœºå‚æ•°ã€‚åœ¨è®­ç»ƒåçš„çœŸå®æ¨¡å‹ä¸­ï¼Œæƒé‡åˆ†å¸ƒä¼šæ›´åŠ å°–é”â€”â€”æ¨¡å‹ä¼šå­¦ä¼šåœ¨éœ€è¦æ—¶èšç„¦äºç‰¹å®šä½ç½®ã€‚\n\n### è§£ç å™¨çš„å®Œæ•´æµç¨‹\n\næœ‰äº†Attentionæœºåˆ¶ï¼Œè§£ç å™¨çš„æ¯ä¸€æ­¥å·¥ä½œæµç¨‹å˜ä¸ºï¼š\n\n1. **è®¡ç®—æ³¨æ„åŠ›æƒé‡** $\\alpha_{ij}$ï¼šåŸºäºå½“å‰è§£ç å™¨çŠ¶æ€å’Œæ‰€æœ‰ç¼–ç å™¨çŠ¶æ€\n2. **è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡** $\\mathbf{c}_i$ï¼šå¯¹ç¼–ç å™¨çŠ¶æ€åŠ æƒæ±‚å’Œ\n3. **æ›´æ–°è§£ç å™¨çŠ¶æ€**ï¼šç»“åˆä¸Šä¸‹æ–‡å‘é‡ã€å‰ä¸€æ­¥è¾“å‡ºã€å‰ä¸€æ­¥çŠ¶æ€\n\n$$\n\\mathbf{s}_i = f(\\mathbf{s}_{i-1}, y_{i-1}, \\mathbf{c}_i)\n$$\n\n4. **ç”Ÿæˆè¾“å‡º**ï¼šåŸºäºæ–°çš„è§£ç å™¨çŠ¶æ€\n\n$$\nP(y_i | y_{<i}, \\mathbf{x}) = g(\\mathbf{s}_i, y_{i-1}, \\mathbf{c}_i)\n$$\n\nå…³é”®åŒºåˆ«æ˜¯ï¼š**æ¯ä¸€æ­¥éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„ä¸Šä¸‹æ–‡å‘é‡ $\\mathbf{c}_i$**ï¼Œå®ƒæ˜¯æ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€è®¡ç®—çš„ã€‚\n\n### å¤æ‚åº¦åˆ†æ\n\n**æ—¶é—´å¤æ‚åº¦**ï¼š\n\n- è®¡ç®—æ‰€æœ‰å¯¹é½åˆ†æ•°ï¼š$O(T_x \\cdot T_y \\cdot d)$\n- å…¶ä¸­ $T_x$ æ˜¯æºåºåˆ—é•¿åº¦ï¼Œ$T_y$ æ˜¯ç›®æ ‡åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯éšè—ç»´åº¦\n\nä¸æ ‡å‡†Seq2Seqç›¸æ¯”ï¼ŒAttentionå¢åŠ äº† $O(T_x \\cdot T_y)$ çš„è®¡ç®—é‡ã€‚å¯¹äºé•¿åºåˆ—ï¼Œè¿™ä¸ªå¼€é”€æ˜¯æ˜¾è‘—çš„ã€‚\n\n**ç©ºé—´å¤æ‚åº¦**ï¼š\n\n- éœ€è¦å­˜å‚¨æ‰€æœ‰ç¼–ç å™¨éšè—çŠ¶æ€ï¼š$O(T_x \\cdot d)$\n- æ ‡å‡†Seq2Seqåªéœ€è¦å­˜å‚¨æœ€ç»ˆçŠ¶æ€ï¼š$O(d)$\n\nè¿™æ˜¯ç”¨ç©ºé—´æ¢å–æ€§èƒ½çš„å…¸å‹ä¾‹å­ã€‚\n\n---\n\n## æ³¨æ„åŠ›å¯è§†åŒ–ï¼šæ¨¡å‹åœ¨\"çœ‹\"ä»€ä¹ˆï¼Ÿ\n\n### å¯¹é½çŸ©é˜µ\n\nAttentionæœºåˆ¶çš„ä¸€ä¸ªç¾å¦™ç‰¹æ€§æ˜¯**å¯è§£é‡Šæ€§**ã€‚æ³¨æ„åŠ›æƒé‡ $\\alpha_{ij}$ ç›´æ¥å‘Šè¯‰æˆ‘ä»¬ï¼šåœ¨ç”Ÿæˆç¬¬ $i$ ä¸ªç›®æ ‡è¯æ—¶ï¼Œæ¨¡å‹å…³æ³¨äº†å“ªäº›æºè¯ã€‚\n\næˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰çš„æ³¨æ„åŠ›æƒé‡æ’åˆ—æˆä¸€ä¸ªçŸ©é˜µï¼Œæ¨ªè½´æ˜¯æºåºåˆ—ï¼Œçºµè½´æ˜¯ç›®æ ‡åºåˆ—ã€‚è¿™ä¸ªçŸ©é˜µè¢«ç§°ä¸º**å¯¹é½çŸ©é˜µï¼ˆalignment matrixï¼‰**ã€‚\n\n![å¯¹é½å¯è§†åŒ–ï¼šå››ä¸ªè‹±æ³•ç¿»è¯‘ä¾‹å­çš„æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ã€‚æ¨ªè½´æ˜¯è‹±è¯­æºå¥å­ï¼Œçºµè½´æ˜¯æ³•è¯­ç›®æ ‡å¥å­ã€‚ç™½è‰²è¡¨ç¤ºé«˜æ³¨æ„åŠ›æƒé‡ï¼Œé»‘è‰²è¡¨ç¤ºä½æƒé‡ã€‚æ³¨æ„å¯¹è§’çº¿æ¨¡å¼ï¼ˆå•è¯ä¸€ä¸€å¯¹åº”ï¼‰å’Œåç¦»å¯¹è§’çº¿çš„åŒºåŸŸï¼ˆè¯­åºè°ƒæ•´ï¼‰ã€‚ä¾‹å¦‚(a)ä¸­\"August\"å¯¹åº”\"aoÃ»t\"ï¼Œ\"European Economic Area\"å¯¹åº”\"zone Ã©conomique europÃ©enne\"ã€‚](figures/chapter-5/original/fig3-alignment-visualization.png){#fig-alignment-visualization width=95%}\n\n::: {.figure-caption}\n*Source: Bahdanau, Cho, & Bengio (2015) \"Neural Machine Translation by Jointly Learning to Align and Translate\", Figure 3. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*\n:::\n\n### å¯¹é½æ¨¡å¼çš„è¯­è¨€å­¦æ„ä¹‰\n\né€šè¿‡è§‚å¯Ÿå¯¹é½çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸€äº›æœ‰è¶£çš„è¯­è¨€å­¦æ¨¡å¼ï¼š\n\n**1. å•è°ƒå¯¹é½**\n\nå¯¹äºè¯­åºç›¸ä¼¼çš„è¯­è¨€å¯¹ï¼ˆå¦‚è‹±è¯­åˆ°å¾·è¯­çš„æŸäº›ç»“æ„ï¼‰ï¼Œå¯¹é½çŸ©é˜µæ¥è¿‘å¯¹è§’çº¿â€”â€”ç¬¬1ä¸ªæºè¯å¯¹åº”ç¬¬1ä¸ªç›®æ ‡è¯ï¼Œç¬¬2ä¸ªå¯¹åº”ç¬¬2ä¸ªï¼Œä¾æ­¤ç±»æ¨ã€‚\n\n**2. è¯­åºè°ƒæ•´**\n\nå½“æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„è¯åºä¸åŒæ—¶ï¼Œå¯¹é½çŸ©é˜µä¼šåç¦»å¯¹è§’çº¿ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„\"red car\"ç¿»è¯‘æˆæ³•è¯­æ˜¯\"voiture rouge\"ï¼ˆè½¦ çº¢ï¼‰ï¼Œå¯¹é½çŸ©é˜µä¼šæ˜¾ç¤ºäº¤å‰æ¨¡å¼ã€‚\n\n**3. ä¸€å¯¹å¤šå’Œå¤šå¯¹ä¸€**\n\næŸäº›è¯æ²¡æœ‰ç›´æ¥å¯¹åº”ï¼Œæˆ–ä¸€ä¸ªè¯å¯¹åº”å¤šä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„\"going to\"å¯èƒ½å¯¹åº”æ³•è¯­çš„å•ä¸ªè¯\"va\"ã€‚\n\n**4. ç©ºå¯¹é½**\n\næŸäº›ç›®æ ‡è¯ï¼ˆå¦‚å† è¯ï¼‰å¯èƒ½æ²¡æœ‰æ˜ç¡®çš„æºè¯å¯¹åº”ï¼Œå®ƒä»¬çš„æ³¨æ„åŠ›æƒé‡ä¼šåˆ†æ•£åœ¨å¤šä¸ªä½ç½®ã€‚\n\n### å¯è§†åŒ–çš„å±€é™æ€§\n\nè™½ç„¶æ³¨æ„åŠ›å¯è§†åŒ–å¾ˆå¸å¼•äººï¼Œä½†æˆ‘ä»¬è¦è°¨æ…è§£è¯»ï¼š\n\n1. **æ³¨æ„åŠ›ä¸ç­‰äºè§£é‡Š**ï¼šé«˜æ³¨æ„åŠ›æƒé‡ä¸ä¸€å®šæ„å‘³ç€æ¨¡å‹\"ç†è§£\"äº†é‚£ä¸ªä½ç½®çš„å†…å®¹\n2. **å¯èƒ½æœ‰å¤šé‡å› ç´ **ï¼šæ¨¡å‹å¯èƒ½é€šè¿‡å…¶ä»–æœºåˆ¶ï¼ˆå¦‚ä½ç½®ä¿¡æ¯ï¼‰åšå‡ºå†³å®š\n3. **è®­ç»ƒç›®æ ‡çš„å½±å“**ï¼šæ³¨æ„åŠ›æƒé‡æ˜¯ä¸ºäº†æœ€å°åŒ–ç¿»è¯‘æŸå¤±è€Œå­¦ä¹ çš„ï¼Œä¸ä¸€å®šåæ˜ äººç±»çš„å¯¹é½ç›´è§‰\n\nåæ¥çš„ç ”ç©¶ï¼ˆå¦‚Jain & Wallace, 2019ï¼‰å¯¹æ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æå‡ºäº†è´¨ç–‘ã€‚ä½†ä½œä¸ºä¸€ä¸ªè¯Šæ–­å·¥å…·ï¼Œæ³¨æ„åŠ›å¯è§†åŒ–ä»ç„¶éå¸¸æœ‰ä»·å€¼ã€‚\n\n---\n\n## å·¥ç¨‹å®è·µï¼šå¸¦Attentionçš„Seq2Seq\n\nè®©æˆ‘ä»¬ç”¨PyTorchå®ç°ä¸€ä¸ªå¸¦Attentionçš„Seq2Seqæ¨¡å‹ã€‚\n\n### ç¼–ç å™¨\n\n::: {#d766c2b5 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.GRU(\n            embed_dim, hidden_dim,\n            num_layers=num_layers,\n            bidirectional=True,  # åŒå‘\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        # å°†åŒå‘çš„éšè—çŠ¶æ€å‹ç¼©åˆ°å•å‘ç»´åº¦\n        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src, src_lengths=None):\n        # src: [batch_size, src_len]\n        embedded = self.dropout(self.embedding(src))  # [batch, src_len, embed_dim]\n\n        if src_lengths is not None:\n            packed = nn.utils.rnn.pack_padded_sequence(\n                embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n            )\n            packed_outputs, hidden = self.rnn(packed)\n            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        else:\n            outputs, hidden = self.rnn(embedded)\n\n        # outputs: [batch, src_len, hidden_dim * 2] (åŒå‘æ‹¼æ¥)\n        # hidden: [num_layers * 2, batch, hidden_dim]\n\n        # åˆå¹¶å‰å‘å’Œåå‘çš„æœ€ç»ˆéšè—çŠ¶æ€\n        # hidden[-2] æ˜¯æœ€åä¸€å±‚å‰å‘ï¼Œhidden[-1] æ˜¯æœ€åä¸€å±‚åå‘\n        hidden = torch.tanh(self.fc(torch.cat([hidden[-2], hidden[-1]], dim=1)))\n        # hidden: [batch, hidden_dim]\n\n        return outputs, hidden\n```\n:::\n\n\n### Attentionå±‚\n\n::: {#5d037926 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nclass BahdanauAttention(nn.Module):\n    def __init__(self, enc_hidden_dim, dec_hidden_dim, attention_dim):\n        super().__init__()\n        # åŠ æ€§æ³¨æ„åŠ›çš„å‚æ•°\n        self.W_a = nn.Linear(dec_hidden_dim, attention_dim, bias=False)\n        self.U_a = nn.Linear(enc_hidden_dim * 2, attention_dim, bias=False)  # åŒå‘ç¼–ç å™¨\n        self.v_a = nn.Linear(attention_dim, 1, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n        \"\"\"\n        decoder_hidden: [batch, dec_hidden]\n        encoder_outputs: [batch, src_len, enc_hidden * 2]\n        mask: [batch, src_len], Trueè¡¨ç¤ºéœ€è¦maskçš„ä½ç½®ï¼ˆpaddingï¼‰\n        \"\"\"\n        batch_size, src_len, _ = encoder_outputs.shape\n\n        # decoder_hidden æ‰©å±•åˆ°æ‰€æœ‰æºä½ç½®\n        # [batch, dec_hidden] -> [batch, src_len, dec_hidden]\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n\n        # è®¡ç®—å¯¹é½åˆ†æ•°\n        # [batch, src_len, attention_dim]\n        energy = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))\n        # [batch, src_len, 1] -> [batch, src_len]\n        attention_scores = self.v_a(energy).squeeze(-1)\n\n        # åº”ç”¨maskï¼ˆå°†paddingä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå°çš„è´Ÿæ•°ï¼‰\n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask, -1e10)\n\n        # Softmaxå½’ä¸€åŒ–\n        attention_weights = F.softmax(attention_scores, dim=1)  # [batch, src_len]\n\n        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡\n        # [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -> [batch, 1, enc_hidden*2]\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n        context = context.squeeze(1)  # [batch, enc_hidden * 2]\n\n        return context, attention_weights\n```\n:::\n\n\n### è§£ç å™¨\n\n::: {#ded5da62 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nclass AttentionDecoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,\n                 attention_dim, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n        # GRUè¾“å…¥æ˜¯ï¼šembedded + context\n        self.rnn = nn.GRU(\n            embed_dim + enc_hidden_dim * 2,  # åŒå‘ç¼–ç å™¨\n            dec_hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n\n        # è¾“å‡ºå±‚\n        self.fc = nn.Linear(dec_hidden_dim + enc_hidden_dim * 2 + embed_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        \"\"\"\n        å•æ­¥è§£ç \n        input_token: [batch] - ä¸Šä¸€æ­¥çš„è¾“å‡ºtoken\n        hidden: [1, batch, dec_hidden] - ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€\n        encoder_outputs: [batch, src_len, enc_hidden * 2]\n        \"\"\"\n        # Embedding\n        embedded = self.dropout(self.embedding(input_token))  # [batch, embed_dim]\n\n        # Attention\n        # hidden[-1] å–æœ€åä¸€å±‚ï¼Œ[batch, dec_hidden]\n        context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)\n\n        # æ‹¼æ¥embeddedå’Œcontextä½œä¸ºRNNè¾“å…¥\n        rnn_input = torch.cat([embedded, context], dim=1).unsqueeze(1)  # [batch, 1, embed+ctx]\n\n        # RNN\n        output, hidden = self.rnn(rnn_input, hidden)\n        output = output.squeeze(1)  # [batch, dec_hidden]\n\n        # è¾“å‡ºå±‚\n        prediction = self.fc(torch.cat([output, context, embedded], dim=1))\n\n        return prediction, hidden, attention_weights\n```\n:::\n\n\n### å®Œæ•´çš„Seq2Seqæ¨¡å‹\n\n::: {#e372dfb6 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nclass Seq2SeqAttention(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        \"\"\"\n        src: [batch, src_len]\n        trg: [batch, trg_len]\n        \"\"\"\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.vocab_size\n\n        # å­˜å‚¨è¾“å‡º\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        attentions = []\n\n        # ç¼–ç \n        encoder_outputs, hidden = self.encoder(src)\n        # hidden: [batch, dec_hidden] -> [1, batch, dec_hidden]\n        hidden = hidden.unsqueeze(0)\n\n        # ç¬¬ä¸€ä¸ªè§£ç è¾“å…¥æ˜¯ <sos> token\n        input_token = trg[:, 0]\n\n        for t in range(1, trg_len):\n            prediction, hidden, attention = self.decoder(\n                input_token, hidden, encoder_outputs\n            )\n            outputs[:, t] = prediction\n            attentions.append(attention)\n\n            # Teacher forcing\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = prediction.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n\n        return outputs, torch.stack(attentions, dim=1)\n\n# åˆ›å»ºæ¨¡å‹ç¤ºä¾‹\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nencoder = Encoder(vocab_size=10000, embed_dim=256, hidden_dim=512)\ndecoder = AttentionDecoder(\n    vocab_size=10000, embed_dim=256,\n    enc_hidden_dim=512, dec_hidden_dim=512, attention_dim=256\n)\nmodel = Seq2SeqAttention(encoder, decoder, device).to(device)\n\nprint(f\"ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in encoder.parameters()):,}\")\nprint(f\"è§£ç å™¨å‚æ•°: {sum(p.numel() for p in decoder.parameters()):,}\")\nprint(f\"æ€»å‚æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nç¼–ç å™¨å‚æ•°: 5,450,240\nè§£ç å™¨å‚æ•°: 23,639,056\næ€»å‚æ•°: 29,089,296\n```\n:::\n:::\n\n\n### å…³é”®å®ç°ç»†èŠ‚\n\n**1. Maskå¤„ç†**\n\nåœ¨å®é™…åº”ç”¨ä¸­ï¼Œbatchä¸­çš„åºåˆ—é•¿åº¦ä¸åŒï¼Œéœ€è¦paddingã€‚è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œpaddingä½ç½®ä¸åº”è¯¥è·å¾—ä»»ä½•æƒé‡ã€‚æˆ‘ä»¬é€šè¿‡maskå°†è¿™äº›ä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå¤§çš„è´Ÿæ•°ï¼Œsoftmaxåå®ƒä»¬çš„æƒé‡è¶‹è¿‘äº0ã€‚\n\n**2. Teacher Forcing**\n\nè®­ç»ƒæ—¶ï¼Œè§£ç å™¨çš„è¾“å…¥å¯ä»¥æ˜¯çœŸå®çš„ä¸Šä¸€ä¸ªè¯ï¼ˆteacher forcingï¼‰æˆ–æ¨¡å‹é¢„æµ‹çš„è¯ã€‚`teacher_forcing_ratio` æ§åˆ¶ä¸¤è€…çš„æ··åˆæ¯”ä¾‹ã€‚è¾ƒé«˜çš„æ¯”ä¾‹åŠ é€Ÿè®­ç»ƒï¼Œä½†å¯èƒ½å¯¼è‡´exposure biasã€‚\n\n**3. åŒå‘ç¼–ç å™¨**\n\næˆ‘ä»¬ä½¿ç”¨åŒå‘GRUï¼Œç¼–ç å™¨è¾“å‡ºçš„ç»´åº¦æ˜¯ `hidden_dim * 2`ã€‚è¿™è®©æ¯ä¸ªä½ç½®éƒ½åŒ…å«å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n\n---\n\n## æ·±å…¥ç†è§£\n\n### ä¸ºä»€ä¹ˆAttentionæœ‰æ•ˆï¼Ÿâ€”â€”ç†è®ºè§†è§’\n\n**1. ä¿¡æ¯è®ºè§†è§’**\n\næ ‡å‡†Seq2Seqçš„ä¸Šä¸‹æ–‡å‘é‡ $\\mathbf{c}$ æ˜¯è¾“å…¥ $\\mathbf{x}$ çš„ä¸€ä¸ª**å……åˆ†ç»Ÿè®¡é‡ï¼ˆsufficient statisticï¼‰**â€”â€”å¦‚æœ $\\mathbf{c}$ å®Œç¾ï¼Œå®ƒåº”è¯¥åŒ…å«å…³äº $\\mathbf{y}$ çš„æ‰€æœ‰å¿…è¦ä¿¡æ¯ã€‚ä½†åœ¨å®è·µä¸­ï¼Œæœ‰é™ç»´åº¦çš„ $\\mathbf{c}$ æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚\n\nAttentioné€šè¿‡è®©è§£ç å™¨è®¿é—®æ‰€æœ‰çš„ $\\mathbf{h}_j$ï¼Œå®é™…ä¸Šæ˜¯åœ¨è¯´ï¼š**ä¸è¦æ±‚ä¸€ä¸ªå……åˆ†ç»Ÿè®¡é‡ï¼Œè€Œæ˜¯è®©æ¨¡å‹åœ¨éœ€è¦æ—¶ç›´æ¥æŸ¥è¯¢åŸå§‹ä¿¡æ¯**ã€‚è¿™ç»•è¿‡äº†ä¿¡æ¯ç“¶é¢ˆã€‚\n\n**2. è®°å¿†å¯»å€è§†è§’**\n\nå¯ä»¥æŠŠç¼–ç å™¨çš„éšè—çŠ¶æ€çœ‹ä½œä¸€ä¸ª**å¤–éƒ¨è®°å¿†ï¼ˆexternal memoryï¼‰**ï¼Œæ¯ä¸ª $\\mathbf{h}_j$ æ˜¯ä¸€ä¸ªè®°å¿†æ§½ã€‚Attentionæœºåˆ¶å®ç°äº†**åŸºäºå†…å®¹çš„è½¯å¯»å€ï¼ˆcontent-based soft addressingï¼‰**â€”â€”æ ¹æ®å½“å‰æŸ¥è¯¢ï¼ˆè§£ç å™¨çŠ¶æ€ï¼‰æ£€ç´¢ç›¸å…³çš„è®°å¿†ã€‚\n\nè¿™ä¸ªè§†è§’åæ¥è¢«æ˜¾å¼åŒ–ä¸ºMemory Networkså’ŒNeural Turing Machineã€‚\n\n**3. æ¢¯åº¦æµè§†è§’**\n\nä»ä¼˜åŒ–è§’åº¦ï¼ŒAttentionæä¾›äº†ä¸€æ¡ä»è§£ç å™¨åˆ°ç¼–ç å™¨ç‰¹å®šä½ç½®çš„**ç›´æ¥è·¯å¾„**ã€‚åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œæ¢¯åº¦è¦ä»è§£ç å™¨æµå›ç¼–ç å™¨ï¼Œå¿…é¡»ç»è¿‡ $\\mathbf{c}$ï¼Œå†ç»è¿‡æ•´ä¸ªç¼–ç è¿‡ç¨‹ã€‚Attentionåˆ›é€ äº†\"æ·å¾„\"â€”â€”æ¢¯åº¦å¯ä»¥é€šè¿‡æ³¨æ„åŠ›æƒé‡ç›´æ¥ä¼ åˆ°ç›¸å…³çš„ç¼–ç å™¨ä½ç½®ã€‚\n\n### è¾¹ç•Œæ¡ä»¶ä¸å¤±æ•ˆæ¨¡å¼\n\n**1. å•è°ƒå¯¹é½å‡è®¾**\n\nBahdanau Attentionéšå«å‡è®¾æºå’Œç›®æ ‡ä¹‹é—´å­˜åœ¨æŸç§å¯¹é½å…³ç³»ã€‚å¯¹äºç¿»è¯‘ä»»åŠ¡è¿™é€šå¸¸æˆç«‹ï¼Œä½†å¯¹äºæŸäº›ä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ï¼‰ï¼Œè¿™ä¸ªå‡è®¾å¯èƒ½ä¸æˆç«‹â€”â€”æ‘˜è¦å¯èƒ½éœ€è¦æ•´åˆåˆ†æ•£åœ¨å„å¤„çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯\"å¯¹é½\"åˆ°ç‰¹å®šä½ç½®ã€‚\n\n**2. å¤æ‚åº¦é™åˆ¶**\n\nå½“æºåºåˆ—å¾ˆé•¿æ—¶ï¼ˆå¦‚æ–‡æ¡£çº§ç¿»è¯‘ï¼‰ï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å˜å¾—æ˜‚è´µã€‚$O(T_x \\cdot T_y)$ çš„å¤æ‚åº¦åœ¨ $T_x = 10000$ æ—¶æ˜¯ä¸å¯æ¥å—çš„ã€‚\n\n**3. åˆ†å¸ƒåç§»**\n\nè®­ç»ƒæ—¶ï¼Œè§£ç å™¨çœ‹åˆ°çš„ä¸Šä¸‹æ–‡å‘é‡åˆ†å¸ƒä¸æ¨ç†æ—¶å¯èƒ½ä¸åŒï¼ˆå› ä¸ºteacher forcingï¼‰ã€‚è¿™å¯èƒ½å¯¼è‡´æ³¨æ„åŠ›æƒé‡åœ¨æ¨ç†æ—¶ä¸å¤Ÿå‡†ç¡®ã€‚\n\n### å¼€æ”¾ç ”ç©¶é—®é¢˜\n\n1. **æœ€ä¼˜å¯¹é½**ï¼šAttentionå­¦åˆ°çš„å¯¹é½ä¸è¯­è¨€å­¦å®¶æ ‡æ³¨çš„å¯¹é½æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿæ˜¯å¦å¯ä»¥ç”¨è¯­è¨€å­¦çŸ¥è¯†æ”¹è¿›Attentionï¼Ÿ\n\n2. **ç¨€ç–æ³¨æ„åŠ›**ï¼šèƒ½å¦å­¦ä¹ æ›´ç¨€ç–çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œåªå…³æ³¨å°‘æ•°å…³é”®ä½ç½®ï¼Œè€Œä¸æ˜¯softåœ°åˆ†å¸ƒåˆ°æ‰€æœ‰ä½ç½®ï¼Ÿ\n\n3. **å±‚æ¬¡åŒ–æ³¨æ„åŠ›**ï¼šå¯¹äºé•¿æ–‡æ¡£ï¼Œèƒ½å¦è®¾è®¡å±‚æ¬¡åŒ–çš„Attentionâ€”â€”å…ˆå…³æ³¨æ®µè½ï¼Œå†å…³æ³¨å¥å­ï¼Œæœ€åå…³æ³¨è¯ï¼Ÿ\n\n---\n\n## å±€é™æ€§ä¸å±•æœ›\n\n### æœ¬ç« æ–¹æ³•çš„æ ¸å¿ƒå±€é™\n\n**1. ä»ç„¶ä¾èµ–RNN**\n\nBahdanau Attentionæ˜¯Seq2Seqçš„\"è¡¥ä¸\"â€”â€”å®ƒå¢å¼ºäº†è§£ç å™¨è®¿é—®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä½†åº•å±‚ä»ç„¶æ˜¯RNNã€‚è¿™æ„å‘³ç€ï¼š\n\n- ä»ç„¶æ˜¯é¡ºåºè®¡ç®—ï¼Œæ— æ³•å¹¶è¡Œ\n- ä»ç„¶å—é™äºRNNçš„é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼ˆè™½ç„¶å› ä¸ºAttentionæœ‰æ‰€ç¼“è§£ï¼‰\n\n**2. æ³¨æ„åŠ›è®¡ç®—ä¸åºåˆ—é•¿åº¦å¹³æ–¹ç›¸å…³**\n\næ¯ä¸ªè§£ç æ­¥éœ€è¦è®¡ç®—å¯¹æ‰€æœ‰ç¼–ç ä½ç½®çš„æ³¨æ„åŠ›ï¼Œæ€»å¤æ‚åº¦æ˜¯ $O(T_x \\cdot T_y)$ã€‚å¯¹äºé•¿åºåˆ—ï¼Œè¿™æ˜¯æ˜¾è‘—çš„å¼€é”€ã€‚\n\n**3. æ²¡æœ‰ä½ç½®æ„ŸçŸ¥**\n\nAttentionæ˜¯åŸºäºå†…å®¹çš„ï¼Œå®ƒä¸ç›´æ¥è€ƒè™‘ä½ç½®ä¿¡æ¯ã€‚è™½ç„¶åŒå‘RNNéšå¼ç¼–ç äº†ä½ç½®ï¼Œä½†Attentionæœ¬èº«å¯¹ä½ç½®æ˜¯\"ç›²ç›®\"çš„ã€‚\n\n### è¿™äº›å±€é™æŒ‡å‘ä»€ä¹ˆï¼Ÿ\n\nAttentionçš„æˆåŠŸå¼•å‘äº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼š**å¦‚æœAttentionå¦‚æ­¤å¼ºå¤§ï¼Œæˆ‘ä»¬è¿˜éœ€è¦RNNå—ï¼Ÿ**\n\nä¸‹ä¸€ç« å°†æ¢è®¨Attentionçš„å„ç§å˜ä½“ï¼ŒåŒ…æ‹¬Luongæå‡ºçš„ä¹˜æ€§æ³¨æ„åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ¢ç´¢æœ€ç»ˆå¯¼å‘äº†ä¸€ä¸ªé©å‘½æ€§çš„ç»“è®ºï¼š**æˆ‘ä»¬å¯ä»¥å®Œå…¨ç”¨Attentionå–ä»£RNN**ã€‚\n\nè¿™å°±æ˜¯ç¬¬8ç« Transformerçš„æ ¸å¿ƒæ€æƒ³â€”â€”\"Attention Is All You Need\"ã€‚åœ¨é‚£é‡Œï¼ŒSelf-Attentionè®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥ä¸å…¶ä»–ä½ç½®äº¤äº’ï¼Œå®Œå…¨æŠ›å¼ƒäº†å¾ªç¯ç»“æ„ï¼Œå®ç°äº†çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ã€‚\n\n> ä»Bahdanau Attentionåˆ°Transformerï¼ŒAttentionä»ä¸€ä¸ª\"è¾…åŠ©æœºåˆ¶\"æ¼”å˜ä¸º\"æ ¸å¿ƒæ¶æ„\"ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ å†å²ä¸Šæœ€é‡è¦çš„èŒƒå¼è½¬å˜ä¹‹ä¸€ã€‚\n\n---\n\n## æœ¬ç« å°ç»“\n\n::: {.callout-important}\n## æ ¸å¿ƒè¦ç‚¹\n\n- **é—®é¢˜**ï¼šSeq2Seqçš„ä¿¡æ¯ç“¶é¢ˆâ€”â€”æ‰€æœ‰è¾“å…¥ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå‘é‡ï¼Œå¯¼è‡´é•¿åºåˆ—ä¿¡æ¯ä¸¢å¤±\n- **æ´å¯Ÿ**ï¼šè§£ç å™¨åº”è¯¥èƒ½å¤ŸåŠ¨æ€åœ°ã€æœ‰é€‰æ‹©åœ°å…³æ³¨è¾“å…¥çš„ä¸åŒä½ç½®\n- **æ–¹æ³•**ï¼šAttentionæœºåˆ¶è®¡ç®—è§£ç å™¨çŠ¶æ€ä¸æ¯ä¸ªç¼–ç å™¨çŠ¶æ€çš„ç›¸å…³æ€§ï¼Œç”Ÿæˆæ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°åŠ¨æ€ä¸Šä¸‹æ–‡å‘é‡\n- **æ„ä¹‰**ï¼šæ‰“ç ´äº†å›ºå®šå‘é‡çš„é™åˆ¶ï¼Œå¤§å¹…æå‡äº†é•¿åºåˆ—ç¿»è¯‘è´¨é‡ï¼Œä¸ºåç»­çš„Transformerå¥ å®šäº†åŸºç¡€\n:::\n\n### å…³é”®å…¬å¼é€ŸæŸ¥\n\n**å¯¹é½åˆ†æ•°ï¼ˆBahdanauåŠ æ€§æ³¨æ„åŠ›ï¼‰**ï¼š\n\n$$\ne_{ij} = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a \\mathbf{s}_{i-1} + \\mathbf{U}_a \\mathbf{h}_j)\n$$\n\n**æ³¨æ„åŠ›æƒé‡**ï¼š\n\n$$\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n$$\n\n**ä¸Šä¸‹æ–‡å‘é‡**ï¼š\n\n$$\n\\mathbf{c}_i = \\sum_{j=1}^{T_x} \\alpha_{ij} \\mathbf{h}_j\n$$\n\n---\n\n## æ€è€ƒé¢˜\n\n1. **[æ¦‚å¿µç†è§£]** ä¸ºä»€ä¹ˆè¯´Attentionå®ç°äº†\"è½¯å¯»å€\"ï¼Ÿå®ƒä¸è®¡ç®—æœºå†…å­˜çš„ç¡¬å¯»å€æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿè¿™ç§è½¯å¯»å€çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\n\n2. **[æ•°å­¦æ¨å¯¼]** è¯æ˜ï¼šå½“æ³¨æ„åŠ›æƒé‡é›†ä¸­åœ¨å•ä¸€ä½ç½®æ—¶ï¼ˆå³ $\\alpha_{ij} \\to 1$ å¯¹æŸä¸ª $j$ï¼Œå…¶ä»–ä¸º0ï¼‰ï¼Œä¸Šä¸‹æ–‡å‘é‡å°±é€€åŒ–ä¸ºé‚£ä¸ªä½ç½®çš„ç¼–ç å™¨çŠ¶æ€ã€‚è¿™ä¸ç¡¬æ³¨æ„åŠ›æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n\n3. **[å·¥ç¨‹å®è·µ]** åœ¨å®ç°Attentionæ—¶ï¼Œä¸ºä»€ä¹ˆè¦å¯¹paddingä½ç½®åº”ç”¨maskï¼Ÿå¦‚æœä¸åšmaskä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿå¦‚ä½•æ­£ç¡®å®ç°maskï¼ˆè€ƒè™‘æ•°å€¼ç¨³å®šæ€§ï¼‰ï¼Ÿ\n\n4. **[æ‰¹åˆ¤æ€è€ƒ]** Attentionçš„å¯è§†åŒ–ç»å¸¸è¢«ç”¨æ¥\"è§£é‡Š\"æ¨¡å‹çš„å†³ç­–ã€‚ä½†è¿™ç§è§£é‡Šæ˜¯å¦å¯é ï¼Ÿè®¾è®¡ä¸€ä¸ªå®éªŒæ¥æ£€éªŒï¼šæ³¨æ„åŠ›æƒé‡é«˜çš„ä½ç½®æ˜¯å¦çœŸçš„å¯¹æ¨¡å‹è¾“å‡ºæœ‰é‡è¦å½±å“ã€‚\n\n5. **[å¼€æ”¾é—®é¢˜]** Bahdanau Attentionéœ€è¦ä¸ºæ¯ä¸ªè§£ç æ­¥è®¡ç®—å¯¹æ‰€æœ‰ç¼–ç ä½ç½®çš„æ³¨æ„åŠ›ï¼Œå¤æ‚åº¦æ˜¯ $O(T_x \\cdot T_y)$ã€‚æœ‰å“ªäº›æ–¹æ³•å¯ä»¥é™ä½è¿™ä¸ªå¤æ‚åº¦ï¼Ÿï¼ˆæç¤ºï¼šè€ƒè™‘ç¨€ç–åŒ–ã€å±€éƒ¨åŒ–ã€æˆ–è¿‘ä¼¼æ–¹æ³•ï¼‰\n\n---\n\n## å»¶ä¼¸é˜…è¯»\n\n### æ ¸å¿ƒè®ºæ–‡ï¼ˆå¿…è¯»ï¼‰\n\n- **[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**\n  - Attentionæœºåˆ¶åœ¨NMTä¸­çš„å¼€åˆ›æ€§å·¥ä½œ\n  - é‡ç‚¹è¯»ï¼šSection 3ï¼ˆæ¨¡å‹æ¶æ„ï¼‰ã€Section 5ï¼ˆå¯è§†åŒ–åˆ†æï¼‰\n  - arXiv: [1409.0473](https://arxiv.org/abs/1409.0473)\n\n### ç†è®ºåŸºç¡€\n\n- **[Graves et al., 2014] Neural Turing Machines**\n  - æå‡ºäº†åŸºäºå†…å®¹çš„è½¯å¯»å€ï¼Œæ˜¯Attentionçš„ç†è®ºå…ˆé©±\n  - é‡ç‚¹è¯»ï¼šSection 3.1ï¼ˆAttentionæœºåˆ¶ï¼‰\n\n### åç»­å‘å±•\n\n- **[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**\n  - æå‡ºä¹˜æ€§æ³¨æ„åŠ›ï¼Œå¯¹æ¯”ä¸åŒæ³¨æ„åŠ›å˜ä½“\n  - è¿™æ˜¯ä¸‹ä¸€ç« çš„æ ¸å¿ƒå†…å®¹\n  - arXiv: [1508.04025](https://arxiv.org/abs/1508.04025)\n\n- **[Vaswani et al., 2017] Attention Is All You Need**\n  - Transformerï¼šå®Œå…¨ç”¨Attentionå–ä»£RNN\n  - è¿™æ˜¯ç¬¬8ç« çš„æ ¸å¿ƒå†…å®¹\n  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)\n\n### å¯¹Attentionå¯è§£é‡Šæ€§çš„è®¨è®º\n\n- **[Jain & Wallace, 2019] Attention is not Explanation**\n  - è´¨ç–‘Attentionæƒé‡ä½œä¸ºè§£é‡Šçš„å¯é æ€§\n  - arXiv: [1902.10186](https://arxiv.org/abs/1902.10186)\n\n- **[Wiegreffe & Pinter, 2019] Attention is not not Explanation**\n  - å¯¹ä¸Šè¿°è®ºæ–‡çš„å›åº”ï¼Œæ›´ç»†è‡´åœ°è®¨è®ºAttentionçš„è§£é‡Šæ€§\n  - arXiv: [1908.04626](https://arxiv.org/abs/1908.04626)\n\n---\n\n## å†å²æ³¨è„š\n\nAttentionæœºåˆ¶çš„çµæ„Ÿéƒ¨åˆ†æ¥è‡ªäººç±»è§†è§‰ç³»ç»Ÿã€‚å½“æˆ‘ä»¬çœ‹ä¸€å¹…å¤æ‚çš„å›¾åƒæ—¶ï¼Œæˆ‘ä»¬ä¸ä¼šåŒæ—¶å¤„ç†æ‰€æœ‰åƒç´ ï¼Œè€Œæ˜¯ä¼š\"èšç„¦\"åœ¨æ„Ÿå…´è¶£çš„åŒºåŸŸã€‚è¿™ç§é€‰æ‹©æ€§æ³¨æ„ï¼ˆselective attentionï¼‰æ˜¯è®¤çŸ¥ç§‘å­¦ç ”ç©¶çš„ç»å…¸è¯¾é¢˜ã€‚\n\nBahdanauåœ¨2014å¹´å°†è¿™ä¸ªæ€æƒ³å¼•å…¥ç¥ç»æœºå™¨ç¿»è¯‘æ—¶ï¼Œå¹¶æ²¡æœ‰é¢„æ–™åˆ°å®ƒä¼šæˆä¸ºæ·±åº¦å­¦ä¹ æœ€æ ¸å¿ƒçš„ç»„ä»¶ä¹‹ä¸€ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä»–ä»¬è°¦è™šåœ°ç§°ä¹‹ä¸º\"å¯¹é½æ¨¡å‹\"ï¼ˆalignment modelï¼‰ï¼Œè€Œä¸æ˜¯\"æ³¨æ„åŠ›\"ã€‚\"Attention\"è¿™ä¸ªæœ¯è¯­æ˜¯åæ¥è¢«ç¤¾åŒºå¹¿æ³›é‡‡ç”¨çš„ã€‚\n\næœ‰è¶£çš„æ˜¯ï¼ŒBahdanau Attentionçš„æˆåŠŸè®©ç ”ç©¶è€…å¼€å§‹æ€è€ƒï¼šå¦‚æœAttentionè¿™ä¹ˆæœ‰æ•ˆï¼Œæˆ‘ä»¬æ˜¯å¦éœ€è¦RNNï¼Ÿä¸¤å¹´åï¼ŒVaswaniç­‰äººç»™å‡ºäº†ç­”æ¡ˆâ€”â€”\"Attention Is All You Need\"ã€‚è¿™ç¯‡è®ºæ–‡ä¸ä»…åœ¨æŠ€æœ¯ä¸Šé©æ–°äº†åºåˆ—å»ºæ¨¡ï¼Œå…¶æ ‡é¢˜æœ¬èº«ä¹Ÿæˆä¸ºäº†æ·±åº¦å­¦ä¹ å†å²ä¸Šæœ€å…·å½±å“åŠ›çš„é‡‘å¥ä¹‹ä¸€ã€‚\n\n",
    "supporting": [
      "ch05-attention-mechanism_files"
    ],
    "filters": [],
    "includes": {}
  }
}