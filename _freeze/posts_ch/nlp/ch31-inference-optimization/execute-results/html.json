{
  "hash": "5600061b29d6b798ed0aabc0d2058445",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第31章：推理优化\"\nsubtitle: \"From Training to Deployment: The Art of Making Giants Fast and Cheap\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-29\"\ncategories: [NLP, Deep Learning, LLM, Inference, Quantization, Speculative Decoding]\ntags: [推理优化, 量化, GPTQ, AWQ, 投机解码, vLLM, PagedAttention, 持续批处理, KV Cache]\ndescription: \"当大语言模型从训练走向部署，推理成本成为新的瓶颈——一个70B模型的FP16推理需要140GB显存，远超单卡容量；自回归生成的token-by-token方式让GPU利用率仅有10-20%。本章系统梳理推理优化的三条主线：量化技术（GPTQ、AWQ）用精度换内存，将70B模型压缩到单卡运行；投机解码用小模型猜测、大模型验证的方式打破自回归瓶颈，实现2-3倍无损加速；持续批处理和PagedAttention通过动态调度最大化硬件利用率。这些技术可以叠加使用，将推理成本降低10倍以上，让'只有大厂用得起'变成'人人可部署'。\"\nimage: \"figures/chapter-31/original/fig-awq-overview.png\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    fig-cap-location: bottom\nbibliography: references.bib\n---\n\n> **核心问题**：当大语言模型从训练走向部署，如何在成本、延迟、质量三者之间取得最优平衡？\n>\n> **历史坐标**：2022–2024 | LLM.int8() (Dettmers, 2022), GPTQ (Frantar, 2022), SmoothQuant (Xiao, 2022), Speculative Decoding (Leviathan & Chen, 2023), AWQ (Lin, 2023), vLLM (Kwon, 2023) | 从算法到系统的推理优化全栈\n\n::: {.callout-tip collapse=\"true\"}\n## 本章参考来源\n\n### 论文\n- **Dettmers et al. (2022)** \"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\" ([arXiv:2208.07339](https://arxiv.org/abs/2208.07339)) — 参考了 outlier 特征的发现和混合精度分解方法\n- **Frantar et al. (2022)** \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\" ([arXiv:2210.17323](https://arxiv.org/abs/2210.17323)) — 参考了基于 Hessian 的逐层量化方法（ICLR 2023）\n- **Xiao et al. (2022)** \"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\" ([arXiv:2211.10438](https://arxiv.org/abs/2211.10438)) — 参考了激活平滑技术\n- **Lin et al. (2023)** \"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\" ([arXiv:2306.00978](https://arxiv.org/abs/2306.00978)) — 参考了激活感知量化方法（MLSys 2024 最佳论文）；从论文提取了1张架构图\n- **Leviathan et al. (2023)** \"Fast Inference from Transformers via Speculative Decoding\" ([arXiv:2211.17192](https://arxiv.org/abs/2211.17192)) — 参考了投机解码的基本框架和数学证明\n- **Chen et al. (2023)** \"Accelerating Large Language Model Decoding with Speculative Sampling\" ([arXiv:2302.01318](https://arxiv.org/abs/2302.01318)) — 参考了 Google 的投机采样实现\n- **Kwon et al. (2023)** \"Efficient Memory Management for Large Language Model Serving with PagedAttention\" ([arXiv:2309.06180](https://arxiv.org/abs/2309.06180)) — 参考了 PagedAttention 和 vLLM 系统设计（SOSP 2023）\n- **Yu et al. (2022)** \"Orca: A Distributed Serving System for Transformer-Based Generative Models\" (OSDI 2022) — 参考了迭代级调度和持续批处理\n- **Ainslie et al. (2023)** \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\" ([arXiv:2305.13245](https://arxiv.org/abs/2305.13245)) — 参考了 Grouped-Query Attention 的设计\n\n### 课程\n- **Stanford CS229S** Systems for Machine Learning (2023) — 参考了推理系统优化的框架\n- **UC Berkeley CS294** Machine Learning Systems (2024) — 参考了量化技术的教学组织\n\n### 代码资源\n- [vllm-project/vllm](https://github.com/vllm-project/vllm) — vLLM 高性能推理引擎\n- [huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference) — TGI 推理服务\n- [IST-DASLab/gptq](https://github.com/IST-DASLab/gptq) — GPTQ 官方实现\n- [mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq) — AWQ 官方实现\n:::\n\n---\n\n## 从上一章说起\n\n上一章我们见证了参数高效微调（PEFT）的革命——从 Adapter 的层间插入，到 LoRA 的低秩分解，再到 QLoRA 的 4-bit 量化微调。这些技术将大模型微调从\"需要一个集群\"变成\"一张消费级 GPU\"。70B 参数的 LLaMA 现在可以在单张 24GB 的 RTX 4090 上完成微调，成本从数万美元降到几百美元。\n\n但微调只是模型生命周期的**一次性成本**。一旦模型上线，真正的成本来自**推理**——每一次用户请求都需要消耗计算资源。对于一个有百万日活用户的应用，推理成本可能是微调成本的数百倍。让我们做一个简单的算术：\n\n假设你运营一个基于 LLaMA-70B 的 AI 助手：\n\n- **微调成本**（一次性）：QLoRA 在单 GPU 上训练，约 $200（云 GPU 租用）\n- **推理成本**（持续）：\n  - 每次请求平均 500 token（输入 + 输出）\n  - 每天 100 万次请求\n  - 70B 模型 FP16 推理需要约 140GB 显存，需要 2 张 A100 80GB\n  - A100 租用成本约 $2/小时 × 2 = $4/小时 = **$96/天**\n  - 每月推理成本：**$2,880**\n\n一年下来，推理成本接近 **$35,000**，是微调成本的 175 倍！\n\n更糟糕的是，这还是在理想情况下的估算。实际部署中，由于自回归生成的顺序特性，GPU 的利用率通常只有 10–30%。大部分时间，昂贵的 A100 GPU 都在\"等待\"——等待上一个 token 生成完毕，才能开始下一个 token 的计算。这是一种巨大的浪费。\n\n推理优化面临三大核心挑战：\n\n**挑战一：显存瓶颈**\n\n一个 70B 模型的 FP16 参数占用 140GB，这还没算 KV Cache。在长上下文场景下（比如 128K token），KV Cache 可能占用数百 GB。即使是 8 张 A100 组成的服务器，也可能被显存卡住。\n\n**挑战二：延迟瓶颈**\n\n自回归生成是天然顺序的——每个 token 都依赖前面所有 token。这意味着生成 100 个 token 需要进行 100 次完整的前向传播。用户看到的是\"逐字蹦出\"的体验，延迟感明显。\n\n**挑战三：吞吐瓶颈**\n\n传统的静态批处理要求所有请求同时开始、同时结束。但实际场景中，用户请求长度参差不齐——有的问\"今天天气\"，有的要求写一篇长文。短请求被迫等待长请求完成，系统吞吐量大打折扣。\n\n> 💡 **本章核心洞察**：推理优化的本质是在**精度、速度、成本**三角形中寻找帕累托最优。量化技术通过降低数值精度换取内存和带宽——INT4 量化可以将 70B 模型压缩到 35GB，单卡可运行。投机解码通过\"猜测-验证\"的并行化打破自回归的顺序瓶颈——小模型一次猜 4 个 token，大模型一次验证全部，实现 2–3 倍无损加速。持续批处理通过迭代级调度最大化硬件利用率——新请求可以随时加入，完成的请求立即退出，不再\"陪跑\"。这些技术可以叠加使用：一个部署 INT4 量化 + 投机解码 + PagedAttention + 持续批处理的系统，相比 naive 实现可以将成本降低 **10 倍以上**。\n\n---\n\n## 问题的本质是什么？\n\n### 推理的成本结构\n\n要优化推理，首先要理解成本从何而来。大模型推理的资源消耗可以分解为三大部分：\n\n**模型参数**\n\n这是最直观的部分。70B 参数在 FP16 下占用 $70 \\times 10^9 \\times 2 = 140$ GB。参数需要从显存读取到计算单元（CUDA Core 或 Tensor Core），这是推理的第一个瓶颈。\n\n**KV Cache**\n\n这是容易被忽视但至关重要的部分。在自回归生成中，为了避免重复计算历史 token 的 Key 和 Value，模型会将它们缓存下来。KV Cache 的大小随序列长度线性增长：\n\n$$\n\\text{KV Cache} = 2 \\times L \\times n \\times d \\times \\text{sizeof(dtype)}\n$$\n\n其中 $L$ 是层数，$n$ 是序列长度，$d$ 是隐藏维度，因子 2 来自 K 和 V 两个矩阵。对于 LLaMA-70B（$L=80$，$d=8192$），128K 上下文的 KV Cache 占用：\n\n$$\n2 \\times 80 \\times 128000 \\times 8192 \\times 2 = 335 \\text{ GB (FP16)}\n$$\n\n这比模型参数本身还大！在长上下文场景，KV Cache 是显存的主要消耗者。\n\n**激活值**\n\n前向传播过程中的中间结果。相比参数和 KV Cache，激活值占用较小（batch size 为 1 时通常只有几个 GB），但在大 batch 场景下也不可忽视。\n\n### 自回归解码的瓶颈分析\n\n为什么大模型推理这么慢？要理解这一点，我们需要深入分析自回归解码的计算特性。\n\n在生成第 $t$ 个 token 时，模型需要：\n\n1. 读取所有模型参数（140GB for 70B model）\n2. 读取 KV Cache（随 $t$ 线性增长）\n3. 计算注意力和 FFN\n4. 输出一个 token 的 logits\n\n关键观察是：**生成单个 token 只需要很少的计算（相对于参数量），但需要读取大量数据（所有参数）**。这让推理变成了一个 **内存带宽受限**（memory-bound）而非**计算受限**（compute-bound）的任务。\n\n让我们算一下。A100 GPU 的规格是：\n\n- 内存带宽：2 TB/s\n- FP16 算力：312 TFLOPS\n\n生成一个 token 需要读取 140GB 参数，按 2 TB/s 带宽计算：\n\n$$\n\\text{读取时间} = \\frac{140 \\text{ GB}}{2000 \\text{ GB/s}} = 70 \\text{ ms}\n$$\n\n而 70B 模型的 FP16 计算量约为 $2 \\times 70 \\times 10^9 = 140$ GFLOPs（每个参数大约 2 次运算），按 312 TFLOPS 计算：\n\n$$\n\\text{计算时间} = \\frac{140 \\text{ GFLOPs}}{312 \\text{ TFLOPs}} = 0.45 \\text{ ms}\n$$\n\n计算只需要 0.45ms，但读取参数需要 70ms——**实际利用率不到 1%**！GPU 的强大算力几乎完全浪费在等待数据传输上。\n\n这就是为什么单纯增加 batch size 可以提升吞吐：多个请求共享同一份参数读取，摊薄了内存带宽的开销。但 batch size 受限于显存（更多请求 = 更多 KV Cache），不能无限增大。\n\n### 我们需要什么样的解决方案？\n\n理解了问题的本质，解决方案的方向就清晰了：\n\n1. **减少需要读取的数据量** → **量化**：把 FP16 参数压缩成 INT8 或 INT4，读取量减半或减到四分之一\n2. **打破顺序依赖** → **投机解码**：一次生成多个 token，减少前向传播次数\n3. **最大化硬件利用率** → **持续批处理**：动态调度请求，让 GPU 时刻保持忙碌\n4. **高效管理 KV Cache** → **PagedAttention**：像操作系统管理内存一样管理 KV Cache\n\n这四类技术相互独立、可以叠加。一个现代推理系统（如 vLLM）会同时使用它们。接下来，我们逐一深入探讨每项技术的原理和实现。\n\n---\n\n## 核心思想与直觉\n\n在进入技术细节之前，让我们先用直觉建立对各项技术的理解。\n\n### 量化的直觉：有损压缩\n\n想象你要通过一条窄带宽的网络传输一张高清照片。原图是 24-bit RGB，每个像素 3 个字节。如果带宽不够，你会怎么做？最直接的方法是**压缩**——JPEG 就是这样做的，它用 8-bit 甚至更低的精度存储像素，肉眼几乎看不出差别。\n\n量化对神经网络做的事情完全类似。预训练的权重是 FP16（16-bit 浮点数），包含了大量\"冗余\"的精度。研究发现，大模型的权重分布高度集中——绝大多数权重的绝对值都很小，只有极少数\"异常值\"（outliers）比较大。这种分布意味着我们可以用更少的 bit 来表示大部分权重，而几乎不损失信息。\n\nINT8 量化将 16-bit 压缩到 8-bit，参数量减半；INT4 量化更激进，压缩到四分之一。代价是轻微的精度损失——就像 JPEG 压缩会引入一些\"噪点\"，但通常在可接受范围内。\n\n### 投机解码的直觉：草稿-审稿流程\n\n想象你是一位繁忙的主编，需要审阅大量稿件。你可以亲自一字一句地写每篇文章，但这太慢了。一个更高效的流程是：让一位初级编辑先写草稿，然后你快速审阅——接受好的部分，修改不满意的部分。\n\n投机解码（Speculative Decoding）正是这个思路。大模型（target model）太慢，每生成一个 token 都要 70ms。我们引入一个小模型（draft model），它虽然不如大模型准确，但生成速度快得多（比如只需 7ms）。流程变成：\n\n1. 小模型快速生成 4 个候选 token：[\"The\", \"cat\", \"sat\", \"on\"]\n2. 大模型**一次性**验证这 4 个 token 是否可接受\n3. 如果前 3 个被接受，第 4 个被拒绝，则保留前 3 个，从拒绝位置重新开始\n\n关键洞察是：大模型验证 4 个 token 只需要**一次**前向传播（约 70ms），而如果大模型自己生成 4 个 token 需要**四次**前向传播（约 280ms）。只要小模型的预测足够准确（被接受的比例够高），整体速度就能提升。\n\n更妙的是，通过精心设计的接受-拒绝机制，投机解码可以保证最终输出分布与直接使用大模型**完全相同**——这是数学上可证明的无损加速！\n\n### 持续批处理的直觉：餐厅动态座位安排\n\n想象一家繁忙的餐厅。传统的做法是\"静态批处理\"：服务员等凑齐 8 位客人，一起安排到一张大桌子；所有人必须等最慢的那位吃完，才能一起离席。如果有人只点了一杯咖啡，他也得陪着吃牛排的人坐到最后。\n\n持续批处理（Continuous Batching）更像现代快餐店：客人随到随坐，吃完就走，座位立刻分配给下一位。这样，快速的请求不用等待慢速的请求，系统吞吐量大幅提升。\n\n在 LLM 推理中，不同请求的输出长度差异巨大——有的回答只需 10 个 token，有的需要 1000 个。静态批处理让短请求\"陪跑\"长请求，GPU 利用率极低。持续批处理允许在每个 iteration（生成一个 token）的粒度上调度：完成的请求立即退出，新请求可以随时加入。\n\n### PagedAttention的直觉：操作系统的内存分页\n\n传统的 KV Cache 管理是\"连续分配\"的：为每个请求预留一大块连续显存，足以容纳其最大可能长度。这就像早期操作系统的内存管理——为每个程序分配一整块连续内存，导致严重的碎片化。\n\nPagedAttention（分页注意力）借鉴了现代操作系统的**虚拟内存**技术：将 KV Cache 切分成固定大小的\"页\"（blocks），按需分配，不要求连续。这样，不同请求可以共享物理显存，碎片化问题迎刃而解。\n\n一个额外的好处是：当多个请求共享相同的 prompt（比如系统提示），它们的 KV Cache 可以共享同一份物理内存，进一步节省空间。\n\n---\n\n## 技术细节\n\n### 量化技术\n\n#### 量化基础\n\n量化的本质是用离散值近似连续值。给定一个浮点数权重 $w$，量化过程可以表示为：\n\n$$\nw_q = \\text{round}\\left(\\frac{w - z}{s}\\right), \\quad w_{dq} = w_q \\cdot s + z\n$$\n\n其中 $s$ 是**缩放因子**（scale），$z$ 是**零点**（zero point），$w_q$ 是量化后的整数，$w_{dq}$ 是反量化后的近似值。\n\n量化有几个关键维度：\n\n**对称 vs 非对称**\n\n- **对称量化**：假设权重分布关于 0 对称，$z = 0$。只需要存储 $s$。\n- **非对称量化**：允许任意分布，需要同时存储 $s$ 和 $z$。更灵活但开销稍大。\n\n**量化粒度**\n\n- **Per-tensor**：整个权重矩阵共享一组 $(s, z)$。最节省空间，但精度最低。\n- **Per-channel**：每个输出通道有独立的 $(s, z)$。平衡了精度和开销。\n- **Per-group**：每 $g$ 个元素共享 $(s, z)$，比如每 128 个元素一组。GPTQ 和 AWQ 常用。\n\n#### 完整数值示例：FP16 → INT4 量化\n\n让我们通过一个具体的例子，手把手走过量化的完整流程。\n\n**设定**：一个 $4 \\times 4$ 的权重矩阵，原始值为 FP16。\n\n$$\nW = \\begin{bmatrix}\n-2.1 & 0.3 & 1.5 & -0.8 \\\\\n0.5 & 2.7 & -1.2 & 0.1 \\\\\n1.8 & -0.4 & 0.9 & 2.3 \\\\\n-1.5 & 1.1 & -0.2 & 0.7\n\\end{bmatrix}\n$$\n\n**Step 1: 计算量化参数**\n\n首先找到权重的范围：\n$$\nw_{\\min} = -2.1, \\quad w_{\\max} = 2.7\n$$\n\nINT4 可以表示 $2^4 = 16$ 个值，范围是 $[0, 15]$（无符号）或 $[-8, 7]$（有符号）。我们使用无符号 INT4，范围 $[0, 15]$。\n\n计算缩放因子和零点：\n$$\ns = \\frac{w_{\\max} - w_{\\min}}{2^4 - 1} = \\frac{2.7 - (-2.1)}{15} = \\frac{4.8}{15} = 0.32\n$$\n\n$$\nz = \\text{round}\\left(-\\frac{w_{\\min}}{s}\\right) = \\text{round}\\left(\\frac{2.1}{0.32}\\right) = \\text{round}(6.56) = 7\n$$\n\n**Step 2: 量化**\n\n对每个权重 $w$，计算量化值：\n$$\nw_q = \\text{clamp}\\left(\\text{round}\\left(\\frac{w}{s}\\right) + z, \\, 0, \\, 15\\right)\n$$\n\n以 $w = 1.5$ 为例：\n$$\nw_q = \\text{round}\\left(\\frac{1.5}{0.32}\\right) + 7 = \\text{round}(4.69) + 7 = 5 + 7 = 12\n$$\n\n完整的量化矩阵：\n$$\nW_q = \\begin{bmatrix}\n0 & 8 & 12 & 4 \\\\\n9 & 15 & 3 & 7 \\\\\n13 & 6 & 10 & 14 \\\\\n2 & 10 & 6 & 9\n\\end{bmatrix}\n$$\n\n**Step 3: 反量化与误差分析**\n\n反量化公式：$w_{dq} = (w_q - z) \\cdot s$\n\n以 $w_q = 12$（原始 $w = 1.5$）为例：\n$$\nw_{dq} = (12 - 7) \\cdot 0.32 = 5 \\cdot 0.32 = 1.60\n$$\n\n量化误差：$|1.5 - 1.60| = 0.10$\n\n**误差分析**：INT4 量化的最大误差为 $s/2 = 0.16$。平均误差通常为最大误差的一半左右，即 $\\sim 0.08$。相比原始权重的典型大小（$\\sim 1$），相对误差约 8%。\n\n**Step 4: 存储空间对比**\n\n- **FP16**：$16 \\times 2 = 32$ bytes\n- **INT4**：$16 \\times 0.5 + 2 \\times 2 = 8 + 4 = 12$ bytes（权重 + scale & zero）\n- **压缩比**：$32 / 12 \\approx 2.7\\times$\n\n对于更大的矩阵，使用 per-group 量化（如每 128 个元素一组），压缩比可以接近理论值 $4\\times$。\n\n#### LLM.int8()：发现异常值\n\n2022 年，Dettmers 等人在尝试直接对 LLM 进行 INT8 量化时，遇到了一个令人惊讶的现象：模型性能**断崖式下降**，远超预期。经过深入分析，他们发现了问题的根源：**异常值特征**（emergent outliers）。\n\n在大模型的某些维度上，激活值会出现极端的异常值——比正常值大 10–100 倍。这些异常值只占全部特征的约 0.1%，但对模型输出至关重要。直接量化会严重损害这些异常值的精度，导致模型崩溃。\n\nLLM.int8() 的解决方案是**混合精度分解**：\n\n1. 检测异常值维度（绝对值超过阈值的维度）\n2. 异常值维度保持 FP16 精度\n3. 其他维度使用 INT8 量化\n\n::: {.callout-note}\n## Algorithm: LLM.int8() 混合精度矩阵乘法 (Dettmers et al., 2022)\n\n**输入**：\n- 激活矩阵 $X \\in \\mathbb{R}^{b \\times d}$\n- 权重矩阵 $W \\in \\mathbb{R}^{d \\times k}$\n- 异常值阈值 $\\alpha$（通常为 6.0）\n\n**输出**：$Y = XW$\n\n```\n1. 检测异常值维度:\n   O ← {i : max|X[:, i]| > α}  // 异常值维度集合\n\n2. 分解输入:\n   X_outlier ← X[:, O]         // 异常值部分 (FP16)\n   X_normal ← X[:, ~O]         // 正常部分\n\n3. 分解权重:\n   W_outlier ← W[O, :]         // 对应异常值的权重 (FP16)\n   W_normal ← W[~O, :]         // 其他权重\n\n4. 量化正常部分:\n   X_q, s_x ← quantize_int8(X_normal)\n   W_q, s_w ← quantize_int8(W_normal)\n\n5. 混合精度计算:\n   Y_outlier ← matmul_fp16(X_outlier, W_outlier)\n   Y_normal ← matmul_int8(X_q, W_q) * s_x * s_w\n\n6. return Y_outlier + Y_normal\n```\n\n**关键设计**：\n- 约 0.1% 的维度是异常值，99.9% 的计算仍在 INT8\n- 内存占用与纯 INT8 相近，推理速度略有下降（约 15–20%）\n\n*Source: Dettmers, T. et al. (2022). \"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\". [arXiv:2208.07339](https://arxiv.org/abs/2208.07339)*\n:::\n\nLLM.int8() 的重要贡献不仅是方法本身，更是对异常值现象的发现和分析。这一发现启发了后续的 SmoothQuant 和 AWQ 等工作。\n\n#### GPTQ：基于 Hessian 的最优量化\n\nGPTQ（Frantar et al., 2022）采用了完全不同的思路：与其简单地四舍五入，不如找到**最优**的量化方式，使量化误差对输出的影响最小化。\n\n核心想法来自经典的**最优脑外科**（Optimal Brain Surgeon）方法：给定一个训练好的网络，如何\"手术式\"地移除一些参数（或降低精度），同时最小化对输出的影响？\n\n数学上，我们希望最小化量化引入的重构误差：\n\n$$\n\\arg\\min_{W_q} \\|WX - W_q X\\|_2^2\n$$\n\n其中 $X$ 是校准数据的激活值。利用二阶 Taylor 展开，这个优化问题可以近似为：\n\n$$\n\\arg\\min_{\\delta W} \\delta W^\\top H \\delta W\n$$\n\n其中 $H = 2XX^\\top$ 是 Hessian 矩阵（或 Fisher 信息矩阵）。直觉是：Hessian 的特征值反映了参数的\"敏感度\"——特征值大的方向，参数变化会显著影响输出；特征值小的方向，参数变化影响较小。\n\nGPTQ 的关键创新是**逐列量化**：不是一次量化整个矩阵，而是一列一列地量化，每量化一列后，用 Hessian 信息更新剩余列，补偿已引入的误差。这种贪心策略在实践中效果极好。\n\n::: {.callout-note}\n## Algorithm: GPTQ 逐列量化 (Frantar et al., 2022)\n\n**输入**：\n- 权重矩阵 $W \\in \\mathbb{R}^{d_{row} \\times d_{col}}$\n- Hessian 矩阵 $H = 2XX^\\top \\in \\mathbb{R}^{d_{col} \\times d_{col}}$\n- 目标量化位宽 $b$（如 4-bit）\n- 分组大小 $B$（如 128）\n\n**输出**：量化后的权重 $\\hat{W}$\n\n```\n1. Cholesky 分解:\n   H ← Cholesky(H)^{-1}     // 高效求逆\n\n2. 初始化:\n   E ← 0                     // 累计误差\n   Q ← empty                 // 量化结果\n\n3. for i = 0 to d_col - 1 in steps of B:\n      // 当前块的列索引\n      cols ← [i, i+1, ..., min(i+B-1, d_col-1)]\n\n      // 量化当前块\n      for j in cols:\n         q_j ← quantize(W[:, j], b)   // 标准量化\n         Q[:, j] ← q_j\n         err ← (W[:, j] - q_j) / H[j, j]\n\n         // 更新后续列（误差补偿）\n         W[:, j+1:] ← W[:, j+1:] - err ⊗ H[j, j+1:]\n\n4. return Q\n```\n\n**关键设计**：\n- 逐列量化 + 误差补偿，利用 Hessian 信息指导更新\n- 分块处理，平衡精度和效率\n- 时间复杂度 $O(d^2 n)$，约 4 小时可量化 175B 模型\n\n*Source: Frantar, E. et al. (2022). \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\". ICLR 2023. [arXiv:2210.17323](https://arxiv.org/abs/2210.17323)*\n:::\n\nGPTQ 的实验结果令人印象深刻：4-bit 量化后，175B 参数的 OPT-175B 模型的困惑度（perplexity）仅上升 0.5，几乎无损。这使得在单张 A100 上运行 175B 模型成为可能。\n\n#### AWQ：激活感知的权重重要性\n\nAWQ（Lin et al., 2023）提出了一个简洁而深刻的观察：**不是所有权重都同等重要**。\n\n![AWQ的核心思想：保护重要的权重通道](figures/chapter-31/original/fig-awq-overview.png){#fig-awq-overview width=90%}\n\n::: {.figure-caption}\n*Source: Lin et al. (2023) \"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\", Figure 1. MLSys 2024 Best Paper.*\n:::\n\n如上图所示，AWQ 的关键洞察是：权重的重要性应该由**激活值**来衡量，而非权重本身的大小。具体来说，如果某个输入通道的激活值很大，那么对应的权重列就很重要——即使这些权重本身数值不大，它们乘以大激活后对输出影响很大。\n\nAWQ 的策略是：\n\n1. 用校准数据统计每个输入通道的平均激活幅度\n2. 对于重要通道（激活大的），放大权重后再量化\n3. 放大系数存下来，推理时对输入做相应的缩小（数学等价变换）\n\n数学上，原始计算是 $Y = XW$。AWQ 引入对角缩放矩阵 $S$：\n\n$$\nY = XW = (XS^{-1})(SW)\n$$\n\n通过选择合适的 $S$，使 $SW$ 更容易量化。具体地，对于重要通道 $i$，设置 $S_{ii} > 1$，这会放大 $W$ 中对应的列，使其在量化时保留更高的精度。\n\nAWQ 在 MLSys 2024 获得最佳论文奖，其简洁的设计和出色的效果代表了量化技术的前沿水平。\n\n#### 量化技术对比\n\n| 方法 | 位宽 | 核心思想 | 校准数据 | 速度 | 精度损失 |\n|------|------|---------|---------|------|---------|\n| LLM.int8() | W8A8 | 混合精度分解 | 无 | +50% | ~0 |\n| SmoothQuant | W8A8 | 平滑激活分布 | ~1000 样本 | +80% | ~0 |\n| GPTQ | W4A16 | Hessian 最优量化 | ~128 样本 | +200% | 0.1-0.5 |\n| AWQ | W4A16 | 激活感知重要性 | ~128 样本 | +200% | 0.1-0.3 |\n\n### 投机解码（Speculative Decoding）\n\n#### 基本原理\n\n投机解码的核心思想惊人地简单：用一个快速但不那么准确的小模型（draft model）生成候选 token 序列，然后用目标大模型（target model）**并行验证**这些候选。\n\n传统自回归生成：\n```\nToken 1 → Token 2 → Token 3 → Token 4\n   70ms      70ms      70ms      70ms\nTotal: 280ms\n```\n\n投机解码：\n```\nDraft: Token 1,2,3,4 (并行生成)  →  Target: 验证全部 (一次前向)\n           28ms                          70ms\nTotal: ~100ms (假设全部接受)\n```\n\n关键问题是：如果 draft 的预测被拒绝怎么办？投机解码使用一种精心设计的**接受-拒绝采样**机制，保证最终输出分布与直接使用 target model 完全相同。\n\n#### 数学原理：为什么能保证无损？\n\n设 $p(x)$ 是 target model 在某位置的概率分布，$q(x)$ 是 draft model 的分布。投机解码的接受规则是：\n\n$$\n\\text{Accept } x \\text{ with probability } \\min\\left(1, \\frac{p(x)}{q(x)}\\right)\n$$\n\n如果 $x$ 被接受，输出就是 $x$。如果被拒绝，从修正分布 $p'(x)$ 中重新采样：\n\n$$\np'(x) = \\text{normalize}\\left(\\max(0, p(x) - q(x))\\right)\n$$\n\n**定理**：在上述接受-拒绝规则下，最终输出的分布恰好是 $p(x)$。\n\n**证明思路**：\n输出 $x$ 的总概率 = (由 draft 生成 $x$ 的概率) × (接受 $x$ 的概率) + (从修正分布采样到 $x$ 的概率)\n\n$$\nP(\\text{output}=x) = q(x) \\cdot \\min\\left(1, \\frac{p(x)}{q(x)}\\right) + P(\\text{reject}) \\cdot p'(x)\n$$\n\n经过仔细计算，可以验证这确实等于 $p(x)$。关键洞察是：当 $q(x) > p(x)$ 时，我们\"过度采样\"了 $x$，需要通过拒绝来修正；当 $q(x) < p(x)$ 时，我们\"欠采样\"了 $x$，通过修正分布来补充。\n\n这个结果非常优雅：它意味着投机解码在数学上是**无损**的，不会改变输出质量，只会提高速度。\n\n#### 完整数值示例：Draft-Verify 流程\n\n让我们通过一个具体例子，完整走过投机解码的一轮验证。\n\n**设定**：\n- Target model：70B 参数\n- Draft model：7B 参数（同系列的小模型）\n- 猜测长度 $K = 4$\n- 当前已生成的前缀：\"The weather today is\"\n\n**Step 1: Draft 生成**\n\nDraft model 并行生成 4 个候选 token 及其概率：\n\n| 位置 | Token | Draft 概率 $q(x)$ |\n|------|-------|-------------------|\n| 1 | \"very\" | 0.60 |\n| 2 | \"nice\" | 0.55 |\n| 3 | \"and\" | 0.40 |\n| 4 | \"warm\" | 0.35 |\n\n**Step 2: Target 验证（一次前向传播）**\n\nTarget model 对序列 \"The weather today is very nice and warm\" 计算条件概率。这是**单次**前向传播，但输出每个位置的概率分布：\n\n| 位置 | Token | Target 概率 $p(x)$ | Draft 概率 $q(x)$ |\n|------|-------|-------------------|-------------------|\n| 1 | \"very\" | 0.70 | 0.60 |\n| 2 | \"nice\" | 0.45 | 0.55 |\n| 3 | \"and\" | 0.50 | 0.40 |\n| 4 | \"warm\" | 0.30 | 0.35 |\n\n**Step 3: 逐位置接受/拒绝**\n\n对每个位置，计算接受概率 $\\min(1, p/q)$ 并掷骰子：\n\n**位置 1 (\"very\")**：\n$$\nP(\\text{accept}) = \\min\\left(1, \\frac{0.70}{0.60}\\right) = \\min(1, 1.17) = 1.0\n$$\n因为 $p > q$，target 比 draft 更喜欢这个 token，一定接受。✅\n\n**位置 2 (\"nice\")**：\n$$\nP(\\text{accept}) = \\min\\left(1, \\frac{0.45}{0.55}\\right) = 0.82\n$$\n掷骰子得到 0.65 < 0.82，接受。✅\n\n**位置 3 (\"and\")**：\n$$\nP(\\text{accept}) = \\min\\left(1, \\frac{0.50}{0.40}\\right) = 1.0\n$$\n一定接受。✅\n\n**位置 4 (\"warm\")**：\n$$\nP(\\text{accept}) = \\min\\left(1, \\frac{0.30}{0.35}\\right) = 0.86\n$$\n掷骰子得到 0.91 > 0.86，拒绝。❌\n\n**Step 4: 处理拒绝**\n\n位置 4 被拒绝，需要从修正分布 $p'$ 中重新采样。假设 target 在位置 4 的完整分布是：\n\n| Token | $p(x)$ | $q(x)$ | $\\max(0, p-q)$ |\n|-------|--------|--------|----------------|\n| \"warm\" | 0.30 | 0.35 | 0.00 |\n| \"sunny\" | 0.25 | 0.20 | 0.05 |\n| \"pleasant\" | 0.20 | 0.15 | 0.05 |\n| 其他 | 0.25 | 0.30 | 0.00 |\n\n归一化后 $p'$：$P(\\text{sunny}) = P(\\text{pleasant}) = 0.5$\n\n假设采样到 \"sunny\"，最终输出是：**\"very nice and sunny\"**\n\n**Step 5: 效率分析**\n\n- **无投机解码**：生成 4 个 token 需要 4 次 target 前向，约 $4 \\times 70 = 280$ ms\n- **有投机解码**：1 次 draft 前向 + 1 次 target 前向，约 $7 + 70 = 77$ ms\n- **实际接受**：3 个 token + 1 个重采样 = 4 个 token\n- **加速比**：$280 / 77 \\approx 3.6\\times$\n\n实际场景中，由于 draft 和 target 的分布差异，接受率通常在 60–80%，整体加速比约 2–3 倍。\n\n### 持续批处理（Continuous Batching）\n\n#### 传统批处理的问题\n\n传统的静态批处理（static batching）要求所有请求同时开始、同时结束：\n\n```\n请求 A: [生成 10 个 token]  ████░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n请求 B: [生成 50 个 token]  ████████████████████████████████\n请求 C: [生成 20 个 token]  ██████████░░░░░░░░░░░░░░░░░░░░░░\n                            ↑                              ↑\n                          开始                            结束\n```\n\n请求 A 和 C 早早完成，却要等待请求 B，白白占用 GPU 资源。对于长度分布差异大的场景（这在实际应用中很常见），静态批处理的 GPU 利用率可能不到 30%。\n\n#### Orca 的迭代级调度\n\nOrca（Yu et al., OSDI 2022）提出了**迭代级调度**（iteration-level scheduling）：在每个 iteration（生成一个 token）的粒度上做调度决策。\n\n```\n请求 A: ████          （10 个 token 后退出，资源释放）\n请求 B: ████████████████████████████████\n请求 C: ██████████    （20 个 token 后退出）\n请求 D:      ████████████████████████    （A 退出后加入）\n请求 E:            ████████████████      （C 退出后加入）\n```\n\n这样，短请求不用\"陪跑\"长请求，系统吞吐量大幅提升。Orca 论文报告了 **36.9 倍**的吞吐提升（与 FasterTransformer 相比，在 GPT-3 175B 上）。\n\n#### PagedAttention 与 vLLM\n\nvLLM（Kwon et al., SOSP 2023）在 Orca 的基础上进一步优化了 KV Cache 管理，提出了 **PagedAttention**。\n\n传统方法为每个请求预分配最大长度的 KV Cache，导致严重浪费：\n\n```\n请求 A (实际 100 token): [████████░░░░░░░░░░░░░░░░░░░░░░░░] 预留 1000 token\n请求 B (实际 500 token): [████████████████████░░░░░░░░░░░░] 预留 1000 token\n                          ↑ 实际使用        ↑ 浪费\n```\n\nPagedAttention 将 KV Cache 切分成固定大小的页（如 16 tokens 一页），按需分配：\n\n```\n请求 A: [Page 1][Page 2][Page 3][Page 4][Page 5][Page 6][Page 7]  (7 页, 实际 100 token)\n请求 B: [Page 8][Page 9]...[Page 39]  (32 页, 实际 500 token)\n\n物理内存: |Page 1|Page 8|Page 2|Page 9|Page 3|... (交错存储，无碎片)\n```\n\n额外好处：当多个请求共享相同的 prompt（如系统指令），它们的 KV Cache 可以共享：\n\n```\n请求 A: [系统提示(共享)][用户问题 A 的 KV]\n请求 B: [系统提示(共享)][用户问题 B 的 KV]\n                  ↑\n            物理内存只存一份\n```\n\nvLLM 结合 PagedAttention 和持续批处理，相比 HuggingFace Transformers 和 FasterTransformer 实现了 **2–4 倍**的吞吐提升。\n\n### 模型并行推理\n\n当单卡显存不足以容纳模型时，需要将模型分布到多个 GPU 上。这与第19章讨论的分布式训练类似，但推理场景有其特殊性。\n\n#### Tensor Parallelism（TP）\n\n将单层的矩阵计算切分到多个 GPU：\n\n```\nGPU 0: W[:, :d/2]   →  计算前半部分输出\nGPU 1: W[:, d/2:]   →  计算后半部分输出\n        ↓ AllReduce ↓\n      合并得到完整输出\n```\n\n优点：每层内并行，延迟低\n缺点：每层都需要 AllReduce 通信\n\n#### Pipeline Parallelism（PP）\n\n将不同层放到不同 GPU：\n\n```\nGPU 0: Layer 0-9    →  处理前 10 层\nGPU 1: Layer 10-19  →  处理中间 10 层\nGPU 2: Layer 20-29  →  处理后 10 层\n```\n\n优点：通信量小（只在层边界传递激活）\n缺点：存在流水线气泡，增加延迟\n\n#### 推理场景的选择\n\n| 场景 | 推荐策略 | 原因 |\n|------|---------|------|\n| 低延迟（实时对话） | 纯 TP | 延迟最低 |\n| 高吞吐（批处理） | TP + PP | 最大化利用率 |\n| 显存极度受限 | PP + 模型卸载 | 允许更大模型 |\n\n---\n\n## 工程实践\n\n### 使用 GPTQ 量化模型\n\n::: {#d1d12709 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n# 加载原始模型\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 配置量化参数\nquantize_config = BaseQuantizeConfig(\n    bits=4,                    # 4-bit 量化\n    group_size=128,            # 每 128 个权重共享量化参数\n    desc_act=True,             # 激活感知排序（提升精度）\n)\n\n# 加载模型并量化\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quantize_config,\n    device_map=\"auto\"\n)\n\n# 准备校准数据\ncalibration_data = [\n    tokenizer(text, return_tensors=\"pt\")\n    for text in [\"校准文本1...\", \"校准文本2...\", ...]\n]\n\n# 执行量化\nmodel.quantize(calibration_data)\n\n# 保存量化后的模型\nmodel.save_quantized(\"llama-2-7b-gptq-4bit\")\n```\n:::\n\n\n### 使用 AWQ 量化模型\n\n::: {#ad49dedc .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\n# 加载模型\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# AWQ 量化配置\nquant_config = {\n    \"zero_point\": True,        # 使用零点\n    \"q_group_size\": 128,       # 分组大小\n    \"w_bit\": 4,                # 4-bit 量化\n    \"version\": \"GEMM\"          # 优化版本\n}\n\n# 执行量化\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# 保存\nmodel.save_quantized(\"llama-2-7b-awq-4bit\")\n```\n:::\n\n\n### vLLM 部署示例\n\n::: {#736b2412 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nfrom vllm import LLM, SamplingParams\n\n# 加载模型（自动启用 PagedAttention 和持续批处理）\nllm = LLM(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    tensor_parallel_size=1,        # 单卡\n    gpu_memory_utilization=0.90,   # GPU 内存利用率\n    max_model_len=4096,            # 最大序列长度\n)\n\n# 配置采样参数\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=256,\n)\n\n# 批量推理\nprompts = [\n    \"What is the capital of France?\",\n    \"Explain quantum computing in simple terms.\",\n    \"Write a haiku about programming.\",\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    print(f\"Prompt: {output.prompt}\")\n    print(f\"Generated: {output.outputs[0].text}\")\n    print(\"-\" * 50)\n```\n:::\n\n\n### TGI 部署（Docker）\n\n```bash\n# 拉取 TGI 镜像\ndocker pull ghcr.io/huggingface/text-generation-inference:latest\n\n# 启动服务\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n    -v $HOME/.cache/huggingface:/data \\\n    ghcr.io/huggingface/text-generation-inference:latest \\\n    --model-id meta-llama/Llama-2-7b-chat-hf \\\n    --quantize gptq \\\n    --max-concurrent-requests 128 \\\n    --max-input-length 2048 \\\n    --max-total-tokens 4096\n\n# 调用 API\ncurl http://localhost:8080/generate \\\n    -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"inputs\": \"What is deep learning?\", \"parameters\": {\"max_new_tokens\": 100}}'\n```\n\n---\n\n## 深入理解\n\n### 为什么有效？——理论视角\n\n#### 量化的信息论基础\n\n从信息论角度，量化的目标是找到一个编码，使重构误差最小化。对于服从某种分布的权重，最优量化（Lloyd-Max 量化）的误差与分布的方差和量化级数相关。\n\n大模型权重的一个关键特性是其分布高度集中：绝大多数权重的绝对值小于某个阈值，呈近似正态分布。这意味着均匀量化（等间距划分）是次优的——大部分量化区间分配给了稀疏的尾部，而密集的中心区域反而精度不足。\n\nGPTQ 和 AWQ 的优越性部分来自于对这种分布特性的利用：通过非均匀的缩放（GPTQ 的 Hessian 加权，AWQ 的激活感知）更好地适应权重分布。\n\n#### 投机解码的概率论基础\n\n投机解码的正确性基于**接受-拒绝采样**（acceptance-rejection sampling）的经典结果。这是蒙特卡洛方法的基础技术，数学上可以追溯到 von Neumann（1951）。\n\n核心定理是：如果提议分布 $q$ 满足 $q(x) > 0$ 对所有 $p(x) > 0$ 的 $x$ 成立，那么接受-拒绝采样可以精确采样任何目标分布 $p$。投机解码的贡献是将这一经典技术巧妙地应用于 LLM 推理加速。\n\n### 边界条件与失效模式\n\n#### 量化的边界条件\n\n**任务敏感性**：某些任务对精度特别敏感。研究发现，数学推理任务在 4-bit 量化后性能下降明显（5–10%），而常规问答和文本生成任务几乎不受影响。\n\n**模型规模**：有趣的是，大模型通常比小模型更容易量化。70B 模型量化后的相对性能损失小于 7B 模型。一种解释是：大模型有更多的冗余，可以更好地\"吸收\"量化噪声。\n\n**校准数据**：GPTQ 和 AWQ 都需要少量校准数据（通常 128–512 样本）。校准数据的领域应该与实际应用匹配——用英文数据校准后处理中文可能效果不佳。\n\n#### 投机解码的边界条件\n\n**Draft-Target 分布差异**：投机解码的加速比高度依赖于 draft 和 target 分布的相似度。当任务需要创意或探索性输出时（如创意写作），draft 的预测接受率会下降，加速比也随之降低。\n\n**Token 长度分布**：对于非常短的输出（< 10 token），投机解码的额外开销可能抵消收益。最适合中等长度的生成任务。\n\n### 开放研究问题\n\n1. **超低 bit 量化**：能否可靠地实现 2-bit 甚至 1-bit 量化？目前的研究表明 3-bit 是一个关键阈值，低于此会显著损失性能。突破这一限制需要新的量化技术或训练方法。\n\n2. **自适应投机解码**：当前的投机解码使用固定的猜测长度 $K$。能否根据上下文动态调整 $K$？在 draft 信心高的位置猜更多，信心低的位置猜更少？\n\n3. **端到端协同优化**：量化、投机解码、批处理目前是独立设计的。它们之间是否存在协同效应或冲突？能否设计一个统一的优化框架？\n\n4. **边缘设备推理**：手机、边缘设备的算力和内存远小于数据中心 GPU。如何将上述技术适配到极端资源受限的场景？\n\n---\n\n## 局限性与未解决的问题\n\n### 本方法的局限\n\n**局限 1：速度-质量权衡不可避免**\n\n尽管量化技术不断进步，4-bit 量化仍然会在某些任务上造成可测量的性能损失。对于追求极致质量的场景（如医疗、法律领域的 AI 应用），需要仔细评估量化的影响。\n\n**局限 2：技术栈复杂性**\n\n完整的推理优化需要叠加多项技术：量化 + 投机解码 + 持续批处理 + KV Cache 优化。每项技术都有自己的超参数和适用条件，组合起来的调优空间巨大。对于资源有限的团队，部署和维护成本不可忽视。\n\n**局限 3：硬件依赖性**\n\n很多优化依赖特定硬件特性：INT4 需要 GPU 支持（如 A100、H100 的 INT4 Tensor Core），FlashAttention 需要足够的 shared memory。跨平台部署（如 CPU、移动端）时，部分优化可能不可用。\n\n### 这些局限导向了什么？\n\n推理优化解决了\"如何高效地运行大模型\"，但没有解决\"大模型本身的局限\"。即使推理成本降到足够低，模型仍然可能：\n\n- 产生幻觉（hallucination），给出看似合理但实际错误的回答\n- 缺乏实时知识，无法回答最新发生的事件\n- 无法处理超长上下文，在需要综合大量信息的任务上表现不佳\n\n这引出了下一章要讨论的**检索增强生成（RAG）**——一种通过外部知识库增强模型能力的范式。推理优化让模型跑得快，RAG 让模型知道更多。两者是互补的技术，共同构成现代 LLM 应用的基础设施。\n\n---\n\n## 本章小结\n\n### 核心要点回顾\n\n1. **问题**：推理成本（显存、延迟、费用）是大模型普及的主要障碍。70B 模型的 FP16 推理需要 140GB 显存，远超单卡容量；自回归生成的 token-by-token 方式让 GPU 利用率仅有 10–30%。\n\n2. **洞察**：推理优化的本质是在精度、速度、成本三角形中寻找帕累托最优。关键在于识别和利用大模型推理的特殊结构——权重分布集中适合量化，自回归生成可以\"猜测-验证\"并行化，请求长度差异适合动态批处理。\n\n3. **方法**：\n   - **量化**（GPTQ、AWQ）：INT8/INT4 压缩，2–4 倍显存节省，几乎无损\n   - **投机解码**：2–3 倍无损加速，数学上可证明正确\n   - **持续批处理**（vLLM）：迭代级调度，2–5 倍吞吐提升\n   - **PagedAttention**：消除 KV Cache 碎片，支持更大 batch\n\n4. **意义**：这些技术可以叠加使用，将推理成本降低 10 倍以上，让\"只有大厂用得起\"变成\"人人可部署\"。\n\n### 关键公式速查\n\n| 公式 | 表达式 |\n|------|--------|\n| 线性量化 | $w_q = \\text{round}((w - z) / s)$ |\n| 投机解码接受概率 | $P_{\\text{accept}} = \\min(1, p(x) / q(x))$ |\n| KV Cache 大小 | $2 \\times L \\times n \\times d \\times \\text{sizeof(dtype)}$ |\n| 内存带宽受限时间 | $T = \\text{参数大小} / \\text{带宽}$ |\n\n### 方法对比速查\n\n| 方法 | 显存影响 | 延迟影响 | 吞吐影响 | 质量影响 |\n|------|---------|---------|---------|---------|\n| INT8 量化 | -50% | -30% | +50% | ~0% |\n| INT4 量化 | -75% | -50% | +100% | 1–3% |\n| 投机解码 | +10%（draft） | -50–60% | 无 | 0%（数学无损） |\n| 持续批处理 | 无 | +20%（首 token） | +200–500% | 0% |\n| PagedAttention | -30–50% | 无 | +50–100% | 0% |\n\n### 思考题\n\n1. **[概念理解]** 解释为什么大模型推理是\"内存带宽受限\"而非\"计算受限\"的任务。这对优化策略有什么启示？\n\n2. **[数学推导]** 证明投机解码的接受-拒绝采样保持目标分布不变。给出完整的概率计算。\n\n3. **[工程实践]** 使用 vLLM 部署一个 INT4 量化的 LLaMA-7B 模型，测量不同 batch size 下的吞吐量（tokens/s）和首 token 延迟。观察并解释瓶颈点。\n\n4. **[开放思考]** 量化、投机解码、持续批处理能否同时使用？设计一个完整的推理系统架构，考虑它们之间的协同与冲突。\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n- **[GPTQ](https://arxiv.org/abs/2210.17323)**：Frantar et al. (2022)，后训练量化的里程碑\n  - 重点读：Section 3（方法）、Section 4（实验）\n  - 可跳过：Appendix 的详细推导\n\n- **[AWQ](https://arxiv.org/abs/2306.00978)**：Lin et al. (2023)，MLSys 2024 最佳论文\n  - 重点读：Section 3（激活感知量化）、Figure 1-2\n\n- **[vLLM/PagedAttention](https://arxiv.org/abs/2309.06180)**：Kwon et al. (2023)，SOSP 2023\n  - 重点读：Section 3（PagedAttention）、Section 4（系统设计）\n\n### 理论基础\n\n- **[LLM.int8()](https://arxiv.org/abs/2208.07339)**：Dettmers et al. (2022)，发现异常值现象\n- **[Speculative Decoding](https://arxiv.org/abs/2211.17192)**：Leviathan et al. (2023)，投机解码的理论框架\n\n### 后续发展\n\n- **[GGUF Format](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)**：llama.cpp 的量化格式，支持 CPU 推理\n- **[GQA](https://arxiv.org/abs/2305.13245)**：Grouped-Query Attention，减少 KV Cache\n\n### 代码资源\n\n- **[vLLM](https://github.com/vllm-project/vllm)**：高性能推理引擎\n- **[TGI](https://github.com/huggingface/text-generation-inference)**：Hugging Face 推理服务\n- **[llama.cpp](https://github.com/ggerganov/llama.cpp)**：CPU 推理，GGUF 格式\n\n---\n\n## 历史注脚\n\n推理优化的需求伴随着 GPT-3 的发布而爆发。2020 年，175B 参数的 GPT-3 震惊了 AI 社区，但也让人们意识到：这样的模型几乎无法在任何单一设备上运行。最初，人们认为这只是\"有钱人的游戏\"——只有 OpenAI、Google 这样的公司才有资源部署如此规模的模型。\n\n转折点出现在 2022 年。Dettmers 的 LLM.int8() 首次展示了大模型可以在\"可承受\"的硬件上运行。紧随其后，GPTQ 将 175B 模型压缩到单卡可运行的程度。2023 年，vLLM 和 TGI 的出现让高效推理成为\"开箱即用\"的能力。短短两年间，大模型推理从\"天方夜谭\"变成了\"人人可及\"。\n\n这段历史告诉我们：AI 的民主化不仅需要开源模型，更需要开源的基础设施。量化、高效推理引擎这些\"不那么性感\"的技术，实际上是让 AI 惠及普通人的关键拼图。\n\n",
    "supporting": [
      "ch31-inference-optimization_files"
    ],
    "filters": [],
    "includes": {}
  }
}