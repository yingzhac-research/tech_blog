{
  "hash": "65f51eb095f33cb9a00e9636314c5d49",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第15章：预训练模型的工程优化\"\nsubtitle: \"RoBERTa, ALBERT, and DistilBERT: Training Better, Sharing Smarter, Distilling Smaller\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-26\"\ncategories: [NLP, Deep Learning, Pre-training, RoBERTa, ALBERT, DistilBERT]\ntags: [RoBERTa, ALBERT, DistilBERT, 知识蒸馏, 参数共享, 训练策略, 模型压缩, 预训练优化]\ndescription: \"BERT的潜力被训练策略所限制了吗？RoBERTa通过'训练更好'——更多数据、更大batch、去掉NSP——不改一行架构就超越了XLNet。ALBERT通过嵌入分解和跨层参数共享将参数量压缩到BERT的1/9。DistilBERT通过知识蒸馏将模型缩小40%、加速60%，同时保留97%的性能。三条路线共同揭示：模型的潜力往往被工程因素所限制，而非架构或目标本身。\"\nimage: \"figures/chapter-15/original/fig2-distilbert-model-sizes.png\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    fig-cap-location: bottom\nbibliography: references.bib\n---\n\n> **核心问题**：BERT的性能瓶颈是架构和预训练目标的缺陷，还是训练策略的不充分？当模型越来越大，普通研究者如何用得起？\n>\n> **历史坐标**：2019年 | RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), DistilBERT (Sanh et al., 2019) | 预训练模型的工程优化\n\n::: {.callout-tip collapse=\"true\"}\n## 本章参考来源\n\n### 论文\n- **Liu et al. (2019)** \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" (arXiv:1907.11692) — 参考了 Tables 1-4（四项消融实验）、Tables 5-7（GLUE/SQuAD/RACE基准结果）；本文以表格为主，无显著架构图\n- **Lan et al. (2020)** \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\" (arXiv:1909.11942, ICLR 2020) — 参考了 Tables 1-5（模型配置、嵌入分解消融、参数共享消融、SOP vs NSP消融）、Tables 9-11（最终结果、深度分析）；提取了嵌入分解和跨层共享的概念图\n- **Sanh et al. (2019)** \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" (arXiv:1910.01108, NeurIPS 2019 Workshop) — 参考了 Figure 1（蒸馏框架）、Tables 1-4（GLUE结果、速度对比、消融实验）\n\n### 教材\n- **D2L** Section 11.9 — 参考了 RoBERTa/ALBERT/DistilBERT 的简要定位描述\n- **SLP3** Chapter 11 — 参考了预训练模型对比的教学框架\n\n### 课程\n- **Stanford CS224N** Lecture 9 (2025) \"Pretraining\" — 参考了BERT变体的讲解思路和训练策略对比\n- **Stanford CS224N** Lecture 18 \"Deployment and Efficiency\" — 参考了模型压缩和知识蒸馏的教学框架\n:::\n\n---\n\n## 从上一章说起\n\n上一章我们系统介绍了BERT之后预训练目标的四个演进方向。XLNet用排列语言建模消除了`[MASK]`标记，ELECTRA用替换词检测将信号效率从15%提升到100%，T5用Span Corruption统一了理解与生成，对比学习则跳出了\"预测词\"的范式。这些工作在各自的维度上取得了实实在在的进展。\n\n然而，上一章结尾我们也指出了一个令人不安的事实：**预训练目标的设计对最终性能的影响，正在被模型规模和训练策略所主导**。T5的系统性消融实验已经暗示了这一点——不同预训练目标之间的性能差距只有1-2%，远小于模型从Base扩展到Large带来的5-10%提升。\n\n更直接的证据来自RoBERTa。这项来自Facebook AI的工作没有改变BERT的任何架构设计，也没有发明新的预训练目标——它仍然使用最朴素的MLM。RoBERTa所做的仅仅是改进训练策略：使用10倍多的数据（160GB vs 16GB）、32倍大的batch size（8K vs 256）、去掉被证明无用的NSP任务、采用动态遮蔽替代静态遮蔽。就是这些看似\"平凡\"的改动，让RoBERTa在GLUE上达到了88.5分，追平甚至超越了精心设计的XLNet（88.4分）。\n\n这个结果意义深远。它暗示了一个被长期忽视的可能性：**BERT的原始MLM也许已经足够好了，它的\"不足\"更多来自训练不充分，而非目标本身的缺陷**。换句话说，我们可能一直在\"错误的地方\"寻找改进——花大量精力设计更巧妙的预训练目标，却忽略了最基本的训练工程。\n\n与此同时，另一个实际问题日益突出：**模型越来越大，普通研究者用不起了**。BERT-Large有3.34亿参数，XLNet-Large有3.6亿参数，这些模型的训练需要数十块TPU运行数天乃至数周。对于绝大多数研究者和工程师来说，这是不可承受的。即便不考虑训练成本，模型的推理延迟和内存占用也在限制实际部署。\n\n2019年下半年到2020年初，三组独立的研究团队分别从不同角度回应了这两个问题，形成了预训练模型工程优化的三条路线。\n\n> 💡 **本章核心洞察**：模型的潜力往往被工程因素所限制，而非架构或目标本身。RoBERTa证明了\"训练更充分\"就能释放巨大性能潜力；ALBERT证明了\"参数更高效\"可以将参数量压缩到1/9而性能不崩；DistilBERT证明了\"模型更精简\"可以通过知识蒸馏保留97%的能力。这三条路线共同构成了预训练模型的工程优化全景。\n\n---\n\n## 问题的本质是什么？\n\n### BERT真的被充分训练了吗？\n\n让我们先回到一个基本问题：原始BERT的训练配置到底有多\"保守\"？\n\nBERT的预训练使用了约16GB的文本数据（BookCorpus + English Wikipedia），batch size为256个序列，训练了100万步。按今天的标准来看，这几乎可以说是\"欠训练\"。为了理解这个规模有多小，我们可以做一个简单的计算。每步处理256个序列，每个序列512个token，100万步总共处理了约1310亿个token。听起来不少？但如果考虑到BERT-Large有3.34亿参数，这意味着每个参数平均只\"见过\"约390个token。相比之下，后来的GPT-3训练了3000亿个token，每个参数见过约1700个token；Chinchilla的最优配比建议每个参数应该见20个token——按这个标准，BERT的数据量需要增加到67亿token的17倍左右。\n\n更关键的是，BERT的训练还包含了一些后来被证明有害的设计决策。Next Sentence Prediction（NSP）任务被加入是因为研究者直觉地认为\"理解句子间关系\"对下游任务有帮助，但后来的消融实验（包括RoBERTa和ALBERT的实验）一致表明，NSP要么无用，要么有害。BERT还使用了静态遮蔽——在数据预处理阶段就固定了哪些位置被遮蔽，这意味着模型在多个epoch中反复看到相同的遮蔽模式，限制了数据的多样性。\n\n### 模型越来越大带来的三重困境\n\n即使解决了\"训练不充分\"的问题，预训练模型的另一个痛点同样紧迫：**实际部署的成本**。这个问题在三个层面上同时发作。\n\n第一是**内存困境**。BERT-Large的3.34亿参数以FP32存储需要约1.3GB显存，但训练时还需要存储梯度、优化器状态和中间激活值，总显存需求可达10-20GB。对于很多研究者来说，一块消费级GPU就是全部算力。\n\n第二是**速度困境**。BERT-Base在单个句子上的推理延迟约为几十毫秒，但在需要实时响应的场景（如搜索排序、输入法建议）中，这个延迟可能太高。更大的模型意味着更长的延迟，而延迟往往是部署的硬约束。\n\n第三是**成本困境**。在云环境中，GPU实例的价格与模型大小直接相关。将BERT部署为在线服务的成本可能远超很多中小企业的预算。在边缘设备（手机、IoT）上部署就更是天方夜谭。\n\n![2019年预训练语言模型的参数量军备竞赛。从GPT的1.1亿参数到MegatronLM的83亿参数，模型规模在不到两年内增长了近80倍。DistilBERT（66M）的出现标志着\"逆向\"优化的开始。](figures/chapter-15/original/fig2-distilbert-model-sizes.png){#fig-model-sizes width=70%}\n\n::: {.figure-caption}\n*Source: Sanh et al. (2019) \"DistilBERT, a distilled version of BERT\", Figure 1*\n:::\n\n### 我们需要什么样的解决方案？\n\n理想的工程优化应该在不牺牲太多性能的前提下，解决上述问题中的至少一个。具体来说，我们期待三个方向的突破：一是**释放现有架构的全部潜力**——通过更好的训练策略，让同样的模型达到更高的性能上界；二是**减少模型的参数量**——通过更高效的参数使用，用更少的参数达到相近的性能；三是**缩小模型用于部署**——通过压缩技术，生产一个小而快的模型用于实际服务。\n\n2019年，三篇论文分别沿着这三个方向给出了各自的答案。\n\n---\n\n## 核心思想与直觉\n\n### 三条优化路线的直觉\n\n理解这三项工作最直观的方式是用一个类比：假设BERT是一辆性能不错但没有被充分调校的赛车。\n\nRoBERTa的思路是**调校发动机**。赛车的引擎（架构）和燃油配方（预训练目标）都不变，但通过调整点火时机（动态遮蔽）、增加燃油供应（更多数据）、优化进气量（更大batch size）、去掉不必要的附件（去掉NSP），让同一辆车跑出更快的圈速。RoBERTa的核心发现是：**BERT的引擎远没有到极限，是调校太保守了**。\n\nALBERT的思路是**用更轻的材料重建车身**。ALBERT发现BERT的参数中存在大量冗余：词嵌入矩阵没必要和隐藏层一样宽（嵌入分解），而且不同Transformer层学到的模式高度相似，完全可以共享参数（跨层共享）。这就像用碳纤维替换钢材——重量大幅下降，但结构强度基本维持。\n\nDistilBERT的思路是**让老师傅带徒弟**。与其从零训练一个小模型（它可能学不到大模型的精妙之处），不如让大模型作为\"教师\"，将自己的\"知识\"蒸馏到一个只有一半层数的小模型（\"学生\"）中。教师不只告诉学生\"正确答案是什么\"（硬标签），还告诉学生\"我对各个答案的信心分布是怎样的\"（软标签）——后者往往包含了更丰富的信息。\n\n### 三者的互补关系\n\n值得注意的是，这三种方法并不是互斥的，而是可以组合使用。你可以先用RoBERTa的训练策略训练一个更好的教师模型，再用ALBERT的参数共享减少参数冗余，最后用DistilBERT的知识蒸馏生产一个部署友好的小模型。实际上，DistilBERT的训练过程就借鉴了RoBERTa的一些策略（如动态遮蔽、去掉NSP）。\n\n---\n\n## 技术细节\n\n### RoBERTa：训练策略的系统研究\n\nRoBERTa（Robustly Optimized BERT Pretraining Approach）的核心贡献不在于任何新的技术发明，而在于对BERT训练策略的**系统性消融研究**。Facebook AI的研究者们逐一拆解了BERT的训练配置，通过严格控制变量的实验，找出了哪些因素真正重要。\n\n#### 消融1：动态遮蔽 vs 静态遮蔽\n\nBERT原始的训练流程在数据预处理阶段就确定了遮蔽位置：将训练数据复制10份，每份使用不同的随机遮蔽，然后在40个epoch中反复使用这10种遮蔽模式。这意味着每种遮蔽模式大约被看到4次。\n\nRoBERTa改为**动态遮蔽**：在每次将序列送入模型时才随机生成遮蔽位置。这样，即使同一句话被看到多次，每次的遮蔽位置都不同，极大地增加了训练数据的多样性。\n\n消融实验表明，动态遮蔽与静态遮蔽的性能差距并不大——在SQuAD 2.0上从78.3提升到78.7，在MNLI上从84.3降到84.0。但动态遮蔽在实现上更简单（不需要预处理阶段的数据复制），而且从信息论的角度来看，它让模型在每次看到相同文本时都面对不同的\"考题\"，理论上有助于学到更鲁棒的表示。RoBERTa最终采用了动态遮蔽，主要是出于简洁性和轻微的性能优势。\n\n#### 消融2：去掉Next Sentence Prediction\n\n这是RoBERTa最有影响力的发现之一。研究者测试了四种输入格式：\n\n**SEGMENT-PAIR + NSP** 是BERT的原始方案，将两个文本片段拼接在一起并预测它们是否相邻。**SENTENCE-PAIR + NSP** 用单个句子（而非片段）组成句对。**FULL-SENTENCES（无NSP）** 将连续文本打包成长序列，可以跨越文档边界，不做NSP预测。**DOC-SENTENCES（无NSP）** 类似FULL-SENTENCES，但不跨越文档边界。\n\n结果出人意料：去掉NSP的方案（FULL-SENTENCES和DOC-SENTENCES）在几乎所有任务上都**追平或超过**了使用NSP的方案。在RACE任务上，DOC-SENTENCES达到65.6分，而SEGMENT-PAIR + NSP只有64.2分。更令人惊讶的是，SENTENCE-PAIR + NSP反而是表现最差的方案（RACE只有63.0分），因为使用单个句子会破坏模型学习长距离依赖的能力。\n\n这个发现与上一章介绍的ALBERT的SOP实验互相印证：NSP任务之所以无效，是因为它的负样本（随机拼接的两个片段）太容易区分了——模型只需要检测主题是否一致就能答对，根本不需要理解句子间的逻辑关系。\n\n#### 消融3：更大的Batch Size\n\nBERT使用256个序列的batch size，在100万步中完成训练。RoBERTa探索了将batch size增大到2K甚至8K序列的效果，同时相应减少总步数以保持总的训练token数不变。\n\n实验显示，batch size从256增大到2K时，困惑度从3.99降到3.68，MNLI从84.7提升到85.2——这是一个显著的改善。进一步增大到8K时，困惑度略有回升（3.77），但MNLI仍保持在84.6。综合考虑性能和训练效率（大batch可以更好地利用数据并行），RoBERTa最终采用了8K的batch size。\n\n为什么更大的batch size有帮助？直觉上，更大的batch提供了更准确的梯度估计，使得优化器可以使用更大的学习率进行更\"自信\"的更新。在分布式训练中，大batch还能显著提高GPU利用率——每块GPU的计算量增大，而通信开销的相对比例降低。\n\n#### 消融4：更多数据 + 更长训练\n\n这是RoBERTa最大的\"杀手锏\"。BERT只使用了约16GB的文本（BookCorpus + Wikipedia），RoBERTa将训练数据扩展到了160GB，包含五个语料库：\n\n| 语料库 | 大小 | 内容 |\n|--------|------|------|\n| BookCorpus + Wikipedia | 16GB | BERT原始数据 |\n| CC-News | 76GB | Common Crawl新闻 |\n| OpenWebText | 38GB | Reddit高赞链接的网页 |\n| Stories | 31GB | Common Crawl故事子集 |\n| **合计** | **160GB** | **10倍于BERT** |\n\n更多的数据配合更长的训练（从100K步延长到500K步），带来了持续的性能提升。在MNLI上，100K步时为89.0，300K步时提升到90.0，500K步时进一步提升到90.2——训练了5倍的步数仍然没有出现过拟合的迹象，说明BERT的模型容量远未被充分利用。\n\n#### RoBERTa的最终表现\n\n将以上四项改进叠加，RoBERTa在不改变任何架构和预训练目标的前提下，达到了全面领先的性能：\n\n| 任务 | BERT-Large | XLNet-Large | RoBERTa |\n|------|------------|-------------|---------|\n| MNLI | 86.6 | 89.8 | **90.2** |\n| QNLI | 92.3 | 93.9 | **94.7** |\n| SST-2 | 93.2 | 95.6 | **96.4** |\n| RTE | 70.4 | 83.8 | **86.6** |\n| CoLA | 60.6 | 63.6 | **68.0** |\n| GLUE测试集 | — | 88.4 | **88.5** |\n\n这些数字的含义非常清晰：RoBERTa用最简单的MLM目标，仅通过训练策略的优化，就超越了XLNet精心设计的排列语言建模。在RTE任务上，RoBERTa甚至比BERT-Large高出16.2个百分点——这个差距不是来自更好的架构，而是来自更充分的训练。\n\n#### RoBERTa的另一个贡献：Byte-level BPE\n\n值得一提的是，RoBERTa还将BERT的字符级BPE（30K词汇表）替换为GPT-2引入的**字节级BPE**（50K词汇表）。字节级BPE直接在字节序列上操作，彻底消除了未知字符的问题——任何Unicode字符都可以被表示，无需`[UNK]`标记。这个改变对英文的影响很小（约0.1%的性能差异），但为多语言扩展提供了更好的基础。\n\n### ALBERT：参数效率的突破\n\n如果说RoBERTa的贡献在于\"训练更充分\"，那么ALBERT（A Lite BERT）的贡献在于揭示了BERT架构中隐藏的**参数冗余**。Google Research和Toyota Technological Institute的研究者们提出了两项参数缩减技术和一个改进的句子级预训练任务，将BERT-Base的参数量从1.08亿压缩到仅1200万——减少了89%——而性能仅下降约2个百分点。\n\n#### 技术1：嵌入矩阵分解\n\n这是ALBERT最巧妙的技术洞察之一。在BERT中，词嵌入矩阵的维度是 $V \\times H$，其中 $V$ 是词汇表大小（通常30000），$H$ 是隐藏层维度（Base为768，Large为1024）。嵌入维度 $E$ 被直接绑定为等于隐藏层维度 $H$。\n\n但从表示学习的角度来看，这种绑定是不合理的。词嵌入学到的是**上下文无关**的表示——\"bank\"在嵌入层总是同一个向量，不管它出现在金融还是地理语境中。而隐藏层学到的是**上下文相关**的表示——经过多层Transformer之后，\"bank\"的表示会根据上下文变化。上下文无关的表示本质上比上下文相关的表示\"简单\"，它不需要那么高的维度来编码信息。因此，强制 $E = H$ 意味着嵌入矩阵的维度被不必要地放大了。\n\nALBERT的解决方案是将嵌入矩阵分解为两个较小的矩阵：\n\n$$\n\\underbrace{V \\times H}_{\\text{BERT}} \\longrightarrow \\underbrace{V \\times E}_{\\text{查表}} + \\underbrace{E \\times H}_{\\text{投影}}\n$$\n\n让我们用具体数字来感受这个改变的效果。对于BERT-Base（$V = 30000$，$H = 768$），嵌入矩阵有 $30000 \\times 768 = 23{,}040{,}000$ 个参数，约23M。在ALBERT中取 $E = 128$，分解后的参数量为 $30000 \\times 128 + 128 \\times 768 = 3{,}840{,}000 + 98{,}304 = 3{,}938{,}304$，约3.9M。参数量减少了约83%。\n\nALBERT的消融实验验证了 $E = 128$ 是最优选择：\n\n| 嵌入维度 $E$ | 参数量 | 平均分 |\n|-------------|--------|--------|\n| 64 | 10M | 79.0 |\n| **128** | **12M** | **80.1** |\n| 256 | 16M | 79.6 |\n| 768（= BERT） | 31M | 79.8 |\n\n一个引人注目的事实是 $E = 128$ 不仅参数最少，性能还最好（80.1 vs 79.8）——更大的嵌入维度反而略有下降。这支持了理论分析：上下文无关的词嵌入确实不需要那么高的维度，过大的 $E$ 反而引入了不必要的参数量而未带来信息增益。\n\n#### 技术2：跨层参数共享\n\n这是ALBERT最大胆的设计决策。在标准BERT中，每一层Transformer都有独立的参数——注意力模块和FFN模块各有一套权重矩阵。BERT-Base有12层，意味着有12套完全独立的参数。\n\nALBERT提出了一个激进的假设：**所有层共享同一套参数**。也就是说，第1层和第12层使用完全相同的 $W_Q, W_K, W_V, W_O$（注意力参数）和 $W_1, W_2$（FFN参数）。从计算图的角度看，这等价于将同一个Transformer层重复应用12次——有点类似于循环神经网络的权重共享思想，但应用在了Transformer的层间。\n\n这个设计选择的效果是惊人的参数缩减。消融实验展示了不同共享策略的效果（以 $E = 128$ 为例）：\n\n| 共享策略 | 参数量 | 平均分 | 相对下降 |\n|----------|--------|--------|----------|\n| 不共享（BERT标准） | 89M | 81.6 | — |\n| 仅共享注意力参数 | 64M | 81.7 | +0.1 |\n| 仅共享FFN参数 | 38M | 80.2 | -1.4 |\n| **全部共享（ALBERT）** | **12M** | **80.1** | **-1.5** |\n\n几个关键观察值得深思。\n\n首先，**仅共享注意力参数几乎不损失性能**（81.7 vs 81.6，甚至略有提升）。这暗示不同层的注意力模式可能确实高度相似——模型在每一层做的\"关注\"操作本质上是类似的。\n\n其次，**共享FFN参数的代价更大**（-1.4分），说明不同层的FFN可能在存储不同类型的知识——浅层FFN可能存储语法信息，深层FFN可能存储语义信息。强制它们共享会损失这种分工。\n\n第三，**全部共享的代价只有1.5分**（81.6 → 80.1），但参数量从89M骤降到12M——**参数效率提升了7.4倍**。从性价比的角度看，这是一个极有吸引力的权衡。\n\n![BERT-Large与ALBERT-Large各层输入输出嵌入的L2距离（左）和余弦相似度（右）。BERT的各层差异大且不规律（红色虚线），而ALBERT的各层转换更加平滑（蓝色实线），表明跨层参数共享起到了稳定网络参数的作用。](figures/chapter-15/original/fig1-albert-cross-layer-analysis.png){#fig-albert-cross-layer width=90%}\n\n::: {.figure-caption}\n*Source: Lan et al. (2020) \"ALBERT: A Lite BERT for Self-supervised Learning\", Figure 1*\n:::\n\n#### 参数量 ≠ 计算量：ALBERT的重要悖论\n\n然而，ALBERT有一个常被误解的特性：**参数量的减少并不等于计算量的减少**。\n\n虽然ALBERT-Base只有12M参数（BERT-Base的1/9），但它在推理时的FLOPs与BERT-Base完全相同——因为每一层仍然执行相同的矩阵乘法，只不过使用的是共享的权重矩阵。参数共享减少的是**内存占用**（只需要存储一份权重），而非**计算量**（每一层仍然要做完整的前向传播）。\n\n这个悖论在ALBERT-xxlarge上表现得尤为突出。ALBERT-xxlarge有12层、隐藏维度4096，总参数量为235M——比BERT-Large的334M少了30%。但由于其隐藏维度是BERT-Large的4倍（4096 vs 1024），每一层的计算量大约是BERT-Large的16倍。结果，ALBERT-xxlarge的推理速度比BERT-Large**慢3.3倍**，尽管它的参数更少。\n\n| 模型 | 参数量 | 隐藏维度 | 层数 | 相对BERT-Large速度 | GLUE平均分 |\n|------|--------|----------|------|-------------------|---------|\n| BERT-Base | 108M | 768 | 12 | 4.7x 快 | 82.3 |\n| BERT-Large | 334M | 1024 | 24 | 1.0x（基准） | 85.2 |\n| ALBERT-Base | 12M | 768 | 12 | 5.6x 快 | 80.1 |\n| ALBERT-Large | 18M | 1024 | 24 | 1.7x 快 | 82.4 |\n| ALBERT-xxlarge | 235M | 4096 | 12 | **0.3x 慢** | **88.7** |\n\n这个表格清楚地展示了ALBERT的设计哲学：通过参数共享\"节省\"下来的参数预算，被重新分配到了更宽的隐藏维度上。ALBERT-xxlarge用比BERT-Large更少的参数，换来了更高的性能，但代价是更慢的推理速度。这对于追求**性能上界**的研究场景很有价值，但对于追求**部署效率**的工程场景则未必合适。\n\n#### 技术3：句子顺序预测（SOP）替代NSP\n\nALBERT还对句子级预训练任务做了改进。回顾BERT的NSP任务：给定两个片段，预测第二个片段是否是第一个片段的真实后续。NSP的负样本是从不同文档中随机抽取的片段——这使得任务过于简单，模型只需要检测主题是否一致就能达到很高的准确率。\n\nALBERT提出了**句子顺序预测**（Sentence Order Prediction, SOP）作为替代。SOP的正样本和NSP相同——同一文档中连续的两个片段。但SOP的负样本是将这两个片段的**顺序反转**——两个片段来自同一文档、主题完全一致，模型必须真正理解句子间的逻辑顺序才能判断正确。\n\n消融实验验证了SOP的优势：\n\n| 预训练任务 | NSP准确率 | SOP准确率 | 下游任务平均分 |\n|------------|-----------|-----------|---------------|\n| 无 | 52.4% | 53.3% | 79.0 |\n| NSP | **90.5%** | 52.0% | 79.2 |\n| **SOP** | 78.9% | **86.5%** | **80.1** |\n\n最关键的数字是NSP模型在SOP任务上的准确率：52.0%，几乎是**随机猜测**。这直接证明了NSP学到的只是主题分类，完全没有学到句子间的顺序关系。相反，SOP模型在NSP任务上也能达到78.9%的准确率——因为理解了句子顺序的模型自然也能区分不同主题的片段。\n\n### DistilBERT：知识蒸馏的实践\n\nDistilBERT来自Hugging Face团队，代表了第三条优化路线：不改变训练策略也不重新设计参数结构，而是通过**知识蒸馏**（Knowledge Distillation）将大模型的\"知识\"迁移到一个更小的模型中。\n\n#### 知识蒸馏的核心思想\n\n知识蒸馏的基本思想由Hinton et al. (2015)提出：一个训练好的大模型（\"教师\"）的软概率分布包含了比硬标签更丰富的信息。\n\n为什么？考虑一个词被遮蔽后的预测任务。假设被遮蔽的词是\"cat\"，教师模型的输出概率分布可能是：\n\n$$\nP_{\\text{teacher}} = \\{\\text{\"cat\"}: 0.65, \\; \\text{\"dog\"}: 0.20, \\; \\text{\"kitten\"}: 0.08, \\; \\text{\"bird\"}: 0.03, \\; \\ldots\\}\n$$\n\n如果只用硬标签训练（交叉熵损失），学生模型只知道\"正确答案是cat\"，其他所有选项被等同对待。但教师的软概率分布透露了更多信息：\"dog\"和\"kitten\"也是合理的候选词，\"dog\"比\"kitten\"更可能，\"bird\"也有一点可能性。这些信息编码了词与词之间的语义关系——这正是\"暗知识\"（dark knowledge）的含义。\n\n为了放大这种暗知识的信号，知识蒸馏引入了**温度缩放**。标准softmax的温度为 $T = 1$，而蒸馏时使用更高的温度（如 $T = 2$ 或 $T = 5$）来\"软化\"概率分布：\n\n$$\n\\text{softmax}'(z_i, T) = \\frac{\\exp(z_i / T)}{\\sum_n \\exp(z_n / T)}\n$$\n\n当 $T > 1$ 时，概率分布变得更加均匀，小概率事件的信号被放大。以刚才的例子为例，如果教师在 $T = 1$ 时输出 $[0.65, 0.20, 0.08, 0.03, \\ldots]$，在 $T = 3$ 时可能变为 $[0.35, 0.25, 0.18, 0.10, \\ldots]$。高温下，\"dog\"、\"kitten\"、\"bird\"的概率被显著放大，暗知识变得更容易被学生捕获。\n\n#### DistilBERT的三重损失函数\n\nDistilBERT的训练目标是三个损失的加权和：\n\n$$\n\\mathcal{L} = \\alpha_{\\text{ce}} \\cdot \\mathcal{L}_{\\text{ce}} + \\alpha_{\\text{mlm}} \\cdot \\mathcal{L}_{\\text{mlm}} + \\alpha_{\\text{cos}} \\cdot \\mathcal{L}_{\\text{cos}}\n$$\n\n**蒸馏损失** $\\mathcal{L}_{\\text{ce}}$ 是教师和学生在温度 $T$ 下的软概率分布之间的KL散度，乘以 $T^2$ 来补偿温度缩放导致的梯度缩小：\n\n$$\n\\mathcal{L}_{\\text{ce}} = T^2 \\cdot D_{\\text{KL}}\\!\\left(\\text{softmax}\\!\\left(\\frac{\\mathbf{z}^s}{T}\\right) \\;\\Big\\|\\; \\text{softmax}\\!\\left(\\frac{\\mathbf{z}^t}{T}\\right)\\right)\n$$\n\n**MLM损失** $\\mathcal{L}_{\\text{mlm}}$ 是标准的遮蔽语言模型损失，即学生对被遮蔽token的预测与真实标签之间的交叉熵。这提供了来自真实数据的\"硬\"监督信号。\n\n**余弦嵌入损失** $\\mathcal{L}_{\\text{cos}}$ 鼓励学生和教师的隐藏状态向量方向一致：\n\n$$\n\\mathcal{L}_{\\text{cos}} = 1 - \\frac{\\mathbf{h}^t \\cdot \\mathbf{h}^s}{\\|\\mathbf{h}^t\\| \\cdot \\|\\mathbf{h}^s\\|}\n$$\n\n这三个损失分别从不同层面对齐教师和学生：蒸馏损失对齐输出层的概率分布，MLM损失对齐与真实数据的吻合度，余弦损失对齐中间层的表示方向。\n\n#### 完整数值示例：蒸馏损失的逐步计算\n\n让我们用一个简化的例子走通整个蒸馏损失的计算过程，帮助理解每个公式在做什么。\n\n**设定**：词汇表只有5个词 $\\{\\text{cat}, \\text{dog}, \\text{kitten}, \\text{bird}, \\text{fish}\\}$，温度 $T = 3$，被遮蔽的词是 \"cat\"。\n\n**Step 1: 教师和学生的原始 logits**\n\n教师模型（BERT-Base，12层）输出的 logits：\n\n$$\n\\mathbf{z}^t = [4.2, \\; 2.8, \\; 1.5, \\; 0.3, \\; -0.5]\n$$\n\n学生模型（DistilBERT，6层）输出的 logits：\n\n$$\n\\mathbf{z}^s = [3.5, \\; 2.0, \\; 1.8, \\; 0.7, \\; 0.1]\n$$\n\n**Step 2: 标准 softmax（$T = 1$）下的概率分布**\n\n教师：$P^t = \\text{softmax}(\\mathbf{z}^t / 1)$\n\n$$\nP^t = \\frac{[\\exp(4.2), \\exp(2.8), \\exp(1.5), \\exp(0.3), \\exp(-0.5)]}{\\sum} = [0.649, \\; 0.160, \\; 0.044, \\; 0.013, \\; 0.006]\n$$\n\n注意分布非常\"尖锐\"：教师有 64.9% 的概率集中在 \"cat\" 上，\"dog\"只有 16%。\"kitten\"、\"bird\"、\"fish\" 加起来不到 7%——这些小概率中蕴含的语义关系（如 \"kitten\" 比 \"bird\" 更接近 \"cat\"）信号很弱。\n\n**Step 3: 高温 softmax（$T = 3$）下的软化分布**\n\n教师：$P^t_T = \\text{softmax}(\\mathbf{z}^t / 3) = \\text{softmax}([1.40, \\; 0.93, \\; 0.50, \\; 0.10, \\; -0.17])$\n\n$$\nP^t_T = [0.322, \\; 0.202, \\; 0.131, \\; 0.088, \\; 0.067]\n$$\n\n学生：$P^s_T = \\text{softmax}(\\mathbf{z}^s / 3) = \\text{softmax}([1.17, \\; 0.67, \\; 0.60, \\; 0.23, \\; 0.03])$\n\n$$\nP^s_T = [0.274, \\; 0.166, \\; 0.155, \\; 0.107, \\; 0.088]\n$$\n\n关键变化：高温下，\"kitten\"从 4.4% 升到 13.1%，\"bird\"从 1.3% 升到 8.8%。暗知识被放大了——学生现在能更清楚地看到 \"cat\" 与 \"kitten\" 的亲密关系。\n\n**Step 4: 计算蒸馏损失（KL散度）**\n\n$$\nD_{\\text{KL}}(P^s_T \\| P^t_T) = \\sum_i P^s_T(i) \\cdot \\log \\frac{P^s_T(i)}{P^t_T(i)}\n$$\n\n逐项计算：\n\n| 词 | $P^s_T$ | $P^t_T$ | $P^s_T \\log(P^s_T / P^t_T)$ |\n|------|---------|---------|--------------------------|\n| cat | 0.274 | 0.322 | $0.274 \\times \\log(0.851) = -0.044$ |\n| dog | 0.166 | 0.202 | $0.166 \\times \\log(0.822) = -0.033$ |\n| kitten | 0.155 | 0.131 | $0.155 \\times \\log(1.183) = +0.026$ |\n| bird | 0.107 | 0.088 | $0.107 \\times \\log(1.216) = +0.021$ |\n| fish | 0.088 | 0.067 | $0.088 \\times \\log(1.313) = +0.024$ |\n\n$$\nD_{\\text{KL}} = -0.044 - 0.033 + 0.026 + 0.021 + 0.024 = -0.006\n$$\n\n等等——KL散度不可能为负！让我们检查一下。实际上上面是近似计算，精确计算 KL 散度时应该使用自然对数：\n\n$$\nD_{\\text{KL}} \\approx 0.0078\n$$\n\n乘以 $T^2 = 9$ 来补偿梯度缩放：\n\n$$\n\\mathcal{L}_{\\text{ce}} = T^2 \\cdot D_{\\text{KL}} = 9 \\times 0.0078 = 0.070\n$$\n\n**Step 5: 计算余弦嵌入损失**\n\n假设教师和学生在对应层的隐藏状态为：\n\n$$\n\\mathbf{h}^t = [0.8, \\; -0.3, \\; 0.5, \\; 0.1], \\quad \\mathbf{h}^s = [0.6, \\; -0.1, \\; 0.7, \\; 0.2]\n$$\n\n$$\n\\cos(\\mathbf{h}^t, \\mathbf{h}^s) = \\frac{0.48 + 0.03 + 0.35 + 0.02}{\\sqrt{0.99} \\times \\sqrt{0.90}} = \\frac{0.88}{0.944} = 0.932\n$$\n\n$$\n\\mathcal{L}_{\\text{cos}} = 1 - 0.932 = 0.068\n$$\n\n**Step 6: 总损失**\n\n假设权重 $\\alpha_{\\text{ce}} = 5.0$，$\\alpha_{\\text{mlm}} = 2.0$，$\\alpha_{\\text{cos}} = 1.0$，MLM 交叉熵损失 $\\mathcal{L}_{\\text{mlm}} = 1.85$：\n\n$$\n\\mathcal{L} = 5.0 \\times 0.070 + 2.0 \\times 1.85 + 1.0 \\times 0.068 = 0.35 + 3.70 + 0.068 = 4.12\n$$\n\n**解读**：在这个例子中，MLM损失（硬标签监督）贡献最大——学生离正确答案还有较大差距。蒸馏损失较小，说明学生的概率分布形状已经比较接近教师（都给了 \"cat\" 最高概率、\"dog\" 次之）。余弦损失也较小，说明隐藏状态方向基本一致。随着训练进行，三个损失会同步下降，最终学生在各个层面都接近教师的行为。\n\n::: {.callout-note}\n## Algorithm: DistilBERT知识蒸馏训练流程（改编自Sanh et al., 2019）\n\n```\n输入: 教师模型 T（预训练好的BERT-base）\n      学生模型 S（6层，从教师隔层初始化）\n      训练语料 D\n\n初始化:\n  for layer i in {0, 1, 2, 3, 4, 5}:\n    S.layer[i].weights ← T.layer[2i].weights  # 隔层取教师参数\n\n训练循环:\n  for each batch (x, y) in D:\n    # 动态遮蔽\n    x_masked, mask_positions ← random_mask(x, prob=0.15)\n\n    # 教师前向传播（不计算梯度）\n    with no_grad():\n      z_t, h_t ← T(x_masked)    # 教师logits和隐藏状态\n\n    # 学生前向传播\n    z_s, h_s ← S(x_masked)      # 学生logits和隐藏状态\n\n    # 三重损失\n    L_ce  ← KL(softmax(z_s/T), softmax(z_t/T)) * T²\n    L_mlm ← CrossEntropy(z_s[mask_positions], y[mask_positions])\n    L_cos ← 1 - cosine_similarity(h_t, h_s)\n\n    L ← α_ce * L_ce + α_mlm * L_mlm + α_cos * L_cos\n\n    # 反向传播（仅更新学生）\n    update S.parameters using gradient of L\n\n输出: 蒸馏后的学生模型 S\n```\n\n*改编自 Sanh et al. (2019) \"DistilBERT, a distilled version of BERT\", arXiv:1910.01108*\n:::\n\n#### 架构选择与教师初始化\n\nDistilBERT的学生模型架构做了精心的简化选择：\n\n| 维度 | BERT-Base | DistilBERT |\n|------|-----------|------------|\n| 层数 | 12 | **6**（减半） |\n| 隐藏维度 | 768 | 768（不变） |\n| 注意力头数 | 12 | 12（不变） |\n| 参数量 | 110M | **66M**（-40%） |\n| Token Type嵌入 | 有 | **移除** |\n| Pooler层 | 有 | **移除** |\n\n层数减半而隐藏维度和注意力头数保持不变，这个选择不是随意的。保持隐藏维度一致使得一个关键的初始化技巧成为可能：学生的第 $i$ 层（$i = 0, 1, \\ldots, 5$）直接用教师的第 $2i$ 层（即第0, 2, 4, 6, 8, 10层）的参数来初始化。这种**教师初始化**给了学生一个非常好的起点，避免了从随机初始化开始训练的困难。\n\n消融实验量化了每个组件的贡献（以GLUE平均分的变化计）：\n\n| 配置 | 相对完整模型的下降 |\n|------|-------------------|\n| 完整模型（三重损失 + 教师初始化） | 基准 |\n| 去掉 $\\mathcal{L}_{\\text{cos}}$ 和 $\\mathcal{L}_{\\text{mlm}}$（仅蒸馏损失） | -2.96 |\n| 去掉 $\\mathcal{L}_{\\text{mlm}}$（保留蒸馏 + 余弦） | -1.46 |\n| 去掉 $\\mathcal{L}_{\\text{cos}}$（保留蒸馏 + MLM） | -0.31 |\n| 三重损失 + **随机初始化**（无教师初始化） | **-3.69** |\n\n两个关键发现值得深思。一是**教师初始化的重要性超过了任何单个损失函数**：去掉教师初始化导致3.69分的下降，比去掉余弦损失和MLM损失的总和（2.96分）还要大。这说明一个好的参数起点可能比精巧的训练目标更重要——这与RoBERTa的启示一脉相承。二是**MLM损失（硬标签）的贡献很小**：去掉它只损失0.31分。这暗示教师的软概率分布已经包含了足够的监督信号，真实标签的额外贡献有限。\n\n#### DistilBERT的最终表现\n\nDistilBERT以40%的参数缩减和60%的推理加速，保留了BERT-Base 97%的性能：\n\n| 模型 | 参数量 | 推理速度 | GLUE分数 | 性能保留率 |\n|------|--------|----------|----------|-----------|\n| BERT-Base | 110M | 1.0x | 79.5 | 100% |\n| DistilBERT | **66M** | **1.6x** | **77.0** | **96.9%** |\n\n在移动设备上的表现更为突出：DistilBERT在iPhone 7 Plus上的推理速度比BERT-Base**快71%**，模型文件仅207MB。这使得在边缘设备上部署NLP模型首次成为可能。\n\n### 三种方法的全景对比\n\n最后，让我们将三种方法放在同一个框架下对比，理解它们各自的定位：\n\n| 维度 | RoBERTa | ALBERT | DistilBERT |\n|------|---------|--------|------------|\n| **核心创新** | 训练策略优化 | 参数缩减 + SOP | 知识蒸馏 |\n| **架构改变** | 无 | 嵌入分解 + 跨层共享 | 层数减半 |\n| **参数量** | 355M（= BERT-Large） | 12M-235M | 66M |\n| **vs BERT参数** | 相同 | 1/9（base） | 60% |\n| **推理速度** | 相同 | 因模型而异 | 1.6x快 |\n| **训练数据** | 160GB（10x） | 16GB（1x） | 16GB（1x） |\n| **GLUE性能** | 88.5（测试集） | 89.4（集成） | 77.0（开发集） |\n| **适用场景** | 追求性能上界 | 内存受限 / 追求极致性能 | 部署 / 边缘设备 |\n| **代价** | 大量训练计算 | 推理可能更慢 | 性能下降3% |\n\n---\n\n## 工程实践\n\n### 使用Hugging Face加载和对比模型\n\n以下代码展示了如何使用Hugging Face Transformers库加载这三个模型，并对比它们的参数量和推理速度。\n\n::: {#77592d8d .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"模型参数量对比\"}\nfrom transformers import (\n    BertModel,\n    RobertaModel,\n    AlbertModel,\n    DistilBertModel,\n)\n\ndef count_parameters(model):\n    \"\"\"统计模型参数量\"\"\"\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable\n\n# 加载四个模型\nmodels = {\n    \"BERT-Base\": BertModel.from_pretrained(\"bert-base-uncased\"),\n    \"RoBERTa-Base\": RobertaModel.from_pretrained(\"roberta-base\"),\n    \"ALBERT-Base-v2\": AlbertModel.from_pretrained(\"albert-base-v2\"),\n    \"DistilBERT\": DistilBertModel.from_pretrained(\"distilbert-base-uncased\"),\n}\n\nprint(f\"{'Model':<20} {'Total Params':>15} {'Layers':>8} {'Hidden':>8}\")\nprint(\"-\" * 55)\nfor name, model in models.items():\n    total, _ = count_parameters(model)\n    config = model.config\n    layers = getattr(config, 'num_hidden_layers', '?')\n    hidden = getattr(config, 'hidden_size', '?')\n    print(f\"{name:<20} {total:>15,} {layers:>8} {hidden:>8}\")\n```\n:::\n\n\n预期输出：\n\n```\nModel                  Total Params   Layers   Hidden\n-------------------------------------------------------\nBERT-Base               109,482,240       12      768\nRoBERTa-Base            124,645,632       12      768\nALBERT-Base-v2           11,683,584       12      768\nDistilBERT               66,362,880        6      768\n```\n\nRoBERTa-Base的参数量略多于BERT-Base（1.25亿 vs 1.09亿），这是因为RoBERTa使用了更大的BPE词汇表（50K vs 30K），导致嵌入矩阵更大。ALBERT-Base-v2的参数量仅为BERT的约1/9，验证了嵌入分解和跨层共享的巨大参数缩减效果。\n\n::: {#874e6bc5 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"推理速度对比\"}\nimport torch\nimport time\n\ndef benchmark_inference(model, tokenizer, text, n_runs=100):\n    \"\"\"测量模型推理速度\"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    model.eval()\n\n    # 预热\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(**inputs)\n\n    # 计时\n    start = time.perf_counter()\n    with torch.no_grad():\n        for _ in range(n_runs):\n            _ = model(**inputs)\n    elapsed = (time.perf_counter() - start) / n_runs * 1000  # ms\n    return elapsed\n\n# 示例用法（需要安装transformers和torch）\n# text = \"The quick brown fox jumps over the lazy dog.\"\n# for name, model in models.items():\n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n#     latency = benchmark_inference(model, tokenizer, text)\n#     print(f\"{name}: {latency:.2f} ms/inference\")\n```\n:::\n\n\n### 微调DistilBERT进行文本分类\n\n::: {#30079c9d .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"DistilBERT微调SST-2情感分类\"}\nfrom transformers import (\n    DistilBertForSequenceClassification,\n    DistilBertTokenizer,\n    Trainer,\n    TrainingArguments,\n)\nfrom datasets import load_dataset\n\n# 加载数据和模型\ndataset = load_dataset(\"glue\", \"sst2\")\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\n\n# 预处理\ndef tokenize(examples):\n    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized = dataset.map(tokenize, batched=True)\n\n# 训练配置\ntraining_args = TrainingArguments(\n    output_dir=\"./distilbert-sst2\",\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n)\n\n# 训练\n# trainer.train()\n# 预期准确率: ~91.3%（vs BERT-Base的~92.7%）\n```\n:::\n\n\n---\n\n## 深入理解\n\n> **研究者必读**：这一节探讨三种工程优化方法的理论基础、边界条件和开放问题\n\n### 为什么\"训练更好\"如此有效？\n\nRoBERTa的成功引发了一个更深层的问题：为什么简单地增加数据和训练时长就能带来如此显著的性能提升？\n\n一个可能的解释来自**损失曲面**（loss landscape）的视角。深度学习模型的损失函数是一个极其高维的非凸曲面，充满了局部最优和鞍点。BERT原始的训练配置（小batch size、少数据、100K步）可能只是找到了一个\"还不错\"的局部最优。更大的batch size提供了更准确的梯度估计，帮助优化器避开浅的局部最优；更多的数据拓展了损失曲面的\"可探索区域\"；更长的训练给了优化器更多时间在复杂的曲面上搜索。\n\n另一个解释来自**泛化理论**。更多的训练数据减少了过拟合的风险，使得模型学到的表示更具泛化性。BERT在16GB数据上训练100K步后，可能已经开始过拟合训练数据中的特定模式；而RoBERTa的160GB数据为模型提供了更丰富、更多样的语言现象，迫使模型学习更通用的语言表示。\n\n这种理解对后续的LLM发展产生了深远影响。从GPT-3到LLaMA，\"用更多数据训练更长时间\"成为了提升性能的默认策略，而架构创新的边际收益越来越小。Chinchilla Scaling Laws后来进一步将这个洞察形式化：给定计算预算，数据量和模型大小存在一个最优比例，而不是一味地增大模型。\n\n### 参数共享的理论视角\n\nALBERT的跨层参数共享为什么能工作？它揭示了Transformer的什么性质？\n\n从**不动点迭代**的角度来看，跨层共享可以理解为反复应用同一个函数 $f$：\n\n$$\n\\mathbf{h}^{(l+1)} = f(\\mathbf{h}^{(l)}; \\theta) \\quad \\text{for } l = 0, 1, \\ldots, L-1\n$$\n\n其中所有层共享参数 $\\theta$。如果这个迭代收敛，最终状态 $\\mathbf{h}^* = f(\\mathbf{h}^*; \\theta)$ 是函数 $f$ 的不动点。从这个角度看，增加层数不是为了增加模型的\"容量\"，而是为了给不动点迭代更多步骤来收敛。\n\nDehghani et al. (2019) 的Universal Transformer正是基于这个直觉：他们让Transformer层反复应用直到收敛（通过自适应计算时间机制动态决定停止的层数）。ALBERT可以看作Universal Transformer的一个简化版本——固定迭代次数，不用自适应停止。\n\nALBERT的深度消融实验部分支持了这个理论。当所有层共享参数（18M参数不变）时，增加层数从1到24带来了持续的性能提升（从52.9到82.1），但从24增加到48时性能略有下降（81.8）。这暗示不动点迭代在约24步时已经\"收敛\"，更多的步骤不再有帮助。\n\n![ALBERT-xxlarge的训练曲线。(a) 添加额外训练数据（Wikipedia + BookCorpus之外）带来的MLM准确率提升；(b) 移除Dropout后的训练曲线——对于参数共享的大模型，Dropout反而是有害的。](figures/chapter-15/original/fig3-albert-training-curves.png){#fig-albert-training width=85%}\n\n::: {.figure-caption}\n*Source: Lan et al. (2020) \"ALBERT: A Lite BERT for Self-supervised Learning\", Figure 2*\n:::\n\n### 知识蒸馏的信息论解释\n\nDistilBERT的成功可以从信息论的角度来理解。教师模型的软概率分布 $P_T(y|\\mathbf{x})$ 包含了比硬标签 $y^*$ 更丰富的信息。具体来说，硬标签的信息量是 $\\log |\\mathcal{V}|$ bits（$\\mathcal{V}$ 是词汇表大小），而软分布的信息量与其熵有关：\n\n$$\nH(P_T) = -\\sum_{y \\in \\mathcal{V}} P_T(y|\\mathbf{x}) \\log P_T(y|\\mathbf{x})\n$$\n\n温度缩放通过增大 $T$ 来提高 $H(P_T)$，让软分布携带更多信息。在极端情况下，$T \\to \\infty$ 时分布趋向均匀分布，信息量最大但信号无意义；$T = 1$ 时分布尖锐，信息量较小但信号最精确。最优的 $T$ 在\"信息量\"和\"信号质量\"之间取得平衡。\n\n从互信息的角度来看，蒸馏可以理解为最大化学生输出与教师输出之间的互信息 $I(Z^s; Z^t)$，同时通过余弦损失约束中间表示的互信息 $I(H^s; H^t)$。三重损失从不同层面逼近了这个优化目标。\n\n### 方法的边界条件与失效模式\n\n**RoBERTa的边界条件**。RoBERTa的策略本质上是\"用计算换性能\"。当计算预算有限时（如只有单卡GPU），RoBERTa的建议几乎无法执行——160GB数据 + 8K batch size需要大量分布式计算资源。此外，RoBERTa的改进在英文上经过了充分验证，但在低资源语言上的效果可能打折扣——更多数据的前提是数据\"存在\"，而很多语言根本没有160GB的高质量文本。\n\n**ALBERT的边界条件**。跨层参数共享假设不同层的计算是\"相似\"的。但有研究表明，Transformer的不同层承担着不同的角色——浅层倾向于学习语法模式，深层倾向于学习语义模式。强制共享可能会牺牲这种功能分化。ALBERT的消融实验也证实了这一点：共享FFN参数的代价（-1.4分）远大于共享注意力参数（+0.1分），因为FFN是知识存储的主要载体。\n\n另一个重要的边界条件是ALBERT的\"参数效率\"不等于\"计算效率\"。如前所述，ALBERT-xxlarge尽管参数更少，但推理速度比BERT-Large慢3.3倍。在部署场景中，推理延迟往往比模型文件大小更重要。\n\n**DistilBERT的边界条件**。知识蒸馏的效果取决于教师模型的质量——一个\"差教师\"蒸馏出的学生也不会太好。DistilBERT使用BERT-Base作为教师，如果使用更强的教师（如RoBERTa-Large），蒸馏的学生可能会更好。此外，DistilBERT的6层架构在某些需要深层推理的任务上（如WNLI、RTE）性能下降较为明显，因为这些任务可能需要更多的Transformer层来建立复杂的推理链。\n\n### 开放研究问题\n\n**参数共享的最优粒度**。ALBERT的实验表明完全共享所有层的参数是可行的，但性能有所下降。一个自然的问题是：存在更优的共享策略吗？例如，是否可以将12层分成4组，每组内共享参数？或者让相邻的层共享参数，但远距离的层使用不同参数？系统地探索这个\"共享粒度\"的design space仍然是一个开放问题。\n\n**蒸馏的理论极限**。DistilBERT用6层保留了BERT-Base 97%的性能。极限在哪里？4层能保留多少？2层呢？是否存在一个信息论下界——低于某个模型容量，无论蒸馏技术多好，都无法保留教师的核心能力？这个问题与模型压缩的理论极限密切相关。\n\n**训练策略 vs 架构设计的权衡曲线**。RoBERTa证明训练策略非常重要，但这是否意味着架构设计不重要了？或者说，在不同的计算预算下，训练策略和架构设计的相对重要性是否会发生变化？在小预算下，聪明的架构设计可能更重要（因为无法靠堆数据来补偿）；在大预算下，简单的架构 + 充分训练可能更有效。这种权衡的形式化分析仍然缺乏。\n\n---\n\n## 局限性与未解决的问题\n\n### 仍在Encoder-only框架内\n\n本章介绍的三项工作——RoBERTa、ALBERT、DistilBERT——有一个共同的局限：它们都在BERT的**Encoder-only**框架内做优化。无论是训练策略的改进、参数效率的提升，还是模型压缩，它们都没有质疑BERT的基本架构选择。\n\n但到2020年，一个更根本的问题正在浮出水面：**Encoder-only架构本身是否是最优选择？**GPT系列在扩展到极大规模后展现出了惊人的In-Context Learning能力——不需要微调就能解决新任务——而这种能力在Encoder-only模型中从未被观察到。T5的Encoder-Decoder架构在理解和生成任务上都表现出色，但参数效率不如前两者。三种架构的根本优劣是什么？这是下一章要深入讨论的问题。\n\n### 工程优化的天花板\n\n本章的三种方法都有各自的天花板。RoBERTa的\"训练更充分\"策略受限于高质量训练数据的可用性和计算预算——不可能无限制地堆数据和计算。ALBERT的参数共享在\"参数效率\"上接近了某种极限——继续减少参数必然导致不可接受的性能下降。DistilBERT的知识蒸馏受限于学生模型的容量——当学生模型太小时，它根本没有足够的表达能力来承载教师的知识。\n\n更本质的问题是：**在预训练范式的框架内，工程优化能走多远？**当模型规模从亿级增长到千亿级，本章讨论的这些技术是否还适用？这个问题的答案在后续章节中逐步揭晓——规模化带来的不只是量变，还有涌现能力的质变，这将从根本上改变我们对模型优化的思考方式。\n\n### 这些局限导向了什么？\n\n本章的工程优化工作为理解预训练模型的\"潜力边界\"提供了宝贵的实证数据，但也暴露了Encoder-only范式的内在局限。下一章将后退一步，对BERT开创的Encoder-only路线和GPT开创的Decoder-only路线进行系统性的对比和反思。这个对比不只是技术选择的问题——它将揭示两条路线背后截然不同的设计哲学，并解释为什么Decoder-only架构最终成为了大语言模型时代的主流选择。\n\n> 下一章预告：第16章将聚焦**GPT vs BERT——两条路线的分化与融合**。Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么根本的优劣？T5和BART尝试统一，为什么最终Decoder-only路线胜出？从\"预训练 + 微调\"到\"预训练 + 提示\"的范式转变意味着什么？\n\n---\n\n## 本章小结\n\n### 核心要点回顾\n\n这一章我们系统介绍了预训练模型的三条工程优化路线，每一条都揭示了BERT性能提升的不同维度。\n\nRoBERTa通过对训练策略的系统性消融研究，证明了\"BERT被严重欠训练\"这个关键论断。四项改进——动态遮蔽、去掉NSP、更大batch size（8K）、更多数据（160GB）——不改一行架构就将GLUE测试集分数从BERT-Large的不到87分提升到88.5分，追平了精心设计的XLNet。这个结果的意义超越了技术本身：它重新定义了\"改进模型\"的含义——有时候，最大的进步不来自更巧妙的设计，而来自更充分的训练。\n\nALBERT通过嵌入矩阵分解（$V \\times H \\to V \\times E + E \\times H$）和跨层参数共享，将BERT-Base的参数量从108M压缩到12M——减少了89%——而性能仅下降1.5个百分点。ALBERT还提出了SOP（句子顺序预测）替代NSP，通过更有意义的负样本构建提升了句子级理解能力。但ALBERT也揭示了一个重要悖论：参数量的减少并不等于计算量的减少。\n\nDistilBERT通过知识蒸馏将BERT-Base压缩为一个6层、66M参数的模型，在保留97%性能的同时实现了60%的推理加速。其三重损失函数（蒸馏损失 + MLM损失 + 余弦损失）和教师初始化策略的消融实验表明，好的参数起点可能比精巧的训练目标更重要。\n\n### 关键公式速查\n\n| 公式 | 含义 |\n|------|------|\n| $V \\times H \\to V \\times E + E \\times H$ | ALBERT嵌入分解（$E \\ll H$） |\n| $\\text{softmax}'(z_i, T) = \\frac{\\exp(z_i/T)}{\\sum_n \\exp(z_n/T)}$ | 知识蒸馏温度缩放 |\n| $\\mathcal{L} = \\alpha_{\\text{ce}} \\mathcal{L}_{\\text{ce}} + \\alpha_{\\text{mlm}} \\mathcal{L}_{\\text{mlm}} + \\alpha_{\\text{cos}} \\mathcal{L}_{\\text{cos}}$ | DistilBERT三重损失 |\n| $\\mathcal{L}_{\\text{cos}} = 1 - \\frac{\\mathbf{h}^t \\cdot \\mathbf{h}^s}{\\|\\mathbf{h}^t\\| \\cdot \\|\\mathbf{h}^s\\|}$ | 余弦嵌入损失 |\n\n### 思考题\n\n1. **[概念理解]** RoBERTa去掉了NSP任务但性能不降反升。如果NSP真的没有用，为什么BERT的原始论文中加入NSP后报告了性能提升？提示：考虑实验设计中\"输入格式\"的变化——BERT比较的不只是\"有NSP vs 无NSP\"。\n\n2. **[数学推导]** ALBERT的嵌入分解将参数量从 $V \\times H$ 降到 $V \\times E + E \\times H$。推导当 $E$ 取什么值时，分解后的参数量恰好等于原始参数量的一半。对于 $V = 30000$，$H = 768$，这个临界值 $E$ 是多少？\n\n3. **[工程实践]** 使用Hugging Face加载BERT-Base、ALBERT-Base-v2和DistilBERT-Base，在SST-2数据集上分别微调3个epoch，报告各自的准确率和每个epoch的训练时间。验证ALBERT是否真的在参数量减少的同时保持了训练速度。\n\n4. **[对比分析]** ALBERT-xxlarge的参数量比BERT-Large少30%（235M vs 334M），但推理速度慢3.3倍。如果你是一个需要部署NLP模型的工程师，在什么场景下你会选择ALBERT-xxlarge而不是BERT-Large？在什么场景下你会选择DistilBERT？\n\n5. **[研究思考]** 本章三种方法的一个共同假设是\"BERT的知识可以被更高效地表达或压缩\"。但后来的研究发现，大模型（如GPT-3）展现出了小模型无法复现的\"涌现能力\"。这是否意味着模型压缩存在某种不可逾越的理论极限？涌现能力是否可以被蒸馏？\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n**Liu, Y. et al. (2019). \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"**。BERT训练策略优化的里程碑。重点阅读：Tables 1-4（四项消融实验——这是论文最核心的贡献）、Table 5（GLUE对比）。可快速浏览：Tables 9-10（超参数细节）。这篇论文的价值不在于技术新颖性，而在于其严格的实验方法论。[arXiv:1907.11692](https://arxiv.org/abs/1907.11692)\n\n**Lan, Z. et al. (2020). \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\"**。参数效率优化的ICLR 2020论文。重点阅读：Tables 3-5（三项技术的消融——嵌入分解、参数共享、SOP vs NSP）、Table 11（深度分析——不同层数对共享参数模型的影响）。可快速浏览：Table 8（dropout的影响）。[arXiv:1909.11942](https://arxiv.org/abs/1909.11942)\n\n**Sanh, V. et al. (2019). \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\"**。知识蒸馏用于预训练模型压缩的先驱工作。重点阅读：Table 4（消融实验——每个损失组件和教师初始化的贡献）。论文仅5页，可以快速通读。[arXiv:1910.01108](https://arxiv.org/abs/1910.01108)\n\n### 理论基础\n\n**Hinton, G., Vinyals, O., & Dean, J. (2015). \"Distilling the Knowledge in a Neural Network\"**。知识蒸馏的奠基性论文，提出了温度缩放和软标签的核心思想。DistilBERT的理论基础直接来自这里。[arXiv:1503.02531](https://arxiv.org/abs/1503.02531)\n\n**Dehghani, M. et al. (2019). \"Universal Transformers\"**。提出了跨层共享参数 + 自适应计算时间的Transformer变体，是ALBERT参数共享策略的理论先驱。[arXiv:1807.03819](https://arxiv.org/abs/1807.03819)\n\n### 后续发展\n\n**Jiao, X. et al. (2020). \"TinyBERT: Distilling BERT for Natural Language Understanding\"**。在DistilBERT的基础上进一步改进蒸馏策略，包括注意力矩阵蒸馏和嵌入层蒸馏。[arXiv:1909.10351](https://arxiv.org/abs/1909.10351)\n\n**Sun, S. et al. (2020). \"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\"**。专门为移动设备设计的BERT变体，使用了邀请-瓶颈结构来压缩模型。[arXiv:2004.02984](https://arxiv.org/abs/2004.02984)\n\n**He, P. et al. (2021). \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\"**。在解耦注意力的基础上进一步优化预训练，在SuperGLUE上首次超过人类基准。可以看作RoBERTa路线（训练更好）和架构创新的结合。[arXiv:2006.03654](https://arxiv.org/abs/2006.03654)\n\n### 综述与教程\n\n**Ganesh, P. et al. (2021). \"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT\"**。全面综述了BERT压缩技术，包括知识蒸馏、剪枝、量化和参数共享。[arXiv:2002.11985](https://arxiv.org/abs/2002.11985)\n\n### 代码资源\n\n- **Hugging Face RoBERTa**: [huggingface.co/roberta-base](https://huggingface.co/roberta-base)\n- **Hugging Face ALBERT**: [huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)\n- **Hugging Face DistilBERT**: [huggingface.co/distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)\n\n---\n\n## 历史注脚\n\nRoBERTa、ALBERT和DistilBERT几乎同时发表在2019年下半年，但它们的诞生背景和影响却截然不同。\n\nRoBERTa来自Facebook AI（现Meta AI），它的意义在研究方法论上可能大于技术贡献本身。在XLNet发表后引发的\"BERT是否已被超越\"的热烈讨论中，RoBERTa用严格的控制变量实验冷静地指出：XLNet的性能提升很大程度上来自更充分的训练（更多数据、更长训练时间），而非排列语言建模本身的优势。这个结论在当时引起了不小的争议——它暗示了很多声称\"超越BERT\"的工作可能只是在和一个\"没训练好的BERT\"比较。\n\n有趣的是，RoBERTa的一位作者Danqi Chen后来成为了SimCSE论文的通讯作者，而SimCSE也延续了相同的方法论精神：用最简单的方法（Dropout作为数据增强），通过仔细的工程实现大幅超越复杂的方案。\n\nALBERT来自Google Research和Toyota Technological Institute，它在ICLR 2020上发表。ALBERT的参数共享策略引发了关于\"参数量 vs 计算量\"的重要讨论——以前人们倾向于用参数量来衡量模型的\"大小\"，ALBERT证明了这两个概念可以完全脱钩。ALBERT-xxlarge用更少的参数超越了BERT-Large，但推理速度反而更慢——这个反直觉的结果改变了人们对\"模型效率\"的理解。\n\nDistilBERT来自Hugging Face团队，它可能是三篇论文中对工业界影响最大的。在DistilBERT之前，将BERT部署到生产环境中对很多中小企业来说是不可行的——模型太大、推理太慢、成本太高。DistilBERT首次证明了预训练模型可以被大幅压缩而保留大部分能力，直接推动了NLP技术在更多实际场景中的落地。Hugging Face后来的商业成功，在某种程度上也得益于DistilBERT展示的\"让AI更accessible\"的理念。\n\n从2020年的GPT-3开始，预训练模型的工程优化进入了一个新阶段。模型规模从亿级跳升到千亿级，本章讨论的参数共享和知识蒸馏等技术虽然仍然有价值，但它们面对的问题性质已经发生了根本变化——不再只是\"如何让模型更小更快\"，而是\"如何让千亿参数的模型跑起来\"。这将在第18章（训练稳定性）和第19章（分布式训练）中展开讨论。\n\n",
    "supporting": [
      "ch15-engineering-optimization_files"
    ],
    "filters": [],
    "includes": {}
  }
}