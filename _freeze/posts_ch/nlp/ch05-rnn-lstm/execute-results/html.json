{
  "hash": "a85b270b9d2d9202197e733a5a2f9b7f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第5章：循环神经网络时代\"\nsubtitle: \"当神经网络学会记忆：从RNN到Seq2Seq的序列建模之路\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-25\"\ncategories: [NLP, RNN, LSTM, GRU, Seq2Seq, 序列建模]\ntags: [循环神经网络, 门控机制, 梯度消失, Encoder-Decoder, 信息瓶颈]\ndescription: \"序列建模的第一个黄金时代：RNN如何学会记忆，LSTM/GRU如何解决梯度消失，以及Seq2Seq架构的信息瓶颈问题。\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\n---\n\n> **核心问题**：如何让神经网络处理变长序列，并捕获序列中的依赖关系？\n>\n> **历史坐标**：1997 (LSTM) → 2014 (GRU, Seq2Seq) | Hochreiter & Schmidhuber, Cho et al., Sutskever et al.\n\n---\n\n## 从上一章说起\n\n上一章我们解决了一个基础但关键的问题：如何将文本切分为模型可处理的单元。无论是BPE、WordPiece还是SentencePiece，Tokenizer的输出都是一个**token序列**。\n\n现在我们面临下一个问题：拿到这个序列后，该怎么办？\n\n回顾第3章的词向量。Word2Vec和GloVe为我们提供了将token映射为向量的方法。于是，一个句子\"I love NLP\"变成了三个向量的序列：$[\\mathbf{v}_\\text{I}, \\mathbf{v}_\\text{love}, \\mathbf{v}_\\text{NLP}]$。\n\n但这里有一个根本性的问题：**这些向量是独立的**。$\\mathbf{v}_\\text{love}$ 不知道它前面是\"I\"还是\"They\"，也不知道后面是\"NLP\"还是\"cats\"。每个词向量都是在真空中计算的。\n\n这在很多任务中是致命的。考虑这两个句子：\n\n- \"The **bank** of the river was muddy.\"\n- \"I went to the **bank** to deposit money.\"\n\n静态词向量给\"bank\"的表示是相同的，但它们的含义完全不同。模型需要看到上下文才能区分。\n\n更广泛地说，语言是**序列性**的。词的顺序至关重要——\"dog bites man\"和\"man bites dog\"含义截然不同。我们需要一种能够**处理序列、捕获上下文**的模型架构。\n\n这就引出了本章的主角：**循环神经网络（RNN）**及其变体。\n\n> 💡 **本章核心洞察**：RNN通过\"隐藏状态\"在时间维度上传递信息，让模型拥有了\"记忆\"。但这种记忆是有限的——长距离依赖会衰减，梯度会消失。LSTM和GRU通过门控机制缓解了这个问题，但并没有根本解决。Seq2Seq架构将RNN推向应用巅峰，也暴露了其终极瓶颈：所有信息必须压缩到一个固定长度的向量中。这个瓶颈将直接催生下一章的Attention机制。\n\n---\n\n## 问题的本质是什么？\n\n### 序列建模的核心挑战\n\n让我们精确地定义我们要解决的问题。\n\n给定一个输入序列 $\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$，我们希望模型能够：\n\n1. **理解上下文**：$x_t$ 的表示应该依赖于 $x_1, \\ldots, x_{t-1}$（以及可能的 $x_{t+1}, \\ldots, x_T$）\n2. **处理变长输入**：不同的序列可能有不同的长度 $T$\n3. **捕获长距离依赖**：$x_1$ 可能影响 $x_{100}$ 的理解\n4. **输出灵活**：可能输出单个标签（分类）、同长序列（标注）、或不同长度序列（翻译）\n\n传统的前馈神经网络（MLP）无法满足这些要求。MLP接受固定大小的输入，输出固定大小的结果，没有任何机制来处理序列结构。\n\n### 为什么不能简单地拼接词向量？\n\n一个直觉的想法是：把所有词向量拼接起来，送入一个大的MLP。\n\n$$\n\\mathbf{h} = \\text{MLP}([\\mathbf{v}_{x_1}; \\mathbf{v}_{x_2}; \\ldots; \\mathbf{v}_{x_T}])\n$$\n\n这个方案有三个致命问题：\n\n**第一，输入维度固定**。如果我们设计网络处理最长100个词，那么99个词的句子需要填充，101个词的句子无法处理。实际文本的长度变化极大——一条推文可能5个词，一篇论文可能5000个词。\n\n**第二，参数爆炸**。假设词向量维度是300，最大序列长度是1000，那么第一层的输入维度就是300,000。这意味着海量的参数和计算。\n\n**第三，没有位置共享**。模型需要单独学习\"第1个位置的'love'\"和\"第50个位置的'love'\"的含义，无法泛化位置信息。一个词在不同位置的处理方式应该是相似的。\n\n我们需要一种更优雅的方案：让模型**逐步处理序列**，在每一步累积信息，用有限的参数处理任意长度的输入。\n\n---\n\n## RNN：时间维度上的权重共享\n\n### 核心思想\n\nRNN的核心洞察是：**用同一组参数处理序列中的每一个位置，通过隐藏状态在时间步之间传递信息**。\n\n想象你在读一本小说。你不会把整本书一次性塞进大脑，而是一个词一个词地读。关键是，当你读到第100页时，你的大脑保留着前99页的\"记忆\"——虽然不是逐字记忆，而是一种压缩的、与当前阅读相关的状态。这个状态会影响你对第100页的理解。\n\nRNN就是这个过程的数学模型：\n\n$$\n\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{x}_t)\n$$\n\n其中 $\\mathbf{h}_t$ 是第 $t$ 步的**隐藏状态**（hidden state），它编码了到目前为止序列的所有信息。$f$ 是一个函数，它接收上一步的隐藏状态和当前的输入，产生新的隐藏状态。\n\n关键点在于：**$f$ 在每一个时间步都是相同的**。这就是\"时间维度上的权重共享\"——不管序列有多长，我们只需要一组参数。\n\n### 数学形式化\n\n::: {.callout-note}\n## Algorithm: Vanilla RNN Forward Pass (Elman, 1990)\n\n```python\ndef rnn_forward(x_sequence, h_0, W_hh, W_xh, b_h):\n    \"\"\"RNN 前向传播（处理整个序列）\"\"\"\n    h_t = h_0  # 初始隐藏状态，通常为零向量\n    hidden_states = []\n\n    for x_t in x_sequence:\n        # 核心公式：线性变换 + 非线性激活\n        h_t = tanh(W_hh @ h_t + W_xh @ x_t + b_h)\n        hidden_states.append(h_t)\n\n    return hidden_states  # 返回所有时间步的隐藏状态\n```\n\n*Reference: Elman (1990) \"Finding Structure in Time\", Cognitive Science 14(2):179-211*\n:::\n\n最简单的RNN（vanilla RNN）使用线性变换加非线性激活：\n\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h)\n$$\n\n其中：\n\n- $\\mathbf{W}_{hh} \\in \\mathbb{R}^{d_h \\times d_h}$：隐藏状态到隐藏状态的权重矩阵\n- $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d_h \\times d_x}$：输入到隐藏状态的权重矩阵\n- $\\mathbf{b}_h \\in \\mathbb{R}^{d_h}$：偏置向量\n- $\\tanh$：激活函数，将输出压缩到 $(-1, 1)$\n\n初始隐藏状态 $\\mathbf{h}_0$ 通常初始化为零向量。\n\n如果需要输出（比如每一步预测下一个词），可以加一个输出层：\n\n$$\n\\mathbf{y}_t = \\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y\n$$\n\n### 完整数值示例：RNN前向传播\n\n让我们用一个极简的例子走一遍RNN的计算过程。\n\n**设定**：\n\n- 输入序列：两个token，维度 $d_x = 2$\n- 隐藏状态维度：$d_h = 3$\n- 输入：$\\mathbf{x}_1 = [1, 0]^T$，$\\mathbf{x}_2 = [0, 1]^T$\n\n**参数**（简化的小数值）：\n\n$$\n\\mathbf{W}_{xh} = \\begin{bmatrix} 0.5 & 0.3 \\\\ 0.2 & 0.4 \\\\ 0.1 & 0.6 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hh} = \\begin{bmatrix} 0.1 & 0.2 & 0.1 \\\\ 0.3 & 0.1 & 0.2 \\\\ 0.2 & 0.3 & 0.1 \\end{bmatrix}, \\quad\n\\mathbf{b}_h = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\n\n**Step 1：初始化**\n\n$$\n\\mathbf{h}_0 = [0, 0, 0]^T\n$$\n\n**Step 2：处理第一个token**\n\n$$\n\\begin{aligned}\n\\mathbf{z}_1 &= \\mathbf{W}_{hh}\\mathbf{h}_0 + \\mathbf{W}_{xh}\\mathbf{x}_1 + \\mathbf{b}_h \\\\\n&= \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.5 \\cdot 1 + 0.3 \\cdot 0 \\\\ 0.2 \\cdot 1 + 0.4 \\cdot 0 \\\\ 0.1 \\cdot 1 + 0.6 \\cdot 0 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 0.5 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix}\n\\end{aligned}\n$$\n\n$$\n\\mathbf{h}_1 = \\tanh(\\mathbf{z}_1) = \\begin{bmatrix} \\tanh(0.5) \\\\ \\tanh(0.2) \\\\ \\tanh(0.1) \\end{bmatrix} \\approx \\begin{bmatrix} 0.462 \\\\ 0.197 \\\\ 0.100 \\end{bmatrix}\n$$\n\n**Step 3：处理第二个token**\n\n$$\n\\begin{aligned}\n\\mathbf{z}_2 &= \\mathbf{W}_{hh}\\mathbf{h}_1 + \\mathbf{W}_{xh}\\mathbf{x}_2 + \\mathbf{b}_h \\\\\n&= \\begin{bmatrix} 0.1 \\cdot 0.462 + 0.2 \\cdot 0.197 + 0.1 \\cdot 0.100 \\\\ 0.3 \\cdot 0.462 + 0.1 \\cdot 0.197 + 0.2 \\cdot 0.100 \\\\ 0.2 \\cdot 0.462 + 0.3 \\cdot 0.197 + 0.1 \\cdot 0.100 \\end{bmatrix} + \\begin{bmatrix} 0.3 \\\\ 0.4 \\\\ 0.6 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 0.046 + 0.039 + 0.010 + 0.3 \\\\ 0.139 + 0.020 + 0.020 + 0.4 \\\\ 0.092 + 0.059 + 0.010 + 0.6 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 0.395 \\\\ 0.579 \\\\ 0.761 \\end{bmatrix}\n\\end{aligned}\n$$\n\n$$\n\\mathbf{h}_2 = \\tanh(\\mathbf{z}_2) \\approx \\begin{bmatrix} 0.375 \\\\ 0.521 \\\\ 0.642 \\end{bmatrix}\n$$\n\n**解读**：$\\mathbf{h}_2$ 编码了整个序列 $[\\mathbf{x}_1, \\mathbf{x}_2]$ 的信息。注意它不仅受 $\\mathbf{x}_2$ 影响，也包含了 $\\mathbf{x}_1$ 通过 $\\mathbf{h}_1$ 传递过来的信息。\n\n### RNN的计算图与参数共享\n\n下图展示了RNN在时间维度上的\"展开\"（unrolling）：\n\n![RNN展开图：左侧是循环表示，右侧是展开后的时间步。每个时间步接收输入 $x_t$，产生隐藏状态 $h_t$。所有时间步共享同一组参数 A。](figures/chapter-5/original/fig-rnn-unrolled-colah.png){#fig-rnn-unrolled width=85%}\n\n::: {.figure-caption}\n*Source: Christopher Olah (2015) \"[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\"*\n:::\n\n关键观察：**所有\"RNN Cell\"共享相同的参数**（$\\mathbf{W}_{hh}, \\mathbf{W}_{xh}, \\mathbf{b}_h$）。这就是权重共享——无论序列有10个token还是1000个token，参数量都是固定的。\n\n另一种理解方式是把RNN\"展开\"（unroll）成时间步：展开后看起来像一个很深的前馈网络，但每一层用的是同一组参数。\n\n---\n\n## 梯度消失与梯度爆炸\n\n### 问题的本质\n\nRNN理论上可以捕获任意长距离的依赖——$\\mathbf{h}_{100}$ 包含了 $\\mathbf{x}_1$ 的信息（通过连续的传递）。但在实践中，这个\"理论上\"往往不成立。\n\n问题出在**梯度的传播**上。\n\n考虑损失函数 $\\mathcal{L}$ 对早期隐藏状态 $\\mathbf{h}_1$ 的梯度。根据链式法则：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_1} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_T} \\cdot \\frac{\\partial \\mathbf{h}_T}{\\partial \\mathbf{h}_{T-1}} \\cdot \\frac{\\partial \\mathbf{h}_{T-1}}{\\partial \\mathbf{h}_{T-2}} \\cdots \\frac{\\partial \\mathbf{h}_2}{\\partial \\mathbf{h}_1}\n$$\n\n关键在于这些连乘的雅可比矩阵 $\\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-1}}$。\n\n对于 vanilla RNN，我们有：\n\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h)\n$$\n\n因此：\n\n$$\n\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} = \\text{diag}(1 - \\mathbf{h}_t^2) \\cdot \\mathbf{W}_{hh}\n$$\n\n其中 $\\text{diag}(1 - \\mathbf{h}_t^2)$ 是 $\\tanh$ 的导数（因为 $\\tanh'(x) = 1 - \\tanh^2(x)$）。\n\n### 数学分析：为什么会消失或爆炸？\n\n当我们连乘 $T-1$ 个这样的雅可比矩阵时：\n\n$$\n\\prod_{t=2}^{T} \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} = \\prod_{t=2}^{T} \\text{diag}(1 - \\mathbf{h}_t^2) \\cdot \\mathbf{W}_{hh}\n$$\n\n简化分析：假设所有 $\\mathbf{h}_t$ 接近零（激活值小），那么 $1 - \\mathbf{h}_t^2 \\approx 1$，连乘大约是：\n\n$$\n\\mathbf{W}_{hh}^{T-1}\n$$\n\n现在，根据 $\\mathbf{W}_{hh}$ 的**特征值**分布：\n\n- 如果最大特征值 $|\\lambda_{\\max}| < 1$：$\\mathbf{W}_{hh}^{T-1} \\to 0$（指数衰减）→ **梯度消失**\n- 如果最大特征值 $|\\lambda_{\\max}| > 1$：$\\mathbf{W}_{hh}^{T-1} \\to \\infty$（指数增长）→ **梯度爆炸**\n\n实际情况更复杂，因为 $\\tanh$ 的导数在激活值大时趋近于0，这会**加剧梯度消失**。\n\n### 直觉理解：信息的\"衰减\"\n\n用一个类比来理解。想象你在玩\"传话游戏\"：第一个人说一句话，传给第二个人，第二个人传给第三个人……传100个人之后，原始信息还剩多少？\n\n在vanilla RNN中，信息每经过一个时间步，都要经过一次\"压缩和混合\"（矩阵乘法+非线性）。如果这个过程是\"有损\"的（信息被衰减），那么经过100步后，第1步的信息几乎完全消失。\n\n这就是为什么vanilla RNN无法捕获长距离依赖：不是模型没有\"记住\"早期信息的能力，而是梯度无法有效地流回早期时间步，导致模型学不到长距离的模式。\n\n### 梯度爆炸的简单修复：梯度裁剪\n\n梯度爆炸相对容易处理：当梯度的范数超过某个阈值时，按比例缩小。\n\n$$\n\\mathbf{g} \\leftarrow \\begin{cases}\n\\mathbf{g} & \\text{if } \\|\\mathbf{g}\\| \\leq \\theta \\\\\n\\frac{\\theta}{\\|\\mathbf{g}\\|} \\mathbf{g} & \\text{if } \\|\\mathbf{g}\\| > \\theta\n\\end{cases}\n$$\n\n这被称为**梯度裁剪**（gradient clipping），几乎是所有RNN训练的标配。\n\n但梯度消失没有这么简单的修复方法。我们需要更根本的架构改变。\n\n---\n\n## LSTM：门控机制的智慧\n\n### 核心洞察\n\n1997年，Hochreiter和Schmidhuber提出了**长短期记忆网络**（Long Short-Term Memory, LSTM）。他们的核心洞察是：\n\n> 问题不在于RNN没有记忆能力，而在于信息在时间步之间传递时被\"过度处理\"了。如果我们能让某些信息**不经修改地直接传递**，就能保留长距离依赖。\n\n这个想法的具体实现是**门控机制**：用可学习的\"门\"来控制信息的流动——哪些信息要保留，哪些要遗忘，哪些要更新。\n\n### 直觉：细胞状态作为\"传送带\"\n\nLSTM引入了一个新的概念：**细胞状态**（cell state）$\\mathbf{c}_t$，它像一条传送带一样在时间步之间传递。\n\n想象一个工厂的传送带：物品（信息）在传送带上移动。沿途有几个工作站：\n\n1. **遗忘门**：决定丢弃传送带上的哪些物品\n2. **输入门**：决定往传送带上放哪些新物品\n3. **输出门**：决定从传送带上取出哪些物品作为当前的输出\n\n关键在于：传送带本身的传递是**近乎恒等的**——没有被遗忘的信息可以不经修改地传到下游。这就避免了梯度消失问题。\n\n下图展示了LSTM的内部结构。与简单RNN只有一个tanh层不同，LSTM有**四个相互作用的层**（三个sigmoid门 + 一个tanh层）：\n\n![LSTM单元结构：黄色方框是神经网络层（σ = sigmoid，tanh = tanh），粉色圆圈是逐元素操作（× = 乘法，+ = 加法）。顶部的横线是细胞状态 $c_t$（\"传送带\"），底部的横线是隐藏状态 $h_t$。](figures/chapter-5/original/fig-lstm-chain-colah.png){#fig-lstm-chain width=90%}\n\n::: {.figure-caption}\n*Source: Christopher Olah (2015) \"[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\"*\n:::\n\n### 数学形式化\n\n::: {.callout-note}\n## Algorithm: LSTM Forward Pass (Hochreiter & Schmidhuber, 1997)\n\n```python\ndef lstm_forward(x_t, h_prev, c_prev, W_f, W_i, W_c, W_o):\n    \"\"\"LSTM 单步前向传播\"\"\"\n    concat = [h_prev, x_t]\n\n    # 遗忘门：决定丢弃多少旧的细胞状态\n    f_t = sigmoid(W_f @ concat)\n\n    # 输入门：决定写入多少新信息\n    i_t = sigmoid(W_i @ concat)\n    c_tilde = tanh(W_c @ concat)  # 候选细胞状态\n\n    # 更新细胞状态：遗忘旧信息 + 写入新信息\n    c_t = f_t * c_prev + i_t * c_tilde\n\n    # 输出门：决定输出多少细胞状态\n    o_t = sigmoid(W_o @ concat)\n    h_t = o_t * tanh(c_t)\n\n    return h_t, c_t\n```\n\n*Source: Hochreiter & Schmidhuber (1997) \"Long Short-Term Memory\", Neural Computation 9(8):1735-1780*\n:::\n\nLSTM在每个时间步计算以下量：\n\n**遗忘门**：决定遗忘多少旧的细胞状态\n\n$$\n\\mathbf{f}_t = \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f)\n$$\n\n**输入门**：决定写入多少新信息\n\n$$\n\\mathbf{i}_t = \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i)\n$$\n\n**候选细胞状态**：新信息的候选值\n\n$$\n\\tilde{\\mathbf{c}}_t = \\tanh(\\mathbf{W}_c [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_c)\n$$\n\n**细胞状态更新**：\n\n$$\n\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t\n$$\n\n**输出门**：决定输出多少细胞状态\n\n$$\n\\mathbf{o}_t = \\sigma(\\mathbf{W}_o [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o)\n$$\n\n**隐藏状态**：\n\n$$\n\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n$$\n\n其中 $\\sigma$ 是sigmoid函数（输出在 $(0, 1)$），$\\odot$ 是逐元素乘法。\n\n### 为什么LSTM解决了梯度消失？\n\n关键在于细胞状态的更新公式：\n\n$$\n\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t\n$$\n\n考虑梯度 $\\frac{\\partial \\mathbf{c}_t}{\\partial \\mathbf{c}_{t-1}}$：\n\n$$\n\\frac{\\partial \\mathbf{c}_t}{\\partial \\mathbf{c}_{t-1}} = \\text{diag}(\\mathbf{f}_t) + \\text{其他项}\n$$\n\n如果遗忘门 $\\mathbf{f}_t$ 接近1，那么 $\\frac{\\partial \\mathbf{c}_t}{\\partial \\mathbf{c}_{t-1}} \\approx \\mathbf{I}$（单位矩阵）。\n\n这意味着：**梯度可以几乎不衰减地流过多个时间步**。只要遗忘门学习到保持信息（$\\mathbf{f}_t \\approx 1$），早期时间步的信息就能对后期产生影响，模型也能学到长距离依赖。\n\n这和ResNet中残差连接的原理非常类似：提供一条\"高速公路\"让梯度直接流动。\n\n### 完整数值示例：LSTM前向传播\n\n**设定**：\n\n- 输入维度 $d_x = 2$，隐藏维度 $d_h = 2$\n- 输入序列：$\\mathbf{x}_1 = [1, 0]^T$，$\\mathbf{x}_2 = [0, 1]^T$\n- 初始状态：$\\mathbf{h}_0 = [0, 0]^T$，$\\mathbf{c}_0 = [0, 0]^T$\n\n**简化参数**（为便于计算，使用小值）：\n\n$$\n\\mathbf{W}_f = \\mathbf{W}_i = \\mathbf{W}_c = \\mathbf{W}_o = \\begin{bmatrix} 0.5 & 0.5 & 0.3 & 0.3 \\\\ 0.3 & 0.3 & 0.5 & 0.5 \\end{bmatrix}\n$$\n\n所有偏置为零。\n\n**Step 1：处理 $\\mathbf{x}_1$**\n\n拼接输入：$[\\mathbf{h}_0, \\mathbf{x}_1] = [0, 0, 1, 0]^T$\n\n计算各个门（省略详细矩阵乘法）：\n\n$$\n\\begin{aligned}\n\\mathbf{f}_1 &= \\sigma([0.3, 0.5]^T) = [0.574, 0.622]^T \\\\\n\\mathbf{i}_1 &= \\sigma([0.3, 0.5]^T) = [0.574, 0.622]^T \\\\\n\\tilde{\\mathbf{c}}_1 &= \\tanh([0.3, 0.5]^T) = [0.291, 0.462]^T \\\\\n\\mathbf{o}_1 &= \\sigma([0.3, 0.5]^T) = [0.574, 0.622]^T\n\\end{aligned}\n$$\n\n更新细胞状态：\n\n$$\n\\mathbf{c}_1 = \\mathbf{f}_1 \\odot \\mathbf{c}_0 + \\mathbf{i}_1 \\odot \\tilde{\\mathbf{c}}_1 = [0, 0] + [0.167, 0.287]^T = [0.167, 0.287]^T\n$$\n\n计算隐藏状态：\n\n$$\n\\mathbf{h}_1 = \\mathbf{o}_1 \\odot \\tanh(\\mathbf{c}_1) = [0.574, 0.622] \\odot [0.165, 0.279] = [0.095, 0.174]^T\n$$\n\n**Step 2：处理 $\\mathbf{x}_2$**\n\n拼接输入：$[\\mathbf{h}_1, \\mathbf{x}_2] = [0.095, 0.174, 0, 1]^T$\n\n（类似计算，省略细节）\n\n最终得到 $\\mathbf{h}_2$ 和 $\\mathbf{c}_2$，编码了整个序列的信息。\n\n**关键观察**：$\\mathbf{c}_1$ 中的信息通过遗忘门控制，部分保留到 $\\mathbf{c}_2$。如果 $\\mathbf{f}_2 \\approx 1$，则 $\\mathbf{x}_1$ 的信息几乎完整传递。\n\n---\n\n## GRU：门控机制的简化\n\n### 动机\n\nLSTM有4个门、两个状态（$\\mathbf{h}$ 和 $\\mathbf{c}$），参数量较大。2014年，Cho等人提出了**门控循环单元**（Gated Recurrent Unit, GRU），用更少的参数达到相似的效果。\n\nGRU的核心简化：\n\n1. **合并细胞状态和隐藏状态**：只保留一个状态 $\\mathbf{h}$\n2. **合并遗忘门和输入门**：用一个\"更新门\"同时控制遗忘和写入\n\n### GRU 单元结构\n\n![GRU hidden unit 结构图：update gate (z) 控制保留多少旧状态，reset gate (r) 控制如何将旧状态与新输入结合。](figures/chapter-5/original/fig-gru-cell.png){#fig-gru-cell width=60%}\n\n::: {.figure-caption}\n*Source: Cho et al. (2014) \"Learning Phrase Representations using RNN Encoder-Decoder\", Figure 2. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)*\n:::\n\n### 数学形式化\n\n::: {.callout-note}\n## Algorithm: GRU Forward Pass (Cho et al., 2014)\n\n```python\ndef gru_forward(x_t, h_prev, W_r, W_z, W_h):\n    \"\"\"GRU 单步前向传播\"\"\"\n    # 重置门：决定忽略多少旧状态\n    r_t = sigmoid(W_r @ concat(h_prev, x_t))\n\n    # 更新门：决定保留多少旧状态\n    z_t = sigmoid(W_z @ concat(h_prev, x_t))\n\n    # 候选状态：用重置门过滤旧状态后，与新输入结合\n    h_tilde = tanh(W_h @ concat(r_t * h_prev, x_t))\n\n    # 最终状态：旧状态与候选状态的插值\n    h_t = z_t * h_prev + (1 - z_t) * h_tilde\n\n    return h_t\n```\n\n*Adapted from: Cho et al. (2014) Equations 5-8. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)*\n:::\n\n**重置门**：决定如何将过去的信息与新输入结合\n\n$$\n\\mathbf{r}_t = \\sigma(\\mathbf{W}_r [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_r)\n$$\n\n**更新门**：决定保留多少旧状态\n\n$$\n\\mathbf{z}_t = \\sigma(\\mathbf{W}_z [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_z)\n$$\n\n**候选隐藏状态**：\n\n$$\n\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_h [\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_h)\n$$\n\n**隐藏状态更新**：\n\n$$\n\\mathbf{h}_t = (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t\n$$\n\n### LSTM vs GRU\n\n| 方面 | LSTM | GRU |\n|------|------|-----|\n| **状态数量** | 2（$\\mathbf{h}$, $\\mathbf{c}$） | 1（$\\mathbf{h}$） |\n| **门数量** | 3（遗忘、输入、输出） | 2（重置、更新） |\n| **参数量** | 较多 | 较少（约75%） |\n| **性能** | 通常略好 | 相当，有时更好 |\n| **训练速度** | 较慢 | 较快 |\n\n实践中的选择：\n\n- **数据量大、任务复杂**：LSTM可能略有优势\n- **计算资源有限、需要快速迭代**：GRU是更好的选择\n- **很多情况下差异不大**：先用GRU快速验证，必要时换LSTM\n\n---\n\n## Seq2Seq：Encoder-Decoder架构\n\n### 核心问题：变长到变长的映射\n\n之前我们讨论的是\"理解\"任务——输入一个序列，输出一个向量（分类）或同长度序列（标注）。但很多重要任务需要**输入和输出都是序列，且长度不同**：\n\n- **机器翻译**：\"I love NLP\" → \"我喜欢自然语言处理\"（3词 → 6词）\n- **文本摘要**：长文章 → 短摘要\n- **对话生成**：问题 → 回答\n\n这就需要一种新的架构：**Seq2Seq**（Sequence-to-Sequence），也称为Encoder-Decoder架构。\n\n### 架构设计\n\nSeq2Seq由两个RNN组成：\n\n1. **编码器（Encoder）**：读取输入序列，将其压缩为一个固定长度的向量\n2. **解码器（Decoder）**：接收这个向量，生成输出序列\n\n![RNN Encoder-Decoder 架构：Encoder 将输入序列 $(x_1, ..., x_T)$ 编码为上下文向量 $c$，Decoder 从 $c$ 生成输出序列 $(y_1, ..., y_{T'})$。](figures/chapter-5/original/fig-encoder-decoder.png){#fig-encoder-decoder width=50%}\n\n::: {.figure-caption}\n*Source: Cho et al. (2014) \"Learning Phrase Representations using RNN Encoder-Decoder\", Figure 1. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)*\n:::\n\n下图展示了 Sutskever 等人的经典 Seq2Seq 架构，注意**输入序列是反转的**（ABC 读取为 CBA），这是他们发现的一个有效技巧：\n\n![Seq2Seq 架构（Sutskever et al., 2014）：输入 \"ABC\" 被反转后读取，输出 \"WXYZ\"。模型在输出 `<EOS>` 后停止生成。](figures/chapter-5/original/fig-seq2seq.png){#fig-seq2seq width=80%}\n\n::: {.figure-caption}\n*Source: Sutskever et al. (2014) \"Sequence to Sequence Learning with Neural Networks\", Figure 1. [arXiv:1409.3215](https://arxiv.org/abs/1409.3215)*\n:::\n\n**编码过程**：\n\n$$\n\\mathbf{h}_t^{enc} = \\text{LSTM}_{enc}(\\mathbf{h}_{t-1}^{enc}, \\mathbf{x}_t)\n$$\n\n$$\n\\mathbf{c} = \\mathbf{h}_T^{enc} \\quad \\text{（最后一步的隐藏状态作为上下文向量）}\n$$\n\n**解码过程**：\n\n$$\n\\mathbf{h}_t^{dec} = \\text{LSTM}_{dec}(\\mathbf{h}_{t-1}^{dec}, \\mathbf{y}_{t-1})\n$$\n\n$$\nP(\\mathbf{y}_t | \\mathbf{y}_{<t}, \\mathbf{x}) = \\text{softmax}(\\mathbf{W}_o \\mathbf{h}_t^{dec})\n$$\n\n解码器的初始隐藏状态设为上下文向量：$\\mathbf{h}_0^{dec} = \\mathbf{c}$。\n\n### 训练：Teacher Forcing\n\n在训练时，我们使用**teacher forcing**：解码器的每一步输入是**真实的前一个词**（ground truth），而不是模型预测的词。\n\n$$\n\\mathbf{h}_t^{dec} = \\text{LSTM}_{dec}(\\mathbf{h}_{t-1}^{dec}, \\mathbf{y}_{t-1}^{*})\n$$\n\n其中 $\\mathbf{y}_{t-1}^{*}$ 是真实的第 $t-1$ 个词。\n\n这样做的原因是：如果用模型预测的词，早期的错误会累积，导致训练不稳定。\n\n但这也带来一个问题：训练和推理时的分布不一致（**exposure bias**）。训练时解码器总是看到正确的前缀，推理时却要依赖自己的预测。\n\n### 推理：贪心搜索与束搜索\n\n在推理时，我们需要生成输出序列。两种常见策略：\n\n**贪心搜索**：每一步选择概率最高的词\n\n$$\n\\hat{y}_t = \\arg\\max_y P(y | \\hat{y}_{<t}, \\mathbf{x})\n$$\n\n简单快速，但不保证全局最优。\n\n**束搜索（Beam Search）**：维护 $k$ 个候选序列，每一步扩展所有候选，保留得分最高的 $k$ 个。\n\n这是一种在搜索空间和计算成本之间的权衡。实践中 $k=4$ 或 $k=5$ 通常足够。\n\n---\n\n## 信息瓶颈：Seq2Seq的终极局限\n\n### 问题描述\n\n回顾Seq2Seq的核心假设：整个输入序列的信息被压缩到**一个固定长度的向量** $\\mathbf{c}$ 中。\n\n这意味着，无论输入是5个词还是500个词，都要塞进同一个维度的向量。\n\n想象一下：你要把一本500页的书的所有信息压缩成一个1024维的向量，然后仅凭这个向量翻译成另一种语言。这能行吗？\n\n实验证据也验证了这个担忧。Sutskever等人(2014)在机器翻译任务上发现：当输入句子超过20个词时，翻译质量急剧下降。\n\n### 信息论视角\n\n从信息论角度，这个问题可以精确表述。\n\n假设输入序列 $\\mathbf{x}$ 的信息熵是 $H(\\mathbf{x})$。上下文向量 $\\mathbf{c}$ 是 $\\mathbf{x}$ 的一个有损压缩。根据数据处理不等式：\n\n$$\nI(\\mathbf{y}; \\mathbf{c}) \\leq I(\\mathbf{y}; \\mathbf{x})\n$$\n\n其中 $I$ 是互信息。也就是说，$\\mathbf{c}$ 中关于 $\\mathbf{y}$ 的信息不可能超过 $\\mathbf{x}$ 中的。\n\n如果 $\\mathbf{c}$ 的维度是 $d$，它最多携带 $O(d \\log |\\mathcal{V}|)$ 比特的信息（$|\\mathcal{V}|$ 是取值范围）。当输入序列很长时，必然有信息丢失。\n\n### 一个思考实验\n\n考虑机器翻译任务。输入是英语句子，输出是法语句子。\n\n假设我们要翻译一个很长的句子，其中第100个词是一个人名\"Claude\"。在输出的法语句子中，这个名字应该保持不变。\n\n但根据Seq2Seq的架构，\"Claude\"这个词首先要编码进隐藏状态 $\\mathbf{h}_{100}^{enc}$，然后经过剩下的编码步骤，最终压缩到上下文向量 $\\mathbf{c}$ 中，再由解码器提取出来。\n\n在这个过程中，\"Claude\"的信息可能与其他词的信息混合、被覆盖、或因梯度消失而无法有效学习。\n\n**我们需要的是什么？**\n\n理想情况下，当解码器生成这个名字时，它应该能够**直接\"看\"到编码器中对应的位置**，而不是只依赖一个压缩后的向量。\n\n这正是Attention机制的核心思想——**让解码器在每一步都能访问编码器的所有隐藏状态**，而不是只有最后一个。\n\n> 这个洞察将在下一章详细展开。Seq2Seq + Attention的组合，将RNN推向了最后的辉煌，也为后来完全抛弃RNN的Transformer铺平了道路。\n\n---\n\n## 工程实践：LSTM文本分类\n\n让我们用PyTorch实现一个简单的LSTM文本分类器，体会RNN的实际应用。\n\n### 模型定义\n\n::: {#75cabcf9 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=1, dropout=0.5):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(\n            embed_dim,\n            hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,  # 双向LSTM\n            dropout=dropout if num_layers > 1 else 0\n        )\n        # 双向LSTM输出维度是 hidden_dim * 2\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, lengths=None):\n        # x: [batch_size, seq_len]\n        embedded = self.dropout(self.embedding(x))  # [batch_size, seq_len, embed_dim]\n\n        # LSTM前向传播\n        if lengths is not None:\n            # 使用pack_padded_sequence处理变长序列\n            packed = nn.utils.rnn.pack_padded_sequence(\n                embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n            )\n            packed_output, (hidden, cell) = self.lstm(packed)\n        else:\n            output, (hidden, cell) = self.lstm(embedded)\n\n        # hidden: [num_layers * 2, batch_size, hidden_dim] (双向)\n        # 取最后一层的前向和后向隐藏状态\n        hidden_forward = hidden[-2, :, :]  # [batch_size, hidden_dim]\n        hidden_backward = hidden[-1, :, :]  # [batch_size, hidden_dim]\n        hidden_concat = torch.cat([hidden_forward, hidden_backward], dim=1)  # [batch_size, hidden_dim * 2]\n\n        output = self.fc(self.dropout(hidden_concat))  # [batch_size, num_classes]\n        return output\n\n# 示例：创建模型\nmodel = LSTMClassifier(\n    vocab_size=10000,\n    embed_dim=128,\n    hidden_dim=256,\n    num_classes=2\n)\nprint(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n模型参数量: 2,071,554\n```\n:::\n:::\n\n\n### 关键实现细节\n\n**1. 双向LSTM**\n\n使用 `bidirectional=True`，模型同时从左到右和从右到左处理序列，输出维度翻倍。这对于分类任务很有用——一个词的语义既依赖前文也依赖后文。\n\n**2. 变长序列处理**\n\n实际文本长度不一，需要填充（padding）到相同长度。`pack_padded_sequence` 和 `pad_packed_sequence` 让LSTM忽略填充位置，避免污染隐藏状态。\n\n**3. 分类策略**\n\n对于分类任务，我们用最后一个时间步的隐藏状态（或所有时间步的平均）作为序列的表示，送入分类层。\n\n### 训练技巧\n\n```python\n# 梯度裁剪（防止梯度爆炸）\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n# 学习率调度\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=2\n)\n```\n\n---\n\n## 深入理解\n\n### 为什么LSTM有效？——更深入的理论视角\n\nLSTM的成功不仅仅是\"门控机制\"这个技巧。从更深的角度看：\n\n**1. 常微分方程视角**\n\n可以把RNN看作离散化的常微分方程：\n\n$$\n\\frac{d\\mathbf{h}}{dt} = f(\\mathbf{h}, \\mathbf{x})\n$$\n\nVanilla RNN对应简单的欧拉方法，而LSTM的细胞状态更新：\n\n$$\n\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t\n$$\n\n可以看作一种**自适应步长**的数值积分——遗忘门控制\"衰减率\"，输入门控制\"增量\"。\n\n**2. 记忆与计算的分离**\n\nLSTM将\"记忆\"（$\\mathbf{c}$）和\"计算\"（$\\mathbf{h}$）分离。细胞状态是长期记忆的载体，隐藏状态是当前的工作记忆。这种分离让模型能够同时保持长期信息和进行复杂的当前计算。\n\n**3. 可学习的遗忘**\n\n遗忘门的引入是关键创新。直觉上，保持信息似乎总是好的。但实际上，选择性遗忘同样重要——不相关的信息会干扰有用的信息。遗忘门让模型学习什么时候清除旧信息。\n\n### 边界条件与失效模式\n\nLSTM并不完美。以下是它的已知局限：\n\n**1. 顺序计算**\n\nLSTM必须按顺序处理序列，无法并行。这在GPU时代是严重的效率瓶颈。一个1000步的序列需要1000次串行计算。\n\n**2. 长距离仍有衰减**\n\n虽然比vanilla RNN好很多，但LSTM在极长序列（>1000步）上仍然会丢失信息。遗忘门不可能永远是1——那样模型就没有\"忘记\"的能力。\n\n**3. 缺乏显式的位置信息**\n\nLSTM通过顺序处理隐式编码位置，但无法像后来的位置编码那样精确地表示绝对或相对位置。\n\n### 开放研究问题\n\n1. **最优门控结构**：LSTM和GRU的门控设计是启发式的，是否存在理论上最优的结构？\n2. **长度泛化**：在短序列上训练的模型能否泛化到更长的序列？\n3. **RNN与Transformer的融合**：是否可以结合RNN的归纳偏置和Transformer的并行性？（如线性RNN、Mamba等近期工作）\n\n---\n\n## 局限性与展望\n\n### 本章方法的核心局限\n\n**1. 信息瓶颈（最严重）**\n\nSeq2Seq将整个输入压缩到一个固定向量，导致：\n- 长序列信息丢失\n- 无法回溯查看输入的特定位置\n\n**2. 顺序计算瓶颈**\n\nRNN必须按时间步串行计算，无法利用现代GPU的并行能力。\n\n**3. 梯度消失/爆炸（部分缓解）**\n\nLSTM/GRU缓解但未根本解决。极长序列仍有问题。\n\n### 这些局限指向什么？\n\n信息瓶颈问题促使研究者思考：**解码器是否必须只看一个向量？能否让它在每一步都访问编码器的所有状态？**\n\n这个问题的答案是**Attention机制**——下一章的主题。\n\nAttention最初被设计为Seq2Seq的补充，让解码器能够\"注意\"编码器的不同位置。它将RNN推向了最后的辉煌。\n\n但更具革命性的发现是：**如果Attention足够强大，我们是否还需要RNN？**\n\n这个问题的答案是Transformer——完全抛弃循环结构，用纯Attention建模序列。这将在第8章详细展开。\n\n---\n\n## 本章小结\n\n::: {.callout-important}\n## 核心要点\n\n- **RNN的核心思想**：用共享参数的循环结构处理序列，通过隐藏状态在时间步之间传递信息\n- **梯度消失问题**：vanilla RNN的梯度在时间维度上指数衰减/爆炸，无法学习长距离依赖\n- **LSTM的解决方案**：通过门控机制（遗忘门、输入门、输出门）和细胞状态，让信息可以不衰减地传递\n- **GRU的简化**：用更少的参数（重置门、更新门）达到类似效果\n- **Seq2Seq架构**：用Encoder-Decoder结构处理变长到变长的映射\n- **信息瓶颈**：Seq2Seq将整个输入压缩到一个向量，导致长序列信息丢失——这个问题催生了Attention机制\n:::\n\n### 关键公式速查\n\n**Vanilla RNN**：\n\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h)\n$$\n\n**LSTM**：\n\n$$\n\\begin{aligned}\n\\mathbf{f}_t &= \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f) \\\\\n\\mathbf{i}_t &= \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i) \\\\\n\\tilde{\\mathbf{c}}_t &= \\tanh(\\mathbf{W}_c [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_c) \\\\\n\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t \\\\\n\\mathbf{o}_t &= \\sigma(\\mathbf{W}_o [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o) \\\\\n\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n\\end{aligned}\n$$\n\n**GRU**：\n\n$$\n\\begin{aligned}\n\\mathbf{r}_t &= \\sigma(\\mathbf{W}_r [\\mathbf{h}_{t-1}, \\mathbf{x}_t]) \\\\\n\\mathbf{z}_t &= \\sigma(\\mathbf{W}_z [\\mathbf{h}_{t-1}, \\mathbf{x}_t]) \\\\\n\\tilde{\\mathbf{h}}_t &= \\tanh(\\mathbf{W}_h [\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}, \\mathbf{x}_t]) \\\\\n\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t\n\\end{aligned}\n$$\n\n---\n\n## 思考题\n\n1. **[概念理解]** 为什么说RNN实现了\"时间维度上的权重共享\"？这与CNN中的空间权重共享有什么异同？\n\n2. **[数学推导]** 证明：如果 $\\mathbf{W}_{hh}$ 的所有特征值的绝对值都小于1，那么 $\\mathbf{W}_{hh}^T \\to \\mathbf{0}$ 当 $T \\to \\infty$。这如何解释vanilla RNN的梯度消失？\n\n3. **[工程实践]** 在PyTorch中，`nn.LSTM` 的 `hidden_size` 和 `num_layers` 参数如何影响模型的容量和计算成本？如果要增加模型容量，增加 `hidden_size` 和增加 `num_layers` 哪个更有效？\n\n4. **[批判思考]** GRU的更新公式 $\\mathbf{h}_t = (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t$ 可以看作是旧状态和新候选状态的插值。这个设计隐含了什么假设？有什么局限性？\n\n5. **[开放问题]** Seq2Seq的信息瓶颈问题有没有其他解决方案，不使用Attention？（提示：考虑使用多个向量表示输入，或使用记忆网络）\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n- **[Hochreiter & Schmidhuber, 1997] Long Short-Term Memory**\n  - LSTM的原始论文，提出门控机制\n  - 重点读：Section 3（LSTM架构）、Section 4（为什么能解决梯度消失）\n\n- **[Cho et al., 2014] Learning Phrase Representations using RNN Encoder-Decoder**\n  - 提出GRU和Encoder-Decoder架构\n  - 重点读：Section 3（GRU公式）、Section 2.2（Encoder-Decoder）\n\n- **[Sutskever et al., 2014] Sequence to Sequence Learning with Neural Networks**\n  - Seq2Seq在机器翻译上的突破性工作\n  - 重点读：Section 3.4（反转输入技巧）、实验结果\n\n### 理论基础\n\n- **[Bengio et al., 1994] Learning Long-Term Dependencies with Gradient Descent is Difficult**\n  - 梯度消失问题的理论分析\n  - 重点读：理论证明部分\n\n### 后续发展\n\n- **[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**\n  - 提出Attention机制，解决信息瓶颈问题\n  - 这是下一章的核心论文\n\n- **[Gu et al., 2023] Mamba: Linear-Time Sequence Modeling with Selective State Spaces**\n  - 结合RNN和Transformer优点的最新尝试\n  - 重点读：与Transformer的对比分析\n\n",
    "supporting": [
      "ch05-rnn-lstm_files"
    ],
    "filters": [],
    "includes": {}
  }
}