{
  "hash": "5683b111e11a16a84df6fb301b7feef0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第3章：表示学习的觉醒\"\nsubtitle: \"从离散符号到分布式语义\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-25\"\ncategories: [NLP, Word2Vec, GloVe, FastText, Representation Learning]\ntags: [词向量, Skip-gram, CBOW, 分布式语义, 负采样, 表示学习]\ndescription: \"从离散符号到分布式语义：Word2Vec、GloVe、FastText如何让机器理解词义，以及静态词向量的根本局限。\"\nimage: \"figures/chapter-3/fig3-cbow-vs-skipgram.png\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\n---\n\n> **核心问题**：如何让计算机理解词的\"含义\"？如何用数学表示\"猫\"和\"狗\"比\"猫\"和\"民主\"更相似？\n>\n> **历史坐标**：2013 | Mikolov et al. (Word2Vec) | 从特征工程到表示学习的范式转变\n\n---\n\n## 从上一章说起\n\n在第1章中，我们回顾了前深度学习时代的NLP——那是一个特征工程的时代，研究者需要手工设计特征，把原始文本转换成机器学习模型可以处理的数值向量。第2章则鸟瞰了NLP的核心任务全景，从文本分类到机器翻译、从序列标注到问答系统。我们看到了传统方法的成功，但也看到了它们共同面临的瓶颈——特征工程不可扩展，每个新任务都需要重新设计特征，专家的时间成为了整个领域的天花板。\n\n更根本的问题是：传统方法把词当作离散的符号来处理。在one-hot编码中，\"cat\"是一个维度上的1，\"dog\"是另一个维度上的1，它们之间没有任何数学关系。模型无法知道\"cat\"和\"dog\"都是动物、都是宠物、都有四条腿。这种离散表示无法捕获语义相似性，导致泛化困难——一个在\"猫追老鼠\"上训练的模型，无法自动迁移到\"狗追松鼠\"。\n\n这就引出了一个核心问题：能不能让词的表示本身就蕴含语义信息？让语义相似的词，在数学上也彼此接近？\n\n2013年，Google的研究员Tomas Mikolov给出了一个惊人简洁的答案。\n\n> 💡 **本章核心洞察**：词的含义可以从它的\"邻居\"中学习——\"You shall know a word by the company it keeps\"。通过预测上下文（或被上下文预测），模型可以自动学习词的分布式表示，让语义相似的词在向量空间中彼此接近。\n\n---\n\n## 问题的本质是什么？\n\n### 表示问题的精确定义\n\n让我们先精确地定义问题。我们希望找到一个映射函数：\n\n$$\nf: \\mathcal{V} \\rightarrow \\mathbb{R}^d\n$$\n\n把词汇表$\\mathcal{V}$中的每个词映射到一个$d$维的实数向量。这个映射应该满足一个关键性质：**语义相似的词，映射后的向量也相似**。\n\n用数学语言说，如果$w_1$和$w_2$语义相似，那么$f(w_1)$和$f(w_2)$的余弦相似度应该高：\n\n$$\n\\text{similarity}(w_1, w_2) \\propto \\cos(f(w_1), f(w_2)) = \\frac{f(w_1) \\cdot f(w_2)}{\\|f(w_1)\\| \\|f(w_2)\\|}\n$$\n\n更进一步，我们希望这种表示能够捕获语义关系。比如，\"国王\"之于\"女王\"的关系，应该类似于\"男人\"之于\"女人\"。这意味着向量空间中应该存在某种规律性的结构。\n\n### 之前的尝试为何失败？\n\n在Word2Vec之前，也有人尝试过学习词的分布式表示，但效果不尽如人意。\n\n**One-hot编码**是最简单的表示方法。如果词汇表大小是$|\\mathcal{V}|$，那么每个词都是一个$|\\mathcal{V}|$维的向量，只有对应位置是1，其他都是0。这种表示的问题是显而易见的：任意两个词的向量都是正交的，$\\cos(\\text{cat}, \\text{dog}) = 0$，没有任何相似性信息。\n\n**共现矩阵**是一个更有意思的尝试。思路是：如果两个词经常在相似的上下文中出现，它们可能是相似的。可以构建一个词-词共现矩阵$X$，其中$X_{ij}$表示词$i$和词$j$在窗口内共同出现的次数。然后对这个矩阵做SVD分解，取前$d$个奇异向量作为词的表示。这种方法确实能捕获一些语义信息，但有几个问题：矩阵太大（$|\\mathcal{V}| \\times |\\mathcal{V}|$可能有数十亿元素），SVD计算昂贵，而且新词需要重新计算整个分解。\n\n**神经语言模型**（Bengio et al., 2003）是最接近现代方法的先驱。他们用神经网络训练语言模型，词的embedding作为模型的第一层被一起学习。这个想法很正确，但当时的计算能力有限，模型很小，无法在大规模语料上训练，因此效果受限。\n\n### 我们需要什么样的解决方案？\n\n回顾这些尝试，理想的解决方案应该具备以下特性。\n\n首先是**可扩展性**。方法应该能在大规模语料（数十亿词）上高效训练。这意味着不能依赖全局矩阵分解，需要能够用随机梯度下降增量式地学习。\n\n其次是**语义保持**。学到的表示应该自然地捕获语义相似性，不需要额外的语义知识库。\n\n第三是**泛化能力**。表示应该在多种下游任务中有用——情感分析、命名实体识别、问答系统...一次训练，到处使用。\n\n最后是**可解释性**。最好能够理解表示学到了什么，向量的不同维度是否有明确的语义。\n\nWord2Vec惊人地同时满足了前三个要求，并在某种程度上满足了第四个——它发现了著名的\"词向量类比\"现象。\n\n---\n\n## 核心思想与直觉\n\n### 分布式假设：语境定义语义\n\nWord2Vec的核心洞察来自语言学中的**分布式假设**（Distributional Hypothesis）：\n\n> \"You shall know a word by the company it keeps.\" — J.R. Firth, 1957\n\n一个词的含义，可以从它出现的上下文中推断出来。考虑两个你可能不熟悉的词：\n\n- \"The **glorp** chased the mouse across the kitchen.\"\n- \"I fed my **glorp** some fish and it purred happily.\"\n\n即使不知道\"glorp\"的定义，你也能推断它可能是某种猫科动物。因为它出现在\"chased mouse\"、\"fed fish\"、\"purred\"这样的上下文中，而这些上下文通常与猫相关。\n\n这个洞察的数学含义是：如果两个词经常出现在相似的上下文中，它们应该有相似的向量表示。Word2Vec正是利用这一点来学习词向量。\n\n### 两种学习范式：CBOW与Skip-gram\n\nWord2Vec提出了两种学习词向量的方法，它们是镜像对称的。\n\n**CBOW（Continuous Bag of Words）**：给定上下文词，预测中心词。比如，给定\"The cat ___ on the mat\"中的上下文词，预测中间的词是\"sat\"。\n\n**Skip-gram**：给定中心词，预测上下文词。比如，给定\"sat\"，预测它周围可能出现\"cat\"、\"on\"、\"mat\"等词。\n\n直觉上，CBOW是\"多个线索猜一个词\"，而Skip-gram是\"一个词猜多个线索\"。实践中，Skip-gram在处理罕见词时效果更好，因为每个罕见词都有多个预测机会（需要预测多个上下文词），获得更多的训练信号。这也是为什么Skip-gram在后来的研究中更常被使用。\n\n![CBOW 与 Skip-gram 架构对比：CBOW 从上下文预测中心词，Skip-gram 从中心词预测上下文。两者是镜像对称的。](figures/chapter-3/fig3-cbow-vs-skipgram.png){#fig-cbow-skipgram width=90%}\n\n::: {.figure-caption}\n*自绘示意图，基于 Mikolov et al. (2013) \"Efficient Estimation of Word Representations in Vector Space\". [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)*\n:::\n\n### 一个直觉性的例子\n\n让我们用一个简化的例子来建立直觉。假设语料库是：\n\n- \"I love cats\"\n- \"I love dogs\"\n- \"cats are cute\"\n- \"dogs are loyal\"\n\n如果我们用Skip-gram训练，以\"love\"为中心词（窗口大小=1），需要预测\"I\"和\"cats\"（来自第一句）以及\"I\"和\"dogs\"（来自第二句）。\n\n训练过程会调整词向量，让\"love\"的向量能够同时预测\"cats\"和\"dogs\"。这意味着\"cats\"和\"dogs\"需要在向量空间中有相似的位置——因为它们都经常出现在\"love\"附近。\n\n类似地，\"cats\"需要预测\"are\"和\"cute\"，\"dogs\"需要预测\"are\"和\"loyal\"。它们都需要预测\"are\"，这进一步拉近了它们在向量空间中的距离。\n\n通过这种方式，即使\"cats\"和\"dogs\"从未在同一个句子中共同出现，模型也能学习到它们的相似性——因为它们出现在相似的上下文模式中。\n\n---\n\n## 技术细节\n\n### Skip-gram模型的数学形式\n\n让我们详细推导Skip-gram模型。设词汇表大小为$|\\mathcal{V}|$，嵌入维度为$d$。模型有两套词向量：\n\n- **中心词向量**（Center word embeddings）：$\\mathbf{v}_w \\in \\mathbb{R}^d$，用于词$w$作为中心词时\n- **上下文词向量**（Context word embeddings）：$\\mathbf{u}_w \\in \\mathbb{R}^d$，用于词$w$作为上下文词时\n\n给定中心词$w_c$和上下文词$w_o$，我们用softmax定义条件概率：\n\n$$\nP(w_o | w_c) = \\frac{\\exp(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c})}{\\sum_{w \\in \\mathcal{V}} \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{w_c})}\n$$\n\n直觉是：如果$\\mathbf{u}_{w_o}$和$\\mathbf{v}_{w_c}$的点积大（向量方向相近），那么$w_o$出现在$w_c$上下文中的概率就高。\n\n给定语料库中的一个位置$t$，中心词是$w_t$，上下文窗口大小是$m$，Skip-gram的目标是最大化：\n\n$$\n\\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{t+j} | w_t)\n$$\n\n对整个语料库，目标函数是：\n\n$$\n\\mathcal{L} = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{t+j} | w_t)\n$$\n\n### 计算瓶颈：分母太贵\n\n展开log概率：\n\n$$\n\\log P(w_o | w_c) = \\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c} - \\log \\sum_{w \\in \\mathcal{V}} \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{w_c})\n$$\n\n问题出在分母——需要对整个词汇表求和。如果词汇表有100万个词，每次计算一个概率就需要100万次点积运算，这是不可接受的。\n\nWord2Vec的关键创新之一就是解决这个计算瓶颈。两种主要方法是**负采样**（Negative Sampling）和**层级Softmax**（Hierarchical Softmax）。\n\n### 负采样：化softmax为二分类\n\n负采样的核心思想是：不去计算完整的softmax，而是把问题转化为二分类——区分\"真实的上下文词\"和\"随机采样的噪声词\"。\n\n对于一个正样本对$(w_c, w_o)$（$w_o$确实出现在$w_c$的上下文中），我们同时采样$k$个负样本$w_1, w_2, \\ldots, w_k$（随机从词汇表中按某种分布采样）。\n\n新的目标函数是最大化：\n\n$$\n\\log \\sigma(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma(-\\mathbf{u}_{w_i}^\\top \\mathbf{v}_{w_c}) \\right]\n$$\n\n其中$\\sigma(x) = \\frac{1}{1+e^{-x}}$是sigmoid函数，$P_n(w)$是负采样分布。\n\n这个目标的直觉是：让真实上下文词的向量与中心词相近（点积为正），让随机噪声词的向量与中心词远离（点积为负）。\n\n**为什么这样做是合理的？**\n\n可以证明，当$k$趋近于无穷时，负采样的目标函数趋近于原始softmax目标的某种近似。直觉上，如果模型能够区分真实上下文和随机噪声，它就学会了词之间的共现模式。\n\n### 负采样分布的设计\n\n负采样分布$P_n(w)$的选择对结果有影响。实践中，Word2Vec使用：\n\n$$\nP_n(w) \\propto f(w)^{3/4}\n$$\n\n其中$f(w)$是词$w$在语料库中的频率。\n\n**为什么用3/4次方？**\n\n如果直接用频率$f(w)$，高频词（如\"the\"、\"a\"）会被过度采样，低频词几乎不会被采样。用$f(w)^{3/4}$是一个\"平滑\"：它降低了高频词的权重，增加了低频词的权重，使得采样分布更均匀。\n\n实验表明，0.75是一个神奇的数字——它在多个数据集上都表现良好。这个选择更多是经验性的，没有很强的理论解释。\n\n::: {.callout-note}\n## Algorithm: Skip-gram with Negative Sampling (SGNS) (Mikolov et al., 2013)\n\n**输入**：语料库 $\\mathcal{D}$，词汇表 $\\mathcal{V}$，嵌入维度 $d$，窗口大小 $m$，负采样数 $k$，学习率 $\\eta$\n\n**输出**：词向量矩阵 $\\mathbf{V} \\in \\mathbb{R}^{|\\mathcal{V}| \\times d}$（中心词），$\\mathbf{U} \\in \\mathbb{R}^{|\\mathcal{V}| \\times d}$（上下文词）\n\n**1. 初始化**：\n```\n随机初始化 V, U ∈ ℝ^(|V| × d)（通常使用小的随机值）\n构建负采样分布 Pn(w) ∝ f(w)^0.75\n```\n\n**2. 训练循环**：\n```\nfor epoch = 1 to num_epochs:\n    for 每个位置 t = 1 to T in 语料库:\n        wc ← 中心词（位置 t 的词）\n\n        for 每个上下文位置 j ∈ [-m, m], j ≠ 0:\n            wo ← 上下文词（位置 t+j 的词）\n\n            # 正样本更新\n            score_pos ← u_wo · v_wc\n            grad_pos ← (1 - σ(score_pos))\n\n            # 负采样\n            for i = 1 to k:\n                wn ← 从 Pn(w) 采样一个负样本词\n                score_neg ← u_wn · v_wc\n                grad_neg ← σ(score_neg)\n\n                # 更新负样本的上下文向量\n                u_wn ← u_wn - η · grad_neg · v_wc\n\n            # 更新正样本的上下文向量\n            u_wo ← u_wo + η · grad_pos · v_wc\n\n            # 更新中心词向量\n            v_wc ← v_wc + η · (grad_pos · u_wo - Σ grad_neg · u_wn)\n```\n\n**3. 输出**：返回 $\\mathbf{V}$（或 $\\frac{\\mathbf{V} + \\mathbf{U}}{2}$）作为最终词向量\n\n**关键参数选择**（来自原论文）：\n\n| 参数 | 小数据集 | 大数据集 |\n|------|----------|----------|\n| 负采样数 $k$ | 5-20 | 2-5 |\n| 窗口大小 $m$ | 5 | 5-10 |\n| 嵌入维度 $d$ | 100-300 | 300 |\n\n*Adapted from: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). \"Distributed Representations of Words and Phrases and their Compositionality\". NeurIPS 2013. [arXiv:1310.4546](https://arxiv.org/abs/1310.4546)*\n:::\n\n### CBOW模型\n\nCBOW与Skip-gram方向相反：给定上下文词，预测中心词。\n\n设上下文窗口内的词是$w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m}$，CBOW首先计算上下文的平均向量：\n\n$$\n\\bar{\\mathbf{v}} = \\frac{1}{2m} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\mathbf{v}_{w_{c+j}}\n$$\n\n然后用这个平均向量预测中心词：\n\n$$\nP(w_c | \\text{context}) = \\frac{\\exp(\\mathbf{u}_{w_c}^\\top \\bar{\\mathbf{v}})}{\\sum_{w \\in \\mathcal{V}} \\exp(\\mathbf{u}_w^\\top \\bar{\\mathbf{v}})}\n$$\n\nCBOW的计算效率比Skip-gram高（每个位置只需一次预测），但在捕获罕见词方面通常不如Skip-gram。\n\n### 数值示例：Skip-gram训练一步\n\n让我们用一个极简的数值例子来理解Skip-gram的梯度更新是如何工作的。\n\n**设定**：\n\n- 词汇表：{I, love, cats, dogs}，大小$|\\mathcal{V}| = 4$\n- 嵌入维度：$d = 2$\n- 学习率：$\\eta = 0.1$\n- 负采样数：$k = 1$\n\n**初始化词向量**（随机小值）：\n\n中心词向量$\\mathbf{v}$：\n\n$$\n\\mathbf{v}_{\\text{I}} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}, \\quad\n\\mathbf{v}_{\\text{love}} = \\begin{bmatrix} 0.3 \\\\ -0.1 \\end{bmatrix}, \\quad\n\\mathbf{v}_{\\text{cats}} = \\begin{bmatrix} -0.2 \\\\ 0.4 \\end{bmatrix}, \\quad\n\\mathbf{v}_{\\text{dogs}} = \\begin{bmatrix} 0.0 \\\\ 0.3 \\end{bmatrix}\n$$\n\n上下文词向量$\\mathbf{u}$（初始化相同值，简化演示）：\n\n$$\n\\mathbf{u}_{\\text{I}} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}, \\quad\n\\mathbf{u}_{\\text{love}} = \\begin{bmatrix} 0.3 \\\\ -0.1 \\end{bmatrix}, \\quad\n\\mathbf{u}_{\\text{cats}} = \\begin{bmatrix} -0.2 \\\\ 0.4 \\end{bmatrix}, \\quad\n\\mathbf{u}_{\\text{dogs}} = \\begin{bmatrix} 0.0 \\\\ 0.3 \\end{bmatrix}\n$$\n\n**训练样本**：句子\"I love cats\"，窗口大小=1\n\n- 中心词：love\n- 正样本上下文词：cats（右边一个词）\n- 负采样词：dogs（随机采样）\n\n**Step 1：计算点积**\n\n正样本：$\\mathbf{u}_{\\text{cats}}^\\top \\mathbf{v}_{\\text{love}}$\n\n$$\n= \\begin{bmatrix} -0.2 & 0.4 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ -0.1 \\end{bmatrix}\n= (-0.2)(0.3) + (0.4)(-0.1) = -0.06 - 0.04 = -0.10\n$$\n\n负样本：$\\mathbf{u}_{\\text{dogs}}^\\top \\mathbf{v}_{\\text{love}}$\n\n$$\n= \\begin{bmatrix} 0.0 & 0.3 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ -0.1 \\end{bmatrix}\n= (0.0)(0.3) + (0.3)(-0.1) = 0 - 0.03 = -0.03\n$$\n\n**Step 2：计算Sigmoid**\n\n$$\n\\sigma(-0.10) = \\frac{1}{1 + e^{0.10}} \\approx \\frac{1}{1.105} \\approx 0.475\n$$\n\n$$\n\\sigma(-0.03) = \\frac{1}{1 + e^{0.03}} \\approx \\frac{1}{1.030} \\approx 0.493\n$$\n\n对于负样本，我们需要$\\sigma(-\\mathbf{u}_{\\text{dogs}}^\\top \\mathbf{v}_{\\text{love}}) = \\sigma(0.03) \\approx 0.507$\n\n**Step 3：计算梯度**\n\n对于负采样目标函数：\n\n$$\n\\mathcal{L} = \\log \\sigma(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c}) + \\log \\sigma(-\\mathbf{u}_{w_{\\text{neg}}}^\\top \\mathbf{v}_{w_c})\n$$\n\n梯度计算：\n\n- $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_{\\text{love}}}$：需要拉近cats，推远dogs\n- $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{\\text{cats}}}$：需要拉近love\n- $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{\\text{dogs}}}$：需要推远love\n\n对于正样本，梯度为$(1 - \\sigma(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c})) \\cdot \\mathbf{u}_{w_o}$：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_{\\text{love}}} \\big|_{\\text{positive}} = (1 - 0.475) \\cdot \\begin{bmatrix} -0.2 \\\\ 0.4 \\end{bmatrix} = 0.525 \\cdot \\begin{bmatrix} -0.2 \\\\ 0.4 \\end{bmatrix} = \\begin{bmatrix} -0.105 \\\\ 0.210 \\end{bmatrix}\n$$\n\n对于负样本，梯度为$-\\sigma(\\mathbf{u}_{w_{\\text{neg}}}^\\top \\mathbf{v}_{w_c}) \\cdot \\mathbf{u}_{w_{\\text{neg}}}$：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_{\\text{love}}} \\big|_{\\text{negative}} = -0.493 \\cdot \\begin{bmatrix} 0.0 \\\\ 0.3 \\end{bmatrix} = \\begin{bmatrix} 0.0 \\\\ -0.148 \\end{bmatrix}\n$$\n\n总梯度：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_{\\text{love}}} = \\begin{bmatrix} -0.105 \\\\ 0.210 \\end{bmatrix} + \\begin{bmatrix} 0.0 \\\\ -0.148 \\end{bmatrix} = \\begin{bmatrix} -0.105 \\\\ 0.062 \\end{bmatrix}\n$$\n\n**Step 4：更新词向量**\n\n$$\n\\mathbf{v}_{\\text{love}}^{\\text{new}} = \\mathbf{v}_{\\text{love}} + \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_{\\text{love}}}\n= \\begin{bmatrix} 0.3 \\\\ -0.1 \\end{bmatrix} + 0.1 \\cdot \\begin{bmatrix} -0.105 \\\\ 0.062 \\end{bmatrix}\n= \\begin{bmatrix} 0.290 \\\\ -0.094 \\end{bmatrix}\n$$\n\n**观察**：\n\n- \"love\"的向量第一维减小了（从0.3到0.29），这会使它与\"cats\"的点积增大（因为cats第一维是负的）\n- \"love\"的向量第二维增大了（从-0.1到-0.094），这也会使它与\"cats\"的点积增大（因为cats第二维是正的）\n- 同时，这些变化会使\"love\"与\"dogs\"的点积变化不大或减小\n\n这就是Word2Vec学习的本质：通过大量这样的小更新，逐渐调整词向量，让共现词的向量彼此接近。\n\n### 词向量类比的神奇现象\n\nWord2Vec最令人惊叹的发现是**词向量类比**（word analogy）。训练好的词向量呈现出惊人的规律性：\n\n$$\n\\mathbf{v}_{\\text{king}} - \\mathbf{v}_{\\text{man}} + \\mathbf{v}_{\\text{woman}} \\approx \\mathbf{v}_{\\text{queen}}\n$$\n\n这意味着\"国王-男人+女人≈女王\"。向量空间中存在一个\"性别方向\"，沿着这个方向移动可以改变词的性别属性，同时保持其他语义属性（如\"皇室成员\"）不变。\n\n类似的关系还有很多：\n\n- $\\mathbf{v}_{\\text{Paris}} - \\mathbf{v}_{\\text{France}} + \\mathbf{v}_{\\text{Italy}} \\approx \\mathbf{v}_{\\text{Rome}}$（首都关系）\n- $\\mathbf{v}_{\\text{walking}} - \\mathbf{v}_{\\text{walk}} + \\mathbf{v}_{\\text{swim}} \\approx \\mathbf{v}_{\\text{swimming}}$（时态变化）\n\n这个发现说明，词向量空间不仅捕获了词的相似性，还学习到了词之间的语义关系。这是传统特征工程无法实现的。\n\n**为什么会出现这种现象？**\n\n一个直觉解释是：语言中的规律性反映在共现模式中。\"king\"和\"queen\"出现在相似的上下文中（都与皇室、统治相关），但\"king\"更常与\"he\"、\"his\"共现，\"queen\"更常与\"she\"、\"her\"共现。类似地，\"man\"和\"woman\"也有这种共现模式的差异。通过大量语料的训练，这些规律性被编码进了向量空间的结构中。\n\n更正式的理论分析将在后面的\"深入理解\"部分讨论。\n\n---\n\n## GloVe与FastText：词向量的改进\n\n### GloVe：全局统计的视角\n\nWord2Vec是一个**预测模型**：它通过预测上下文来学习词向量。2014年，Stanford的Pennington等人提出了GloVe（Global Vectors），从一个不同的角度来看这个问题。\n\nGloVe的核心洞察是：词向量应该直接编码共现统计信息。\n\n设$X_{ij}$是词$i$和词$j$在窗口内共同出现的次数，定义$P_{ij} = P(j|i) = X_{ij} / X_i$为词$j$出现在词$i$上下文中的概率。GloVe关注的是**概率比值**：\n\n$$\n\\frac{P_{ik}}{P_{jk}} = \\frac{P(k|i)}{P(k|j)}\n$$\n\n这个比值揭示了词之间的关系。考虑词\"ice\"和\"steam\"，以及探测词\"solid\"和\"gas\"：\n\n| | $P(\\cdot|\\text{ice})$ | $P(\\cdot|\\text{steam})$ | 比值 |\n|---|---|---|---|\n| solid | 高（冰是固体） | 低 | 大 |\n| gas | 低 | 高（蒸汽是气体） | 小 |\n| water | 高 | 高 | ≈1 |\n| fashion | 低 | 低 | ≈1 |\n\nGloVe的目标是让词向量能够编码这种比值关系：\n\n$$\n\\mathbf{w}_i^\\top \\mathbf{w}_j + b_i + b_j = \\log X_{ij}\n$$\n\n最终的损失函数是：\n\n$$\n\\mathcal{L} = \\sum_{i,j=1}^{|\\mathcal{V}|} f(X_{ij}) \\left( \\mathbf{w}_i^\\top \\mathbf{w}_j + b_i + b_j - \\log X_{ij} \\right)^2\n$$\n\n其中$f(x)$是权重函数，用于降低高频共现对的影响（否则\"the\"、\"a\"等词会主导训练）。\n\n**GloVe vs Word2Vec**：\n\nGloVe是**基于统计**的：它直接利用全局共现矩阵，一步到位地优化。Word2Vec是**基于预测**的：它通过局部的滑动窗口，增量式地学习。\n\n实践中，两者效果相当。GloVe的优势是利用了全局统计信息，可能在某些任务上更稳定。Word2Vec的优势是可以流式训练，不需要先构建完整的共现矩阵。\n\n### FastText：子词的力量\n\nWord2Vec和GloVe都把词当作原子单位。但语言中有很多词共享相同的词根或词缀，比如\"teach\"、\"teacher\"、\"teaching\"、\"teaches\"。能不能利用这种结构？\n\n2016年，Facebook的Bojanowski等人提出了FastText，核心思想是：把词表示为**子词（subword）的组合**。\n\n具体来说，FastText把每个词分解成字符n-gram。比如，\"where\"（加上边界符号<和>）分解为：\n\n- 3-gram: <wh, whe, her, ere, re>\n- 4-gram: <whe, wher, here, ere>\n- 5-gram: <wher, where, here>\n\n词\"where\"的向量是所有这些n-gram向量的和，再加上词本身的向量（如果存在的话）：\n\n$$\n\\mathbf{v}_{\\text{where}} = \\mathbf{z}_{\\text{where}} + \\sum_{g \\in G(\\text{where})} \\mathbf{z}_g\n$$\n\n**FastText的优势**：\n\n首先是**处理OOV（Out-of-Vocabulary）词**。传统词向量无法处理训练时没见过的词。但FastText可以：即使\"unfriendliness\"没出现过，只要它的子词（如\"un\"、\"friend\"、\"ness\"）出现过，就能组合出一个合理的向量。\n\n其次是**形态丰富的语言**。对于德语、土耳其语等形态变化丰富的语言，同一个词根可能有几十种变形。FastText通过共享子词表示，可以更有效地学习这些语言。\n\n第三是**处理拼写错误**。\"freind\"和\"friend\"共享很多子词，因此它们的向量会很相似，模型对拼写错误更鲁棒。\n\n**FastText vs Word2Vec**：\n\nFastText的代价是参数量增加（需要存储所有n-gram的向量），训练也更慢。在英语等形态变化较少的语言上，FastText相对Word2Vec的优势不太明显。但在形态丰富的语言上，FastText通常显著更好。\n\n下图展示了 FastText 如何通过子词捕获词之间的相似性。热力图显示了不同 n-gram 之间的相似度：共享更多子词的词对（如 \"rarity\" 和 \"scarceness\"）在向量空间中更接近。\n\n![FastText 子词相似度热力图：展示了 \"rarity\" 和 \"scarceness\" 两个词的各个 n-gram 之间的相似度。共享相似子词结构的词会有更高的相似度。](figures/chapter-3/original/fig4-fasttext-similarity-rarity-scarceness.png){#fig-fasttext-similarity width=70%}\n\n::: {.figure-caption}\n*Source: Bojanowski et al. (2017) \"Enriching Word Vectors with Subword Information\", Figure 1*\n:::\n\n---\n\n## 工程实践：词向量训练与可视化\n\n### 使用Gensim训练Word2Vec\n\n::: {#16dffdcf .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nimport logging\n\n# 设置日志，观察训练进度\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# 准备训练数据：每行一个句子，词用空格分隔\n# 这里用一个简单的示例，实际应该用大规模语料\nsentences = [\n    ['I', 'love', 'machine', 'learning'],\n    ['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning'],\n    ['natural', 'language', 'processing', 'uses', 'deep', 'learning'],\n    ['word', 'vectors', 'are', 'learned', 'representations'],\n    ['word2vec', 'learns', 'word', 'vectors', 'from', 'text'],\n    ['cats', 'and', 'dogs', 'are', 'pets'],\n    ['cats', 'chase', 'mice'],\n    ['dogs', 'chase', 'cats', 'sometimes'],\n]\n\n# 训练Word2Vec模型\nmodel = Word2Vec(\n    sentences,\n    vector_size=50,      # 词向量维度\n    window=3,            # 上下文窗口大小\n    min_count=1,         # 忽略频率低于此值的词\n    workers=4,           # 训练线程数\n    sg=1,                # 1=Skip-gram, 0=CBOW\n    negative=5,          # 负采样数量\n    epochs=100           # 训练轮数\n)\n\n# 查看词向量\nprint(\"'learning'的词向量前5维:\", model.wv['learning'][:5])\n\n# 找相似词\nprint(\"\\n与'learning'最相似的词:\")\nfor word, score in model.wv.most_similar('learning', topn=5):\n    print(f\"  {word}: {score:.4f}\")\n\n# 词向量运算\nif 'cats' in model.wv and 'dogs' in model.wv:\n    similarity = model.wv.similarity('cats', 'dogs')\n    print(f\"\\n'cats'和'dogs'的相似度: {similarity:.4f}\")\n\n# 保存和加载模型\nmodel.save(\"word2vec.model\")\n# loaded_model = Word2Vec.load(\"word2vec.model\")\n```\n:::\n\n\n### 使用预训练词向量\n\n实际应用中，通常使用在大规模语料上预训练好的词向量，而不是自己从头训练。\n\n::: {#6db3ed52 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport gensim.downloader as api\n\n# 下载预训练的Word2Vec模型（Google News, 300维, 300万词）\n# 注意：这个模型约1.6GB，首次下载需要时间\n# model = api.load(\"word2vec-google-news-300\")\n\n# 或者使用更小的GloVe模型\n# model = api.load(\"glove-wiki-gigaword-100\")\n\n# 演示用一个小模型\nmodel = api.load(\"glove-twitter-25\")  # 25维，较小\n\n# 词向量运算示例\nprint(\"king - man + woman = ?\")\nresult = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=3)\nfor word, score in result:\n    print(f\"  {word}: {score:.4f}\")\n\nprint(\"\\nParis - France + Italy = ?\")\nresult = model.most_similar(positive=['paris', 'italy'], negative=['france'], topn=3)\nfor word, score in result:\n    print(f\"  {word}: {score:.4f}\")\n\n# 找不相似的词\nprint(\"\\n哪个词不属于这一组？['breakfast', 'lunch', 'dinner', 'computer']\")\nodd_one = model.doesnt_match(['breakfast', 'lunch', 'dinner', 'computer'])\nprint(f\"  答案: {odd_one}\")\n```\n:::\n\n\n### 词向量可视化\n\n高维词向量无法直接可视化，需要用降维技术投影到2D或3D。\n\n::: {#23b7118b .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport gensim.downloader as api\n\n# 加载预训练模型\nmodel = api.load(\"glove-twitter-25\")\n\n# 选择一些词进行可视化\nwords = [\n    # 动物\n    'cat', 'dog', 'bird', 'fish', 'horse',\n    # 国家\n    'china', 'japan', 'france', 'germany', 'italy',\n    # 颜色\n    'red', 'blue', 'green', 'yellow', 'black',\n    # 数字\n    'one', 'two', 'three', 'four', 'five'\n]\n\n# 获取词向量\nword_vectors = np.array([model[w] for w in words if w in model])\nwords = [w for w in words if w in model]\n\n# t-SNE降维\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nvectors_2d = tsne.fit_transform(word_vectors)\n\n# 可视化\nplt.figure(figsize=(12, 8))\nplt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.7)\n\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n                 fontsize=12, alpha=0.8)\n\nplt.title(\"Word Vectors Visualization (t-SNE)\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.tight_layout()\nplt.savefig(\"word_vectors_tsne.png\", dpi=150)\nplt.show()\n```\n:::\n\n\n### 评估词向量质量\n\n词向量的质量可以通过多种基准测试评估。\n\n::: {#940e55aa .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nfrom gensim.models import KeyedVectors\nimport gensim.downloader as api\n\n# 加载模型\nmodel = api.load(\"glove-wiki-gigaword-100\")\n\n# 1. 词类比任务（Word Analogy）\n# 格式：A:B :: C:D，测试 A-B+C=D\nanalogies = [\n    ('king', 'man', 'woman', 'queen'),\n    ('paris', 'france', 'berlin', 'germany'),\n    ('good', 'better', 'bad', 'worse'),\n]\n\nprint(\"词类比测试:\")\nfor a, b, c, expected in analogies:\n    try:\n        result = model.most_similar(positive=[a, c], negative=[b], topn=1)[0][0]\n        correct = \"✓\" if result == expected else \"✗\"\n        print(f\"  {a}:{b} :: {c}:? → {result} (期望: {expected}) {correct}\")\n    except KeyError as e:\n        print(f\"  词不在词汇表中: {e}\")\n\n# 2. 词相似度任务（Word Similarity）\n# 与人类判断的相关性\nword_pairs = [\n    ('car', 'automobile', 'high'),\n    ('car', 'bicycle', 'medium'),\n    ('car', 'democracy', 'low'),\n]\n\nprint(\"\\n词相似度测试:\")\nfor w1, w2, expected_level in word_pairs:\n    try:\n        sim = model.similarity(w1, w2)\n        print(f\"  {w1} - {w2}: {sim:.4f} (预期: {expected_level})\")\n    except KeyError as e:\n        print(f\"  词不在词汇表中: {e}\")\n\n# 3. 聚类质量\n# 语义相似的词应该聚在一起\nfrom sklearn.cluster import KMeans\n\nwords_by_category = {\n    'animals': ['cat', 'dog', 'bird', 'fish', 'horse'],\n    'countries': ['china', 'japan', 'france', 'germany', 'italy'],\n    'colors': ['red', 'blue', 'green', 'yellow', 'black'],\n}\n\nall_words = []\ntrue_labels = []\nfor i, (category, words) in enumerate(words_by_category.items()):\n    for word in words:\n        if word in model:\n            all_words.append(word)\n            true_labels.append(i)\n\nvectors = np.array([model[w] for w in all_words])\n\n# K-means聚类\nkmeans = KMeans(n_clusters=3, random_state=42)\npred_labels = kmeans.fit_predict(vectors)\n\n# 计算聚类纯度\nfrom sklearn.metrics import adjusted_rand_score\nari = adjusted_rand_score(true_labels, pred_labels)\nprint(f\"\\n聚类质量 (Adjusted Rand Index): {ari:.4f}\")\n```\n:::\n\n\n---\n\n## 深入理解\n\n> **研究者必读**：这一节探讨词向量的理论基础、边界条件和开放问题\n\n### 为什么有效？——理论视角\n\n词向量的成功有深刻的理论基础。\n\n**分布式假设的数学化**。Levy和Goldberg（2014）的重要工作表明，Word2Vec的Skip-gram负采样目标实际上等价于对点互信息（PMI）矩阵的隐式分解：\n\n$$\n\\mathbf{w}_i \\cdot \\mathbf{c}_j = \\text{PMI}(i, j) - \\log k\n$$\n\n其中PMI定义为：\n\n$$\n\\text{PMI}(i, j) = \\log \\frac{P(i, j)}{P(i)P(j)} = \\log \\frac{\\#(i, j) \\cdot |D|}{\\#(i) \\cdot \\#(j)}\n$$\n\n这意味着Word2Vec本质上是在学习词对的PMI——一种统计相关性度量。PMI高意味着两个词比随机共现更频繁地一起出现，这正是语义相关的信号。\n\n**与矩阵分解的联系**。进一步，可以证明GloVe也在做类似的事情：对共现统计的某种变换进行低秩分解。这解释了为什么两种表面上不同的方法能得到相似的结果——它们都在捕获相同的统计结构。\n\n**线性结构的来源**。词向量类比（king - man + woman ≈ queen）的线性结构可以从PMI的性质推导。如果\"king\"和\"queen\"有相似的上下文，但\"king\"更常与\"he\"共现、\"queen\"更常与\"she\"共现，那么：\n\n$$\n\\text{PMI}(\\text{king}, \\text{he}) - \\text{PMI}(\\text{queen}, \\text{he}) \\approx \\text{PMI}(\\text{man}, \\text{he}) - \\text{PMI}(\\text{woman}, \\text{he})\n$$\n\n这种对称性导致了向量空间中的平行结构。\n\n### 为什么有效？——实证视角\n\n大量实验揭示了词向量有效的条件。\n\n**数据量的影响**。更多的训练数据通常带来更好的词向量。Mikolov等人报告，在Google News语料（1000亿词）上训练的向量显著优于在较小语料上训练的向量。这并不意外——更多数据意味着更准确的共现统计估计。\n\n**维度的影响**。词向量维度通常在100-300之间效果最好。太低维度无法捕获足够的信息，太高维度可能过拟合或引入噪声。300维似乎是一个\"甜点\"，在多数任务上表现良好。\n\n**窗口大小的影响**。较小的窗口（2-5）倾向于捕获语法关系（如词性），较大的窗口（5-10）倾向于捕获语义关系（如主题）。选择取决于下游任务。\n\n**负采样数量的影响**。对于小数据集，较多的负样本（15-20）可能有帮助。对于大数据集，较少的负样本（5-10）通常就够了。\n\n### 方法的边界条件\n\n词向量有几个重要的隐含假设和局限性。\n\n**静态表示假设**。每个词只有一个向量，不论它出现在什么上下文中。这意味着一词多义无法处理。\"bank\"（银行）和\"bank\"（河岸）有相同的向量，这显然不合理。这个问题在第11章（ELMo）和第13章（BERT）才得到解决。\n\n**词袋假设**。词向量训练通常忽略词序，把上下文当作词袋处理。这丢失了语法信息，比如\"dog bites man\"和\"man bites dog\"的区分。\n\n**频率偏差**。高频词（如\"the\"、\"is\"）可能主导训练，导致它们的向量质量过高，而低频词（真正有语义内容的词）质量不足。虽然负采样分布的设计试图缓解这个问题，但偏差仍然存在。\n\n**文化和历史偏见**。词向量会学习语料库中的偏见。如果语料库中\"doctor\"更常与\"he\"共现、\"nurse\"更常与\"she\"共现，词向量就会编码这种性别偏见。这在某些应用中可能是问题。\n\n### 变体与扩展\n\n词向量的成功催生了大量后续工作。\n\n**句子向量**。如何表示整个句子？简单的方法是对词向量取平均，但这丢失了词序信息。Le和Mikolov（2014）提出了Doc2Vec，用类似Word2Vec的方法学习文档向量。\n\n**跨语言词向量**。能否让不同语言的词在同一个向量空间中？如果能，机器翻译就变成了简单的最近邻查找。研究表明，通过少量双语词典或平行语料，可以学习跨语言的词向量对齐。\n\n**知识增强**。纯粹从文本学习可能错过一些常识知识。研究者尝试将知识图谱信息融入词向量，比如让\"巴黎\"和\"法国\"的向量编码\"首都\"关系。\n\n### 开放研究问题\n\n如果你要在词向量方向做研究，可以考虑这些问题。\n\n第一，**最优的训练目标是什么？** Skip-gram、CBOW、GloVe、FastText...都有各自的优缺点。是否存在一个理论最优的目标函数？目前还没有定论。\n\n第二，**如何处理一词多义？** 静态词向量的根本局限是无法区分同一个词的不同含义。在ELMo和BERT之前，有一些工作尝试为每个词学习多个向量（如Sense2Vec），但效果有限。\n\n第三，**词向量编码了什么信息？** 虽然我们知道词向量能完成类比任务，但它们到底学到了什么语言知识？是语法？语义？世界知识？这仍然是一个活跃的研究问题。\n\n第四，**如何减少偏见？** 词向量中的性别、种族偏见如何检测和消除？简单的\"去偏\"方法（如减去性别方向）可能只是表面上消除偏见，深层的偏见仍然存在。\n\n---\n\n## 局限性与未解决的问题\n\n### 静态表示的根本缺陷\n\n词向量最大的局限是**静态性**：每个词只有一个固定的向量表示，不论它出现在什么上下文中。\n\n考虑\"bank\"这个词：\n\n- \"I went to the **bank** to deposit money.\"（银行）\n- \"We sat on the **bank** of the river.\"（河岸）\n\n这两个\"bank\"显然有不同的含义，但在Word2Vec中它们只有一个向量。这个向量是所有含义的某种\"平均\"，对于每个具体用法都不够精确。\n\n问题在高度多义词上尤其严重。英语中很多常用词都有多个含义（\"run\"有超过100个义项），一个静态向量无法捕获这种复杂性。\n\n### 上下文信息的缺失\n\n词向量训练使用的是固定窗口内的局部上下文，而忽略了更广泛的语篇信息。\n\n考虑这个例子：\n\n- \"The trophy would not fit in the suitcase because **it** was too big.\"\n- \"The trophy would not fit in the suitcase because **it** was too small.\"\n\n第一句中\"it\"指代trophy，第二句中\"it\"指代suitcase。理解这种指代需要跨越整个句子的推理，而不仅仅是局部窗口内的共现。\n\n词向量的训练目标（预测局部上下文）天然无法捕获这种长距离依赖。\n\n### 这些局限导向了什么？\n\n静态词向量的成功和局限同时存在，这引出了一个自然的问题：能不能让词的表示**动态地依赖于上下文**？\n\n这正是接下来几章要讨论的问题。第11章的ELMo将展示如何用双向LSTM生成上下文相关的词表示，第13章的BERT将用Transformer的双向注意力实现更强大的上下文编码。这些方法被称为\"上下文化词向量\"（Contextualized Word Embeddings），它们建立在Word2Vec的基础上，但解决了静态表示的根本缺陷。\n\n> 下一章预告：第4章将讨论一个常被忽视但极其重要的问题——Tokenization。词向量假设我们已经有了\"词\"，但什么是一个\"词\"？中文要不要分词？形态丰富的语言如何处理？这些问题比看起来复杂得多，而Tokenizer的设计直接影响模型的能力。\n\n---\n\n## 本章小结\n\n### 核心要点回顾\n\n这一章我们见证了NLP历史上的一次重要突破：从手工设计特征到自动学习表示。\n\nWord2Vec的核心洞察极其简洁：**一个词的含义由它的上下文定义**。通过训练模型预测词的上下文（Skip-gram）或从上下文预测词（CBOW），模型自动学习到了词的分布式表示。这些表示捕获了语义相似性——\"cat\"和\"dog\"的向量彼此接近，因为它们出现在相似的上下文中。\n\n更神奇的是，词向量空间呈现出规律的线性结构：king - man + woman ≈ queen。这说明向量空间不仅编码了相似性，还编码了语义关系。\n\nGloVe从全局统计的角度重新推导了词向量，FastText通过子词分解解决了OOV问题。这些改进丰富了词向量的生态。\n\n然而，所有这些方法都面临一个根本局限：**静态表示无法处理一词多义**。每个词只有一个向量，无论它出现在什么上下文中。这个问题的解决要等到ELMo和BERT的出现。\n\n### 关键公式速查\n\n| 公式 | 含义 |\n|------|------|\n| $P(w_o \\| w_c) = \\frac{\\exp(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c})}{\\sum_w \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{w_c})}$ | Skip-gram的Softmax概率 |\n| $\\log \\sigma(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c}) + \\sum_{i=1}^{k} \\log \\sigma(-\\mathbf{u}_{w_i}^\\top \\mathbf{v}_{w_c})$ | 负采样目标函数 |\n| $P_n(w) \\propto f(w)^{3/4}$ | 负采样分布 |\n| $\\mathbf{w}_i^\\top \\mathbf{w}_j + b_i + b_j = \\log X_{ij}$ | GloVe目标 |\n\n### 思考题\n\n1. **[概念理解]** 为什么Skip-gram在处理罕见词时比CBOW更有效？从训练信号的角度分析。\n\n2. **[数学推导]** 推导负采样目标函数的梯度。给定正样本$(w_c, w_o)$和负样本$w_1, \\ldots, w_k$，写出$\\partial \\mathcal{L} / \\partial \\mathbf{v}_{w_c}$的表达式。\n\n3. **[工程实践]** 使用Gensim在一个中等规模的语料库（如Wikipedia dump的一部分）上训练Word2Vec模型。比较不同超参数（维度、窗口大小、负采样数）对词类比任务准确率的影响。\n\n4. **[研究思考]** 词向量捕获了语料库中的偏见（如性别偏见）。你认为应该如何处理这个问题？完全去除偏见是否可行或可取？这个问题有什么更深层的哲学含义？\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n- **Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013)**：Word2Vec的原始论文\n  - 重点读：Section 4（Skip-gram和CBOW的定义）\n  - 可跳过：实验的细节\n\n- **Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013)**：引入负采样的论文\n  - 重点读：Section 2.2（负采样）、Section 4（短语向量）\n  - 这篇论文比第一篇更重要，因为负采样成为了标准做法\n\n### 理论分析\n\n- **Neural Word Embedding as Implicit Matrix Factorization (Levy & Goldberg, 2014)**：证明Word2Vec等价于PMI矩阵分解\n  - 这篇论文揭示了Word2Vec成功的数学原因\n  - 对理解为什么不同方法效果相近很有帮助\n\n### 改进方法\n\n- **GloVe: Global Vectors for Word Representation (Pennington et al., 2014)**：GloVe的原始论文\n  - 从不同角度推导词向量，与Word2Vec殊途同归\n\n- **Enriching Word Vectors with Subword Information (Bojanowski et al., 2017)**：FastText\n  - 解决OOV问题的优雅方案\n\n### 综述与教程\n\n- **Word2Vec Tutorial (Chris McCormick)**：最通俗的Word2Vec解释\n  - 适合入门，有很好的可视化\n\n- **Speech and Language Processing, Chapter 6 (Jurafsky & Martin)**：词向量的教科书级介绍\n\n### 代码资源\n\n- **Gensim库**：Word2Vec/FastText的标准Python实现\n- **Stanford的GloVe官方代码**：C实现，效率很高\n- **Hugging Face的预训练模型**：可以直接加载各种预训练词向量\n\n---\n\n## 历史注脚\n\nWord2Vec的成功有一个有趣的历史背景。Tomas Mikolov在发表Word2Vec之前，已经在循环神经网络语言模型上做了多年工作。他发现RNN语言模型的隐藏层可以作为词的表示，但训练太慢、无法扩展。Word2Vec可以看作是把这个思想极简化——用更简单的模型（没有非线性隐藏层），换取在大规模数据上训练的能力。\n\n另一个有趣的细节是，Word2Vec论文被ICLR 2013拒稿了。审稿人认为模型太简单、理论贡献不足。后来Mikolov把论文放到arXiv上，它迅速成为NLP领域引用最多的论文之一。这个故事说明，有时候最有影响力的工作不是最\"复杂\"的，而是找到了正确的简化。\n\n词向量类比的发现也颇为戏剧性。据说Mikolov在调试代码时偶然发现 king - man + woman 会得到接近queen的向量，起初以为是bug。后来才意识到这是一个深刻的发现——向量空间自动学到了语义关系的线性结构。这个发现极大地推动了对词向量的兴趣，因为它暗示着向量空间中可能编码了我们尚未完全理解的语言知识。\n\n",
    "supporting": [
      "ch03-representation-learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}