{
  "hash": "df1d2e0235afbd09a00aa0c1695aa6df",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第6章：注意力机制的变体演进\"\nsubtitle: \"从加性到乘性、从全局到局部、从软到硬\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-25\"\ncategories: [NLP, Attention, Luong, 机器翻译, Seq2Seq]\ntags: [乘性注意力, 点积注意力, Global Attention, Local Attention, Hard Attention, Soft Attention]\ndescription: \"注意力机制的设计空间：从加性到乘性、从全局到局部、从软到硬，Luong的系统性探索与效率-表达力权衡。\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\n---\n\n> **核心问题**：Bahdanau的加性注意力虽然有效，但是否有更简洁、更高效的注意力计算方式？不同的设计选择会带来怎样的权衡？\n>\n> **历史坐标**：2015 | Luong, Pham, Manning | 注意力机制的系统性探索\n\n---\n\n## 从上一章说起\n\n上一章我们见证了Attention机制的诞生。Bahdanau等人通过让解码器在每一步都能\"回头看\"编码器的所有位置，彻底打破了Seq2Seq的信息瓶颈。这个突破性的想法迅速在机器翻译领域引发了连锁反应——如果Attention如此有效，是否还有其他方式来计算\"注意力\"？\n\nBahdanau的设计使用了一个**加性**的对齐函数：先将解码器状态和编码器状态分别经过线性变换，然后相加，最后通过一个单层网络得到标量分数。这个设计有效，但计算量不小——每次计算对齐分数都需要一个前馈网络。一个自然的问题浮现：能否用更简单的操作，比如直接计算向量的点积，来衡量两个状态的相关性？\n\n2015年，斯坦福大学的Luong、Pham和Manning发表了一篇系统性的研究，探索了多种注意力机制的变体。他们不仅提出了计算效率更高的**乘性注意力（multiplicative attention）**，还探讨了**全局注意力**与**局部注意力**的权衡、不同对齐函数的对比，以及注意力在解码器中的最佳使用位置。\n\n> 💡 **本章核心洞察**：注意力机制的设计空间远比想象中丰富。**加性 vs 乘性**决定了表达能力与计算效率的权衡；**全局 vs 局部**决定了长序列处理的策略；**软 vs 硬**决定了端到端可训练性。理解这些设计选择，是理解后来Transformer中Scaled Dot-Product Attention的关键。\n\n---\n\n## 问题的本质是什么？\n\n### 问题的精确定义\n\nBahdanau Attention虽然有效，但在实践中面临几个设计问题：\n\n**计算效率问题**：加性注意力需要为每对（解码器状态，编码器状态）计算一个前馈网络的输出：\n\n$$\ne_{ij} = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a \\mathbf{s}_{i-1} + \\mathbf{U}_a \\mathbf{h}_j)\n$$\n\n这涉及两个矩阵乘法（$\\mathbf{W}_a \\mathbf{s}$ 和 $\\mathbf{U}_a \\mathbf{h}$）、一个非线性激活（$\\tanh$）、一个向量点积（$\\mathbf{v}_a^\\top (\\cdot)$）。当序列很长时，这个计算量是可观的。\n\n**设计空间未充分探索**：Bahdanau做了一系列设计选择——使用加性对齐、在每个解码步之前计算注意力、关注所有源位置——但这些选择是否最优？有没有更好的替代方案？\n\n**长序列的挑战**：对于很长的源序列，计算对所有位置的注意力可能是浪费的。直觉上，在翻译某个词时，我们只需要关注源序列的一小部分，而不是全部。能否只计算\"局部\"的注意力？\n\n### 我们需要什么样的解决方案？\n\n一个系统性的探索应该回答以下问题：\n\n1. **对齐函数**：除了加性，还有哪些方式计算两个向量的相关性？它们的表达能力和计算效率如何权衡？\n2. **注意力范围**：是否需要关注所有位置？能否只关注一个局部窗口？\n3. **使用位置**：注意力应该在解码的哪个阶段使用？是在计算解码器状态之前，还是之后？\n4. **软 vs 硬**：是否可以用确定性的\"硬\"选择替代概率分布的\"软\"选择？\n\n---\n\n## 核心思想与直觉\n\n### Luong Attention：乘性替代加性\n\nLuong等人提出的核心改进是用**乘性（multiplicative）**操作替代加性操作来计算对齐分数。最简单的形式是直接计算点积：\n\n$$\ne_{ij} = \\mathbf{s}_i^\\top \\mathbf{h}_j\n$$\n\n这被称为**点积注意力（dot-product attention）**。与Bahdanau的加性注意力相比，它没有任何可学习参数——只是两个向量的内积。\n\n直觉上，点积衡量的是两个向量的\"相似度\"。如果解码器状态 $\\mathbf{s}_i$ 和编码器状态 $\\mathbf{h}_j$ 指向相似的方向，点积就大；如果它们正交，点积就是零。这正是我们想要的：找到与当前解码状态最\"相关\"的编码位置。\n\n### 三种对齐函数的对比\n\nLuong的论文系统比较了三种对齐函数：\n\n| 名称 | 公式 | 参数 | 计算效率 |\n|------|------|------|----------|\n| **Dot** | $\\mathbf{s}^\\top \\mathbf{h}$ | 无 | 最快 |\n| **General** | $\\mathbf{s}^\\top \\mathbf{W}_a \\mathbf{h}$ | $\\mathbf{W}_a$ | 中等 |\n| **Concat** | $\\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a [\\mathbf{s}; \\mathbf{h}])$ | $\\mathbf{v}_a, \\mathbf{W}_a$ | 最慢 |\n\n**Dot（点积）**：最简单，计算最快，但要求解码器和编码器的隐藏维度必须相同。\n\n**General（一般形式）**：引入一个可学习的矩阵 $\\mathbf{W}_a$，可以处理不同维度的状态，也增加了模型的表达能力。这实际上是在问：\"$\\mathbf{s}$ 和 $\\mathbf{W}_a \\mathbf{h}$（$\\mathbf{h}$ 的一个线性变换）有多相似？\"\n\n**Concat（拼接）**：这就是Bahdanau的加性注意力，将两个向量拼接后通过一个单层网络。表达能力最强，但计算最慢。\n\n### 另一个关键差异：注意力的使用位置\n\n除了对齐函数的不同，Luong还指出了另一个重要差异：**注意力应该在解码器的什么位置使用？**\n\n**Bahdanau的方式**：先计算注意力，得到上下文向量 $\\mathbf{c}_i$，然后将 $\\mathbf{c}_i$ 和上一步输出 $y_{i-1}$ 一起输入RNN，计算新的隐藏状态 $\\mathbf{s}_i$。\n\n$$\n\\mathbf{s}_i = f(\\mathbf{s}_{i-1}, y_{i-1}, \\mathbf{c}_i)\n$$\n\n**Luong的方式**：先用RNN计算新的隐藏状态 $\\mathbf{s}_i$，然后基于 $\\mathbf{s}_i$ 计算注意力，得到上下文向量 $\\mathbf{c}_i$，最后将两者结合生成输出。\n\n$$\n\\mathbf{s}_i = f(\\mathbf{s}_{i-1}, y_{i-1})\n$$\n$$\n\\mathbf{c}_i = \\text{Attention}(\\mathbf{s}_i, \\mathbf{H})\n$$\n$$\n\\tilde{\\mathbf{s}}_i = \\tanh(\\mathbf{W}_c [\\mathbf{c}_i; \\mathbf{s}_i])\n$$\n\n这看起来是个细节差异，但Luong的方式更加模块化——RNN和Attention是分离的，便于分析和调试。\n\n---\n\n## 技术细节\n\n### Luong Attention的三种变体\n\n让我们详细看看三种对齐函数的数学形式和实现。\n\n**变体1：Dot-Product Attention**\n\n$$\n\\text{score}(\\mathbf{s}_i, \\mathbf{h}_j) = \\mathbf{s}_i^\\top \\mathbf{h}_j\n$$\n\n这是最简单的形式。两个向量的点积可以用矩阵乘法高效实现：如果我们有所有编码器状态组成的矩阵 $\\mathbf{H} \\in \\mathbb{R}^{T_x \\times d}$，那么所有对齐分数可以一次计算：\n\n$$\n\\mathbf{e}_i = \\mathbf{H} \\mathbf{s}_i \\in \\mathbb{R}^{T_x}\n$$\n\n**变体2：General Attention**\n\n$$\n\\text{score}(\\mathbf{s}_i, \\mathbf{h}_j) = \\mathbf{s}_i^\\top \\mathbf{W}_a \\mathbf{h}_j\n$$\n\n其中 $\\mathbf{W}_a \\in \\mathbb{R}^{d_s \\times d_h}$ 是可学习参数。这允许解码器和编码器有不同的隐藏维度，同时让模型学习一个\"相似度度量\"。\n\n**变体3：Concat Attention（与Bahdanau类似）**\n\n$$\n\\text{score}(\\mathbf{s}_i, \\mathbf{h}_j) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a [\\mathbf{s}_i; \\mathbf{h}_j])\n$$\n\n其中 $[\\mathbf{s}_i; \\mathbf{h}_j]$ 表示向量拼接。这是最具表达能力的形式，因为它可以学习任意的对齐函数。\n\n::: {.callout-note}\n## Algorithm: Luong Attention Variants (Luong et al., 2015)\n\n```python\ndef luong_attention(decoder_state, encoder_outputs, method='dot', W_a=None, v_a=None):\n    \"\"\"\n    Luong 注意力机制的三种变体\n\n    参数:\n        decoder_state: 解码器当前隐藏状态 [batch, dec_hidden]\n        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]\n        method: 'dot', 'general', 或 'concat'\n        W_a: 可学习参数（general和concat需要）\n        v_a: 可学习参数（concat需要）\n\n    返回:\n        context: 上下文向量 [batch, enc_hidden]\n        attention_weights: 注意力权重 [batch, src_len]\n    \"\"\"\n    if method == 'dot':\n        # 点积: s^T h\n        # [batch, dec_hidden] @ [batch, enc_hidden, src_len] -> [batch, src_len]\n        scores = torch.bmm(decoder_state.unsqueeze(1),\n                          encoder_outputs.transpose(1, 2)).squeeze(1)\n\n    elif method == 'general':\n        # 一般形式: s^T W h\n        # 先计算 W @ h: [batch, src_len, dec_hidden]\n        transformed = encoder_outputs @ W_a.T\n        scores = torch.bmm(decoder_state.unsqueeze(1),\n                          transformed.transpose(1, 2)).squeeze(1)\n\n    elif method == 'concat':\n        # 拼接形式: v^T tanh(W [s; h])\n        # 扩展 decoder_state 到所有位置\n        s_expanded = decoder_state.unsqueeze(1).expand(-1, encoder_outputs.size(1), -1)\n        concat = torch.cat([s_expanded, encoder_outputs], dim=-1)\n        scores = v_a @ torch.tanh(concat @ W_a.T).transpose(1, 2)\n        scores = scores.squeeze(1)\n\n    # Softmax 归一化\n    attention_weights = F.softmax(scores, dim=-1)\n\n    # 加权求和\n    context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n\n    return context, attention_weights\n```\n\n*Source: Luong, Pham, & Manning (2015) \"Effective Approaches to Attention-based Neural Machine Translation\", EMNLP 2015. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*\n:::\n\n### 完整数值示例：对比三种对齐函数\n\n让我们用同一组数据，比较三种对齐函数计算出的分数。\n\n**设定**：\n\n- 解码器状态：$\\mathbf{s} = [0.5, -0.3, 0.8, 0.2]^\\top$\n- 编码器状态（3个位置）：\n  - $\\mathbf{h}_1 = [0.2, 0.4, 0.1, -0.3]^\\top$\n  - $\\mathbf{h}_2 = [0.6, -0.1, 0.7, 0.3]^\\top$\n  - $\\mathbf{h}_3 = [-0.2, 0.5, 0.3, 0.1]^\\top$\n\n**Dot-Product 计算**：\n\n$$\ne_1 = \\mathbf{s}^\\top \\mathbf{h}_1 = 0.5 \\times 0.2 + (-0.3) \\times 0.4 + 0.8 \\times 0.1 + 0.2 \\times (-0.3)\n$$\n$$\n= 0.10 - 0.12 + 0.08 - 0.06 = 0.00\n$$\n\n$$\ne_2 = \\mathbf{s}^\\top \\mathbf{h}_2 = 0.5 \\times 0.6 + (-0.3) \\times (-0.1) + 0.8 \\times 0.7 + 0.2 \\times 0.3\n$$\n$$\n= 0.30 + 0.03 + 0.56 + 0.06 = 0.95\n$$\n\n$$\ne_3 = \\mathbf{s}^\\top \\mathbf{h}_3 = 0.5 \\times (-0.2) + (-0.3) \\times 0.5 + 0.8 \\times 0.3 + 0.2 \\times 0.1\n$$\n$$\n= -0.10 - 0.15 + 0.24 + 0.02 = 0.01\n$$\n\n**Softmax 归一化**：\n\n$$\n\\alpha_1 = \\frac{e^{0.00}}{e^{0.00} + e^{0.95} + e^{0.01}} = \\frac{1.00}{1.00 + 2.59 + 1.01} = \\frac{1.00}{4.60} \\approx 0.22\n$$\n\n$$\n\\alpha_2 = \\frac{e^{0.95}}{4.60} = \\frac{2.59}{4.60} \\approx 0.56\n$$\n\n$$\n\\alpha_3 = \\frac{e^{0.01}}{4.60} = \\frac{1.01}{4.60} \\approx 0.22\n$$\n\n**解读**：使用点积注意力，模型将56%的注意力放在位置2，这是因为 $\\mathbf{h}_2$ 与 $\\mathbf{s}$ 的点积最大——它们在向量空间中最\"相似\"。\n\n**General Attention 计算**（假设 $\\mathbf{W}_a$ 是单位矩阵）：\n\n当 $\\mathbf{W}_a = \\mathbf{I}$ 时，General退化为Dot-Product。在一般情况下，$\\mathbf{W}_a$ 可以学习一个变换，使得模型能够发现更复杂的相关性模式。\n\n### Global vs Local Attention\n\nLuong还提出了另一个重要的设计选择：**注意力的范围**。\n\n**Global Attention**：关注所有源位置。这是Bahdanau的做法，也是上面讨论的默认方式。\n\n$$\n\\mathbf{c}_i = \\sum_{j=1}^{T_x} \\alpha_{ij} \\mathbf{h}_j\n$$\n\n**Local Attention**：只关注源序列的一个**窗口**。\n\n核心思想是：在每个解码步，先预测一个对齐位置 $p_i$，然后只计算以 $p_i$ 为中心、宽度为 $2D+1$ 的窗口内的注意力。\n\n$$\n\\mathbf{c}_i = \\sum_{j=p_i-D}^{p_i+D} \\alpha_{ij} \\mathbf{h}_j\n$$\n\n对齐位置 $p_i$ 可以通过两种方式确定：\n\n**Local-m（单调）**：假设源和目标大致对齐，简单设置 $p_i = i$。\n\n**Local-p（预测）**：学习一个函数来预测 $p_i$：\n\n$$\np_i = T_x \\cdot \\sigma(\\mathbf{v}_p^\\top \\tanh(\\mathbf{W}_p \\mathbf{s}_i))\n$$\n\n其中 $\\sigma$ 是sigmoid函数，确保 $p_i \\in [0, T_x]$。\n\n为了让注意力在窗口中心附近更集中，Local Attention还引入了一个高斯偏置：\n\n$$\n\\alpha_{ij} = \\text{align}(\\mathbf{s}_i, \\mathbf{h}_j) \\cdot \\exp\\left(-\\frac{(j - p_i)^2}{2\\sigma^2}\\right)\n$$\n\n![Luong论文中的Global vs Local Attention对比。左边是Global Attention：解码器状态 $h_t$ 与所有源位置计算注意力，生成上下文向量 $c_t$。右边是Local Attention：先预测对齐位置 $p_t$，只计算窗口 $[p_t - D, p_t + D]$ 内的注意力。](figures/chapter-6/original/fig-global-local-attention.png){#fig-global-local width=80%}\n\n::: {.figure-caption}\n*Source: Luong, Pham, & Manning (2015) \"Effective Approaches to Attention-based Neural Machine Translation\", Figure 2 & 3. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*\n:::\n\n### Hard Attention vs Soft Attention\n\n除了Global/Local的区分，还有另一个重要维度：**软注意力（Soft Attention）** vs **硬注意力（Hard Attention）**。\n\n**Soft Attention**：计算所有位置的注意力权重（一个概率分布），然后加权求和。这是我们一直在讨论的方式。\n\n$$\n\\mathbf{c}_i = \\sum_j \\alpha_{ij} \\mathbf{h}_j = \\mathbb{E}_{p(j | \\mathbf{s}_i, \\mathbf{H})}[\\mathbf{h}_j]\n$$\n\n**Hard Attention**：从注意力分布中**采样**一个位置 $j^*$，只使用那个位置的信息。\n\n$$\nj^* \\sim \\text{Categorical}(\\alpha_{i1}, \\alpha_{i2}, \\ldots, \\alpha_{iT_x})\n$$\n$$\n\\mathbf{c}_i = \\mathbf{h}_{j^*}\n$$\n\n两者的核心区别在于**可微分性**。\n\n**Soft Attention 是可微分的**：加权求和是一个连续操作，梯度可以通过 $\\alpha_{ij}$ 流向对齐函数的参数。这意味着我们可以用标准的反向传播进行端到端训练。\n\n**Hard Attention 不可微分**：采样操作是离散的，梯度无法直接通过。要训练Hard Attention，需要使用强化学习方法（如REINFORCE），用期望梯度的蒙特卡洛估计来近似。这带来了高方差和训练不稳定的问题。\n\n::: {.callout-warning}\n## Hard Attention的训练困难\n\nHard Attention虽然在概念上更接近人类的\"注意\"（我们真的只看一个地方，而不是模糊地看所有地方），但它的训练需要强化学习技术：\n\n$$\n\\nabla_\\theta J = \\mathbb{E}_{j^* \\sim p(j|\\theta)} \\left[ \\nabla_\\theta \\log p(j^* | \\theta) \\cdot R(j^*) \\right]\n$$\n\n其中 $R(j^*)$ 是选择位置 $j^*$ 带来的\"奖励\"。这个梯度估计的方差很大，需要大量采样才能稳定。\n\n实践中，**Soft Attention几乎总是更好的选择**，因为：\n\n1. 端到端可微分，训练简单\n2. 梯度估计没有方差问题\n3. 可以同时利用多个位置的信息\n:::\n\n### 复杂度分析\n\n不同注意力变体的计算复杂度：\n\n| 变体 | 对齐计算 | 总复杂度 |\n|------|----------|----------|\n| **Global + Dot** | $O(T_x \\cdot d)$ per step | $O(T_x \\cdot T_y \\cdot d)$ |\n| **Global + General** | $O(T_x \\cdot d^2)$ per step | $O(T_x \\cdot T_y \\cdot d^2)$ |\n| **Global + Concat** | $O(T_x \\cdot d^2)$ per step | $O(T_x \\cdot T_y \\cdot d^2)$ |\n| **Local** | $O(D \\cdot d)$ per step | $O(D \\cdot T_y \\cdot d)$ |\n\n其中 $T_x$ 是源序列长度，$T_y$ 是目标序列长度，$d$ 是隐藏维度，$D$ 是局部窗口大小。\n\nLocal Attention的优势在长序列时尤为明显：当 $T_x = 1000$ 而 $D = 50$ 时，计算量减少了20倍。\n\n---\n\n## 工程实践\n\n### 实现Luong Attention\n\n::: {#f1fdc0c4 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LuongAttention(nn.Module):\n    \"\"\"\n    Luong 注意力机制，支持三种对齐方式\n    \"\"\"\n    def __init__(self, enc_hidden_dim, dec_hidden_dim, method='dot'):\n        super().__init__()\n        self.method = method\n        self.enc_hidden_dim = enc_hidden_dim\n        self.dec_hidden_dim = dec_hidden_dim\n\n        if method == 'general':\n            self.W_a = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)\n        elif method == 'concat':\n            self.W_a = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim, bias=False)\n            self.v_a = nn.Linear(dec_hidden_dim, 1, bias=False)\n\n    def forward(self, decoder_state, encoder_outputs, mask=None):\n        \"\"\"\n        decoder_state: [batch, dec_hidden]\n        encoder_outputs: [batch, src_len, enc_hidden]\n        mask: [batch, src_len], True表示padding位置\n        \"\"\"\n        batch_size, src_len, _ = encoder_outputs.shape\n\n        if self.method == 'dot':\n            # 点积: s^T h\n            # 需要 dec_hidden == enc_hidden\n            scores = torch.bmm(\n                decoder_state.unsqueeze(1),  # [batch, 1, dec_hidden]\n                encoder_outputs.transpose(1, 2)  # [batch, enc_hidden, src_len]\n            ).squeeze(1)  # [batch, src_len]\n\n        elif self.method == 'general':\n            # 一般形式: s^T W h\n            # W 将 enc_hidden 映射到 dec_hidden\n            transformed = self.W_a(encoder_outputs)  # [batch, src_len, dec_hidden]\n            scores = torch.bmm(\n                decoder_state.unsqueeze(1),\n                transformed.transpose(1, 2)\n            ).squeeze(1)\n\n        elif self.method == 'concat':\n            # 拼接形式: v^T tanh(W [s; h])\n            decoder_expanded = decoder_state.unsqueeze(1).expand(-1, src_len, -1)\n            concat = torch.cat([decoder_expanded, encoder_outputs], dim=-1)\n            energy = torch.tanh(self.W_a(concat))  # [batch, src_len, dec_hidden]\n            scores = self.v_a(energy).squeeze(-1)  # [batch, src_len]\n\n        # 应用 mask\n        if mask is not None:\n            scores = scores.masked_fill(mask, -1e10)\n\n        # Softmax\n        attention_weights = F.softmax(scores, dim=-1)\n\n        # 上下文向量\n        context = torch.bmm(\n            attention_weights.unsqueeze(1),\n            encoder_outputs\n        ).squeeze(1)\n\n        return context, attention_weights\n```\n:::\n\n\n### 实现Local Attention\n\n::: {#9a405fd0 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nclass LocalAttention(nn.Module):\n    \"\"\"\n    Luong 的 Local Attention（预测型）\n    \"\"\"\n    def __init__(self, enc_hidden_dim, dec_hidden_dim, window_size=10):\n        super().__init__()\n        self.window_size = window_size  # D: 窗口半径\n        self.enc_hidden_dim = enc_hidden_dim\n\n        # 位置预测网络\n        self.W_p = nn.Linear(dec_hidden_dim, dec_hidden_dim)\n        self.v_p = nn.Linear(dec_hidden_dim, 1)\n\n        # 对齐函数（使用 general）\n        self.W_a = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)\n\n        # 高斯标准差\n        self.sigma = window_size / 2\n\n    def forward(self, decoder_state, encoder_outputs, mask=None):\n        \"\"\"\n        decoder_state: [batch, dec_hidden]\n        encoder_outputs: [batch, src_len, enc_hidden]\n        \"\"\"\n        batch_size, src_len, _ = encoder_outputs.shape\n        device = decoder_state.device\n\n        # Step 1: 预测对齐位置 p\n        # p = S * sigmoid(v^T tanh(W_p s))\n        p = src_len * torch.sigmoid(\n            self.v_p(torch.tanh(self.W_p(decoder_state)))\n        ).squeeze(-1)  # [batch]\n\n        # Step 2: 计算所有位置的对齐分数\n        transformed = self.W_a(encoder_outputs)  # [batch, src_len, dec_hidden]\n        scores = torch.bmm(\n            decoder_state.unsqueeze(1),\n            transformed.transpose(1, 2)\n        ).squeeze(1)  # [batch, src_len]\n\n        # Step 3: 应用高斯窗口\n        # 生成位置索引 [0, 1, 2, ..., src_len-1]\n        positions = torch.arange(src_len, device=device).float()\n        positions = positions.unsqueeze(0).expand(batch_size, -1)  # [batch, src_len]\n\n        # 高斯权重: exp(-(j - p)^2 / (2 * sigma^2))\n        gaussian = torch.exp(-((positions - p.unsqueeze(1)) ** 2) / (2 * self.sigma ** 2))\n\n        # Step 4: 窗口mask（只保留 [p-D, p+D] 范围内的位置）\n        window_mask = (positions >= (p.unsqueeze(1) - self.window_size)) & \\\n                      (positions <= (p.unsqueeze(1) + self.window_size))\n\n        # 应用窗口mask\n        scores = scores.masked_fill(~window_mask, -1e10)\n\n        # Step 5: Softmax + 高斯加权\n        attention_weights = F.softmax(scores, dim=-1) * gaussian\n\n        # 重新归一化\n        attention_weights = attention_weights / (attention_weights.sum(dim=-1, keepdim=True) + 1e-10)\n\n        # 上下文向量\n        context = torch.bmm(\n            attention_weights.unsqueeze(1),\n            encoder_outputs\n        ).squeeze(1)\n\n        return context, attention_weights, p\n```\n:::\n\n\n### 对比实验\n\n::: {#b2e8eee4 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\n# 创建测试数据\nbatch_size = 2\nsrc_len = 10\nenc_hidden = 64\ndec_hidden = 64\n\nencoder_outputs = torch.randn(batch_size, src_len, enc_hidden)\ndecoder_state = torch.randn(batch_size, dec_hidden)\n\n# 测试三种 Luong Attention\nfor method in ['dot', 'general', 'concat']:\n    attn = LuongAttention(enc_hidden, dec_hidden, method=method)\n    context, weights = attn(decoder_state, encoder_outputs)\n    print(f\"{method:8s}: context shape = {context.shape}, weights sum = {weights.sum(dim=-1)}\")\n\n# 测试 Local Attention\nlocal_attn = LocalAttention(enc_hidden, dec_hidden, window_size=3)\ncontext, weights, p = local_attn(decoder_state, encoder_outputs)\nprint(f\"{'local':8s}: context shape = {context.shape}, predicted p = {p.tolist()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndot     : context shape = torch.Size([2, 64]), weights sum = tensor([1., 1.])\ngeneral : context shape = torch.Size([2, 64]), weights sum = tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\nconcat  : context shape = torch.Size([2, 64]), weights sum = tensor([1., 1.], grad_fn=<SumBackward1>)\nlocal   : context shape = torch.Size([2, 64]), predicted p = [4.633298397064209, 5.614270210266113]\n```\n:::\n:::\n\n\n---\n\n## 深入理解\n\n### 为什么点积注意力能工作？——理论视角\n\n点积注意力的有效性可以从多个角度理解。\n\n**余弦相似度视角**：当向量被归一化后，点积就是余弦相似度：\n\n$$\n\\mathbf{s}^\\top \\mathbf{h} = \\|\\mathbf{s}\\| \\|\\mathbf{h}\\| \\cos(\\theta)\n$$\n\n余弦相似度是衡量两个向量\"方向一致性\"的经典指标。神经网络在训练过程中，会学习让相关的状态指向相似的方向。\n\n**核方法视角**：点积可以看作一个线性核（linear kernel）。在核方法的框架下，注意力权重实际上是在一个特征空间中计算相似度。General Attention引入的可学习矩阵 $\\mathbf{W}_a$ 相当于学习一个Mahalanobis距离。\n\n**信息检索视角**：点积注意力可以类比为向量空间模型中的查询-文档匹配。解码器状态是\"查询\"，编码器状态是\"文档\"，点积衡量查询与文档的相关性。\n\n### 为什么需要缩放？——Scaled Dot-Product的预兆\n\nLuong的论文没有讨论这个问题，但后来的Transformer论文（Vaswani et al., 2017）指出了点积注意力的一个潜在问题：\n\n当向量维度 $d$ 很大时，点积的方差会很大。假设 $\\mathbf{s}$ 和 $\\mathbf{h}$ 的每个分量都是独立的、均值为0、方差为1的随机变量，那么：\n\n$$\n\\text{Var}(\\mathbf{s}^\\top \\mathbf{h}) = d\n$$\n\n当 $d = 512$ 时，点积的标准差是 $\\sqrt{512} \\approx 22.6$。这意味着点积可能产生很大的正值或负值，导致softmax输出接近one-hot分布，梯度变得很小。\n\n解决方案是**缩放**：\n\n$$\n\\text{score}(\\mathbf{s}, \\mathbf{h}) = \\frac{\\mathbf{s}^\\top \\mathbf{h}}{\\sqrt{d}}\n$$\n\n这就是Transformer中的**Scaled Dot-Product Attention**。Luong的论文使用的维度较小（500左右），问题不太明显；但在Transformer的大维度设置下，缩放变得必要。\n\n### 方法的边界条件\n\n**Dot-Product Attention的局限**：\n\n1. **维度必须匹配**：解码器和编码器的隐藏维度必须相同，否则无法计算点积\n2. **表达能力有限**：无法学习复杂的相关性模式，只能捕捉\"方向相似\"的关系\n\n**Local Attention的局限**：\n\n1. **需要预测对齐位置**：如果位置预测错误，会错过重要信息\n2. **不适合非单调对齐**：对于语序差异大的语言对，局部窗口可能覆盖不到正确位置\n3. **窗口大小是超参数**：选择不当会影响性能\n\n**General/Concat的局限**：\n\n1. **计算开销**：额外的矩阵乘法增加了计算量\n2. **过拟合风险**：更多参数可能导致在小数据集上过拟合\n\n### 开放研究问题\n\n1. **对齐函数的最优选择**：在什么条件下应该选择哪种对齐函数？是否有理论指导？\n\n2. **动态窗口**：Local Attention使用固定窗口大小，能否根据内容动态调整？\n\n3. **多粒度注意力**：能否同时使用全局和局部注意力，在不同粒度上捕获信息？\n\n---\n\n## 局限性与展望\n\n### 本章方法的核心局限\n\n**1. 注意力仍然是RNN的\"附属品\"**\n\n无论是Bahdanau还是Luong的注意力，都是Seq2Seq架构的增强组件。编码和解码的核心仍然依赖RNN。这意味着：\n\n- 顺序计算无法避免：RNN必须逐步处理序列\n- 长距离依赖仍然困难：虽然Attention提供了捷径，但RNN本身的问题没有解决\n\n**2. 注意力只在编码器-解码器之间**\n\n当前的Attention只让解码器关注编码器。但一个自然的问题是：**编码器内部的各个位置能否相互关注？** 一个词的理解可能依赖于同一句话中的其他词，而当前的架构没有提供这种机制。\n\n**3. 位置信息是隐式的**\n\nAttention本身不包含位置信息。位置信息完全依赖RNN的顺序处理来隐式编码。如果抛弃RNN，位置信息将完全丢失。\n\n### 这些局限指向什么？\n\nLuong的工作完成了对Seq2Seq Attention的系统性探索，确立了一些最佳实践（如点积注意力的效率优势）。但更深层的问题浮现：\n\n**能否让Attention独立于RNN？**\n\n如果Attention如此有效，为什么还需要RNN？能否设计一个完全基于Attention的架构？\n\n**能否让序列中的每个位置相互关注？**\n\n当前的Attention是\"跨序列\"的（解码器关注编码器）。如果让同一序列内的位置相互关注——这就是**Self-Attention**——会发生什么？\n\n这些问题的答案，将在接下来的两章揭晓：第7章将介绍Self-Attention的诞生，第8章将介绍革命性的Transformer架构——\"Attention Is All You Need\"。\n\n> 从加性到乘性，从全局到局部，Luong的探索为Attention机制建立了理论和实践的基础。但真正的革命还在后面：当研究者意识到**注意力本身就可以成为架构的核心**，深度学习的历史翻开了新的一页。\n\n---\n\n## 本章小结\n\n::: {.callout-important}\n## 核心要点\n\n- **问题**：Bahdanau的加性注意力有效但计算较慢，注意力机制的设计空间还有很多未探索的选择\n- **洞察**：点积注意力可以用简单的向量内积计算相关性，大幅提高计算效率；局部注意力可以减少长序列的计算开销\n- **方法**：Luong系统比较了dot/general/concat三种对齐函数，以及global/local两种范围策略\n- **意义**：确立了点积注意力的效率优势，为后来Transformer的Scaled Dot-Product Attention奠定基础\n:::\n\n### 关键公式速查\n\n**Dot-Product Attention**：\n\n$$\n\\text{score}(\\mathbf{s}, \\mathbf{h}) = \\mathbf{s}^\\top \\mathbf{h}\n$$\n\n**General Attention**：\n\n$$\n\\text{score}(\\mathbf{s}, \\mathbf{h}) = \\mathbf{s}^\\top \\mathbf{W}_a \\mathbf{h}\n$$\n\n**Concat (Additive) Attention**：\n\n$$\n\\text{score}(\\mathbf{s}, \\mathbf{h}) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a [\\mathbf{s}; \\mathbf{h}])\n$$\n\n**Local Attention位置预测**：\n\n$$\np_i = T_x \\cdot \\sigma(\\mathbf{v}_p^\\top \\tanh(\\mathbf{W}_p \\mathbf{s}_i))\n$$\n\n---\n\n## 思考题\n\n1. **[概念理解]** 点积注意力和加性注意力在表达能力上有什么本质区别？设计一个简单的例子，展示加性注意力能学习而点积注意力无法学习的相关性模式。\n\n2. **[数学推导]** 证明：当 $\\mathbf{W}_a$ 是单位矩阵时，General Attention退化为Dot-Product Attention。更一般地，如果 $\\mathbf{W}_a = \\mathbf{U}\\mathbf{V}^\\top$（秩-$r$分解），这对注意力模式有什么影响？\n\n3. **[工程实践]** 实现一个支持多头（multi-head）的Luong Attention。每个头使用不同的 $\\mathbf{W}_a$，最后拼接所有头的输出。对比单头和多头在翻译任务上的表现。\n\n4. **[批判思考]** Local Attention假设对齐是大致单调的（源和目标的位置对应）。对于哪些语言对或任务，这个假设会严重失效？能否设计一种\"非单调局部注意力\"？\n\n5. **[开放问题]** Hard Attention虽然训练困难，但它有一个优势：稀疏性可以提高可解释性。有没有方法既保持Soft Attention的可微分性，又能获得接近Hard Attention的稀疏性？（提示：考虑稀疏softmax、Gumbel-softmax）\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n- **[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**\n  - 本章的核心论文，系统比较不同注意力变体\n  - 重点读：Section 3（Global vs Local）、Section 4（实验对比）\n  - arXiv: [1508.04025](https://arxiv.org/abs/1508.04025)\n\n### 理论基础\n\n- **[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**\n  - 上一章的核心，Attention的开山之作\n  - arXiv: [1409.0473](https://arxiv.org/abs/1409.0473)\n\n### 后续发展\n\n- **[Vaswani et al., 2017] Attention Is All You Need**\n  - 提出Scaled Dot-Product Attention和Multi-Head Attention\n  - 完全抛弃RNN，只用Attention构建模型\n  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)\n\n- **[Xu et al., 2015] Show, Attend and Tell**\n  - Hard Attention在图像描述中的应用\n  - 对比Soft和Hard Attention的效果\n  - arXiv: [1502.03044](https://arxiv.org/abs/1502.03044)\n\n### 技术报告\n\n- **[Britz et al., 2017] Massive Exploration of Neural Machine Translation Architectures**\n  - 大规模对比NMT的各种设计选择\n  - 包括注意力变体的实证对比\n  - arXiv: [1703.03906](https://arxiv.org/abs/1703.03906)\n\n---\n\n## 历史注脚\n\nLuong的论文发表于2015年EMNLP，距离Bahdanau的论文仅一年。在这短短一年里，Attention迅速成为NMT的标准配置，各种变体层出不穷。Luong的工作之所以重要，不仅在于提出了新的变体，更在于它**系统性地比较和总结**了当时的各种方法，为后来者提供了清晰的设计指南。\n\n有趣的是，Luong论文中提到的点积注意力（Dot-Product）因为过于简单而没有受到太多关注。当时的主流仍然是参数化的对齐函数。但两年后，当Transformer论文提出\"Scaled Dot-Product Attention\"时，点积注意力终于登上了历史舞台的中央——它不仅简单高效，而且在大规模设置下与更复杂的对齐函数表现相当。\n\n从某种意义上说，Luong的论文是Attention发展史上的一个\"中场休息\"——它总结了第一阶段的探索，为第二阶段（Self-Attention和Transformer）的革命铺平了道路。\n\n",
    "supporting": [
      "ch06-attention-variants_files"
    ],
    "filters": [],
    "includes": {}
  }
}