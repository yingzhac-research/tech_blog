{
  "hash": "243456ecd3deebe54854a61517555225",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ç¬¬7ç« ï¼šSelf-Attentionçš„çªç ´\"\nsubtitle: \"å½“åºåˆ—å¼€å§‹å®¡è§†è‡ªå·±\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-25\"\ncategories: [NLP, Attention, Self-Attention, Memory Networks, ä½ç½®ç¼–ç ]\ntags: [è‡ªæ³¨æ„åŠ›, Q-K-V, ä½ç½®ç¼–ç , æ­£å¼¦ç¼–ç , ç½®æ¢ä¸å˜æ€§, Decomposable Attention]\ndescription: \"ä»è·¨åºåˆ—æ³¨æ„åŠ›åˆ°è‡ªæ³¨æ„åŠ›ï¼šåºåˆ—å¦‚ä½•å®¡è§†è‡ªå·±ï¼ŒQ-K-Væ¡†æ¶çš„å»ºç«‹ï¼Œä»¥åŠä½ç½®ä¿¡æ¯ç¼ºå¤±çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\n---\n\n> **æ ¸å¿ƒé—®é¢˜**ï¼šåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®èƒ½å¦ç›´æ¥å…³æ³¨åŒä¸€åºåˆ—ä¸­çš„å…¶ä»–ä½ç½®ï¼Ÿå¦‚æœå¯ä»¥ï¼Œè¿™ç§\"è‡ªæˆ‘å®¡è§†\"çš„æœºåˆ¶ä¼šå¸¦æ¥ä»€ä¹ˆçªç ´ï¼Œåˆä¼šä¸¢å¤±ä»€ä¹ˆï¼Ÿ\n>\n> **å†å²åæ ‡**ï¼š2015-2017 | Memory Networks, Decomposable Attention | ä»è¾…åŠ©æœºåˆ¶åˆ°æ ¸å¿ƒæ¶æ„çš„è½¬å˜\n\n---\n\n## ä»ä¸Šä¸€ç« è¯´èµ·\n\nä¸Šä¸€ç« æˆ‘ä»¬ç³»ç»Ÿæ¢ç´¢äº†Attentionæœºåˆ¶çš„è®¾è®¡ç©ºé—´ï¼šåŠ æ€§ä¸ä¹˜æ€§å¯¹é½å‡½æ•°çš„æƒè¡¡ã€å…¨å±€ä¸å±€éƒ¨èŒƒå›´çš„å–èˆã€è½¯æ³¨æ„åŠ›ä¸ç¡¬æ³¨æ„åŠ›çš„å·®å¼‚ã€‚Luongçš„å·¥ä½œç¡®ç«‹äº†ç‚¹ç§¯æ³¨æ„åŠ›çš„æ•ˆç‡ä¼˜åŠ¿ï¼Œä¸ºåæ¥çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚ä½†æ— è®ºæ˜¯Bahdanauè¿˜æ˜¯Luongï¼Œä»–ä»¬çš„Attentionéƒ½æœ‰ä¸€ä¸ªå…±åŒçš„ç‰¹ç‚¹â€”â€”å®ƒæ˜¯**è·¨åºåˆ—**çš„ï¼Œè®©è§£ç å™¨å…³æ³¨ç¼–ç å™¨ã€‚\n\nç„¶è€Œï¼Œä¸Šä¸€ç« çš„ç»“å°¾ç•™ä¸‹äº†ä¸¤ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚\n\nç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼š**Attentionèƒ½å¦ç‹¬ç«‹äºRNNï¼Ÿ** å½“å‰çš„Attentionåªæ˜¯RNNæ¶æ„çš„å¢å¼ºç»„ä»¶ã€‚ç¼–ç å™¨æ˜¯RNNï¼Œè§£ç å™¨ä¹Ÿæ˜¯RNNï¼ŒAttentionåªæ˜¯åœ¨å®ƒä»¬ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚é¡ºåºè®¡ç®—çš„ç“¶é¢ˆã€é•¿è·ç¦»ä¾èµ–çš„æ¢¯åº¦é—®é¢˜ï¼Œè¿™äº›RNNçš„å›ºæœ‰ç¼ºé™·å¹¶æ²¡æœ‰è¢«Attentionè§£å†³ã€‚\n\nç¬¬äºŒä¸ªé—®é¢˜æ›´åŠ æ ¹æœ¬ï¼š**åŒä¸€åºåˆ—å†…éƒ¨çš„ä½ç½®èƒ½å¦ç›¸äº’å…³æ³¨ï¼Ÿ** è€ƒè™‘è¿™å¥è¯ï¼š\"The animal didn't cross the street because it was too tired.\" è¦ç†è§£\"it\"æŒ‡ä»£ä»€ä¹ˆï¼Œéœ€è¦å»ºç«‹\"it\"ä¸\"animal\"ä¹‹é—´çš„è”ç³»ã€‚åœ¨å½“å‰çš„RNNç¼–ç å™¨ä¸­ï¼Œè¿™ç§è”ç³»åªèƒ½é€šè¿‡é€æ­¥ä¼ é€’æ¥éšå¼å»ºç«‹ã€‚ä½†å¦‚æœ\"it\"èƒ½å¤Ÿç›´æ¥\"çœ‹åˆ°\"å¥å­ä¸­çš„å…¶ä»–è¯ï¼Œç›´æ¥è®¡ç®—ä¸\"animal\"å’Œ\"street\"çš„ç›¸å…³æ€§ï¼Œç†è§£ä¸æ˜¯ä¼šæ›´åŠ ç›´æ¥å—ï¼Ÿ\n\nè¿™å°±å¼•å‡ºäº†æœ¬ç« çš„ä¸»è§’ï¼š**Self-Attention**â€”â€”è®©åºåˆ—\"å®¡è§†è‡ªå·±\"çš„æœºåˆ¶ã€‚\n\n> ğŸ’¡ **æœ¬ç« æ ¸å¿ƒæ´å¯Ÿ**ï¼šSelf-Attentionè®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®å¯ä»¥ç›´æ¥å…³æ³¨åŒä¸€åºåˆ—çš„æ‰€æœ‰å…¶ä»–ä½ç½®ï¼Œä¸€æ­¥åˆ°ä½åœ°å»ºç«‹ä»»æ„è·ç¦»çš„ä¾èµ–å…³ç³»ã€‚è¿™ä¸ä»…çªç ´äº†RNNçš„é¡ºåºç“¶é¢ˆï¼Œè¿˜ä¸ºå®Œå…¨å¹¶è¡Œçš„è®¡ç®—æ‰“å¼€äº†å¤§é—¨ã€‚ä½†è¿™ç§èƒ½åŠ›æ˜¯æœ‰ä»£ä»·çš„â€”â€”Self-Attentionå¤©ç”Ÿä¸¢å¤±äº†ä½ç½®ä¿¡æ¯ï¼Œå¦‚ä½•å¼¥è¡¥è¿™ä¸€ç¼ºé™·å°†æˆä¸ºå…³é”®çš„è®¾è®¡æŒ‘æˆ˜ã€‚\n\n---\n\n## é—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ\n\n### é—®é¢˜çš„ç²¾ç¡®å®šä¹‰\n\næˆ‘ä»¬è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼š**å¦‚ä½•è®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®éƒ½èƒ½è·å¾—å…³äºæ•´ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Ÿ**\n\næ›´æ­£å¼åœ°è¯´ï¼Œç»™å®šè¾“å…¥åºåˆ— $\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n)$ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºæ¯ä¸ªä½ç½® $i$ ç”Ÿæˆä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤º $\\mathbf{z}_i$ï¼Œä½¿å¾— $\\mathbf{z}_i$ èƒ½å¤Ÿç¼–ç æ¥è‡ªæ•´ä¸ªåºåˆ—çš„ç›¸å…³ä¿¡æ¯ã€‚\n\nåœ¨RNNä¸­ï¼Œè¿™æ˜¯é€šè¿‡é¡ºåºè®¡ç®—å®ç°çš„ï¼š\n\n$$\n\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{x}_t)\n$$\n\néšè—çŠ¶æ€ $\\mathbf{h}_t$ éšå¼åœ°\"å‹ç¼©\"äº†ä½ç½® $1$ åˆ° $t$ çš„æ‰€æœ‰ä¿¡æ¯ã€‚åŒå‘RNNåˆ™åŒæ—¶è€ƒè™‘å‰å‘å’Œåå‘çš„ä¿¡æ¯ã€‚ä½†è¿™ç§æ–¹å¼æœ‰ä¸¤ä¸ªæ ¹æœ¬é—®é¢˜ï¼š\n\n**é¡ºåºä¾èµ–**ï¼š$\\mathbf{h}_t$ å¿…é¡»ç­‰ $\\mathbf{h}_{t-1}$ è®¡ç®—å®Œæˆï¼Œæ— æ³•å¹¶è¡Œã€‚\n\n**é—´æ¥è¿æ¥**ï¼šä½ç½® $1$ çš„ä¿¡æ¯è¦åˆ°è¾¾ä½ç½® $n$ï¼Œå¿…é¡»ç»è¿‡ $n-1$ æ¬¡éçº¿æ€§å˜æ¢ã€‚æ¯æ¬¡å˜æ¢éƒ½ä¼šå¸¦æ¥ä¿¡æ¯æŸå¤±ã€‚\n\n### ä¸€ä¸ªå…³é”®çš„æ€æƒ³å®éªŒ\n\nè®©æˆ‘ä»¬åšä¸€ä¸ªæ€æƒ³å®éªŒã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¥å­ \"I love natural language processing\"ï¼ŒåŒ…å«5ä¸ªè¯ã€‚ç°åœ¨è€ƒè™‘ä¸¤ç§æ–¹å¼æ¥å»ºç«‹è¯ä¸è¯ä¹‹é—´çš„è”ç³»ï¼š\n\n**RNNæ–¹å¼**ï¼š\n\n```\nI â†’ love â†’ natural â†’ language â†’ processing\n```\n\n\"I\" çš„ä¿¡æ¯è¦åˆ°è¾¾ \"processing\"ï¼Œéœ€è¦ç»è¿‡ 4 æ­¥ä¼ é€’ã€‚æ¯ä¸€æ­¥ï¼Œä¿¡æ¯éƒ½ä¸æ–°çš„è¾“å…¥æ··åˆï¼Œç»è¿‡éçº¿æ€§å˜æ¢ã€‚æœ€ç»ˆåˆ°è¾¾ \"processing\" æ—¶ï¼Œå…³äº \"I\" çš„ä¿¡æ¯å·²ç»è¢«å¤§é‡ç¨€é‡Šã€‚\n\n**ç†æƒ³æ–¹å¼**ï¼š\n\n```\n     I â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ processing\n       â†–                       â†—\n         love â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ language\n               â†–       â†—\n                 natural\n```\n\næ¯ä¸ªè¯éƒ½å¯ä»¥ç›´æ¥\"çœ‹åˆ°\"å…¶ä»–æ‰€æœ‰è¯ã€‚\"processing\" æƒ³è¦çŸ¥é“ \"I\" çš„ä¿¡æ¯ï¼Ÿç›´æ¥æŸ¥è¯¢ï¼Œä¸€æ­¥åˆ°ä½ã€‚\n\nè¿™å°±æ˜¯Self-Attentionçš„æ ¸å¿ƒæ€æƒ³ï¼š**æŠŠ $O(n)$ çš„é—´æ¥è·¯å¾„å˜æˆ $O(1)$ çš„ç›´æ¥è·¯å¾„**ã€‚\n\n### æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„è§£å†³æ–¹æ¡ˆï¼Ÿ\n\nä»ä¸Šè¿°åˆ†æå¯ä»¥çœ‹å‡ºï¼Œç†æƒ³çš„åºåˆ—å»ºæ¨¡æœºåˆ¶åº”è¯¥æ»¡è¶³ä»¥ä¸‹ç‰¹æ€§ï¼š\n\n1. **ç›´æ¥è¿æ¥**ï¼šä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’æ˜¯ä¸€æ­¥åˆ°ä½çš„ï¼Œè€Œéé€æ­¥ä¼ é€’\n2. **å¹¶è¡Œè®¡ç®—**ï¼šä¸åŒä½ç½®çš„è¡¨ç¤ºå¯ä»¥åŒæ—¶è®¡ç®—ï¼Œä¸éœ€è¦ç­‰å¾…å‰åºè®¡ç®—å®Œæˆ\n3. **åŠ¨æ€æƒé‡**ï¼šå…³æ³¨çš„å¼ºåº¦åº”è¯¥å–å†³äºå†…å®¹çš„ç›¸å…³æ€§ï¼Œè€Œéå›ºå®šçš„ä½ç½®å…³ç³»\n4. **å…¨å±€è§†é‡**ï¼šæ¯ä¸ªä½ç½®éƒ½èƒ½çœ‹åˆ°æ•´ä¸ªåºåˆ—ï¼Œè€Œéåªæœ‰å±€éƒ¨çª—å£\n\nSelf-Attentionæ­£æ˜¯ä¸ºäº†åŒæ—¶æ»¡è¶³è¿™å››ä¸ªéœ€æ±‚è€Œè®¾è®¡çš„ã€‚\n\n---\n\n## æ ¸å¿ƒæ€æƒ³ä¸ç›´è§‰\n\n### ä»Cross-Attentionåˆ°Self-Attention\n\nå›é¡¾ä¸€ä¸‹Seq2Seqä¸­çš„Attentionæœºåˆ¶ï¼š\n\n$$\n\\mathbf{c}_i = \\sum_{j=1}^{T_x} \\alpha_{ij} \\mathbf{h}_j^{(enc)}\n$$\n\nå…¶ä¸­ $\\alpha_{ij}$ è¡¡é‡è§£ç å™¨ä½ç½® $i$ å¯¹ç¼–ç å™¨ä½ç½® $j$ çš„å…³æ³¨ç¨‹åº¦ã€‚è¿™é‡Œçš„å…³é”®æ˜¯ï¼šQueryæ¥è‡ªè§£ç å™¨ï¼ŒKeyå’ŒValueæ¥è‡ªç¼–ç å™¨â€”â€”è¿™æ˜¯**è·¨åºåˆ—**çš„æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰ã€‚\n\nSelf-Attentionçš„æƒ³æ³•æå…¶ç®€å•ï¼š**è®©Queryã€Keyã€Valueéƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—**ã€‚\n\n$$\n\\mathbf{z}_i = \\sum_{j=1}^{n} \\alpha_{ij} \\mathbf{x}_j\n$$\n\nå…¶ä¸­ $\\alpha_{ij}$ è¡¡é‡ä½ç½® $i$ å¯¹ä½ç½® $j$ çš„å…³æ³¨ç¨‹åº¦ã€‚ç°åœ¨ï¼Œåºåˆ—åœ¨\"å®¡è§†è‡ªå·±\"â€”â€”æ¯ä¸ªä½ç½®éƒ½åœ¨é—®ï¼š\"æˆ‘åº”è¯¥å…³æ³¨åºåˆ—ä¸­çš„å“ªäº›å…¶ä»–ä½ç½®ï¼Ÿ\"\n\n### Query-Key-Valueçš„ä¸‰å…ƒç»„è§†è§’\n\nä¸ºäº†è®©Self-Attentionæ›´åŠ çµæ´»ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ç”¨åŸå§‹è¡¨ç¤ºè®¡ç®—æ³¨æ„åŠ›ï¼Œè€Œæ˜¯å…ˆè¿›è¡Œçº¿æ€§å˜æ¢ï¼š\n\n$$\n\\mathbf{q}_i = \\mathbf{W}_Q \\mathbf{x}_i, \\quad \\mathbf{k}_i = \\mathbf{W}_K \\mathbf{x}_i, \\quad \\mathbf{v}_i = \\mathbf{W}_V \\mathbf{x}_i\n$$\n\nç„¶åï¼š\n\n$$\n\\alpha_{ij} = \\text{softmax}_j\\left(\\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j}{\\sqrt{d_k}}\\right)\n$$\n\n$$\n\\mathbf{z}_i = \\sum_{j=1}^{n} \\alpha_{ij} \\mathbf{v}_j\n$$\n\nè¿™ç§Query-Key-Valueçš„ä¸‰å…ƒç»„ç»“æ„æœ‰æ·±åˆ»çš„ç›´è§‰æ„ä¹‰ï¼š\n\n- **Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼šä»£è¡¨\"æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ\"â€”â€”ä½ç½® $i$ æƒ³è¦è·å–ä»€ä¹ˆæ ·çš„ä¿¡æ¯\n- **Keyï¼ˆé”®ï¼‰**ï¼šä»£è¡¨\"æˆ‘æ˜¯ä»€ä¹ˆï¼Ÿ\"â€”â€”ä½ç½® $j$ çš„èº«ä»½æ ‡è¯†ï¼Œç”¨äºåŒ¹é…æŸ¥è¯¢\n- **Valueï¼ˆå€¼ï¼‰**ï¼šä»£è¡¨\"æˆ‘èƒ½æä¾›ä»€ä¹ˆï¼Ÿ\"â€”â€”ä½ç½® $j$ å®é™…è´¡çŒ®çš„å†…å®¹\n\nè¿™ä¸ªè®¾è®¡å…è®¸åŒä¸€ä¸ªä½ç½®ä»¥ä¸åŒçš„\"èº«ä»½\"å‚ä¸è®¡ç®—ï¼šå½“å®ƒä½œä¸ºæŸ¥è¯¢è€…æ—¶ï¼Œå®ƒåœ¨å¯»æ‰¾ç›¸å…³ä¿¡æ¯ï¼›å½“å®ƒä½œä¸ºè¢«æŸ¥è¯¢è€…æ—¶ï¼Œå®ƒçš„Keyå†³å®šæ˜¯å¦è¢«é€‰ä¸­ï¼Œå®ƒçš„Valueå†³å®šè´¡çŒ®ä»€ä¹ˆå†…å®¹ã€‚\n\n### ä¸€ä¸ªå…·ä½“çš„ä¾‹å­\n\nè€ƒè™‘å¥å­ \"The cat sat on the mat\"ï¼Œæˆ‘ä»¬æƒ³ç†è§£è¯ \"sat\" çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚\n\nåœ¨Self-Attentionä¸­ï¼Œ\"sat\" ä¼šï¼š\n\n1. ç”Ÿæˆä¸€ä¸ªQueryå‘é‡ï¼Œç¼–ç \"åŠ¨è¯'sat'åœ¨å¯»æ‰¾å®ƒçš„ä¸»è¯­å’Œå®¾è¯­\"\n2. ä¸æ‰€æœ‰è¯çš„Keyå‘é‡è®¡ç®—ç›¸ä¼¼åº¦\n3. ä¸ \"cat\" çš„Keyé«˜åº¦åŒ¹é…ï¼ˆ\"cat\"æ˜¯ä¸»è¯­ï¼‰ï¼Œä¸ \"mat\" ä¹Ÿæœ‰ä¸€å®šåŒ¹é…ï¼ˆ\"mat\"æ˜¯ä»‹è¯å®¾è¯­çš„å¯¹è±¡ï¼‰\n4. æœ€ç»ˆçš„è¡¨ç¤ºæ˜¯æ‰€æœ‰è¯çš„Valueçš„åŠ æƒå’Œï¼Œ\"cat\" å’Œ \"mat\" è´¡çŒ®è¾ƒå¤š\n\nè¿™ä¸ªè¿‡ç¨‹å®Œå…¨æ˜¯åŸºäºå†…å®¹çš„â€”â€”æ¨¡å‹å­¦ä¹ åˆ°\"åŠ¨è¯é€šå¸¸éœ€è¦å…³æ³¨å®ƒçš„ä¸»è¯­å’Œå®¾è¯­\"ï¼Œè€Œè¿™ç§å­¦ä¹ æ˜¯ä»æ•°æ®ä¸­è‡ªåŠ¨è·å¾—çš„ï¼Œæ— éœ€äººå·¥å®šä¹‰è¯­æ³•è§„åˆ™ã€‚\n\n---\n\n## Memory Networksï¼šSelf-Attentionçš„æ€æƒ³å…ˆé©±\n\n### é—®ç­”ç³»ç»Ÿçš„æŒ‘æˆ˜\n\nåœ¨Self-Attentionè¢«æ˜ç¡®æå‡ºä¹‹å‰ï¼Œæœ‰ä¸€æ¡å¹³è¡Œçš„ç ”ç©¶è„‰ç»œåœ¨æ¢ç´¢ç±»ä¼¼çš„æƒ³æ³•ï¼š**Memory Networks**ã€‚\n\n2014-2015å¹´ï¼ŒFacebook AI Researchï¼ˆSukhbaatar, Szegedy, Westonç­‰äººï¼‰æå‡ºäº†Memory Networksæ¥è§£å†³éœ€è¦å¤šè·³æ¨ç†çš„é—®ç­”ä»»åŠ¡ã€‚è€ƒè™‘ä»¥ä¸‹ä¾‹å­ï¼š\n\n**æ•…äº‹**ï¼š\n> Mary moved to the bathroom.\n> John went to the hallway.\n> Mary traveled to the office.\n\n**é—®é¢˜**ï¼šWhere is Mary?\n\n**ç­”æ¡ˆ**ï¼šoffice\n\nè¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ï¼š\n1. ä»è®°å¿†ä¸­æ‰¾åˆ°ä¸\"Mary\"ç›¸å…³çš„ä¿¡æ¯\n2. è¯†åˆ«å‡ºæœ€æ–°çš„ä½ç½®ä¿¡æ¯\n3. ç»¼åˆè¿™äº›ä¿¡æ¯å¾—å‡ºç­”æ¡ˆ\n\nè¿™æ¯”ç®€å•çš„æ–‡æœ¬åŒ¹é…å¤æ‚å¾—å¤šâ€”â€”éœ€è¦åœ¨å¤šæ¡ä¿¡æ¯ä¹‹é—´å»ºç«‹è”ç³»ï¼Œè¿›è¡Œæ¨ç†ã€‚\n\n### Memory Networksçš„è®¾è®¡\n\nMemory Networksçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå°†ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ªå¤–éƒ¨è®°å¿†ä¸­ï¼Œç„¶åé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ¥è¯»å–ç›¸å…³ä¿¡æ¯ã€‚\n\n![End-To-End Memory Networksçš„æ¶æ„ã€‚è¾“å…¥å¥å­è¢«ç¼–ç ä¸ºè®°å¿†å‘é‡ï¼ˆçº¢è‰²ï¼‰ï¼Œé—®é¢˜è¢«ç¼–ç ä¸ºæŸ¥è¯¢å‘é‡ï¼ˆè“è‰²ï¼‰ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä»è®°å¿†ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚](figures/chapter-7/original/fig-memory-network.png){#fig-memory-network width=70%}\n\n::: {.figure-caption}\n*Source: Sukhbaatar et al. (2015) \"End-To-End Memory Networks\", Figure 1. [arXiv:1503.08895](https://arxiv.org/abs/1503.08895)*\n:::\n\nå…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ç»„è®°å¿† $\\{\\mathbf{m}_1, \\mathbf{m}_2, \\ldots, \\mathbf{m}_n\\}$ å’Œä¸€ä¸ªæŸ¥è¯¢ $\\mathbf{q}$ï¼š\n\n**Step 1: è®¡ç®—æ³¨æ„åŠ›æƒé‡**\n\n$$\np_i = \\text{softmax}(\\mathbf{q}^\\top \\mathbf{m}_i)\n$$\n\n**Step 2: è¯»å–è®°å¿†**\n\n$$\n\\mathbf{o} = \\sum_{i=1}^{n} p_i \\mathbf{c}_i\n$$\n\nå…¶ä¸­ $\\mathbf{c}_i$ æ˜¯è®°å¿† $i$ çš„è¾“å‡ºè¡¨ç¤ºï¼ˆå¯ä»¥ä¸ $\\mathbf{m}_i$ ä¸åŒï¼‰ã€‚\n\n**Step 3: æ›´æ–°æŸ¥è¯¢å¹¶è¿­ä»£**\n\n$$\n\\mathbf{q}' = \\mathbf{q} + \\mathbf{o}\n$$\n\nç„¶åç”¨ $\\mathbf{q}'$ å†æ¬¡æŸ¥è¯¢è®°å¿†ï¼Œè¿›è¡Œå¤šè·³æ¨ç†ã€‚\n\n### Memory Networksä¸Self-Attentionçš„è”ç³»\n\nçœ‹å‡ºæ¥äº†å—ï¼ŸMemory Networksä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¸Self-AttentionæƒŠäººåœ°ç›¸ä¼¼ï¼š\n\n| Memory Networks | Self-Attention |\n|-----------------|----------------|\n| æŸ¥è¯¢ $\\mathbf{q}$ | Query $\\mathbf{Q}$ |\n| è®°å¿† $\\mathbf{m}_i$ | Key $\\mathbf{K}$ |\n| è¾“å‡º $\\mathbf{c}_i$ | Value $\\mathbf{V}$ |\n| $p_i = \\text{softmax}(\\mathbf{q}^\\top \\mathbf{m}_i)$ | $\\alpha = \\text{softmax}(\\mathbf{Q}\\mathbf{K}^\\top / \\sqrt{d})$ |\n| $\\mathbf{o} = \\sum_i p_i \\mathbf{c}_i$ | $\\mathbf{Z} = \\alpha \\mathbf{V}$ |\n\nå…³é”®çš„è”ç³»åœ¨äºï¼š**Self-Attentionå¯ä»¥çœ‹ä½œæ˜¯åºåˆ—å¯¹è‡ªå·±è¿›è¡ŒMemory Networkå¼çš„æŸ¥è¯¢**ã€‚\n\næ¯ä¸ªä½ç½®éƒ½æ˜¯ä¸€ä¸ª\"è®°å¿†æ§½\"ï¼Œæ¯ä¸ªä½ç½®ä¹Ÿéƒ½æ˜¯ä¸€ä¸ª\"æŸ¥è¯¢è€…\"ã€‚å½“ä½ç½® $i$ è®¡ç®—å®ƒçš„è¾“å‡ºæ—¶ï¼Œå®ƒå°±åƒåœ¨ç”¨è‡ªå·±çš„Queryå»æ£€ç´¢å…¶ä»–ä½ç½®ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰å­˜å‚¨çš„ä¿¡æ¯ã€‚\n\nè¿™ä¸ªè§†è§’è§£é‡Šäº†ä¸ºä»€ä¹ˆSelf-Attentionå¦‚æ­¤å¼ºå¤§ï¼šå®ƒæœ¬è´¨ä¸Šæ˜¯åœ¨åš**åŸºäºå†…å®¹çš„å…³è”è®°å¿†æ£€ç´¢**ï¼Œè€Œè¿™æ­£æ˜¯æ¨ç†å’Œç†è§£çš„æ ¸å¿ƒèƒ½åŠ›ã€‚\n\n### ä»Memory Networksåˆ°Transformer\n\nMemory Networksçš„å·¥ä½œå¯¹åæ¥çš„Transformeræœ‰æ·±è¿œå½±å“ï¼š\n\n1. **ç«¯åˆ°ç«¯å¯å¾®åˆ†**ï¼šæ—©æœŸçš„Memory Networkséœ€è¦å¼ºç›‘ç£ï¼ˆå‘Šè¯‰æ¨¡å‹åº”è¯¥å…³æ³¨å“ªäº›è®°å¿†ï¼‰ï¼Œåæ¥çš„End-to-End Memory Networksé€šè¿‡è½¯æ³¨æ„åŠ›å®ç°äº†ç«¯åˆ°ç«¯è®­ç»ƒâ€”â€”è¿™æ­£æ˜¯Transformeræ‰€é‡‡ç”¨çš„æ–¹å¼ã€‚\n\n2. **å¤šè·³æ¨ç†**ï¼šMemory Networksé€šè¿‡å¤šæ¬¡æŸ¥è¯¢è®°å¿†æ¥å®ç°å¤æ‚æ¨ç†ã€‚Transformerä¸­çš„å¤šå±‚å †å ä¹Ÿå¯ä»¥ç†è§£ä¸ºå¤šè·³æ¨ç†â€”â€”æ¯ä¸€å±‚çš„Self-Attentionéƒ½æ˜¯ä¸€æ¬¡\"æŸ¥è¯¢\"ã€‚\n\n3. **Key-Valueåˆ†ç¦»**ï¼šMemory Networksä¸­ $\\mathbf{m}_i$ï¼ˆç”¨äºè®¡ç®—æ³¨æ„åŠ›ï¼‰å’Œ $\\mathbf{c}_i$ï¼ˆç”¨äºè¾“å‡ºï¼‰å¯ä»¥ä¸åŒã€‚è¿™å¯å‘äº†Self-Attentionä¸­Keyå’ŒValueçš„åˆ†ç¦»è®¾è®¡ã€‚\n\n---\n\n## æŠ€æœ¯ç»†èŠ‚\n\n### Self-Attentionçš„æ•°å­¦å½¢å¼\n\nè®©æˆ‘ä»¬æ­£å¼å®šä¹‰Self-Attentionçš„è®¡ç®—è¿‡ç¨‹ã€‚\n\nç»™å®šè¾“å…¥åºåˆ— $\\mathbf{X} = [\\mathbf{x}_1; \\mathbf{x}_2; \\ldots; \\mathbf{x}_n]^\\top \\in \\mathbb{R}^{n \\times d}$ï¼ŒSelf-Attentionçš„è®¡ç®—å¦‚ä¸‹ï¼š\n\n**Step 1: çº¿æ€§æŠ•å½±**\n\n$$\n\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X} \\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X} \\mathbf{W}_V\n$$\n\nå…¶ä¸­ $\\mathbf{W}_Q, \\mathbf{W}_K \\in \\mathbb{R}^{d \\times d_k}$ï¼Œ$\\mathbf{W}_V \\in \\mathbb{R}^{d \\times d_v}$ã€‚\n\n**Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**\n\n$$\n\\mathbf{S} = \\mathbf{Q} \\mathbf{K}^\\top \\in \\mathbb{R}^{n \\times n}\n$$\n\nçŸ©é˜µ $\\mathbf{S}$ çš„å…ƒç´  $S_{ij} = \\mathbf{q}_i^\\top \\mathbf{k}_j$ è¡¡é‡ä½ç½® $i$ å¯¹ä½ç½® $j$ çš„\"å…³æ³¨ç¨‹åº¦\"ã€‚\n\n**Step 3: ç¼©æ”¾å’Œå½’ä¸€åŒ–**\n\n$$\n\\mathbf{A} = \\text{softmax}\\left(\\frac{\\mathbf{S}}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{n \\times n}\n$$\n\næ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¡¨ç¤ºè¯¥ä½ç½®å¯¹æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ã€‚\n\n**Step 4: åŠ æƒèšåˆ**\n\n$$\n\\mathbf{Z} = \\mathbf{A} \\mathbf{V} \\in \\mathbb{R}^{n \\times d_v}\n$$\n\nè¾“å‡º $\\mathbf{z}_i = \\sum_{j=1}^{n} A_{ij} \\mathbf{v}_j$ æ˜¯æ‰€æœ‰Valueå‘é‡çš„åŠ æƒå’Œã€‚\n\n::: {.callout-note}\n## Algorithm: Scaled Dot-Product Self-Attention (Vaswani et al., 2017)\n\n```python\ndef self_attention(X, W_Q, W_K, W_V):\n    \"\"\"\n    Self-Attention è®¡ç®—\n\n    å‚æ•°:\n        X: è¾“å…¥åºåˆ— [batch, seq_len, d_model]\n        W_Q, W_K: æŠ•å½±çŸ©é˜µ [d_model, d_k]\n        W_V: æŠ•å½±çŸ©é˜µ [d_model, d_v]\n\n    è¿”å›:\n        Z: è¾“å‡ºåºåˆ— [batch, seq_len, d_v]\n        A: æ³¨æ„åŠ›æƒé‡ [batch, seq_len, seq_len]\n    \"\"\"\n    # Step 1: çº¿æ€§æŠ•å½±\n    Q = X @ W_Q  # [batch, seq_len, d_k]\n    K = X @ W_K  # [batch, seq_len, d_k]\n    V = X @ W_V  # [batch, seq_len, d_v]\n\n    # Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n    d_k = Q.shape[-1]\n    scores = Q @ K.transpose(-2, -1)  # [batch, seq_len, seq_len]\n\n    # Step 3: ç¼©æ”¾ + Softmax\n    scores = scores / math.sqrt(d_k)\n    A = F.softmax(scores, dim=-1)\n\n    # Step 4: åŠ æƒèšåˆ\n    Z = A @ V  # [batch, seq_len, d_v]\n\n    return Z, A\n```\n\n*Adapted from: Vaswani, A. et al. (2017). \"Attention Is All You Need\". NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*\n:::\n\n### å®Œæ•´æ•°å€¼ç¤ºä¾‹ï¼šä»è¾“å…¥åˆ°è¾“å‡º\n\nè®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå°è§„æ¨¡çš„ä¾‹å­å®Œæ•´æ¼”ç¤ºSelf-Attentionçš„è®¡ç®—è¿‡ç¨‹ã€‚\n\n**è®¾å®š**ï¼šå¥å­ \"I love NLP\"ï¼Œå…±3ä¸ªè¯ã€‚$d_{model} = 4$ï¼Œ$d_k = d_v = 4$ã€‚\n\n**Step 1: è¾“å…¥è¡¨ç¤º**ï¼ˆå‡è®¾å·²ç»é€šè¿‡embeddingè·å¾—ï¼‰\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\n\\mathbf{x}_I \\\\\n\\mathbf{x}_{love} \\\\\n\\mathbf{x}_{NLP}\n\\end{bmatrix} = \\begin{bmatrix}\n1.0 & 0.0 & 1.0 & 0.0 \\\\\n0.0 & 1.0 & 0.5 & 0.5 \\\\\n1.0 & 1.0 & 0.0 & 0.0\n\\end{bmatrix}\n$$\n\n**Step 2: æŠ•å½±çŸ©é˜µ**ï¼ˆä¸ºç®€åŒ–ï¼Œä½¿ç”¨æ¥è¿‘å•ä½çŸ©é˜µçš„å€¼ï¼‰\n\n$$\n\\mathbf{W}_Q = \\mathbf{W}_K = \\mathbf{W}_V = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n$$\n\nå› æ­¤ $\\mathbf{Q} = \\mathbf{K} = \\mathbf{V} = \\mathbf{X}$ã€‚\n\n**Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° $\\mathbf{Q}\\mathbf{K}^\\top$**\n\n$$\n\\mathbf{S} = \\mathbf{X} \\mathbf{X}^\\top = \\begin{bmatrix}\n\\mathbf{x}_I^\\top \\mathbf{x}_I & \\mathbf{x}_I^\\top \\mathbf{x}_{love} & \\mathbf{x}_I^\\top \\mathbf{x}_{NLP} \\\\\n\\mathbf{x}_{love}^\\top \\mathbf{x}_I & \\mathbf{x}_{love}^\\top \\mathbf{x}_{love} & \\mathbf{x}_{love}^\\top \\mathbf{x}_{NLP} \\\\\n\\mathbf{x}_{NLP}^\\top \\mathbf{x}_I & \\mathbf{x}_{NLP}^\\top \\mathbf{x}_{love} & \\mathbf{x}_{NLP}^\\top \\mathbf{x}_{NLP}\n\\end{bmatrix}\n$$\n\né€ä¸ªè®¡ç®—ï¼š\n- $\\mathbf{x}_I^\\top \\mathbf{x}_I = 1^2 + 0^2 + 1^2 + 0^2 = 2.0$\n- $\\mathbf{x}_I^\\top \\mathbf{x}_{love} = 1 \\times 0 + 0 \\times 1 + 1 \\times 0.5 + 0 \\times 0.5 = 0.5$\n- $\\mathbf{x}_I^\\top \\mathbf{x}_{NLP} = 1 \\times 1 + 0 \\times 1 + 1 \\times 0 + 0 \\times 0 = 1.0$\n- $\\mathbf{x}_{love}^\\top \\mathbf{x}_{love} = 0^2 + 1^2 + 0.5^2 + 0.5^2 = 1.5$\n- $\\mathbf{x}_{love}^\\top \\mathbf{x}_{NLP} = 0 \\times 1 + 1 \\times 1 + 0.5 \\times 0 + 0.5 \\times 0 = 1.0$\n- $\\mathbf{x}_{NLP}^\\top \\mathbf{x}_{NLP} = 1^2 + 1^2 + 0^2 + 0^2 = 2.0$\n\n$$\n\\mathbf{S} = \\begin{bmatrix}\n2.0 & 0.5 & 1.0 \\\\\n0.5 & 1.5 & 1.0 \\\\\n1.0 & 1.0 & 2.0\n\\end{bmatrix}\n$$\n\n**Step 4: ç¼©æ”¾ï¼ˆé™¤ä»¥ $\\sqrt{d_k} = 2$ï¼‰**\n\n$$\n\\frac{\\mathbf{S}}{\\sqrt{4}} = \\begin{bmatrix}\n1.0 & 0.25 & 0.5 \\\\\n0.25 & 0.75 & 0.5 \\\\\n0.5 & 0.5 & 1.0\n\\end{bmatrix}\n$$\n\n**Step 5: Softmaxï¼ˆæŒ‰è¡Œï¼‰**\n\nç¬¬ä¸€è¡Œï¼š$[e^{1.0}, e^{0.25}, e^{0.5}] = [2.72, 1.28, 1.65]$ï¼Œå’Œä¸º $5.65$\n\n$$\n\\alpha_{I} = [2.72/5.65, 1.28/5.65, 1.65/5.65] = [0.48, 0.23, 0.29]\n$$\n\nç±»ä¼¼è®¡ç®—å…¶ä»–è¡Œï¼š\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n0.48 & 0.23 & 0.29 \\\\\n0.26 & 0.43 & 0.31 \\\\\n0.31 & 0.31 & 0.38\n\\end{bmatrix}\n$$\n\n**Step 6: åŠ æƒèšåˆ $\\mathbf{Z} = \\mathbf{A}\\mathbf{V}$**\n\n$$\n\\mathbf{z}_I = 0.48 \\times \\mathbf{x}_I + 0.23 \\times \\mathbf{x}_{love} + 0.29 \\times \\mathbf{x}_{NLP}\n$$\n\n$$\n= 0.48 \\times [1, 0, 1, 0] + 0.23 \\times [0, 1, 0.5, 0.5] + 0.29 \\times [1, 1, 0, 0]\n$$\n\n$$\n= [0.48 + 0 + 0.29, 0 + 0.23 + 0.29, 0.48 + 0.115 + 0, 0 + 0.115 + 0]\n$$\n\n$$\n= [0.77, 0.52, 0.60, 0.12]\n$$\n\n**è§£è¯»**ï¼š\n\n\"I\" çš„Self-Attentionè¾“å‡ºä¸å†åªæ˜¯è‡ªå·±çš„è¡¨ç¤ºï¼Œè€Œæ˜¯èåˆäº†æ•´ä¸ªå¥å­çš„ä¿¡æ¯ã€‚æ³¨æ„åŠ›åˆ†å¸ƒ $[0.48, 0.23, 0.29]$ æ˜¾ç¤ºï¼š\n- \"I\" æœ€å…³æ³¨è‡ªå·±ï¼ˆ0.48ï¼‰â€”â€”è¿™æ˜¯åˆç†çš„ï¼Œè‡ªå·±çš„ä¿¡æ¯æœ€ç›¸å…³\n- å¯¹ \"NLP\"ï¼ˆ0.29ï¼‰çš„å…³æ³¨ç•¥é«˜äº \"love\"ï¼ˆ0.23ï¼‰â€”â€”åœ¨è¿™ä¸ªç®€åŒ–ä¾‹å­ä¸­ï¼Œè¿™æ¥è‡ªäºå‘é‡ç›¸ä¼¼åº¦\n\nåœ¨çœŸå®çš„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè¿™äº›æƒé‡ä¼šå­¦ä¹ åˆ°è¯­è¨€çš„ç»“æ„ï¼šä¸»è¯­ä¼šå…³æ³¨åŠ¨è¯ï¼ŒåŠ¨è¯ä¼šå…³æ³¨å®¾è¯­ï¼Œä»£è¯ä¼šå…³æ³¨å®ƒçš„æŒ‡ä»£å¯¹è±¡ã€‚\n\n### Self-Attentionçš„å¤æ‚åº¦åˆ†æ\n\n**æ—¶é—´å¤æ‚åº¦**ï¼š\n\n- æŠ•å½±ï¼š$O(n \\cdot d \\cdot d_k)$ï¼Œä¸‰æ¬¡æŠ•å½±å…± $O(n \\cdot d^2)$\n- è®¡ç®— $\\mathbf{Q}\\mathbf{K}^\\top$ï¼š$O(n^2 \\cdot d_k)$\n- Softmaxï¼š$O(n^2)$\n- è®¡ç®— $\\mathbf{A}\\mathbf{V}$ï¼š$O(n^2 \\cdot d_v)$\n\næ€»ä½“ï¼š$O(n^2 \\cdot d + n \\cdot d^2)$\n\nå½“ $n$ å¾ˆå¤§æ—¶ï¼Œ$O(n^2)$ æ˜¯ä¸»å¯¼é¡¹â€”â€”è¿™æ˜¯Self-Attentionçš„ä¸»è¦ç“¶é¢ˆã€‚\n\n**ç©ºé—´å¤æ‚åº¦**ï¼š\n\néœ€è¦å­˜å‚¨ $n \\times n$ çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå› æ­¤ç©ºé—´å¤æ‚åº¦ä¸º $O(n^2)$ã€‚\n\n**ä¸RNNçš„å¯¹æ¯”**ï¼š\n\n| æŒ‡æ ‡ | RNN | Self-Attention |\n|------|-----|----------------|\n| æ—¶é—´å¤æ‚åº¦ | $O(n \\cdot d^2)$ | $O(n^2 \\cdot d)$ |\n| å¹¶è¡Œåº¦ | $O(1)$ | $O(n)$ |\n| æœ€é•¿è·¯å¾„ | $O(n)$ | $O(1)$ |\n| ç©ºé—´å¤æ‚åº¦ | $O(d)$ | $O(n^2)$ |\n\nSelf-Attentionç”¨ $O(n^2)$ çš„å¤æ‚åº¦æ¢æ¥äº†ï¼š\n1. å®Œå…¨å¹¶è¡ŒåŒ–ï¼ˆä» $O(1)$ åˆ° $O(n)$ï¼‰\n2. ä»»æ„ä½ç½®ä¹‹é—´çš„ç›´æ¥è·¯å¾„ï¼ˆä» $O(n)$ åˆ° $O(1)$ï¼‰\n\nè¿™ä¸ªæƒè¡¡åœ¨å¤§å¤šæ•°NLPä»»åŠ¡ä¸­æ˜¯å€¼å¾—çš„ï¼Œå› ä¸ºå¥å­é•¿åº¦é€šå¸¸ä¸è¶…è¿‡å‡ ç™¾ä¸ªè¯ã€‚ä½†å¯¹äºè¶…é•¿åºåˆ—ï¼ˆå¦‚æ–‡æ¡£ã€ä»£ç ï¼‰ï¼Œ$O(n^2)$ ä¼šæˆä¸ºä¸¥é‡ç“¶é¢ˆâ€”â€”è¿™å‚¬ç”Ÿäº†åæ¥çš„é«˜æ•ˆæ³¨æ„åŠ›å˜ä½“ï¼ˆSparse Attentionã€Linear Attentionç­‰ï¼‰ã€‚\n\n---\n\n## Self-Attentionçš„è‡´å‘½ç¼ºé™·ï¼šä½ç½®ä¿¡æ¯ä¸¢å¤±\n\n### é—®é¢˜çš„æœ¬è´¨\n\nä»”ç»†è§‚å¯ŸSelf-Attentionçš„è®¡ç®—å…¬å¼ï¼š\n\n$$\n\\mathbf{z}_i = \\sum_{j=1}^{n} \\text{softmax}\\left(\\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j}{\\sqrt{d_k}}\\right) \\mathbf{v}_j\n$$\n\nè¿™é‡Œæœ‰ä¸€ä¸ªè‡´å‘½çš„é—®é¢˜ï¼š**è®¡ç®—å®Œå…¨ä¸ä½ç½®æ— å…³**ã€‚\n\nå¦‚æœæˆ‘ä»¬æ‰“ä¹±è¾“å…¥åºåˆ—çš„é¡ºåºï¼Œæ¯”å¦‚æŠŠ \"I love NLP\" å˜æˆ \"NLP I love\"ï¼Œæ¯ä¸ªè¯çš„æ³¨æ„åŠ›æƒé‡åªæ˜¯ç›¸åº”åœ°æ‰“ä¹±ï¼Œæœ€ç»ˆè¾“å‡ºåªæ˜¯é‡æ–°æ’åˆ—ï¼Œä¸åŸæ¥ä¸€ä¸€å¯¹åº”ã€‚\n\næ›´æ­£å¼åœ°è¯´ï¼ŒSelf-Attentionæ˜¯**ç½®æ¢ç­‰å˜çš„ï¼ˆpermutation equivariantï¼‰**ï¼š\n\n$$\n\\text{SelfAttn}(\\mathbf{P}\\mathbf{X}) = \\mathbf{P} \\cdot \\text{SelfAttn}(\\mathbf{X})\n$$\n\nå…¶ä¸­ $\\mathbf{P}$ æ˜¯ä»»æ„ç½®æ¢çŸ©é˜µã€‚\n\nè¿™æ„å‘³ç€ï¼šSelf-Attentionå®Œå…¨ä¸çŸ¥é“\"è°åœ¨è°å‰é¢\"ã€‚\n\n### ä¸ºä»€ä¹ˆè¿™æ˜¯è‡´å‘½çš„ï¼Ÿ\n\nåœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œé¡ºåºæºå¸¦ç€å…³é”®ä¿¡æ¯ï¼š\n\n1. **è¯­æ³•è§’è‰²**ï¼š\"The dog bit the man\" vs \"The man bit the dog\" æ„æ€å®Œå…¨ä¸åŒ\n\n2. **æ—¶æ€**ï¼š\"I will go\" vs \"I go\" vs \"I went\" ä¾èµ–äºè¯çš„ç›¸å¯¹ä½ç½®\n\n3. **æŒ‡ä»£æ¶ˆè§£**ï¼š\"John told Bill that he...\" ä¸­ \"he\" é€šå¸¸æŒ‡ä»£è¾ƒè¿‘çš„åè¯\n\n4. **å¦å®šèŒƒå›´**ï¼š\"I didn't go to school\" vs \"I went to not school\" å¦å®šè¯çš„ä½ç½®å†³å®šå¦å®šçš„èŒƒå›´\n\nå¦‚æœæ¨¡å‹ä¸çŸ¥é“è¯çš„é¡ºåºï¼Œå®ƒæ€ä¹ˆå¯èƒ½ç†è§£è¯­è¨€ï¼Ÿ\n\n### ç›´è§‰æ¼”ç¤ºï¼šä½ç½®ä¿¡æ¯ä¸ºä»€ä¹ˆé‡è¦\n\nè®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç›´è§‚æ„Ÿå—ä½ç½®ä¿¡æ¯çš„é‡è¦æ€§ã€‚\n\nè€ƒè™‘ä¸¤ä¸ªå¥å­ï¼š\n- A: \"The cat chased the mouse\"\n- B: \"The mouse chased the cat\"\n\nåœ¨çº¯Self-Attentionä¸­ï¼ˆæ²¡æœ‰ä½ç½®ç¼–ç ï¼‰ï¼Œä¸¤ä¸ªå¥å­çš„è¡¨ç¤ºæ˜¯**å®Œå…¨ç›¸åŒçš„**ï¼Œå› ä¸ºåŒ…å«çš„è¯é›†åˆç›¸åŒï¼Œåªæ˜¯é¡ºåºä¸åŒã€‚\n\nä½†è¿™ä¸¤ä¸ªå¥å­çš„æ„æ€å®Œå…¨ç›¸åï¼å¦‚æœæ¨¡å‹ä¸èƒ½åŒºåˆ†å®ƒä»¬ï¼Œå°±æ— æ³•æ­£ç¡®ç†è§£è¯­è¨€ã€‚\n\n---\n\n## ä½ç½®ç¼–ç ï¼šå¼¥è¡¥ç¼ºå¤±çš„é¡ºåºä¿¡æ¯\n\n### è®¾è®¡ç›®æ ‡\n\næˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•ï¼Œå°†ä½ç½®ä¿¡æ¯æ³¨å…¥åˆ°Self-Attentionä¸­ã€‚è¿™ç§æ–¹æ³•åº”è¯¥æ»¡è¶³ï¼š\n\n1. **å”¯ä¸€æ€§**ï¼šä¸åŒä½ç½®çš„ç¼–ç åº”è¯¥ä¸åŒ\n2. **å¯æ³›åŒ–**ï¼šæ¨¡å‹åº”è¯¥èƒ½å¤Ÿå¤„ç†è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åº¦\n3. **ç›¸å¯¹å…³ç³»**ï¼šç¼–ç åº”è¯¥èƒ½è¡¨è¾¾ä½ç½®ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ï¼Œè€Œéä»…ä»…æ˜¯ç»å¯¹ä½ç½®\n4. **æœ‰ç•Œæ€§**ï¼šç¼–ç å€¼ä¸åº”éšä½ç½®æ— é™å¢é•¿\n\n### æ–¹æ¡ˆä¸€ï¼šå¯å­¦ä¹ çš„ä½ç½®åµŒå…¥\n\næœ€ç®€å•çš„æƒ³æ³•ï¼šä¸ºæ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªå‘é‡ã€‚\n\n$$\n\\mathbf{x}_i' = \\mathbf{x}_i + \\mathbf{p}_i\n$$\n\nå…¶ä¸­ $\\mathbf{p}_i \\in \\mathbb{R}^d$ æ˜¯ä½ç½® $i$ çš„å¯å­¦ä¹ åµŒå…¥ã€‚\n\n**ä¼˜ç‚¹**ï¼š\n- ç®€å•ç›´è§‚\n- æ¨¡å‹å¯ä»¥è‡ªç”±å­¦ä¹ ä½ç½®è¡¨ç¤º\n\n**ç¼ºç‚¹**ï¼š\n- æ— æ³•æ³›åŒ–åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åº¦\n- å¦‚æœæœ€é•¿è®­ç»ƒåºåˆ—æ˜¯512ï¼Œå°±æ— æ³•å¤„ç†513é•¿åº¦çš„è¾“å…¥\n- ä¸èƒ½è¡¨è¾¾ç›¸å¯¹ä½ç½®å…³ç³»\n\nè¿™ç§æ–¹å¼è¢«BERTç­‰æ¨¡å‹é‡‡ç”¨ï¼Œä½†éœ€è¦é…åˆå›ºå®šçš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚\n\n### æ–¹æ¡ˆäºŒï¼šæ­£å¼¦ä½ç½®ç¼–ç \n\nTransformerè®ºæ–‡æå‡ºäº†ä¸€ç§ä¼˜é›…çš„æ–¹æ¡ˆï¼šä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ã€‚\n\n$$\nPE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n$$\n\n$$\nPE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n$$\n\nå…¶ä¸­ $pos$ æ˜¯ä½ç½®ï¼Œ$i$ æ˜¯ç»´åº¦ç´¢å¼•ã€‚\n\nè¿™ä¸ªçœ‹èµ·æ¥å¤æ‚çš„å…¬å¼å®é™…ä¸Šæœ‰ç€æ·±åˆ»çš„è®¾è®¡æ€æƒ³ï¼š\n\n**1. å”¯ä¸€æ€§**ï¼šæ¯ä¸ªä½ç½®æœ‰å”¯ä¸€çš„ç¼–ç å‘é‡\n\n**2. æœ‰ç•Œæ€§**ï¼šæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å€¼åŸŸæ˜¯ $[-1, 1]$ï¼Œä¸ä¼šéšä½ç½®çˆ†ç‚¸\n\n**3. é•¿åº¦æ³›åŒ–**ï¼šå‡½æ•°æ˜¯è¿ç»­çš„ï¼Œä»»ä½•é•¿åº¦éƒ½å¯ä»¥è®¡ç®—\n\n**4. ç›¸å¯¹ä½ç½®å¯è¡¨è¾¾**ï¼šè¿™æ˜¯æœ€ç²¾å¦™çš„éƒ¨åˆ†ã€‚å¯ä»¥è¯æ˜ï¼Œä½ç½® $pos + k$ çš„ç¼–ç å¯ä»¥è¡¨ç¤ºä¸ºä½ç½® $pos$ çš„ç¼–ç çš„çº¿æ€§å˜æ¢ï¼š\n\n$$\nPE_{pos+k} = T_k \\cdot PE_{pos}\n$$\n\nå…¶ä¸­ $T_k$ æ˜¯åªä¾èµ–äº $k$ï¼ˆåç§»é‡ï¼‰çš„çŸ©é˜µã€‚è¿™æ„å‘³ç€æ¨¡å‹æœ‰å¯èƒ½å­¦ä¹ åˆ°ç›¸å¯¹ä½ç½®å…³ç³»ã€‚\n\n### æ­£å¼¦ç¼–ç çš„å‡ ä½•ç›´è§‰\n\næƒ³è±¡ä¸€ä¸ªæ—¶é’Ÿï¼š\n\n- ç§’é’ˆè½¬ä¸€åœˆæ˜¯60ç§’ï¼ˆé«˜é¢‘ï¼‰\n- åˆ†é’ˆè½¬ä¸€åœˆæ˜¯60åˆ†é’Ÿï¼ˆä¸­é¢‘ï¼‰\n- æ—¶é’ˆè½¬ä¸€åœˆæ˜¯12å°æ—¶ï¼ˆä½é¢‘ï¼‰\n\nä¸åŒçš„æŒ‡é’ˆåœ¨ä¸åŒçš„\"é¢‘ç‡\"ä¸Šè¿åŠ¨ï¼Œç»„åˆèµ·æ¥å¯ä»¥å”¯ä¸€åœ°è¡¨ç¤ºæ—¶é—´ã€‚\n\næ­£å¼¦ä½ç½®ç¼–ç çš„åŸç†ç±»ä¼¼ï¼š\n- ä½ç»´åº¦ï¼ˆå° $i$ï¼‰çš„æ­£å¼¦æ³¢é¢‘ç‡é«˜ï¼Œå˜åŒ–å¿«ï¼Œç¼–ç **å±€éƒ¨ä½ç½®ä¿¡æ¯**\n- é«˜ç»´åº¦ï¼ˆå¤§ $i$ï¼‰çš„æ­£å¼¦æ³¢é¢‘ç‡ä½ï¼Œå˜åŒ–æ…¢ï¼Œç¼–ç **å…¨å±€ä½ç½®ä¿¡æ¯**\n\nç»„åˆæ‰€æœ‰ç»´åº¦ï¼Œå°±èƒ½å”¯ä¸€æ ‡è¯†æ¯ä¸ªä½ç½®ã€‚\n\n::: {#baffb8fe .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sinusoidal_position_encoding(max_len, d_model):\n    \"\"\"è®¡ç®—æ­£å¼¦ä½ç½®ç¼–ç \"\"\"\n    pe = np.zeros((max_len, d_model))\n    position = np.arange(max_len)[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n\n    return pe\n\n# å¯è§†åŒ–\npe = sinusoidal_position_encoding(100, 64)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# å·¦å›¾ï¼šçƒ­åŠ›å›¾\nim = axes[0].imshow(pe, cmap='RdBu', aspect='auto')\naxes[0].set_xlabel('Dimension')\naxes[0].set_ylabel('Position')\naxes[0].set_title('Sinusoidal Position Encoding')\nplt.colorbar(im, ax=axes[0])\n\n# å³å›¾ï¼šä¸åŒç»´åº¦çš„æ³¢å½¢\nfor dim in [0, 10, 20, 30]:\n    axes[1].plot(pe[:, dim], label=f'dim {dim}')\naxes[1].set_xlabel('Position')\naxes[1].set_ylabel('Value')\naxes[1].set_title('Different Dimensions = Different Frequencies')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ch07-self-attention_files/figure-html/cell-2-output-1.png){width=1334 height=470}\n:::\n:::\n\n\n### æ–¹æ¡ˆä¸‰ï¼šç›¸å¯¹ä½ç½®ç¼–ç \n\næ­£å¼¦ç¼–ç è™½ç„¶ç²¾å·§ï¼Œä½†å®ƒç¼–ç çš„æ˜¯**ç»å¯¹ä½ç½®**ã€‚ä¸€äº›ç ”ç©¶è€…è®¤ä¸ºï¼Œå¯¹äºè¯­è¨€ç†è§£æ¥è¯´ï¼Œ**ç›¸å¯¹ä½ç½®**ï¼ˆä¸¤ä¸ªè¯ä¹‹é—´çš„è·ç¦»ï¼‰å¯èƒ½æ¯”ç»å¯¹ä½ç½®æ›´é‡è¦ã€‚\n\nç›¸å¯¹ä½ç½®ç¼–ç çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¿®æ”¹æ³¨æ„åŠ›è®¡ç®—ï¼Œè®©å®ƒç›´æ¥è€ƒè™‘ä½ç½®å·®ï¼š\n\n$$\n\\alpha_{ij} \\propto \\exp\\left(\\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j + \\mathbf{q}_i^\\top \\mathbf{r}_{j-i}}{\\sqrt{d_k}}\\right)\n$$\n\nå…¶ä¸­ $\\mathbf{r}_{j-i}$ æ˜¯è¡¨ç¤ºç›¸å¯¹ä½ç½® $j-i$ çš„å‘é‡ã€‚\n\nè¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æ˜¯ï¼š\n- ç›´æ¥å»ºæ¨¡ç›¸å¯¹å…³ç³»\n- ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„ç›¸å¯¹ä½ç½®åå¥½\n- ç†è®ºä¸Šå¯ä»¥æ³›åŒ–åˆ°ä»»æ„é•¿åº¦\n\nåæ¥çš„RoPEï¼ˆRotary Position Embeddingï¼‰è¿›ä¸€æ­¥å‘å±•äº†è¿™ä¸ªæ€æƒ³ï¼Œæˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ‡é…â€”â€”æˆ‘ä»¬å°†åœ¨ç¬¬26ç« è¯¦ç»†è®¨è®ºã€‚\n\n### ä½ç½®ç¼–ç çš„æ³¨å…¥æ–¹å¼\n\næœ‰äº†ä½ç½®ç¼–ç  $\\mathbf{PE}$ï¼Œå¦‚ä½•å°†å…¶ä¸è¾“å…¥ç»“åˆï¼Ÿä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼š\n\n**åŠ æ³•æ³¨å…¥ï¼ˆTransformeré‡‡ç”¨ï¼‰**ï¼š\n\n$$\n\\mathbf{X}' = \\mathbf{X} + \\mathbf{PE}\n$$\n\nå°†ä½ç½®ç¼–ç ç›´æ¥åŠ åˆ°è¯åµŒå…¥ä¸Šã€‚è¿™å‡è®¾ä½ç½®ä¿¡æ¯å’Œè¯­ä¹‰ä¿¡æ¯å¯ä»¥åœ¨åŒä¸€ç©ºé—´ä¸­è¡¨è¾¾å’Œæ··åˆã€‚\n\n**æ‹¼æ¥æ³¨å…¥**ï¼š\n\n$$\n\\mathbf{X}' = [\\mathbf{X}; \\mathbf{PE}]\n$$\n\nå°†ä½ç½®ç¼–ç ä¸è¯åµŒå…¥æ‹¼æ¥ï¼Œä¿æŒä¸¤è€…åˆ†ç¦»ã€‚è¿™éœ€è¦æ›´å¤šå‚æ•°ï¼Œä½†é¿å…äº†ä¿¡æ¯æ··æ·†ã€‚\n\nå®è·µä¸­ï¼ŒåŠ æ³•æ³¨å…¥æ›´å¸¸ç”¨ï¼Œå› ä¸ºå®ƒæ›´ç®€æ´ä¸”æ•ˆæœè‰¯å¥½ã€‚\n\n---\n\n## å·¥ç¨‹å®è·µ\n\n### å®ç°Self-Attention\n\n::: {#b6127518 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    \"\"\"\n    Self-Attention æ¨¡å—\n    \"\"\"\n    def __init__(self, d_model, d_k=None, d_v=None):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_k if d_k is not None else d_model\n        self.d_v = d_v if d_v is not None else d_model\n\n        # Q, K, V æŠ•å½±çŸ©é˜µ\n        self.W_Q = nn.Linear(d_model, self.d_k, bias=False)\n        self.W_K = nn.Linear(d_model, self.d_k, bias=False)\n        self.W_V = nn.Linear(d_model, self.d_v, bias=False)\n\n        # ç¼©æ”¾å› å­\n        self.scale = math.sqrt(self.d_k)\n\n    def forward(self, X, mask=None):\n        \"\"\"\n        X: [batch, seq_len, d_model]\n        mask: [batch, seq_len, seq_len], True è¡¨ç¤ºéœ€è¦ mask çš„ä½ç½®\n\n        è¿”å›:\n            output: [batch, seq_len, d_v]\n            attention_weights: [batch, seq_len, seq_len]\n        \"\"\"\n        # Step 1: çº¿æ€§æŠ•å½±\n        Q = self.W_Q(X)  # [batch, seq_len, d_k]\n        K = self.W_K(X)  # [batch, seq_len, d_k]\n        V = self.W_V(X)  # [batch, seq_len, d_v]\n\n        # Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale  # [batch, seq_len, seq_len]\n\n        # Step 3: åº”ç”¨ mask\n        if mask is not None:\n            scores = scores.masked_fill(mask, -1e9)\n\n        # Step 4: Softmax\n        attention_weights = F.softmax(scores, dim=-1)\n\n        # Step 5: åŠ æƒèšåˆ\n        output = torch.bmm(attention_weights, V)\n\n        return output, attention_weights\n```\n:::\n\n\n### å®ç°ä½ç½®ç¼–ç \n\n::: {#2274baf1 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    æ­£å¼¦ä½ç½®ç¼–ç \n    \"\"\"\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # é¢„è®¡ç®—ä½ç½®ç¼–ç \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        x: [batch, seq_len, d_model]\n        \"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\n\nclass LearnablePositionalEmbedding(nn.Module):\n    \"\"\"\n    å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥\n    \"\"\"\n    def __init__(self, d_model, max_len=512, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.position_embedding = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        \"\"\"\n        x: [batch, seq_len, d_model]\n        \"\"\"\n        seq_len = x.size(1)\n        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        x = x + self.position_embedding(positions)\n        return self.dropout(x)\n```\n:::\n\n\n### å®Œæ•´æ¼”ç¤ºï¼šSelf-Attentionçš„æ•ˆæœ\n\n::: {#ad5ab18e .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n# åˆ›å»ºæµ‹è¯•æ•°æ®\nbatch_size = 2\nseq_len = 5\nd_model = 64\n\n# éšæœºè¾“å…¥\nX = torch.randn(batch_size, seq_len, d_model)\n\n# åˆ›å»ºæ¨¡å‹\nself_attn = SelfAttention(d_model)\npos_enc = PositionalEncoding(d_model)\n\n# æ·»åŠ ä½ç½®ç¼–ç \nX_with_pos = pos_enc(X)\n\n# è®¡ç®— Self-Attention\noutput, attention_weights = self_attn(X_with_pos)\n\nprint(f\"è¾“å…¥å½¢çŠ¶: {X.shape}\")\nprint(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\nprint(f\"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}\")\nprint(f\"\\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›çŸ©é˜µ:\\n{attention_weights[0].detach().numpy().round(2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nè¾“å…¥å½¢çŠ¶: torch.Size([2, 5, 64])\nè¾“å‡ºå½¢çŠ¶: torch.Size([2, 5, 64])\næ³¨æ„åŠ›æƒé‡å½¢çŠ¶: torch.Size([2, 5, 5])\n\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›çŸ©é˜µ:\n[[0.11 0.33 0.32 0.09 0.15]\n [0.13 0.31 0.16 0.19 0.22]\n [0.14 0.22 0.21 0.22 0.22]\n [0.14 0.27 0.21 0.11 0.27]\n [0.18 0.19 0.44 0.1  0.09]]\n```\n:::\n:::\n\n\n### å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼\n\n::: {#473322c5 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nimport matplotlib.pyplot as plt\n\n# åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¾‹å­\nwords = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\nseq_len = len(words)\n\n# æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒå¥½çš„æ³¨æ„åŠ›çŸ©é˜µ\n# è¿™ä¸ªçŸ©é˜µå±•ç¤ºäº†ä¸€äº›è¯­è¨€å­¦ä¸Šåˆç†çš„æ¨¡å¼\nattention_pattern = torch.tensor([\n    [0.5, 0.3, 0.1, 0.05, 0.05],  # \"The\" ä¸»è¦å…³æ³¨è‡ªå·±å’Œ \"cat\"\n    [0.2, 0.4, 0.3, 0.05, 0.05],  # \"cat\" å…³æ³¨ \"sat\"ï¼ˆåŠ¨è¯ï¼‰\n    [0.1, 0.4, 0.2, 0.2, 0.1],    # \"sat\" å…³æ³¨ \"cat\"ï¼ˆä¸»è¯­ï¼‰å’Œ \"on\"\n    [0.05, 0.1, 0.3, 0.3, 0.25],  # \"on\" å…³æ³¨ \"sat\" å’Œ \"mat\"\n    [0.05, 0.1, 0.1, 0.25, 0.5],  # \"mat\" å…³æ³¨ \"on\" å’Œè‡ªå·±\n])\n\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(attention_pattern, cmap='Blues')\n\nax.set_xticks(range(seq_len))\nax.set_yticks(range(seq_len))\nax.set_xticklabels(words)\nax.set_yticklabels(words)\nax.set_xlabel('Key (attended to)')\nax.set_ylabel('Query (attending from)')\nax.set_title('Self-Attention Weights: \"The cat sat on mat\"')\n\n# æ·»åŠ æ•°å€¼æ ‡æ³¨\nfor i in range(seq_len):\n    for j in range(seq_len):\n        ax.text(j, i, f'{attention_pattern[i, j]:.2f}',\n                ha='center', va='center', color='black' if attention_pattern[i, j] < 0.3 else 'white')\n\nplt.colorbar(im)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ch07-self-attention_files/figure-html/cell-6-output-1.png){width=659 height=566}\n:::\n:::\n\n\n---\n\n## æ·±å…¥ç†è§£\n\n### ä¸ºä»€ä¹ˆSelf-Attentionæœ‰æ•ˆï¼Ÿâ€”â€”ç†è®ºè§†è§’\n\n**è¡¨è¾¾èƒ½åŠ›åˆ†æ**\n\nSelf-Attentionå¯ä»¥çœ‹ä½œä¸€ç§ç‰¹æ®Šçš„å›¾ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­å›¾æ˜¯å®Œå…¨è¿æ¥çš„ï¼Œè¾¹æƒé‡ç”±æ³¨æ„åŠ›å†³å®šã€‚Yun et al. (2020) è¯æ˜äº†ï¼šåœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼ŒSelf-Attentionæ˜¯**é€šç”¨å‡½æ•°é€¼è¿‘å™¨**â€”â€”å®ƒå¯ä»¥é€¼è¿‘ä»»ä½•è¿ç»­çš„ç½®æ¢ç­‰å˜å‡½æ•°ã€‚\n\n**ä¸å·ç§¯çš„å¯¹æ¯”**\n\nå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹Ÿå¯ä»¥å¤„ç†åºåˆ—ï¼Œä½†å®ƒæœ‰å›ºå®šçš„æ„Ÿå—é‡ã€‚ä¸€ä¸ª3-gramå·ç§¯åªèƒ½çœ‹åˆ°ç›¸é‚»çš„3ä¸ªè¯ï¼›è¦çœ‹åˆ°æ›´è¿œçš„è¯ï¼Œéœ€è¦å †å å¤šå±‚ã€‚Self-Attentionåˆ™ä¸€æ­¥å°±èƒ½çœ‹åˆ°å…¨å±€ã€‚\n\n| ç‰¹æ€§ | CNN | Self-Attention |\n|------|-----|----------------|\n| æ„Ÿå—é‡ | å±€éƒ¨ï¼ˆé€å±‚æ‰©å¤§ï¼‰ | å…¨å±€ï¼ˆä¸€å±‚åˆ°ä½ï¼‰ |\n| è®¡ç®—å¤æ‚åº¦ | $O(n \\cdot k^2 \\cdot d)$ | $O(n^2 \\cdot d)$ |\n| ä½ç½®åç½® | å¼ºï¼ˆç›¸å¯¹ä½ç½®å…³ç³»å›ºå®šï¼‰ | å¼±ï¼ˆéœ€è¦ä½ç½®ç¼–ç ï¼‰ |\n| å‚æ•°å…±äº« | è·¨ä½ç½®å…±äº« | Q/K/VæŠ•å½±å…±äº« |\n\n**Memory Networkè§†è§’**\n\nå¦‚å‰æ‰€è¿°ï¼ŒSelf-Attentionå¯ä»¥ç†è§£ä¸ºåºåˆ—å¯¹è‡ªå·±è¿›è¡ŒMemory Networkå¼çš„æŸ¥è¯¢ã€‚æ¯ä¸€å±‚Self-Attentionéƒ½æ˜¯ä¸€æ¬¡\"å¤šè·³æ¨ç†\"â€”â€”å‰ä¸€å±‚çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€å±‚çš„\"è®°å¿†\"ã€‚\n\n### æ–¹æ³•çš„è¾¹ç•Œæ¡ä»¶\n\n**Self-Attentionçš„éšå«å‡è®¾**ï¼š\n\n1. **å…¨å±€ç›¸å…³æ€§å‡è®¾**ï¼šæ¯ä¸ªä½ç½®éƒ½å¯èƒ½ä¸ä»»ä½•å…¶ä»–ä½ç½®ç›¸å…³ã€‚ä½†å¯¹äºæŸäº›ä»»åŠ¡ï¼ˆå¦‚æ—¶é—´åºåˆ—é¢„æµ‹ï¼‰ï¼Œå±€éƒ¨ç›¸å…³æ€§å¯èƒ½æ›´é‡è¦ã€‚\n\n2. **å‡åŒ€è®¡ç®—å‡è®¾**ï¼šæ‰€æœ‰ä½ç½®ä¹‹é—´çš„äº¤äº’ç”¨ç›¸åŒçš„è®¡ç®—é‡ã€‚ä½†ç›´è§‰ä¸Šï¼Œ\"é‡è¦\"çš„äº¤äº’å¯èƒ½éœ€è¦æ›´å¤šè®¡ç®—ã€‚\n\n3. **çº¿æ€§å¯åˆ†å‡è®¾**ï¼šç›¸å…³æ€§å¯ä»¥é€šè¿‡å‘é‡ç‚¹ç§¯æ¥è¡¡é‡ã€‚ä½†æœ‰äº›å¤æ‚çš„å…³ç³»å¯èƒ½æ— æ³•ç”¨ç‚¹ç§¯æ•è·ã€‚\n\n**å¤±æ•ˆæ¡ä»¶**ï¼š\n\n1. **è¶…é•¿åºåˆ—**ï¼šå½“ $n > 10000$ æ—¶ï¼Œ$O(n^2)$ å¤æ‚åº¦å˜å¾—ä¸å¯æ¥å—ã€‚\n2. **å¼ºä½ç½®ä¾èµ–ä»»åŠ¡**ï¼šå¯¹äºéœ€è¦ç²¾ç¡®ä½ç½®æ¨ç†çš„ä»»åŠ¡ï¼ˆå¦‚ç®—æœ¯ï¼‰ï¼ŒSelf-Attentionå¸¸å¸¸å¤±è´¥ã€‚\n3. **æ•°æ®é‡ä¸è¶³**ï¼šSelf-Attentionçš„å¼±å½’çº³åç½®éœ€è¦å¤§é‡æ•°æ®æ¥è¡¥å¿ã€‚\n\n### å¼€æ”¾ç ”ç©¶é—®é¢˜\n\n1. **ä½ç½®ç¼–ç çš„æœ€ä¼˜è®¾è®¡**ï¼šæ­£å¼¦ç¼–ç ã€å¯å­¦ä¹ ç¼–ç ã€ç›¸å¯¹ä½ç½®ç¼–ç â€”â€”å“ªç§æœ€å¥½ï¼Ÿæ˜¯å¦å­˜åœ¨\"æœ€ä¼˜\"çš„ä½ç½®ç¼–ç ï¼Ÿ\n\n2. **Self-Attentionçš„å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡çœŸçš„ä»£è¡¨äº†\"é‡è¦æ€§\"å—ï¼Ÿè¿˜æ˜¯åªæ˜¯è®¡ç®—çš„å‰¯äº§å“ï¼Ÿ\n\n3. **æ•ˆç‡ä¸è¡¨è¾¾èƒ½åŠ›çš„æƒè¡¡**ï¼šèƒ½å¦è®¾è®¡å‡º $O(n)$ å¤æ‚åº¦ä½†ä¿æŒ $O(n^2)$ è¡¨è¾¾èƒ½åŠ›çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ\n\n4. **å½’çº³åç½®çš„è®¾è®¡**ï¼šå¦‚ä½•åœ¨Self-Attentionä¸­å¼•å…¥åˆé€‚çš„å½’çº³åç½®ï¼Œå‡å°‘å¯¹æ•°æ®é‡çš„ä¾èµ–ï¼Ÿ\n\n---\n\n## å±€é™æ€§ä¸å±•æœ›\n\n### Self-Attentionçš„æ ¸å¿ƒå±€é™\n\n**1. ä½ç½®ä¿¡æ¯ä»æ˜¯\"è¡¥ä¸\"**\n\nè™½ç„¶ä½ç½®ç¼–ç è§£å†³äº†é—®é¢˜ï¼Œä½†å®ƒæ˜¯ä¸€ç§äº‹åè¡¥æ•‘ã€‚Self-Attentionæœ¬èº«ä¸å…·å¤‡ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½ç½®ä¿¡æ¯æ˜¯å¤–éƒ¨æ³¨å…¥çš„ã€‚è¿™ç§è®¾è®¡æ˜¯å¦æœ€ä¼˜ï¼Ÿæ˜¯å¦æœ‰æ›´ä¼˜é›…çš„æ–¹å¼è®©æ³¨æ„åŠ›æœºåˆ¶å¤©ç”Ÿå…·æœ‰ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Ÿ\n\n**2. $O(n^2)$ å¤æ‚åº¦**\n\nå…¨å±€æ³¨æ„åŠ›çš„ä»£ä»·æ˜¯ $n \\times n$ çš„æ³¨æ„åŠ›çŸ©é˜µã€‚å½“åºåˆ—é•¿åº¦è¾¾åˆ°æ•°ä¸‡ç”šè‡³æ•°åä¸‡æ—¶ï¼Œè¿™å˜å¾—ä¸å¯è¡Œã€‚å¦‚ä½•åœ¨ä¿æŒå…¨å±€è§†é‡çš„åŒæ—¶é™ä½å¤æ‚åº¦ï¼Ÿ\n\n**3. å•ä¸€çš„æ³¨æ„åŠ›æ¨¡å¼**\n\nåœ¨ä¸€å±‚Self-Attentionä¸­ï¼Œæ¯ä¸ªä½ç½®åªç”Ÿæˆä¸€ç»„Queryã€Keyã€Valueã€‚ä½†ç›´è§‰ä¸Šï¼Œä¸€ä¸ªä½ç½®å¯èƒ½éœ€è¦åŒæ—¶å…³æ³¨å¤šç§ä¸åŒç±»å‹çš„ä¿¡æ¯â€”â€”æ¯”å¦‚ä¸»è¯­éœ€è¦å…³æ³¨åŠ¨è¯ï¼ˆè¯­æ³•ï¼‰ï¼Œä¹Ÿéœ€è¦å…³æ³¨è¯­å¢ƒä¸­çš„ç›¸å…³å®ä½“ï¼ˆè¯­ä¹‰ï¼‰ã€‚èƒ½å¦è®©æ¨¡å‹åŒæ—¶æ•è·å¤šç§å…³æ³¨æ¨¡å¼ï¼Ÿ\n\n### è¿™äº›å±€é™æŒ‡å‘ä»€ä¹ˆï¼Ÿ\n\nSelf-Attentionä¸ºåºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€ï¼Œä½†è¦æ„å»ºå®Œæ•´çš„æ¶æ„ï¼Œè¿˜éœ€è¦å›ç­”å‡ ä¸ªå…³é”®é—®é¢˜ï¼š\n\n**å¦‚ä½•å †å å¤šå±‚ï¼Ÿ** ç®€å•åœ°å †å Self-Attentionå±‚ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿéœ€è¦ä»€ä¹ˆé¢å¤–çš„ç»„ä»¶æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼Ÿ\n\n**å¦‚ä½•å¤„ç†ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Ÿ** æœºå™¨ç¿»è¯‘éœ€è¦ç¼–ç å™¨ç†è§£æºè¯­è¨€ï¼Œè§£ç å™¨ç”Ÿæˆç›®æ ‡è¯­è¨€ã€‚Self-Attentionå¦‚ä½•åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ä½¿ç”¨ï¼Ÿè§£ç å™¨å¦‚ä½•æ—¢å…³æ³¨è‡ªå·±å·²ç”Ÿæˆçš„å†…å®¹ï¼Œåˆå…³æ³¨ç¼–ç å™¨çš„è¾“å‡ºï¼Ÿ\n\n**å¦‚ä½•å®ç°\"å¤šå¤´\"æ³¨æ„åŠ›ï¼Ÿ** èƒ½å¦è®©å¤šä¸ªæ³¨æ„åŠ›\"å¤´\"åŒæ—¶å·¥ä½œï¼Œæ¯ä¸ªå¤´å…³æ³¨ä¸åŒçš„å­ç©ºé—´ï¼Œæœ€åç»¼åˆå®ƒä»¬çš„è¾“å‡ºï¼Ÿ\n\nè¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œå°†åœ¨ä¸‹ä¸€ç« æ­æ™“ã€‚2017å¹´ï¼ŒGoogleçš„ç ”ç©¶å›¢é˜Ÿåœ¨è®ºæ–‡\"Attention Is All You Need\"ä¸­æå‡ºäº†**Transformer**æ¶æ„â€”â€”ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚å®ƒä¸ä»…å›ç­”äº†ä¸Šè¿°æ‰€æœ‰é—®é¢˜ï¼Œè¿˜ä»¥æƒŠäººçš„æ•ˆæœè¯æ˜äº†ä¸€ä¸ªå¤§èƒ†çš„è®ºæ–­ï¼š**æ³¨æ„åŠ›ï¼Œå°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡**ã€‚\n\n> Self-Attentionè®©åºåˆ—å­¦ä¼šäº†\"å®¡è§†è‡ªå·±\"ï¼Œæ‰“ç ´äº†RNNçš„é¡ºåºæ·é”ã€‚ä½†å®ƒåªæ˜¯é©å‘½çš„åºæ›²ã€‚å½“ç ”ç©¶è€…æ„è¯†åˆ°å¯ä»¥å®Œå…¨æŠ›å¼ƒå¾ªç¯ç»“æ„ï¼Œåªç”¨æ³¨æ„åŠ›æ¥æ„å»ºæ•´ä¸ªæ¨¡å‹æ—¶ï¼Œæ·±åº¦å­¦ä¹ çš„å†å²ç¿»å¼€äº†æ–°çš„ä¸€é¡µã€‚\n\n---\n\n## æœ¬ç« å°ç»“\n\n::: {.callout-important}\n## æ ¸å¿ƒè¦ç‚¹\n\n- **é—®é¢˜**ï¼šå¦‚ä½•è®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®è·å¾—å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶é¿å…RNNçš„é¡ºåºè®¡ç®—ç“¶é¢ˆï¼Ÿ\n- **æ´å¯Ÿ**ï¼šè®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ç›´æ¥å…³æ³¨æ‰€æœ‰å…¶ä»–ä½ç½®ï¼ˆSelf-Attentionï¼‰ï¼Œå°† $O(n)$ çš„é—´æ¥è·¯å¾„å˜æˆ $O(1)$ çš„ç›´æ¥è·¯å¾„\n- **æ–¹æ³•**ï¼šQuery-Key-Valueä¸‰å…ƒç»„ç»“æ„ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º\n- **ä»£ä»·**ï¼šSelf-Attentionä¸¢å¤±ä½ç½®ä¿¡æ¯ï¼Œéœ€è¦ä½ç½®ç¼–ç æ¥å¼¥è¡¥ï¼›å¤æ‚åº¦ä¸º $O(n^2)$\n- **æ„ä¹‰**ï¼šSelf-Attentionä¸ºå®Œå…¨æŠ›å¼ƒRNNã€æ„å»ºçº¯æ³¨æ„åŠ›æ¶æ„å¥ å®šäº†åŸºç¡€\n:::\n\n### å…³é”®å…¬å¼é€ŸæŸ¥\n\n**Self-Attention**ï¼š\n\n$$\n\\text{SelfAttn}(\\mathbf{X}) = \\text{softmax}\\left(\\frac{\\mathbf{X}\\mathbf{W}_Q (\\mathbf{X}\\mathbf{W}_K)^\\top}{\\sqrt{d_k}}\\right) \\mathbf{X}\\mathbf{W}_V\n$$\n\n**æ­£å¼¦ä½ç½®ç¼–ç **ï¼š\n\n$$\nPE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n$$\n\n**ä¸Cross-Attentionçš„å¯¹æ¯”**ï¼š\n\n| ç±»å‹ | Queryæ¥æº | Key/Valueæ¥æº | ç”¨é€” |\n|------|-----------|---------------|------|\n| Cross-Attention | è§£ç å™¨ | ç¼–ç å™¨ | è·¨åºåˆ—å…³æ³¨ |\n| Self-Attention | åŒä¸€åºåˆ— | åŒä¸€åºåˆ— | åºåˆ—å†…éƒ¨å»ºæ¨¡ |\n\n---\n\n## æ€è€ƒé¢˜\n\n1. **[æ¦‚å¿µç†è§£]** Self-Attentionä¸ºä»€ä¹ˆæ˜¯\"ç½®æ¢ç­‰å˜\"çš„ï¼Ÿè®¾è®¡ä¸€ä¸ªç®€å•çš„å®éªŒæ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®©æ¨¡å‹åŒºåˆ† \"dog bites man\" å’Œ \"man bites dog\"ï¼Œä»…é Self-Attentionï¼ˆä¸åŠ ä½ç½®ç¼–ç ï¼‰èƒ½åšåˆ°å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n\n2. **[æ•°å­¦æ¨å¯¼]** è¯æ˜æ­£å¼¦ä½ç½®ç¼–ç çš„ç›¸å¯¹ä½ç½®æ€§è´¨ï¼š$PE_{pos+k}$ å¯ä»¥è¡¨ç¤ºä¸º $PE_{pos}$ çš„çº¿æ€§å˜æ¢ã€‚å…·ä½“å†™å‡ºå˜æ¢çŸ©é˜µçš„å½¢å¼ã€‚ï¼ˆæç¤ºï¼šåˆ©ç”¨ä¸‰è§’å‡½æ•°çš„å’Œå·®å…¬å¼ï¼‰\n\n3. **[å·¥ç¨‹å®è·µ]** å®ç°ä¸€ä¸ªç®€å•çš„Self-Attentionæ–‡æœ¬åˆ†ç±»å™¨ï¼ˆä¸ä½¿ç”¨RNNï¼‰ï¼š\n   - è¾“å…¥ï¼šIMDBç”µå½±è¯„è®º\n   - æ¶æ„ï¼šEmbedding â†’ Self-Attention â†’ å¹³å‡æ± åŒ– â†’ åˆ†ç±»å¤´\n   - å¯¹æ¯”æœ‰æ— ä½ç½®ç¼–ç çš„æ•ˆæœå·®å¼‚\n\n4. **[æ‰¹åˆ¤æ€è€ƒ]** Memory Networkså’ŒSelf-Attentionåœ¨å½¢å¼ä¸Šéå¸¸ç›¸ä¼¼ã€‚å®ƒä»¬çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼ŸMemory Networksçš„\"å¤–éƒ¨è®°å¿†\"å’ŒSelf-Attentionçš„\"åºåˆ—ä½œä¸ºè®°å¿†\"æœ‰ä»€ä¹ˆä¸åŒçš„è®¾è®¡è€ƒé‡ï¼Ÿ\n\n5. **[å¼€æ”¾é—®é¢˜]** Self-Attentionçš„ $O(n^2)$ å¤æ‚åº¦æ˜¯ä¸€ä¸ªæ ¹æœ¬é™åˆ¶å—ï¼Ÿæœ‰æ²¡æœ‰å¯èƒ½è®¾è®¡å‡º $O(n)$ å¤æ‚åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶ä¿æŒ $O(1)$ çš„æœ€é•¿è·¯å¾„ï¼Ÿç°æœ‰çš„çº¿æ€§æ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰ä¸ºä»€ä¹ˆä¼šæœ‰æ€§èƒ½æŸå¤±ï¼Ÿ\n\n---\n\n## å»¶ä¼¸é˜…è¯»\n\n### æ ¸å¿ƒè®ºæ–‡ï¼ˆå¿…è¯»ï¼‰\n\n- **[Vaswani et al., 2017] Attention Is All You Need**\n  - æå‡ºTransformerå’ŒScaled Dot-Product Attention\n  - é‡ç‚¹è¯»ï¼šSection 3.2ï¼ˆAttentionï¼‰ã€Section 3.5ï¼ˆä½ç½®ç¼–ç ï¼‰\n  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)\n\n- **[Sukhbaatar et al., 2015] End-To-End Memory Networks**\n  - Self-Attentionçš„æ€æƒ³å…ˆé©±\n  - é‡ç‚¹è¯»ï¼šSection 2ï¼ˆæ¨¡å‹æ¶æ„ï¼‰\n  - arXiv: [1503.08895](https://arxiv.org/abs/1503.08895)\n\n### ç†è®ºåŸºç¡€\n\n- **[Yun et al., 2020] Are Transformers universal approximators of sequence-to-sequence functions?**\n  - è¯æ˜Transformerçš„é€šç”¨é€¼è¿‘æ€§è´¨\n  - arXiv: [1912.10077](https://arxiv.org/abs/1912.10077)\n\n- **[Ramsauer et al., 2020] Hopfield Networks is All You Need**\n  - å°†Attentionè§£é‡Šä¸ºç°ä»£Hopfieldç½‘ç»œ\n  - arXiv: [2008.02217](https://arxiv.org/abs/2008.02217)\n\n### åç»­å‘å±•\n\n- **[Shaw et al., 2018] Self-Attention with Relative Position Representations**\n  - ç›¸å¯¹ä½ç½®ç¼–ç çš„æ—©æœŸå·¥ä½œ\n  - arXiv: [1803.02155](https://arxiv.org/abs/1803.02155)\n\n- **[Su et al., 2021] RoFormer: Enhanced Transformer with Rotary Position Embedding**\n  - æå‡ºRoPEï¼Œæˆä¸ºç°ä»£LLMçš„æ ‡é…\n  - arXiv: [2104.09864](https://arxiv.org/abs/2104.09864)\n\n### å¯è§†åŒ–èµ„æº\n\n- **[Jay Alammar] The Illustrated Transformer**\n  - æœ€ä½³å¯è§†åŒ–æ•™ç¨‹\n  - ç½‘å€: [jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)\n\n---\n\n## å†å²æ³¨è„š\n\nSelf-Attentionçš„æƒ³æ³•å¹¶éå‡­ç©ºå‡ºç°ã€‚åœ¨2017å¹´Transformerè®ºæ–‡ä¹‹å‰ï¼Œå·²ç»æœ‰å¤šæ¡ç ”ç©¶è„‰ç»œåœ¨æ¢ç´¢ç±»ä¼¼çš„æ€æƒ³ï¼š\n\n**Memory Networks (2014-2015)**ï¼šFacebook AI Researchçš„å›¢é˜Ÿæå‡ºäº†ç”¨æ³¨æ„åŠ›æœºåˆ¶ä»å¤–éƒ¨è®°å¿†ä¸­æ£€ç´¢ä¿¡æ¯çš„æƒ³æ³•ã€‚è™½ç„¶ä»–ä»¬çš„ç›®æ ‡æ˜¯é—®ç­”ç³»ç»Ÿè€Œéåºåˆ—å»ºæ¨¡ï¼Œä½†Key-Valueåˆ†ç¦»ã€è½¯æ³¨æ„åŠ›æ£€ç´¢ç­‰è®¾è®¡æ·±åˆ»å½±å“äº†åæ¥çš„å‘å±•ã€‚\n\n**Neural Turing Machines (2014)**ï¼šDeepMindæå‡ºçš„ç¥ç»å›¾çµæœºä¹Ÿä½¿ç”¨äº†ç±»ä¼¼æ³¨æ„åŠ›çš„æœºåˆ¶æ¥è¯»å†™å¤–éƒ¨è®°å¿†ã€‚è™½ç„¶æ›´åŠ å¤æ‚ï¼Œä½†å®ƒå±•ç¤ºäº†ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ ç±»ä¼¼\"æŸ¥æ‰¾è¡¨\"çš„æ“ä½œã€‚\n\n**Decomposable Attention (2016)**ï¼šParikhç­‰äººæå‡ºäº†ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›çš„è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œä¸ä½¿ç”¨ä»»ä½•RNNã€‚è¿™æ˜¯\"çº¯æ³¨æ„åŠ›\"æ¨¡å‹çš„æ—©æœŸæˆåŠŸæ¡ˆä¾‹ã€‚\n\n**Transformerçš„è´¡çŒ®**ï¼šä¸æ˜¯å‘æ˜Self-Attentionï¼Œè€Œæ˜¯å°†å…¶ç³»ç»ŸåŒ–å¹¶è¯æ˜å®ƒå¯ä»¥å®Œå…¨æ›¿ä»£RNNã€‚\"Attention Is All You Need\"è¿™ä¸ªå¤§èƒ†çš„æ ‡é¢˜ï¼Œæ—¢æ˜¯æŠ€æœ¯å£°æ˜ï¼Œä¹Ÿæ˜¯ç ”ç©¶å®£è¨€ã€‚\n\næœ‰è¶£çš„æ˜¯ï¼ŒTransformerè®ºæ–‡çš„ä½œè€…ä¹‹ä¸€Ashish Vaswaniåæ¥å›å¿†è¯´ï¼Œä»–ä»¬æœ€åˆå¹¶ä¸ç¡®å®šå®Œå…¨æŠ›å¼ƒRNNæ˜¯å¦å¯è¡Œã€‚å®éªŒç»“æœçš„æƒŠäººæ•ˆæœè¶…å‡ºäº†æ‰€æœ‰äººçš„é¢„æœŸâ€”â€”ä¸ä»…æ•ˆæœæ›´å¥½ï¼Œè®­ç»ƒè¿˜å¿«äº†ä¸€ä¸ªæ•°é‡çº§ã€‚è¿™ä¸ªç»“æœæ”¹å˜äº†æ•´ä¸ªé¢†åŸŸçš„æ–¹å‘ã€‚\n\nä»Memory Networksåˆ°Transformerï¼Œä»è¾…åŠ©æœºåˆ¶åˆ°æ ¸å¿ƒæ¶æ„ï¼ŒSelf-Attentionå®Œæˆäº†ä»\"é…è§’\"åˆ°\"ä¸»è§’\"çš„åä¸½è½¬èº«ã€‚ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†è§è¯è¿™åœºé©å‘½çš„é«˜æ½®ï¼šTransformerâ€”â€”\"Attention Is All You Need\"ã€‚\n\n",
    "supporting": [
      "ch07-self-attention_files"
    ],
    "filters": [],
    "includes": {}
  }
}