{
  "hash": "2d683c5fd16f132074b8cfd447c6c8fe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"第8章：Transformer——注意力即一切\"\nsubtitle: \"Attention Is All You Need\"\nauthor: \"Ying Zha\"\ndate: \"2026-01-22\"\ncategories: [NLP, Deep Learning, Transformer, Attention]\nimage: \"figures/transformer-banner.png\"\ntoc: true\ntoc-depth: 3\nnumber-sections: true\ncode-fold: true\ncode-tools: true\nformat:\n  html:\n    css: styles.css\n    fig-cap-location: bottom\nbibliography: references.bib\n---\n\n> **核心问题**：能否用纯注意力机制替代循环结构，构建一个并行高效、能捕获任意距离依赖的序列建模架构？\n>\n> **历史坐标**：2017年 | Vaswani et al. \"Attention Is All You Need\" | Google Brain/Google Research\n\n---\n\n## 从上一章说起\n\n上一章我们见证了Self-Attention的诞生。这是一个概念性的突破：序列中的每个位置可以直接关注其他任意位置，不再需要通过RNN的逐步传递来建立长距离依赖。Self-Attention彻底打破了循环网络的顺序枷锁，让任意两个位置可以一步到位地直接交互。这不仅解决了长距离依赖的问题，还带来了一个意想不到的副产品：既然不同位置之间的注意力计算彼此独立，它们就可以完全并行——GPU终于能发挥它真正的实力了。更妙的是，注意力权重本身就是可解释的，你可以直接看到模型在处理某个词时\"关注\"了哪些其他词，这在黑箱横行的深度学习时代尤为珍贵。\n\n然而，Self-Attention仍然面临几个关键问题。首先是位置信息的缺失：纯Self-Attention是置换不变的（permutation invariant），打乱输入顺序，输出也只是相应打乱，模型完全不知道\"谁在谁前面\"。其次，之前的Self-Attention通常只是作为RNN的辅助模块，而非独立架构。最后，如何堆叠多层？如何处理编码器-解码器结构？如何保证训练稳定？这些架构设计问题都还悬而未决。\n\n2017年，Google的研究团队在论文\"Attention Is All You Need\"中给出了一个大胆的回答。\n\n> 💡 **本章核心洞察**：完全抛弃循环结构，用精心设计的注意力模块（Scaled Dot-Product Attention + Multi-Head）配合位置编码、残差连接、层归一化，构建一个完整的序列到序列架构——Transformer。\n\n这个决定在当时看来相当激进。RNN已经统治序列建模领域多年，放弃它意味着放弃一种直觉上合理的归纳偏置——毕竟，顺序处理符合人类阅读的方式。但实验结果令人震惊：Transformer不仅在机器翻译任务上大幅超越了当时最好的RNN模型，训练速度还快了一个数量级。\n\n这一章，我们将深入理解Transformer的每个设计决策：为什么要这样做？还有什么其他选择？这些选择带来了什么后果？\n\n---\n\n## 问题的本质是什么？\n\n### 问题的精确定义\n\n我们要解决的核心问题是序列到序列建模（Sequence-to-Sequence）：给定输入序列 $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$，生成输出序列 $\\mathbf{y} = (y_1, y_2, \\ldots, y_m)$。这个问题为什么重要？因为几乎所有的自然语言处理任务都可以转化为某种形式的序列到序列问题。机器翻译是把英文句子变成中文句子，文本摘要是把长文档变成短摘要，对话系统是把用户输入变成系统回复，代码生成是把自然语言描述变成代码。找到一个通用的序列建模架构，就等于找到了NLP的\"万能钥匙\"。\n\n### RNN-based Seq2Seq的局限\n\n在Transformer之前，主流的Seq2Seq架构是基于RNN的Encoder-Decoder加上Attention机制。这个架构我们在第4-6章已经详细讨论过。\n\n![RNN-based Seq2Seq with Attention](figures/chapter-8/fig-rnn-seq2seq.png){#fig-rnn-seq2seq}\n\n这个架构有三个根本性的问题。\n\n第一个问题是顺序计算瓶颈。RNN必须按顺序计算：$h_t = f(h_{t-1}, x_t)$，第$t$步必须等第$t-1$步完成。这意味着一个1000词的句子需要1000个顺序步骤，GPU明明擅长并行计算，却被迫做串行运算，利用率极低。\n\n第二个问题是长距离依赖的梯度问题。尽管LSTM和GRU缓解了梯度消失，但信息仍需逐步传递。考虑一个100词的句子，第1个词要影响第100个词的表示，信息需要经过99次非线性变换。每次变换都会带来信息损失和梯度衰减，这是RNN架构的固有缺陷。\n\n第三个问题是Attention仍然受限于RNN架构。虽然Attention可以直接连接编码器和解码器的任意位置，但Encoder内部仍是顺序处理，Decoder内部也是顺序处理。Attention只是RNN的\"外挂\"，而非核心架构。\n\n### 我们需要什么样的架构？\n\n从上述分析可以看出，理想的序列建模架构应该满足几个关键需求。首先是并行计算能力，这样才能充分利用GPU加速训练。其次是直接的长距离连接，让任意两个位置可以一步到位地交互。同时必须保留位置信息，因为序列顺序对语言理解至关重要。当然，表达能力要足够强，至少能匹配或超越RNN。最后，训练要稳定，深层网络要能收敛。\n\nTransformer的设计正是为了同时满足这些需求。\n\n---\n\n## 核心思想与直觉\n\n### 革命性洞察：注意力本身就是计算\n\n传统观点把注意力看作一种选择机制——从一堆信息中挑选重要的部分。Transformer的洞察更加深刻：注意力可以是计算本身，不只是选择信息，而是通过加权聚合来生成新的表示。\n\n让我用一个图书馆的类比来解释这个区别。RNN的方式就像你按顺序阅读书架上的每本书，一本接一本，用笔记记录累积的理解。读到第100本书时，你对第1本的记忆已经模糊了。Transformer的方式则完全不同：你同时把所有书摊开在桌上，对于每个问题（Query），快速扫视所有书（Key），找出相关的（高注意力权重），然后综合这些相关内容（Value的加权和）得到答案。这种方式不仅更高效，而且不会遗忘。\n\n### 核心架构概览\n\nTransformer由两个主要部分组成：Encoder负责理解输入序列，Decoder负责生成输出序列。\n\n![The Transformer — model architecture.](figures/chapter-8/original/fig1-transformer-architecture.png){#fig-transformer-overview width=70%}\n\n::: {.figure-caption}\n*Source: Vaswani et al. (2017) \"Attention Is All You Need\", Figure 1*\n:::\n\n整个架构由五种核心组件构成。Scaled Dot-Product Attention负责高效地计算注意力。Multi-Head Attention让多个注意力\"头\"关注不同的子空间。Positional Encoding注入位置信息。Feed-Forward Network提供逐位置的非线性变换。Residual Connection和Layer Norm则保证深层训练的稳定性。\n\n### 设计动机：为什么这些组件？\n\n每个组件都是为了解决特定问题而存在的。Scaled Dot-Product用点积计算注意力，比加性注意力更高效，因为可以直接利用矩阵乘法的硬件优化。缩放因子$\\sqrt{d_k}$防止softmax饱和导致的梯度消失。Multi-Head让模型能同时捕获多种不同类型的关系模式。Positional Encoding弥补了纯注意力机制丢失的位置信息。FFN提供必要的非线性变换能力，因为单纯的注意力操作本质上是线性的。Residual和LayerNorm则是深层网络训练的标配，没有它们模型根本无法收敛。\n\n---\n\n## 技术细节\n\n### Scaled Dot-Product Attention\n\n#### 基本公式\n\n给定Query矩阵 $Q \\in \\mathbb{R}^{n \\times d_k}$，Key矩阵 $K \\in \\mathbb{R}^{m \\times d_k}$，Value矩阵 $V \\in \\mathbb{R}^{m \\times d_v}$，Scaled Dot-Product Attention的计算公式是：\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n$$\n\n这里$n$是Query的数量，$m$是Key/Value的数量，$d_k$是Key的维度，$d_v$是Value的维度。整个计算流程可以用下图表示：\n\n![Scaled Dot-Product Attention](figures/chapter-8/original/fig2-scaled-attention.png){#fig-scaled-dot-product width=35%}\n\n::: {.figure-caption}\n*Source: Vaswani et al. (2017) \"Attention Is All You Need\", Figure 2 (left)*\n:::\n\n#### 为什么要除以 $\\sqrt{d_k}$？\n\n这是Transformer论文中最精妙的细节之一，值得我们从数学上仔细理解。\n\n问题的根源在于：当$d_k$较大时，点积$q \\cdot k$的方差会变大。假设$q$和$k$的每个分量都是独立同分布的，均值为0，方差为1。那么点积的方差是多少呢？\n\n$$\n\\text{Var}(q \\cdot k) = \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i k_i\\right) = \\sum_{i=1}^{d_k} \\text{Var}(q_i k_i) = d_k\n$$\n\n当$d_k = 512$时，点积的标准差约为$\\sqrt{512} \\approx 22.6$。这意味着点积值可能非常大或非常小。\n\n这会带来什么后果？如果某个点积远大于其他点积，softmax输出会接近one-hot分布，梯度会趋近于零（因为softmax在饱和区的梯度很小），学习就会停滞。\n\n解决方案很优雅：除以$\\sqrt{d_k}$，将方差重新标准化为1。\n\n$$\n\\text{Var}\\left(\\frac{q \\cdot k}{\\sqrt{d_k}}\\right) = \\frac{d_k}{d_k} = 1\n$$\n\n这保证了无论$d_k$多大，注意力分数的分布都保持稳定。在$d_k = 64$的典型设置下，不缩放时点积标准差约为8，softmax容易饱和；缩放后标准差为1，softmax梯度稳定。\n\n#### Mask的作用\n\n在解码器的Self-Attention中，我们需要因果遮罩（Causal Mask）来保证模型只能看到过去，不能看到未来。Mask矩阵的定义是：当$i \\geq j$时为0，当$i < j$时为$-\\infty$。加上Mask后：\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + \\text{Mask}\\right)V\n$$\n\n$-\\infty$经过softmax变成0，实现了\"看不到未来\"的效果。这对于自回归生成至关重要——在生成第$t$个词时，模型只能依赖已经生成的前$t-1$个词。\n\n### Multi-Head Attention\n\n#### 动机：为什么需要多头？\n\n单头注意力有一个根本性的局限：每个位置只能有一种\"关注模式\"。但在自然语言中，一个词可能同时需要关注多种不同类型的信息。比如动词需要关注它的主语（语法关系），代词需要关注它的先行词（指代关系），同义词之间也需要相互关注（语义关系）。单头难以同时捕获这些不同类型的依赖，这就是Multi-Head的动机。\n\n#### 数学形式\n\nMulti-Head Attention的核心思想是将$d_{model}$维的Q、K、V分别投影到$h$个子空间，在每个子空间独立计算注意力，最后拼接起来：\n\n$$\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n$$\n\n其中每个head的计算是：\n\n$$\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n$$\n\n参数矩阵的维度设计很讲究：$W_i^Q$和$W_i^K$都是$d_{model} \\times d_k$，$W_i^V$是$d_{model} \\times d_v$，输出投影$W^O$是$hd_v \\times d_{model}$。通常设置$d_k = d_v = d_{model}/h$，这样保证了总参数量不变。\n\n![Multi-Head Attention consists of several attention layers running in parallel.](figures/chapter-8/original/fig2-multi-head.png){#fig-multi-head width=50%}\n\n::: {.figure-caption}\n*Source: Vaswani et al. (2017) \"Attention Is All You Need\", Figure 2 (right)*\n:::\n\n#### Head数量的选择\n\nTransformer base使用$h=8$，large使用$h=16$。选择的考量涉及一个权衡：头太少，表达能力不足；头太多，每个head的维度$d_k = d_{model}/h$就太小，信息容量不足。经验法则是$d_k \\geq 64$通常是下限。\n\n后续研究发现不同head确实学到了不同的模式。有的head关注局部（相邻词），有的关注全局（长距离依赖）；有的关注语法结构，有的关注语义相似性。这印证了Multi-Head设计的合理性。\n\n### 位置编码\n\n#### 为什么需要位置编码？\n\nSelf-Attention有一个数学上的特性：它是置换等变的（permutation equivariant）。如果输入序列$(x_1, x_2, x_3)$变成$(x_2, x_3, x_1)$，输出也只是相应重排为$(y_2, y_3, y_1)$。模型完全不知道\"谁在谁前面\"。\n\n但对于语言来说，顺序至关重要。\"狗咬人\"和\"人咬狗\"完全不同，\"我爱你\"和\"你爱我\"意思相反。我们必须显式地告诉模型位置信息。\n\n#### 正弦位置编码\n\nTransformer原论文使用正弦函数生成位置编码。对于位置$pos$和维度索引$i$：\n\n$$\nPE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n$$\n\n$$\nPE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n$$\n\n::: {.figure-placeholder}\n**[图 8.1: Positional Encoding Visualization]**\n\n![placeholder](figures/chapter-8/positional-encoding.png)\n\n<details>\n<summary>📊 NotebookLM 生成指令（点击展开）</summary>\n\n**图片类型**：热力图/可视化\n\n**核心内容**：\n展示正弦位置编码的模式。横轴是位置（0-100），纵轴是维度（0-512），颜色表示PE值（-1到1）。应该能清晰看到：低维度变化快（高频），高维度变化慢（低频）。\n\n**详细元素**：\n1. 热力图主体：位置 × 维度的矩阵\n2. 颜色条：-1（深蓝）到 +1（深红）\n3. 标注：低维度区域标注\"High frequency\"，高维度区域标注\"Low frequency\"\n4. 可选：在右侧展示几个特定维度的正弦曲线\n\n**视觉风格**：\n- 布局：热力图为主，右侧或下方加颜色条\n- 配色：蓝-白-红渐变\n- 重点标注：用虚线框标出高频区和低频区\n\n**参考描述**：\n> Create a heatmap visualization of sinusoidal positional encoding. X-axis shows positions 0-100, Y-axis shows embedding dimensions 0-512. Color represents PE values from -1 (blue) to +1 (red). The pattern should clearly show that lower dimensions have higher frequency (rapid oscillation) while higher dimensions have lower frequency (slow oscillation). Add labels \"High frequency\" for lower dimensions and \"Low frequency\" for higher dimensions. Include a color bar on the right side.\n\n</details>\n:::\n\n为什么选择正弦函数？首先是有界性：值始终在[-1, 1]之间，不会爆炸。其次是周期性：每个维度有不同的波长，从$2\\pi$到$10000 \\cdot 2\\pi$，形成了一个\"频率谱\"。最重要的是，正弦函数有一个优美的数学性质：对于任意固定的偏移$k$，存在线性变换使得$PE_{pos+k} = f(PE_{pos})$。这意味着模型可以学习到\"位置+3\"或\"位置-2\"这样的相对关系。\n\n一个直觉的理解方式是把位置编码想象成\"频率分解\"。低维度是高频信号，变化快，用来区分相邻位置；高维度是低频信号，变化慢，用来区分远距离位置。就像钟表：秒针转得快用来区分秒，时针转得慢用来区分小时。\n\n#### 可学习位置编码\n\n另一种选择是让位置编码作为可学习参数：$PE_{pos} = E_{pos} \\in \\mathbb{R}^{d_{model}}$，其中$E$是可学习的嵌入矩阵。\n\n这两种方案各有优劣。正弦编码不需要额外参数，理论上可以外推到任意长度；可学习编码需要$L \\times d_{model}$个参数，只能用于训练见过的长度。实际效果上两者相当，BERT等模型使用可学习编码也取得了很好的效果。后续工作如RoPE和ALiBi（见第26章）提出了更优的位置编码方案，但这已经是后话了。\n\n### Feed-Forward Network\n\n每个Transformer层都包含一个逐位置的前馈网络，它的形式很简单：两层线性变换夹一个ReLU激活：\n\n$$\n\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n$$\n\n其中$W_1 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$，$W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$，通常$d_{ff} = 4 \\times d_{model}$。\n\nFFN是一个被低估的组件。直觉上，Self-Attention负责\"信息交流\"——决定收集哪些信息，进行不同位置间的加权聚合；FFN负责\"信息处理\"——决定如何处理信息，对每个位置的表示做非线性变换。可以把这种分工类比为：Attention是\"开会讨论\"，FFN是\"个人思考\"。\n\n一个令人惊讶的事实是，在标准Transformer中，FFN的参数量占模型总参数的约三分之二。FFN参数量是$2 \\times d_{model} \\times d_{ff} = 8d_{model}^2$，而Attention参数量只有$4 \\times d_{model}^2$。后续研究发现FFN可能起到\"记忆\"的作用——存储知识，不同层的FFN关注不同类型的信息。稀疏化FFN（如Mixture of Experts）成为了扩展模型规模的重要方向。\n\n### 残差连接与Layer Normalization\n\n#### 残差连接\n\n每个子层（Attention或FFN）都被残差连接包裹。残差连接解决的是梯度流动问题：它提供了一条\"高速公路\"让梯度直接传回前层，使得深层网络训练成为可能。Transformer base有6层，large有12层；后来的模型到96层甚至更多，没有残差连接这是不可想象的。\n\n#### Layer Normalization\n\n与Batch Normalization不同，Layer Norm在特征维度而非batch维度上归一化：\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta\n$$\n\n其中$\\mu$和$\\sigma$是$x$在特征维度上的均值和标准差。\n\n为什么用LayerNorm而非BatchNorm？原因有三。第一，序列长度不固定，不同序列长度不同，batch统计不稳定。第二，LayerNorm不依赖batch统计，训练和推理行为一致。第三，每个样本独立归一化，对并行更友好。\n\n#### Pre-Norm vs Post-Norm\n\n原始Transformer使用Post-Norm，先子层后归一化：$x_{l+1} = \\text{LayerNorm}(x_l + \\text{Sublayer}(x_l))$。后来的模型多采用Pre-Norm，先归一化后子层：$x_{l+1} = x_l + \\text{Sublayer}(\\text{LayerNorm}(x_l))$。\n\n![Pre-Norm vs Post-Norm](figures/chapter-8/fig-pre-post-norm.png){#fig-pre-post-norm}\n\nPre-Norm的优势在于更稳定的训练，尤其是深层模型，甚至不需要warmup也能收敛。GPT系列、LLaMA等都采用了Pre-Norm。\n\n### Encoder与Decoder的结构差异\n\nTransformer的Encoder和Decoder有三个关键区别。\n\n第一个区别是Self-Attention的类型。Encoder使用双向Self-Attention，每个位置可以看到整个输入序列：\n\n$$\n\\text{Attention}_{ij} = \\frac{\\exp(q_i \\cdot k_j / \\sqrt{d_k})}{\\sum_l \\exp(q_i \\cdot k_l / \\sqrt{d_k})}\n$$\n\n没有Mask，第$i$个位置可以关注任意位置$j$。而Decoder使用因果Self-Attention，第$i$个位置只能关注位置$1, 2, \\ldots, i$：\n\n$$\n\\text{Attention}_{ij} = \\begin{cases}\n\\frac{\\exp(q_i \\cdot k_j / \\sqrt{d_k})}{\\sum_{l \\leq i} \\exp(q_i \\cdot k_l / \\sqrt{d_k})} & j \\leq i \\\\\n0 & j > i\n\\end{cases}\n$$\n\n这保证了生成时只用到已生成的token，符合自回归的要求。\n\n第二个区别是Decoder有一个额外的Cross-Attention层，通过它连接到Encoder。Query来自Decoder当前层的输出，Key和Value来自Encoder最后一层的输出。这使得生成每个词时都可以参考整个输入序列。\n\n![Cross-Attention connects Decoder to Encoder](figures/chapter-8/fig-cross-attention.png){#fig-cross-attention}\n\n第三个区别是用途：Encoder负责理解输入序列，产生上下文表示；Decoder负责基于这些表示生成输出序列。\n\n### 复杂度分析\n\n设序列长度为$n$，模型维度为$d$。Self-Attention的时间复杂度是$O(n^2 d)$，瓶颈在于$n^2$的注意力矩阵计算。FFN的时间复杂度是$O(n d^2)$，瓶颈在于$d^2$的线性变换。总计是$O(n^2 d + n d^2)$，当序列很长时$n^2$项主导。\n\n与RNN的$O(n d^2)$相比，Transformer在$n < d$时更快，$n > d$时更慢。但关键区别在于并行性：RNN需要$O(n)$个顺序步骤，完全无法并行；Transformer只需要$O(1)$个顺序步骤（只是层数），计算完全并行。这是Transformer训练速度远超RNN的根本原因。\n\n空间复杂度方面，最大的开销是注意力矩阵，需要$O(n^2)$的空间。对于长序列，比如处理整本书，这会成为严重问题。这为第9章的高效注意力研究埋下了伏笔。\n\n---\n\n## 工程实践\n\n### 从零实现Transformer\n\n以下是Transformer核心模块的PyTorch实现，附详细注释：\n\n::: {#99972be3 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention\n    核心公式: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V\n    \"\"\"\n    def __init__(self, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, Q, K, V, mask=None):\n        d_k = Q.size(-1)\n\n        # 计算注意力分数: [batch, heads, seq_len, seq_len]\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n        # 应用mask（用于因果注意力或padding mask）\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # Softmax归一化\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # 加权求和\n        output = torch.matmul(attention_weights, V)\n        return output, attention_weights\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-Head Attention\n    将d_model拆成h个头，每个头独立计算attention，最后拼接\n    \"\"\"\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model必须能被num_heads整除\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Q, K, V的线性投影\n        self.W_Q = nn.Linear(d_model, d_model)\n        self.W_K = nn.Linear(d_model, d_model)\n        self.W_V = nn.Linear(d_model, d_model)\n        self.W_O = nn.Linear(d_model, d_model)\n\n        self.attention = ScaledDotProductAttention(dropout)\n\n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n\n        # 线性投影并reshape为多头格式\n        Q = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        # 计算attention\n        attn_output, _ = self.attention(Q, K, V, mask)\n\n        # 拼接所有头并做最终投影\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        return self.W_O(attn_output)\n\n\nclass PositionwiseFeedForward(nn.Module):\n    \"\"\"FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2\"\"\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"正弦位置编码\"\"\"\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1), :])\n\n\nclass TransformerEncoderLayer(nn.Module):\n    \"\"\"Transformer Encoder层：Self-Attention -> Add & Norm -> FFN -> Add & Norm\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-Attention + Residual + LayerNorm\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        # FFN + Residual + LayerNorm\n        x = self.norm2(x + self.dropout(self.ffn(x)))\n        return x\n\n\nclass TransformerDecoderLayer(nn.Module):\n    \"\"\"Transformer Decoder层\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n        # Masked Self-Attention\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, self_attn_mask)))\n        # Cross-Attention\n        x = self.norm2(x + self.dropout(self.cross_attn(x, encoder_output, encoder_output, cross_attn_mask)))\n        # FFN\n        x = self.norm3(x + self.dropout(self.ffn(x)))\n        return x\n\n\nclass Transformer(nn.Module):\n    \"\"\"完整的Transformer模型\"\"\"\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048,\n                 max_len=5000, dropout=0.1):\n        super().__init__()\n\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n\n        self.encoder_layers = nn.ModuleList([\n            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_encoder_layers)\n        ])\n        self.decoder_layers = nn.ModuleList([\n            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n\n        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n        self.scale = math.sqrt(d_model)\n\n    def encode(self, src, src_mask=None):\n        x = self.pos_encoding(self.src_embedding(src) * self.scale)\n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        return x\n\n    def decode(self, tgt, encoder_output, tgt_mask=None, src_mask=None):\n        x = self.pos_encoding(self.tgt_embedding(tgt) * self.scale)\n        for layer in self.decoder_layers:\n            x = layer(x, encoder_output, tgt_mask, src_mask)\n        return x\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        encoder_output = self.encode(src, src_mask)\n        decoder_output = self.decode(tgt, encoder_output, tgt_mask, src_mask)\n        return self.output_projection(decoder_output)\n\n\ndef create_causal_mask(seq_len):\n    \"\"\"创建因果mask，防止看到未来的token\"\"\"\n    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n    return ~mask.unsqueeze(0).unsqueeze(0)\n```\n:::\n\n\n### 使用Hugging Face Transformers\n\n对于实际应用，推荐使用经过优化的库：\n\n::: {#8f439565 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# 加载预训练的T5模型（Encoder-Decoder Transformer）\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 翻译任务示例\ninput_text = \"translate English to German: Hello, how are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids, max_length=50)\ntranslated = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Translation: {translated}\")\n```\n:::\n\n\n### 复现论文的关键细节\n\n原始Transformer论文中有一些容易忽略但至关重要的细节。\n\n首先是Embedding缩放：输入embedding需要乘以$\\sqrt{d_{model}}$，因为embedding初始化通常较小，缩放后与位置编码的量级匹配。\n\n其次是权重共享的可能性：原论文中，encoder和decoder的embedding可以共享；如果源语言和目标语言相同，output projection也可以与embedding共享，这样可以减少参数量。\n\nDropout的位置也有讲究：在attention分数计算后、残差加法前、以及embedding之后都要加dropout。\n\n学习率策略使用了一个特殊的warmup schedule：\n```\nlrate = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n```\n前warmup_steps步线性增加，之后按步数平方根衰减。这种设计对训练稳定性至关重要。\n\n最后，论文使用了0.1的label smoothing，这有助于提高泛化性能。\n\n### 实验验证\n\n验证实现是否正确有几个关键步骤。首先检查各模块输入输出维度是否正确。然后在几个样本上尝试过拟合——如果模型正确，应该能完美过拟合小数据集。接着可视化attention pattern，检查是否合理。最后与Hugging Face等权威实现对比中间结果。\n\n---\n\n## 深入理解\n\n> **研究者必读**：这一节探讨Transformer的理论基础、边界条件和开放问题\n\n### 为什么有效？——理论视角\n\n#### 表达能力分析\n\nTransformer是通用函数逼近器吗？Yun et al. (2019) 给出了肯定的答案：在合适的条件下，Transformer可以以任意精度逼近任何连续的序列到序列函数。具体地，一个$O(n)$层的Transformer可以逼近任何Lipschitz连续的permutation equivariant函数。\n\n与RNN相比，RNN也是通用逼近器，但需要无限精度的隐状态。Transformer的优势在于可以用有限宽度实现相同的逼近能力。\n\n#### 归纳偏置的视角\n\n不同架构有不同的归纳偏置。RNN假设顺序处理和短程依赖，这符合语言直觉，但长距离依赖困难。CNN假设局部模式和层级结构，擅长捕获n-gram，但感受野有限。Transformer假设全局连接和位置无关，非常灵活且可并行，但需要学习位置关系，计算量也更大。\n\nTransformer的归纳偏置可以说是\"更弱\"的——它不假设顺序或局部性，而是让模型自己学习这些模式。这意味着需要更多数据来学习显式的位置关系，但也更灵活，可以学到任意模式。当规模够大时，弱归纳偏置反而可能成为优势。\n\n#### 与Hopfield网络的联系\n\nRamsauer et al. (2020) 发现了一个有趣的联系：Transformer的attention机制可以理解为现代Hopfield网络的一步更新。这提供了一个全新的视角：Hopfield网络是联想记忆模型，Attention可以看作从\"记忆\"（Key-Value对）中\"检索\"与Query相关的内容。这也解释了为什么FFN可能起到存储知识的作用——它就是Transformer的\"记忆体\"。\n\n### 为什么有效？——实证视角\n\n#### Attention真的在学习有意义的模式吗？\n\n通过可视化不同head的attention pattern，研究者发现确实如此。有些head学会了关注语法依赖，比如主谓关系、动宾关系。有些head学会了关注固定偏移，比如总是关注前一个词或后一个词。还有些head专门关注特殊token，如句号或[SEP]标记。\n\n::: {.figure-placeholder}\n**[图 8.2: Attention Visualization across Layers and Heads]**\n\n![placeholder](figures/chapter-8/attention-visualization.png)\n\n<details>\n<summary>📊 NotebookLM 生成指令（点击展开）</summary>\n\n**图片类型**：热力图网格\n\n**核心内容**：\n展示一个句子（如\"The cat sat on the mat\"）在不同层、不同head的attention pattern。应该能看到：浅层更关注局部（相邻词），深层更关注全局（语法关系）。\n\n**详细元素**：\n1. 3×4网格布局：3层 × 4个head\n2. 每个小热力图：6×6的attention矩阵\n3. 行/列标签：句子中的词\n4. 颜色：白（低attention）到深蓝（高attention）\n5. 标题：Layer 1 Head 1, Layer 1 Head 2, ...\n\n**视觉风格**：\n- 布局：规整的网格，清晰的边界\n- 配色：白-蓝渐变\n- 重点标注：在典型的\"语法head\"和\"位置head\"处加标注\n\n**参考描述**：\n> Create a grid of attention heatmaps showing how different layers and heads attend to words in the sentence \"The cat sat on the mat\". Layout: 3 rows (Layer 1, 2, 3) × 4 columns (Head 1, 2, 3, 4). Each cell is a 6×6 heatmap with word labels. Color scale: white (0) to dark blue (1). Show that Layer 1 heads tend to attend to nearby words (diagonal pattern), while Layer 3 heads attend to syntactically related words (subject-verb, etc.). Add annotations for notable patterns.\n\n</details>\n:::\n\n#### 哪些组件是必要的？\n\n通过消融实验，研究者得出了明确的结论。减少Multi-Head的数量会导致性能下降，说明多头是重要的。移除FFN会导致大幅下降，说明它不可或缺。移除Residual会导致模型无法训练。移除LayerNorm会导致训练不稳定。移除Positional Encoding会导致输出几乎随机。\n\n一个有趣的发现是，部分head可以被剪枝而不显著影响性能，说明模型存在一定的冗余。\n\n### 方法的边界条件\n\n#### 隐含假设\n\nTransformer隐含地做了几个假设。第一，位置信息可以通过加法注入——位置编码与内容编码相加，假设它们在同一语义空间可以混合。第二，全局交互是必要的——每个位置都与所有其他位置交互；如果任务只需要局部信息，这其实是一种浪费。第三，数据足够多——弱归纳偏置需要更多数据来补偿。\n\n#### 失效条件\n\n在某些情况下，Transformer表现并不好。当数据量太小时，带有更强归纳偏置的模型（如CNN、RNN）可能表现更好。当序列太长时，$O(n^2)$的复杂度会成为瓶颈，超过几千token时需要特殊处理。对于需要精确位置推理的任务，如算术运算、数字排序，Transformer常常失败。对于强顺序依赖的任务，如解析嵌套结构，Transformer也可能不如专门设计的架构。\n\n#### 已知的Failure Modes\n\n研究文献中报告了几种典型的失败模式。长度泛化失败：在短序列上训练的Transformer难以泛化到长序列。位置编码外推困难：超出训练长度后，正弦编码的外推效果不佳。对重复模式敏感：输入中的重复可能导致注意力过度集中。\n\n### 变体与扩展\n\n从架构角度看，Transformer衍生出了三大变体。Encoder-only的BERT用于理解任务。Decoder-only的GPT用于生成任务。Encoder-Decoder的T5和BART用于seq2seq任务。后续发展表明，Decoder-only架构在规模化后表现最好，这将在第16章详细讨论。\n\n从注意力机制角度看，Sparse Attention只关注部分位置，Linear Attention将复杂度降到$O(n)$，相对位置编码如RoPE和ALiBi解决了外推问题。这些改进将在第9章和第26章展开。\n\n### 开放研究问题\n\n如果你要在这个方向写一篇论文，可以从哪些问题切入呢？\n\n第一个方向是数据效率：为什么Transformer需要这么多数据？如何设计更好的归纳偏置？数据效率能提升多少？\n\n第二个方向是位置编码：最优设计是什么？正弦、可学习、相对位置...哪种最好？能否设计出完美外推的位置编码？\n\n第三个方向是FFN的作用：它到底在做什么？是在存储知识吗？能否用其他结构替代？\n\n第四个方向是可解释性：Attention权重真的代表\"关注\"吗？如何更好地解释Transformer的决策？\n\n第五个方向是效率：能否在保持性能的同时降低复杂度？稀疏注意力的极限在哪里？\n\n---\n\n## 局限性与未解决的问题\n\n### 计算复杂度：$O(n^2)$的诅咒\n\nTransformer最大的局限是Self-Attention的二次复杂度。考虑处理一本10万词的书：\n\n$$\n\\text{注意力矩阵大小} = 100000^2 = 10^{10} \\text{ 元素}\n$$\n\n即使用FP16，也需要20GB显存仅用于存储注意力矩阵！这直接限制了长文档处理、长程对话、以及需要大上下文的任务如代码生成和法律分析。\n\n### 位置编码的外推问题\n\n原始正弦位置编码在训练长度之外表现不佳。如果模型在512长度上训练，面对1024长度的输入时，位置512-1023的编码模型从未见过，性能会急剧下降。这限制了模型的实际应用灵活性。\n\n### 缺乏显式的层级结构\n\n自然语言有天然的层级结构：词组成短语，短语组成句子，句子组成段落，段落组成文档。Transformer的flat attention没有显式建模这种层级，完全依赖模型自己学习。这可能是某些结构化任务上表现不佳的原因。\n\n### 这些局限导向了什么？\n\n上述问题推动了后续研究。$O(n^2)$复杂度的问题催生了第9章的高效注意力机制，包括Longformer、BigBird、Performer等。位置编码外推问题催生了第26章的RoPE、ALiBi等新型位置编码。长上下文需求催生了Flash Attention和KV Cache优化等系统级方案。\n\n> 下一章预告：第9章将深入探讨如何打破$O(n^2)$的魔咒，介绍各种高效注意力机制及其背后的设计思想。\n\n---\n\n## 本章小结\n\n### 核心要点回顾\n\n这一章我们探讨了Transformer的完整设计。核心问题是如何构建一个并行高效、能捕获任意距离依赖的序列建模架构。核心洞察是完全抛弃循环结构，用注意力机制本身作为核心计算单元。\n\n具体方法包括Scaled Dot-Product Attention提供高效稳定的注意力计算，Multi-Head Attention在多个子空间捕获多种模式，Positional Encoding注入位置信息，FFN配合Residual和LayerNorm保证表达能力和训练稳定性。\n\n这项工作的意义是开创性的：它建立了完全基于注意力的架构，奠定了预训练大模型的基础。后续的GPT、BERT、LLaMA等划时代模型都是基于Transformer。\n\n### 关键公式速查\n\n| 公式 | 表达式 |\n|------|--------|\n| Scaled Dot-Product Attention | $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$ |\n| Multi-Head | $\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$ |\n| Positional Encoding | $PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$, $PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$ |\n\n### 思考题\n\n1. **[概念理解]** 为什么说Transformer的Self-Attention是\"置换等变\"的？这与RNN有什么本质区别？\n\n2. **[数学推导]** 证明：当$q$和$k$的分量独立同分布且方差为1时，$q \\cdot k$的方差等于$d_k$。\n\n3. **[工程实践]** 实现一个简化版Transformer，在IWSLT14德英翻译数据集上训练，达到BLEU > 25。\n\n4. **[研究思考]** 有人认为Transformer的成功主要归功于规模（更多参数、更多数据），而非架构本身。你如何看待这个观点？有什么证据支持或反驳？\n\n---\n\n## 延伸阅读\n\n### 核心论文（必读）\n\n**Vaswani et al. (2017). \"Attention Is All You Need\"** 是这一切的起点。重点阅读Section 3的模型架构描述和Section 5.4关于为什么选择Self-Attention的讨论。实验细节可以快速浏览。\n\n### 理论基础\n\n**Yun et al. (2019). \"Are Transformers universal approximators of sequence-to-sequence functions?\"** 证明了Transformer的通用逼近性质，为理解其表达能力提供了理论基础。\n\n**Ramsauer et al. (2020). \"Hopfield Networks is All You Need\"** 揭示了Attention与Hopfield网络的深层联系，提供了一个全新的理解视角。\n\n### 后续发展\n\n**Clark et al. (2019). \"What Does BERT Look At?\"** 深入分析了BERT的attention pattern，帮助我们理解模型到底在关注什么。\n\n**Michel et al. (2019). \"Are Sixteen Heads Really Better than One?\"** 发现多数head可以被剪枝而不显著影响性能，揭示了模型的冗余性。\n\n### 代码资源\n\n官方实现在tensor2tensor仓库。PyTorch入门者推荐阅读The Annotated Transformer，它用详细注释的方式解读了完整实现。实际应用推荐使用Hugging Face的transformers库，它提供了经过优化的生产级实现。\n\n---\n\n## 历史注脚\n\n\"Attention Is All You Need\"这篇论文最初在Google内部也引起了争议。当时RNN仍是主流，有人质疑完全抛弃循环结构是否明智。论文的标题也颇有挑衅意味——\"注意力就是你所需要的全部\"，仿佛在向整个RNN阵营宣战。\n\n有趣的是，论文的八位作者后来分散到了不同的地方：有的创办了AI公司，有的加入了OpenAI，有的继续在Google做研究。这篇论文成为了整个AI时代的基石。\n\n\"Transformer\"这个名字的灵感据说来自于变形金刚（Transformers）——既有\"变换\"的含义，又朗朗上口。另一种说法是它暗示了从RNN到全新架构的\"变革\"（transformation）。\n\n截至2024年，这篇论文的引用次数超过10万次，是近年来引用最高的AI论文之一。几乎所有现代大语言模型——GPT-4、Claude、Gemini、LLaMA——都是基于Transformer架构。当年那个\"激进\"的设计决策，已经成为了整个领域的基础设施。\n\n",
    "supporting": [
      "ch08-transformer-v2_files"
    ],
    "filters": [],
    "includes": {}
  }
}