<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-01-20">
<meta name="description" content="在尽量精简背景的前提下，系统讲清 DeepSeek-R1：它如何从普通大模型出发，通过推理轨迹、奖励模型和强化学习，把「会认真思考」变成一个可训练的工程流程。">

<title>DeepSeek-R1：推理增强的大语言模型（codex 版） – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#引言ai-推理能力的新纪元" id="toc-引言ai-推理能力的新纪元" class="nav-link active" data-scroll-target="#引言ai-推理能力的新纪元"><span class="header-section-number">1</span> 引言：AI 推理能力的新纪元</a>
  <ul class="collapse">
  <li><a href="#本文的阅读路线" id="toc-本文的阅读路线" class="nav-link" data-scroll-target="#本文的阅读路线"><span class="header-section-number">1.1</span> 本文的阅读路线</a></li>
  </ul></li>
  <li><a href="#最小背景语言模型强化学习与注意力" id="toc-最小背景语言模型强化学习与注意力" class="nav-link" data-scroll-target="#最小背景语言模型强化学习与注意力"><span class="header-section-number">2</span> 最小背景：语言模型、强化学习与注意力</a>
  <ul class="collapse">
  <li><a href="#语言模型与训练目标从概率到交叉熵" id="toc-语言模型与训练目标从概率到交叉熵" class="nav-link" data-scroll-target="#语言模型与训练目标从概率到交叉熵"><span class="header-section-number">2.1</span> 语言模型与训练目标：从概率到交叉熵</a></li>
  <li><a href="#强化学习基础从反馈中学习推理策略" id="toc-强化学习基础从反馈中学习推理策略" class="nav-link" data-scroll-target="#强化学习基础从反馈中学习推理策略"><span class="header-section-number">2.2</span> 强化学习基础：从反馈中学习推理策略</a></li>
  <li><a href="#注意力机制transformer-的核心算子" id="toc-注意力机制transformer-的核心算子" class="nav-link" data-scroll-target="#注意力机制transformer-的核心算子"><span class="header-section-number">2.3</span> 注意力机制：Transformer 的核心算子</a></li>
  </ul></li>
  <li><a href="#传统大模型的局限为什么需要-deepseek-r1" id="toc-传统大模型的局限为什么需要-deepseek-r1" class="nav-link" data-scroll-target="#传统大模型的局限为什么需要-deepseek-r1"><span class="header-section-number">3</span> 传统大模型的局限：为什么需要 DeepSeek-R1？</a>
  <ul class="collapse">
  <li><a href="#一次性生成的困境信息瓶颈" id="toc-一次性生成的困境信息瓶颈" class="nav-link" data-scroll-target="#一次性生成的困境信息瓶颈"><span class="header-section-number">3.1</span> 一次性生成的困境：信息瓶颈</a></li>
  <li><a href="#缺乏可见的推理过程黑箱问题" id="toc-缺乏可见的推理过程黑箱问题" class="nav-link" data-scroll-target="#缺乏可见的推理过程黑箱问题"><span class="header-section-number">3.2</span> 缺乏可见的推理过程：黑箱问题</a></li>
  <li><a href="#监督学习的瓶颈数据与组合爆炸" id="toc-监督学习的瓶颈数据与组合爆炸" class="nav-link" data-scroll-target="#监督学习的瓶颈数据与组合爆炸"><span class="header-section-number">3.3</span> 监督学习的瓶颈：数据与组合爆炸</a></li>
  </ul></li>
  <li><a href="#deepseek-r1-的核心创新" id="toc-deepseek-r1-的核心创新" class="nav-link" data-scroll-target="#deepseek-r1-的核心创新"><span class="header-section-number">4</span> DeepSeek-R1 的核心创新</a>
  <ul class="collapse">
  <li><a href="#思维链推理让思考过程可见" id="toc-思维链推理让思考过程可见" class="nav-link" data-scroll-target="#思维链推理让思考过程可见"><span class="header-section-number">4.1</span> 4.1 思维链推理：让思考过程可见</a></li>
  <li><a href="#强化学习驱动从试错中学习推理" id="toc-强化学习驱动从试错中学习推理" class="nav-link" data-scroll-target="#强化学习驱动从试错中学习推理"><span class="header-section-number">4.2</span> 4.2 强化学习驱动：从试错中学习推理</a></li>
  <li><a href="#ppo算法稳定的策略优化" id="toc-ppo算法稳定的策略优化" class="nav-link" data-scroll-target="#ppo算法稳定的策略优化"><span class="header-section-number">4.3</span> 4.3 PPO算法：稳定的策略优化</a></li>
  <li><a href="#过程奖励模型精细化的反馈" id="toc-过程奖励模型精细化的反馈" class="nav-link" data-scroll-target="#过程奖励模型精细化的反馈"><span class="header-section-number">4.4</span> 4.4 过程奖励模型：精细化的反馈</a></li>
  <li><a href="#知识蒸馏平衡性能与效率" id="toc-知识蒸馏平衡性能与效率" class="nav-link" data-scroll-target="#知识蒸馏平衡性能与效率"><span class="header-section-number">4.5</span> 4.5 知识蒸馏：平衡性能与效率</a></li>
  </ul></li>
  <li><a href="#架构实现细节性能优化的数学基础" id="toc-架构实现细节性能优化的数学基础" class="nav-link" data-scroll-target="#架构实现细节性能优化的数学基础"><span class="header-section-number">5</span> 5. 架构实现细节：性能优化的数学基础</a>
  <ul class="collapse">
  <li><a href="#分组查询注意力grouped-query-attention-gqa" id="toc-分组查询注意力grouped-query-attention-gqa" class="nav-link" data-scroll-target="#分组查询注意力grouped-query-attention-gqa"><span class="header-section-number">5.1</span> 5.1 分组查询注意力（Grouped Query Attention, GQA）</a></li>
  <li><a href="#旋转位置编码rope" id="toc-旋转位置编码rope" class="nav-link" data-scroll-target="#旋转位置编码rope"><span class="header-section-number">5.2</span> 5.2 旋转位置编码（RoPE）</a></li>
  <li><a href="#多阶段训练流程" id="toc-多阶段训练流程" class="nav-link" data-scroll-target="#多阶段训练流程"><span class="header-section-number">5.3</span> 5.3 多阶段训练流程</a></li>
  </ul></li>
  <li><a href="#设计动机为什么需要这么复杂的架构" id="toc-设计动机为什么需要这么复杂的架构" class="nav-link" data-scroll-target="#设计动机为什么需要这么复杂的架构"><span class="header-section-number">6</span> 6. 设计动机：为什么需要这么复杂的架构？</a>
  <ul class="collapse">
  <li><a href="#认知科学视角双系统理论" id="toc-认知科学视角双系统理论" class="nav-link" data-scroll-target="#认知科学视角双系统理论"><span class="header-section-number">6.1</span> 6.1 认知科学视角：双系统理论</a></li>
  <li><a href="#学习理论视角突破监督学习的天花板" id="toc-学习理论视角突破监督学习的天花板" class="nav-link" data-scroll-target="#学习理论视角突破监督学习的天花板"><span class="header-section-number">6.2</span> 6.2 学习理论视角：突破监督学习的天花板</a></li>
  <li><a href="#可解释性与可信度高风险应用的需求" id="toc-可解释性与可信度高风险应用的需求" class="nav-link" data-scroll-target="#可解释性与可信度高风险应用的需求"><span class="header-section-number">6.3</span> 6.3 可解释性与可信度：高风险应用的需求</a></li>
  <li><a href="#效率与可扩展性分层部署策略" id="toc-效率与可扩展性分层部署策略" class="nav-link" data-scroll-target="#效率与可扩展性分层部署策略"><span class="header-section-number">6.4</span> 6.4 效率与可扩展性：分层部署策略</a></li>
  <li><a href="#泛化能力组合推理的涌现" id="toc-泛化能力组合推理的涌现" class="nav-link" data-scroll-target="#泛化能力组合推理的涌现"><span class="header-section-number">6.5</span> 6.5 泛化能力：组合推理的涌现</a></li>
  <li><a href="#设计哲学总结" id="toc-设计哲学总结" class="nav-link" data-scroll-target="#设计哲学总结"><span class="header-section-number">6.6</span> 6.6 设计哲学总结</a></li>
  </ul></li>
  <li><a href="#实验结果与深度分析" id="toc-实验结果与深度分析" class="nav-link" data-scroll-target="#实验结果与深度分析"><span class="header-section-number">7</span> 7. 实验结果与深度分析</a>
  <ul class="collapse">
  <li><a href="#主要benchmark结果" id="toc-主要benchmark结果" class="nav-link" data-scroll-target="#主要benchmark结果"><span class="header-section-number">7.1</span> 7.1 主要Benchmark结果</a></li>
  <li><a href="#局限性与失败案例分析" id="toc-局限性与失败案例分析" class="nav-link" data-scroll-target="#局限性与失败案例分析"><span class="header-section-number">7.2</span> 7.3 局限性与失败案例分析</a></li>
  <li><a href="#与人类专家的对比" id="toc-与人类专家的对比" class="nav-link" data-scroll-target="#与人类专家的对比"><span class="header-section-number">7.3</span> 7.4 与人类专家的对比</a></li>
  <li><a href="#实际应用场景的表现" id="toc-实际应用场景的表现" class="nav-link" data-scroll-target="#实际应用场景的表现"><span class="header-section-number">7.4</span> 7.5 实际应用场景的表现</a></li>
  <li><a href="#局限性总结" id="toc-局限性总结" class="nav-link" data-scroll-target="#局限性总结"><span class="header-section-number">7.5</span> 7.6 局限性总结</a></li>
  </ul></li>
  <li><a href="#总结与展望ai推理的下一个十年" id="toc-总结与展望ai推理的下一个十年" class="nav-link" data-scroll-target="#总结与展望ai推理的下一个十年"><span class="header-section-number">8</span> 8. 总结与展望：AI推理的下一个十年</a>
  <ul class="collapse">
  <li><a href="#核心创新的系统性回顾" id="toc-核心创新的系统性回顾" class="nav-link" data-scroll-target="#核心创新的系统性回顾"><span class="header-section-number">8.1</span> 8.1 核心创新的系统性回顾</a></li>
  <li><a href="#理论贡献与科学意义" id="toc-理论贡献与科学意义" class="nav-link" data-scroll-target="#理论贡献与科学意义"><span class="header-section-number">8.2</span> 8.2 理论贡献与科学意义</a></li>
  <li><a href="#实践意义与应用前景" id="toc-实践意义与应用前景" class="nav-link" data-scroll-target="#实践意义与应用前景"><span class="header-section-number">8.3</span> 8.3 实践意义与应用前景</a></li>
  <li><a href="#未来研究方向" id="toc-未来研究方向" class="nav-link" data-scroll-target="#未来研究方向"><span class="header-section-number">8.4</span> 8.4 未来研究方向</a></li>
  <li><a href="#哲学思考ai是否能真正理解" id="toc-哲学思考ai是否能真正理解" class="nav-link" data-scroll-target="#哲学思考ai是否能真正理解"><span class="header-section-number">8.5</span> 8.5 哲学思考：AI是否能真正”理解”？</a></li>
  <li><a href="#最终的思考" id="toc-最终的思考" class="nav-link" data-scroll-target="#最终的思考"><span class="header-section-number">8.6</span> 8.6 最终的思考</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DeepSeek-R1：推理增强的大语言模型（codex 版）</h1>
  <div class="quarto-categories">
    <div class="quarto-category">深度学习</div>
    <div class="quarto-category">大语言模型</div>
    <div class="quarto-category">强化学习</div>
    <div class="quarto-category">推理</div>
  </div>
  </div>

<div>
  <div class="description">
    在尽量精简背景的前提下，系统讲清 DeepSeek-R1：它如何从普通大模型出发，通过推理轨迹、奖励模型和强化学习，把「会认真思考」变成一个可训练的工程流程。
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 20, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="引言ai-推理能力的新纪元" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="引言ai-推理能力的新纪元"><span class="header-section-number">1</span> 引言：AI 推理能力的新纪元</h2>
<p>2025 年 1 月，DeepSeek 团队发布了 DeepSeek-R1，在大语言模型的推理能力上实现了一个显著的台阶提升。<br>
它不只是「参数更大、数据更多」的常规升级，而是<strong>刻意地为推理设计了一条训练流水线</strong>：</p>
<ul>
<li>先让模型写出一整条「思考过程」（推理轨迹），<br>
</li>
<li>用奖励模型给这些过程打分，<br>
</li>
<li>再用强化学习去鼓励那些「既对又写得好的推理路径」。</li>
</ul>
<p>这篇 codex 版文章的目标是：</p>
<ul>
<li>背景部分<strong>只保留直觉 + 关键公式</strong>，不变成一本通用教科书；</li>
<li>尽量在每个概念后面都加一句：<strong>“这在 DeepSeek-R1 里具体对应什么？”</strong>；</li>
<li>让有工程经验的读者，能直接把文中的符号和 PyTorch 里的 <code>tensor</code> / 训练代码对上。</li>
</ul>
<section id="本文的阅读路线" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="本文的阅读路线"><span class="header-section-number">1.1</span> 本文的阅读路线</h3>
<ul>
<li>如果你已经熟悉语言模型训练（MLE / 交叉熵）、基本强化学习（MDP / 策略梯度）和 Transformer 注意力，可以<strong>直接从第 3 章开始</strong>，从 DeepSeek-R1 的训练流水线往后看。</li>
<li>如果你想顺便系统梳理这些背景，可以按顺序阅读第 2 章；其中的公式会控制在<strong>直观 + 关键结果</strong>，不做冗长推导。</li>
</ul>
<hr>
</section>
</section>
<section id="最小背景语言模型强化学习与注意力" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="最小背景语言模型强化学习与注意力"><span class="header-section-number">2</span> 最小背景：语言模型、强化学习与注意力</h2>
<p>在看 DeepSeek-R1 的细节前，我们先快速统一三类概念：</p>
<ol type="1">
<li>语言模型的训练目标：为什么几乎所有大模型都在最小化交叉熵 / 负对数似然？</li>
<li>强化学习的基本框架：MDP、策略梯度，以及它们如何用在「训练会思考的模型」上。</li>
<li>注意力机制：Transformer 的核心算子，后面 GQA / RoPE 等优化的出发点。</li>
</ol>
<blockquote class="blockquote">
<p>如果你对这些内容已经比较熟，可以略读本章，只关注每节末尾与 R1 的<strong>对应关系</strong>。</p>
</blockquote>
<section id="语言模型与训练目标从概率到交叉熵" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="语言模型与训练目标从概率到交叉熵"><span class="header-section-number">2.1</span> 语言模型与训练目标：从概率到交叉熵</h3>
<section id="自回归语言模型的视角" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="自回归语言模型的视角"><span class="header-section-number">2.1.1</span> 自回归语言模型的视角</h4>
<p>一个自回归语言模型的核心假设是：一句话的概率可以被分解成「逐 token 的条件概率」：</p>
<p><span class="math display">\[
p_\theta(\mathbf{x}) = \prod_{t=1}^T p_\theta(x_t \mid x_{&lt;t})
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_T)\)</span>：一整句 token 序列；</li>
<li><span class="math inline">\(x_{&lt;t}\)</span>：当前位置之前的所有 token；</li>
<li><span class="math inline">\(\theta\)</span>：模型参数（比如 Transformer 的权重）。</li>
</ul>
<p>模型要学的，就是在看到前缀 <span class="math inline">\(x_{&lt;t}\)</span> 后，给出一个合理的下一个 token 分布。</p>
</section>
<section id="最大似然与交叉熵损失" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="最大似然与交叉熵损失"><span class="header-section-number">2.1.2</span> 最大似然与交叉熵损失</h4>
<p>训练时，我们有一个由真实文本构成的数据集：</p>
<p><span class="math display">\[
\mathcal{D} = \{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)}\}
\]</span></p>
<p>最自然的目标是：让模型在这些样本上给出尽可能高的概率，也就是<strong>最大化似然</strong>，等价于最小化负对数似然：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{NLL}}(\theta)
= - \frac{1}{N} \sum_{i=1}^N \log p_\theta(\mathbf{x}^{(i)})
\]</span></p>
<p>把序列展开到 token 级别：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{NLL}}(\theta)
= - \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T_i}
   \log p_\theta(x_t^{(i)} \mid x_{&lt;t}^{(i)})
\]</span></p>
<p>在实现层面，这就是大家熟悉的<strong>交叉熵损失</strong>：对每个位置，拿真实 token 的 one-hot 分布与模型给出的 softmax 分布做交叉熵，再在 batch 和时间维度上取平均。</p>
<p>从 PyTorch 的角度看，一条前向大致是：</p>
<ul>
<li>输入：<code>X</code>，shape 约为 <code>B × T</code>（token id）；</li>
<li>输出 logits：<code>Z</code>，shape 为 <code>B × T × V</code>（词表大小为 <code>V</code>）；</li>
<li>用 <code>F.cross_entropy(Z.view(-1, V), target.view(-1))</code> 求交叉熵，对应上面的公式。</li>
</ul>
</section>
<section id="这在-deepseek-r1-里对应什么" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="这在-deepseek-r1-里对应什么"><span class="header-section-number">2.1.3</span> 这在 DeepSeek-R1 里对应什么？</h4>
<ul>
<li>预训练、监督微调（SFT）、以及奖励模型（RM）的训练，<strong>底层都在最小化交叉熵</strong>，只是输入 / 输出的含义不同。</li>
<li>换句话说，DeepSeek-R1 虽然在推理能力上加入了 RL 和过程奖励，但它的语言建模部分，仍然是一个标准的「交叉熵大模型」，可以直接复用成熟的训练基础设施。</li>
</ul>
<hr>
</section>
</section>
<section id="强化学习基础从反馈中学习推理策略" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="强化学习基础从反馈中学习推理策略"><span class="header-section-number">2.2</span> 强化学习基础：从反馈中学习推理策略</h3>
<p>DeepSeek-R1 的第二个基石是强化学习。它不再只关心「给定输入，输出什么」，而是关心「整条推理轨迹是否高质量」，并用<strong>奖励</strong>鼓励好的推理方式。</p>
<section id="mdp把推理看成决策过程" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="mdp把推理看成决策过程"><span class="header-section-number">2.2.1</span> MDP：把推理看成决策过程</h4>
<p>标准强化学习把问题抽象为马尔可夫决策过程（MDP）：</p>
<ul>
<li>状态 <span class="math inline">\(s_t\)</span>：当前所处的情景；</li>
<li>动作 <span class="math inline">\(a_t\)</span>：在该情景下采取的决策；</li>
<li>策略 <span class="math inline">\(\pi_\theta(a_t \mid s_t)\)</span>：在状态 <span class="math inline">\(s_t\)</span> 下，对动作的概率分布；</li>
<li>轨迹 <span class="math inline">\(\tau = (s_0, a_0, \ldots, s_T, a_T)\)</span>：从开始到结束的一整条决策序列；</li>
<li>回报 <span class="math inline">\(R(\tau)\)</span>：对整条轨迹质量的打分（越大越好）。</li>
</ul>
<p>强化学习的目标是最大化期望回报：</p>
<p><span class="math display">\[
J(\theta) =
\mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)] \,.
\]</span></p>
</section>
<section id="策略梯度的关键形式" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="策略梯度的关键形式"><span class="header-section-number">2.2.2</span> 策略梯度的关键形式</h4>
<p>策略梯度（Policy Gradient）提供了一类直接优化策略的方式。一个典型的形式是：</p>
<p><span class="math display">\[
\nabla_\theta J(\theta)
\approx
\mathbb{E}_{\tau}
\big[
  A(\tau)\,
  \nabla_\theta \log \pi_\theta(\tau)
\big]
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(\pi_\theta(\tau)\)</span> 是整条轨迹的概率；</li>
<li><span class="math inline">\(A(\tau)\)</span> 是优势（advantage），衡量这条轨迹比平均水平好多少。</li>
</ul>
<p>像 PPO / GRPO 这类算法，主要是在这个基本形式上加入了<strong>裁剪 / 重要性采样 / 值函数正则</strong>等工程细节，以保证更新稳定、样本利用率高。</p>
</section>
<section id="这在-deepseek-r1-里对应什么-1" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="这在-deepseek-r1-里对应什么-1"><span class="header-section-number">2.2.3</span> 这在 DeepSeek-R1 里对应什么？</h4>
<p>在 DeepSeek-R1 中，MDP 的各个元素可以具体化为：</p>
<ul>
<li><strong>状态 <span class="math inline">\(s_t\)</span></strong>：题目 + 已生成的推理前缀（包括 <code>&lt;思考&gt;</code> 段落中的部分内容）；</li>
<li><strong>动作 <span class="math inline">\(a_t\)</span></strong>：在第 <span class="math inline">\(t\)</span> 步生成的下一个 token；</li>
<li><strong>策略 <span class="math inline">\(\pi_\theta\)</span></strong>：当前的语言模型本身（R1 的 policy）；</li>
<li><strong>回报 <span class="math inline">\(R(\tau)\)</span></strong>：由「答案是否正确 + 推理过程质量（过程奖励）+ 长度惩罚」组合而成；</li>
<li>强化学习阶段就是在这个 MDP 上，用 PPO / GRPO 等方法，不断更新策略参数 <span class="math inline">\(\theta\)</span>。</li>
</ul>
<p>直观上，R1 的 RL 阶段就是：</p>
<blockquote class="blockquote">
<p>一遍遍让模型写出完整的推理过程，<br>
用奖励模型 + 规则给每条过程打分，<br>
然后鼓励模型多走那些「高分推理路径」。</p>
</blockquote>
<hr>
</section>
</section>
<section id="注意力机制transformer-的核心算子" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="注意力机制transformer-的核心算子"><span class="header-section-number">2.3</span> 注意力机制：Transformer 的核心算子</h3>
<p>最后，我们快速回顾注意力机制，它是所有后续架构优化的基础。</p>
<section id="缩放点积注意力" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="缩放点积注意力"><span class="header-section-number">2.3.1</span> 缩放点积注意力</h4>
<p>对于一个长度为 <span class="math inline">\(L\)</span> 的序列，Transformer 会把每个位置映射到三个向量：</p>
<ul>
<li>Query：<span class="math inline">\(\mathbf{Q} \in \mathbb{R}^{L \times d_k}\)</span>；</li>
<li>Key：<span class="math inline">\(\mathbf{K} \in \mathbb{R}^{L \times d_k}\)</span>；</li>
<li>Value：<span class="math inline">\(\mathbf{V} \in \mathbb{R}^{L \times d_v}\)</span>。</li>
</ul>
<p>标准的缩放点积注意力为：</p>
<p><span class="math display">\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
= \text{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}
\]</span></p>
<p>直觉是：</p>
<ul>
<li><span class="math inline">\(\mathbf{Q}\mathbf{K}^\top\)</span> 计算所有 token 之间的相关性打分；</li>
<li>softmax 把打分变成注意力分布；</li>
<li>与 <span class="math inline">\(\mathbf{V}\)</span> 相乘，相当于从所有位置「加权汇总」信息。</li>
</ul>
<p>多头注意力（Multi-Head Attention）则是用多个不同的线性投影得到多组 <span class="math inline">\((\mathbf{Q}, \mathbf{K}, \mathbf{V})\)</span>，分别算注意力后再拼接，帮助模型同时关注不同类型的模式。</p>
</section>
<section id="这在-deepseek-r1-里对应什么-2" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="这在-deepseek-r1-里对应什么-2"><span class="header-section-number">2.3.2</span> 这在 DeepSeek-R1 里对应什么？</h4>
<ul>
<li>DeepSeek-R1 仍然基于 Transformer 架构，注意力是最核心的计算单元。</li>
<li>在后面的实现细节中（尤其是架构部分），我们会看到它如何在此基础上做进一步优化，比如<strong>分组查询注意力（GQA）</strong>、<strong>旋转位置编码（RoPE）</strong> 和多阶段训练 pipeline，以支撑更长、更复杂的推理过程。</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="传统大模型的局限为什么需要-deepseek-r1" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="传统大模型的局限为什么需要-deepseek-r1"><span class="header-section-number">3</span> 传统大模型的局限：为什么需要 DeepSeek-R1？</h2>
<p>有了第 2 章的基本坐标系，我们可以更清楚地说明：<strong>为什么仅靠「一次性生成答案 + 监督学习」很难获得强推理能力</strong>，以及 DeepSeek-R1 具体要解决哪些问题。</p>
<section id="一次性生成的困境信息瓶颈" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="一次性生成的困境信息瓶颈"><span class="header-section-number">3.1</span> 一次性生成的困境：信息瓶颈</h3>
<p>传统大模型（如 GPT-3、LLaMA）在推理任务上的典型工作方式是：</p>
<ol type="1">
<li>读入题目（作为 token 序列）；<br>
</li>
<li>通过几十层 Transformer 进行一次前向计算；<br>
</li>
<li>直接在最后一层 hidden state 上用线性层 + softmax 输出答案 token。</li>
</ol>
<p>这相当于要求模型在<strong>一次前向传播</strong>中，把所有推理步骤都「挤」进最后一个 hidden state 里，然后用一个线性层把它映射成答案。这会带来几个问题：</p>
<ul>
<li><strong>信息瓶颈</strong>：复杂推理过程包含很多中间状态和分支，全部压缩到一个有限维度的向量里容易丢失信息；</li>
<li><strong>错误难以定位</strong>：一旦答案错了，很难知道是「理解错误」还是「中间推导哪一步出了问题」；</li>
<li><strong>不利于搜索</strong>：模型缺少显式的中间推理节点，难以做「多路径探索 + 打分」。</li>
</ul>
<p>DeepSeek-R1 的第一步改动，就是引入<strong>显式的推理轨迹</strong>：让模型先写出 <code>&lt;思考&gt;</code> 段落，再给出最终答案，从而突破这种「一次性压缩」的瓶颈。</p>
</section>
<section id="缺乏可见的推理过程黑箱问题" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="缺乏可见的推理过程黑箱问题"><span class="header-section-number">3.2</span> 缺乏可见的推理过程：黑箱问题</h3>
<p>人类在解题时，通常会经历一个明确的思考过程：</p>
<ol type="1">
<li><strong>理解题目</strong>：提取关键信息；<br>
</li>
<li><strong>调用知识</strong>：想起相关公式或定理；<br>
</li>
<li><strong>多步推导</strong>：一步步变形、计算；<br>
</li>
<li><strong>得到答案并检查</strong>。</li>
</ol>
<p>传统语言模型在训练时，只被要求「从题目到答案」，而不是「从题目到推理过程再到答案」。这会导致：</p>
<ul>
<li>可解释性差：我们不知道模型是<strong>真的理解了规律</strong>，还是只是<strong>模式匹配</strong>；</li>
<li>泛化性受限：一旦题目形式有变化（比如多加条件、换个表述），模型就可能失效。</li>
</ul>
<p>DeepSeek-R1 引入的思维链（Chain-of-Thought）输出，让模型在生成答案前，先生成一段详细的<strong>推理文本</strong>，这样：</p>
<ul>
<li>人类可以审查、修改这些中间步骤；<br>
</li>
<li>奖励模型可以<strong>对过程本身打分</strong>，而不仅仅是看最后一行数字；<br>
</li>
<li>在 RL 阶段，可以优先保留那些「过程清晰、答案正确」的轨迹，逐步强化模型的推理习惯。</li>
</ul>
</section>
<section id="监督学习的瓶颈数据与组合爆炸" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="监督学习的瓶颈数据与组合爆炸"><span class="header-section-number">3.3</span> 监督学习的瓶颈：数据与组合爆炸</h3>
<p>传统 SFT（Supervised Fine-Tuning）的训练数据通常形如：</p>
<p><span class="math display">\[
\mathcal{D}_{\text{SFT}} = \{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{x}_i\)</span> 是输入（题目），<span class="math inline">\(\mathbf{y}_i\)</span> 是目标输出（答案）。训练目标是最小化：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{SFT}}(\theta)
= -\frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{|\mathbf{y}_i|}
  \log p_\theta(y_{i,t} \mid \mathbf{x}_i, y_{i,&lt;t}) \,.
\]</span></p>
<p>这种训练方式有两个根本限制：</p>
<ol type="1">
<li><strong>数据覆盖有限</strong>：你不可能为所有难题都准备标注答案，更不可能为所有题目都标注完整的推理过程；</li>
<li><strong>组合爆炸</strong>：多步推理涉及大量中间状态组合，仅靠「输入-输出对」很难覆盖到足够多的推理路径。</li>
</ol>
<p>结果就是：即使模型在训练集分布附近表现很好，一旦问题稍作变化，它就可能因为没有真正学会「推理套路」而失败。</p>
<p>DeepSeek-R1 的思路是：</p>
<ul>
<li>用少量高质量的思维链标注 + 大量自动生成的推理轨迹，配合奖励模型，把「好的推理过程」学习为一种<strong>策略</strong>；<br>
</li>
<li>再用强化学习，超越固定训练数据集，让模型在推理空间里主动<strong>探索新的解题路径</strong>。</li>
</ul>
<hr>
<p>到这里，我们已经解释了三件事：</p>
<ol type="1">
<li>传统自回归语言模型是如何训练的；<br>
</li>
<li>强化学习如何把「写推理过程」变成一个可以优化的策略问题；<br>
</li>
<li>为什么「一次性生成答案 + 纯监督学习」难以支撑强推理能力。</li>
</ol>
<p>接下来，如果你想继续往下，建议把注意力放在：<strong>DeepSeek-R1 到底是怎么把「推理轨迹 + 奖励模型 + RL」组合起来的？</strong><br>
这会在后续章节（可以基于你原来的 deepseek-r1-cn.qmd / 详解版）中展开：包括生成推理轨迹、奖励模型设计、PPO / GRPO 优化、以及 GQA / RoPE 等架构细节。</p>
</section>
</section>
<section id="deepseek-r1-的核心创新" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="deepseek-r1-的核心创新"><span class="header-section-number">4</span> DeepSeek-R1 的核心创新</h2>
<p>理解了传统模型的局限后，我们现在可以深入探讨 DeepSeek-R1 是如何通过一系列巧妙的创新来突破这些困境的。这些创新不是孤立的技术点，而是相互配合、层层递进的完整系统。</p>
<section id="思维链推理让思考过程可见" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="思维链推理让思考过程可见"><span class="header-section-number">4.1</span> 4.1 思维链推理：让思考过程可见</h3>
<section id="核心思想" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="核心思想"><span class="header-section-number">4.1.1</span> 核心思想</h4>
<p>DeepSeek-R1 的第一个关键创新是<strong>让模型学会像人类一样”思考”</strong>——在给出最终答案之前，先生成一个详细的、可检查的推理过程。</p>
<p>这个想法看似简单，但其背后的数学建模却并不trivial。让我们从形式化定义开始。</p>
</section>
<section id="数学建模从直接输出到两阶段生成" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="数学建模从直接输出到两阶段生成"><span class="header-section-number">4.1.2</span> 数学建模：从直接输出到两阶段生成</h4>
<p>传统模型的生成过程是：</p>
<p><span class="math display">\[
p_\theta(y \mid x) = \prod_{t=1}^{T_y} p_\theta(y_t \mid x, y_{&lt;t})
\]</span></p>
<p>其中： - <span class="math inline">\(x\)</span>：输入问题（例如：“正方形对角线长度是10，求面积”） - <span class="math inline">\(y\)</span>：直接答案（例如：“50”） - <span class="math inline">\(T_y\)</span>：答案的长度（可能很短，只有几个token）</p>
<p>DeepSeek-R1 引入了一个<strong>中间推理链</strong> <span class="math inline">\(c\)</span>（chain-of-thought），将生成过程变为两阶段：</p>
<p><span class="math display">\[
p_\theta(c, y \mid x) = \underbrace{p_\theta(c \mid x)}_{\text{生成推理链}} \cdot \underbrace{p_\theta(y \mid c, x)}_{\text{基于推理得出答案}}
\]</span></p>
<p>让我们详细分解这个公式：</p>
<p><strong>第一阶段：生成推理链 <span class="math inline">\(c\)</span></strong></p>
<p><span class="math display">\[
p_\theta(c \mid x) = \prod_{t=1}^{T_c} p_\theta(c_t \mid x, c_{&lt;t})
\]</span></p>
<p>其中： - <span class="math inline">\(c = (c_1, c_2, \ldots, c_{T_c})\)</span>：推理链，是一个token序列 - <span class="math inline">\(T_c\)</span>：推理链的长度（通常比答案长得多，可能有几百个token） - <span class="math inline">\(c_t\)</span>：推理链中第 <span class="math inline">\(t\)</span> 个token</p>
<p><strong>第二阶段：生成最终答案 <span class="math inline">\(y\)</span></strong></p>
<p><span class="math display">\[
p_\theta(y \mid c, x) = \prod_{t=1}^{T_y} p_\theta(y_t \mid x, c, y_{&lt;t})
\]</span></p>
<p>这个两阶段设计的好处是： 1. <strong>显式推理</strong>：推理过程 <span class="math inline">\(c\)</span> 是可见的、可检查的 2. <strong>更长的”思考时间”</strong>：模型可以用 <span class="math inline">\(T_c\)</span> 步来处理复杂推理，而不是一步到位 3. <strong>中间结果可用</strong>：推理链 <span class="math inline">\(c\)</span> 中的中间结果可以被后续步骤利用</p>
</section>
<section id="具体例子从问题到答案" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="具体例子从问题到答案"><span class="header-section-number">4.1.3</span> 具体例子：从问题到答案</h4>
<p>让我们用刚才的几何问题来说明。DeepSeek-R1 的完整输出会是这样的：</p>
<p><strong>输入 <span class="math inline">\(x\)</span></strong>：</p>
<pre><code>正方形对角线长度是10，求面积。</code></pre>
<p><strong>推理链 <span class="math inline">\(c\)</span></strong>（约150 tokens）：</p>
<pre><code>&lt;思考&gt;
这是一个关于正方形的几何问题。让我分步解决：

第一步：理解已知条件
- 图形是正方形
- 对角线长度 d = 10

第二步：回忆相关公式
正方形的对角线与边长的关系：
设边长为 a，对角线为 d
根据勾股定理：d² = a² + a² = 2a²
因此：d = a√2 或 a = d/√2

第三步：代入数值计算边长
a = 10/√2
  = 10/√2 · √2/√2
  = 10√2/2
  = 5√2
  ≈ 7.071

第四步：计算面积
A = a²
  = (5√2)²
  = 25 × 2
  = 50

第五步：验证
如果 a = 5√2，则 d = a√2 = 5√2 × √2 = 5 × 2 = 10 ✓
&lt;/思考&gt;</code></pre>
<p><strong>最终答案 <span class="math inline">\(y\)</span></strong>（约5 tokens）：</p>
<pre><code>答案是 50 平方单位。</code></pre>
<p>注意到推理链 <span class="math inline">\(c\)</span> 比最终答案 <span class="math inline">\(y\)</span> 长得多。这就是”思考时间”的体现。</p>
</section>
<section id="思维链带来的三大优势" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="思维链带来的三大优势"><span class="header-section-number">4.1.4</span> 思维链带来的三大优势</h4>
<p><strong>优势1：中间步骤可检查</strong></p>
<p>由于推理过程是显式的，我们可以验证每一步的正确性。假设模型在某一步出错：</p>
<pre><code>第三步：代入数值计算边长
a = 10/√2
  = 10/2  ← 错误！忘记了分母的√2
  = 5</code></pre>
<p>我们可以立即发现这个错误发生在第三步，而不是像黑箱模型那样只看到错误的最终答案”25”。</p>
<p>从数学上，这意味着我们可以对推理链的每一步进行验证：</p>
<p><span class="math display">\[
\text{Correct}(c) = \bigwedge_{t=1}^{T_c} \text{Valid}(c_t \mid c_{&lt;t}, x)
\]</span></p>
<p>其中 <span class="math inline">\(\text{Valid}(\cdot)\)</span> 是一个验证函数，检查步骤 <span class="math inline">\(c_t\)</span> 在给定前文的情况下是否逻辑正确。</p>
<p><strong>优势2：推理可泛化</strong></p>
<p>模型学习的不再是从特定问题到特定答案的映射，而是学习<strong>通用的推理模式</strong>。</p>
<p>例如，模型可能学会： - 推理模式1：“遇到几何问题 → 画图 → 标注已知量 → 寻找公式 → 代入计算” - 推理模式2：“遇到代数问题 → 设未知数 → 列方程 → 求解 → 验证”</p>
<p>这些模式可以<strong>组合和迁移</strong>到新问题上。数学上，我们希望模型学习的是：</p>
<p><span class="math display">\[
f_\theta(x) = g_K \circ g_{K-1} \circ \cdots \circ g_1 (x)
\]</span></p>
<p>其中每个 <span class="math inline">\(g_i\)</span> 是一个可复用的推理步骤（如”应用勾股定理”、“求解二次方程”等）。</p>
<p><strong>优势3：自我纠错能力</strong></p>
<p>在生成推理链的过程中，模型可以”回头检查”之前的步骤，发现并修正错误。例如：</p>
<pre><code>第三步：代入数值
a = 10/√2 = 5

等等，这样不对。让我重新算：
a = 10/√2
  = 10/√2 · √2/√2
  = 10√2/2
  = 5√2

对，现在正确了。</code></pre>
<p>这种自我纠错在传统的一次性生成中是不可能的，因为模型没有机会”反思”。</p>
</section>
</section>
<section id="强化学习驱动从试错中学习推理" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="强化学习驱动从试错中学习推理"><span class="header-section-number">4.2</span> 4.2 强化学习驱动：从试错中学习推理</h3>
<p>思维链解决了”如何表示推理”的问题，但随之而来的是另一个挑战：<strong>如何让模型学会生成高质量的推理链？</strong></p>
<section id="监督学习的困境" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="监督学习的困境"><span class="header-section-number">4.2.1</span> 监督学习的困境</h4>
<p>最直接的方法是监督学习：收集大量 <span class="math inline">\((x, c, y)\)</span> 三元组，其中 <span class="math inline">\(c\)</span> 是人工标注的推理链，然后训练模型：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{SFT}} = -\mathbb{E}_{(x, c, y) \sim \mathcal{D}} [\log p_\theta(c, y \mid x)]
\]</span></p>
<p>但这有几个问题：</p>
<ol type="1">
<li><strong>标注成本极高</strong>：一个数学推理样本可能需要 20-30 分钟标注详细推理过程</li>
<li><strong>推理多样性有限</strong>：人类标注者倾向于使用某些常见方法，模型无法探索更优的推理路径</li>
<li><strong>难以覆盖长链推理</strong>：对于需要 50 步以上推理的问题，人工标注几乎不可行</li>
</ol>
<p>DeepSeek-R1 采用了<strong>强化学习</strong>来突破这些限制。</p>
</section>
<section id="将推理建模为mdp" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="将推理建模为mdp"><span class="header-section-number">4.2.2</span> 将推理建模为MDP</h4>
<p>回顾第2.2节介绍的马尔可夫决策过程（MDP）。我们将推理过程精确地映射到MDP框架：</p>
<p><strong>状态 <span class="math inline">\(s_t\)</span>（State）</strong></p>
<p>在时刻 <span class="math inline">\(t\)</span>，状态是”到目前为止生成的所有内容”：</p>
<p><span class="math display">\[
s_t = (x, c_1, c_2, \ldots, c_t)
\]</span></p>
<p>其中： - <span class="math inline">\(x\)</span>：原始问题 - <span class="math inline">\((c_1, \ldots, c_t)\)</span>：已生成的推理链的前 <span class="math inline">\(t\)</span> 个token</p>
<p>状态的维度是动态的：<span class="math inline">\(s_t \in \mathcal{V}^{t+1}\)</span>（<span class="math inline">\(\mathcal{V}\)</span> 是词汇表）。</p>
<p><strong>动作 <span class="math inline">\(a_t\)</span>（Action）</strong></p>
<p>在状态 <span class="math inline">\(s_t\)</span> 下，动作是”选择生成哪个token”：</p>
<p><span class="math display">\[
a_t \in \mathcal{V}
\]</span></p>
<p>即从词汇表中选择一个词作为推理链的下一个token。</p>
<p><strong>转移 <span class="math inline">\(P(s_{t+1} \mid s_t, a_t)\)</span>（Transition）</strong></p>
<p>这个转移是确定性的：</p>
<p><span class="math display">\[
s_{t+1} = s_t \oplus a_t = (x, c_1, \ldots, c_t, a_t)
\]</span></p>
<p>其中 <span class="math inline">\(\oplus\)</span> 表示拼接操作。</p>
<p><strong>奖励 <span class="math inline">\(R(s, a)\)</span>（Reward）</strong></p>
<p>这是强化学习的核心。DeepSeek-R1 使用<strong>稀疏奖励</strong>：大部分时间步奖励为0，只在生成结束时给出奖励。</p>
<p><span class="math display">\[
r_t = \begin{cases}
0 &amp; \text{if } t &lt; T \\
r_{\text{final}} &amp; \text{if } t = T
\end{cases}
\]</span></p>
<p>其中： <span class="math display">\[
r_{\text{final}} = \begin{cases}
+1 &amp; \text{if answer is correct} \\
-1 &amp; \text{if answer is wrong}
\end{cases}
\]</span></p>
<p><strong>策略 <span class="math inline">\(\pi_\theta(a \mid s)\)</span>（Policy）</strong></p>
<p>策略就是语言模型本身：</p>
<p><span class="math display">\[
\pi_\theta(a_t \mid s_t) = p_\theta(a_t \mid x, c_{&lt;t})
\]</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是模型参数。</p>
</section>
<section id="训练目标最大化期望奖励" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="训练目标最大化期望奖励"><span class="header-section-number">4.2.3</span> 训练目标：最大化期望奖励</h4>
<p>我们的目标是找到最优策略 <span class="math inline">\(\pi^*\)</span>，使得期望奖励最大：</p>
<p><span class="math display">\[
\theta^* = \arg\max_\theta J(\theta)
\]</span></p>
<p>其中： <span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
\]</span></p>
<p>展开期望： <span class="math display">\[
J(\theta) = \sum_{\tau} p_\theta(\tau) R(\tau)
\]</span></p>
<p>这里： - <span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)\)</span>：一条完整的轨迹 - <span class="math inline">\(p_\theta(\tau) = \prod_{t=0}^T \pi_\theta(a_t \mid s_t)\)</span>：轨迹的概率 - <span class="math inline">\(R(\tau) = \sum_{t=0}^T \gamma^t r_t = \gamma^T r_{\text{final}}\)</span>：轨迹的总回报（由于只有最后一步有奖励）</p>
</section>
<section id="为什么强化学习有效" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="为什么强化学习有效"><span class="header-section-number">4.2.4</span> 为什么强化学习有效？</h4>
<p>强化学习允许模型<strong>通过试错来发现有效的推理策略</strong>，而不依赖于穷举所有可能的标注样本。</p>
<p><strong>直觉解释</strong>：</p>
<p>想象模型在解决一个数学问题。它可能会尝试多种推理路径：</p>
<p><strong>尝试1</strong>（失败）：</p>
<pre><code>直接猜测答案是25 → 检查发现错误 → 获得奖励 -1</code></pre>
<p><strong>尝试2</strong>（成功）：</p>
<pre><code>应用勾股定理 → 求出边长 → 计算面积50 → 检查正确 → 获得奖励 +1</code></pre>
<p><strong>尝试3</strong>（成功但冗长）：</p>
<pre><code>列出10种不同的几何定理 → 逐一尝试 → 最终用勾股定理 → 答案50 → 获得奖励 +0.5</code></pre>
<p>（由于折扣因子，冗长的推理链会得到较低的奖励）</p>
<p>通过多次尝试，模型会学到： - 应用勾股定理是有效的（尝试2的成功率高） - 直接猜测通常失败（尝试1的成功率低） - 冗长的推理虽然可行但不高效（尝试3的奖励较低）</p>
<p>数学上，策略会逐渐向高奖励的轨迹倾斜：</p>
<p><span class="math display">\[
\pi_{\theta_{t+1}}(a \mid s) \propto \pi_{\theta_t}(a \mid s) \cdot \exp(\alpha \cdot A(s, a))
\]</span></p>
<p>其中 <span class="math inline">\(A(s, a)\)</span> 是优势函数，表示动作 <span class="math inline">\(a\)</span> 比平均好多少。</p>
</section>
</section>
<section id="ppo算法稳定的策略优化" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="ppo算法稳定的策略优化"><span class="header-section-number">4.3</span> 4.3 PPO算法：稳定的策略优化</h3>
<p>理解了强化学习的基本框架后，一个关键问题是：<strong>如何具体地优化策略 <span class="math inline">\(\pi_\theta\)</span>？</strong>这就是Proximal Policy Optimization (PPO) 算法发挥作用的地方。PPO是DeepSeek-R1训练的核心算法，让我们深入理解它的数学原理。</p>
<section id="策略优化的挑战" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="策略优化的挑战"><span class="header-section-number">4.3.1</span> 策略优化的挑战</h4>
<p>在第2.2节，我们介绍了简单的REINFORCE算法。它的更新规则是：</p>
<p><span class="math display">\[
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
\]</span></p>
<p>其中梯度为：</p>
<p><span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot A_t \right]
\]</span></p>
<p>这里 <span class="math inline">\(A_t\)</span> 是优势函数。</p>
<p>但REINFORCE有两个严重问题：</p>
<p><strong>问题1：样本效率低</strong></p>
<p>每次更新都需要新的采样轨迹 <span class="math inline">\(\tau \sim \pi_\theta\)</span>。一旦参数更新（<span class="math inline">\(\theta \to \theta'\)</span>），之前采样的轨迹就”过期”了，不能再用于下一次更新。</p>
<p>这在大语言模型的场景下尤其昂贵：生成一条完整的推理链可能需要几百步，消耗大量计算。</p>
<p><strong>问题2：不稳定</strong></p>
<p>如果某次更新的步长太大（<span class="math inline">\(\theta\)</span> 变化太多），新策略 <span class="math inline">\(\pi_{\theta'}\)</span> 可能与旧策略 <span class="math inline">\(\pi_\theta\)</span> 差异巨大，导致性能突然崩溃。</p>
<p>数学上，这是因为梯度估计 <span class="math inline">\(\hat{g}\)</span> 只在 <span class="math inline">\(\theta\)</span> 附近是可靠的。当我们移动太远时，<span class="math inline">\(\hat{g}\)</span> 不再指向正确的方向。</p>
</section>
<section id="重要性采样提高样本效率" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="重要性采样提高样本效率"><span class="header-section-number">4.3.2</span> 重要性采样：提高样本效率</h4>
<p>PPO的第一个关键技巧是<strong>重要性采样</strong>（Importance Sampling），它允许我们用旧策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 采样的数据来更新新策略 <span class="math inline">\(\pi_\theta\)</span>。</p>
<p><strong>重要性采样的基本原理</strong></p>
<p>假设我们想计算期望 <span class="math inline">\(\mathbb{E}_{x \sim p}[f(x)]\)</span>，但只能从分布 <span class="math inline">\(q\)</span> 采样。重要性采样告诉我们：</p>
<p><span class="math display">\[
\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right]
\]</span></p>
<p>证明很简单： <span class="math display">\[
\mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right] = \int q(x) \cdot \frac{p(x)}{q(x)} f(x) dx = \int p(x) f(x) dx = \mathbb{E}_{x \sim p}[f(x)]
\]</span></p>
<p><strong>应用到策略优化</strong></p>
<p>我们想优化： <span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
\]</span></p>
<p>但只有从 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 采样的轨迹。利用重要性采样：</p>
<p><span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}} \left[\frac{p_\theta(\tau)}{p_{\theta_{\text{old}}}(\tau)} R(\tau)\right]
\]</span></p>
<p>轨迹的概率比可以分解：</p>
<p><span class="math display">\[
\frac{p_\theta(\tau)}{p_{\theta_{\text{old}}}(\tau)} = \frac{\prod_{t=0}^T \pi_\theta(a_t \mid s_t)}{\prod_{t=0}^T \pi_{\theta_{\text{old}}}(a_t \mid s_t)} = \prod_{t=0}^T \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
\]</span></p>
<p>定义<strong>概率比</strong> <span class="math inline">\(r_t(\theta)\)</span>：</p>
<p><span class="math display">\[
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
\]</span></p>
<p>这个比率告诉我们：在新策略下，动作 <span class="math inline">\(a_t\)</span> 的概率相对于旧策略变化了多少倍。</p>
<ul>
<li>如果 <span class="math inline">\(r_t(\theta) &gt; 1\)</span>：新策略更倾向于选择 <span class="math inline">\(a_t\)</span></li>
<li>如果 <span class="math inline">\(r_t(\theta) &lt; 1\)</span>：新策略更不倾向于选择 <span class="math inline">\(a_t\)</span></li>
<li>如果 <span class="math inline">\(r_t(\theta) = 1\)</span>：新旧策略对 <span class="math inline">\(a_t\)</span> 的偏好相同</li>
</ul>
</section>
<section id="替代目标函数" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="替代目标函数"><span class="header-section-number">4.3.3</span> 替代目标函数</h4>
<p>利用重要性采样，我们可以定义一个<strong>替代目标</strong>（surrogate objective）：</p>
<p><span class="math display">\[
L^{\text{CPI}}(\theta) = \mathbb{E}_{t} \left[ r_t(\theta) \hat{A}_t \right]
\]</span></p>
<p>其中： - CPI stands for “Conservative Policy Iteration” - <span class="math inline">\(\hat{A}_t\)</span> 是优势函数 <span class="math inline">\(A(s_t, a_t)\)</span> 的估计值 - 期望 <span class="math inline">\(\mathbb{E}_t\)</span> 是对所有采样的 <span class="math inline">\((s_t, a_t)\)</span> 求平均</p>
<p>让我们理解这个公式的含义：</p>
<p><strong>当 <span class="math inline">\(\hat{A}_t &gt; 0\)</span> （好动作）</strong>： - 如果 <span class="math inline">\(r_t(\theta) &gt; 1\)</span>（新策略增加了这个动作的概率）→ 贡献正值 → 好！ - 如果 <span class="math inline">\(r_t(\theta) &lt; 1\)</span>（新策略减少了这个动作的概率）→ 贡献负值 → 不好</p>
<p><strong>当 <span class="math inline">\(\hat{A}_t &lt; 0\)</span> （坏动作）</strong>： - 如果 <span class="math inline">\(r_t(\theta) &lt; 1\)</span>（新策略减少了这个动作的概率）→ 贡献正值 → 好！ - 如果 <span class="math inline">\(r_t(\theta) &gt; 1\)</span>（新策略增加了这个动作的概率）→ 贡献负值 → 不好</p>
<p>所以最大化 <span class="math inline">\(L^{\text{CPI}}\)</span> 会增加好动作的概率，减少坏动作的概率。</p>
<p><strong>但这还不够！</strong> 如果不加限制地优化 <span class="math inline">\(L^{\text{CPI}}\)</span>，<span class="math inline">\(r_t(\theta)\)</span> 可能变得非常大或非常小，导致策略变化过大。</p>
</section>
<section id="裁剪机制保持稳定" class="level4" data-number="4.3.4">
<h4 data-number="4.3.4" class="anchored" data-anchor-id="裁剪机制保持稳定"><span class="header-section-number">4.3.4</span> 裁剪机制：保持稳定</h4>
<p>PPO的核心创新是<strong>裁剪</strong>（clipping）机制，它防止策略更新幅度过大。</p>
<p>定义裁剪后的概率比：</p>
<p><span class="math display">\[
\text{clip}(r_t, 1-\epsilon, 1+\epsilon) = \begin{cases}
1 - \epsilon &amp; \text{if } r_t &lt; 1-\epsilon \\
r_t &amp; \text{if } 1-\epsilon \leq r_t \leq 1+\epsilon \\
1 + \epsilon &amp; \text{if } r_t &gt; 1+\epsilon
\end{cases}
\]</span></p>
<p>其中 <span class="math inline">\(\epsilon\)</span> 是超参数（通常取 <span class="math inline">\(\epsilon = 0.2\)</span>）。</p>
<p>这个函数的作用是： - 如果 <span class="math inline">\(r_t\)</span> 偏离1不太远（在 <span class="math inline">\([1-\epsilon, 1+\epsilon]\)</span> 范围内），保持原值 - 如果 <span class="math inline">\(r_t\)</span> 偏离1太远，强制拉回到边界</p>
<p>PPO的目标函数是：</p>
<p><span class="math display">\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right) \right]
\]</span></p>
<p>让我们仔细分析这个 <span class="math inline">\(\min\)</span> 操作在不同情况下的行为：</p>
<p><strong>情况1：优势为正 (<span class="math inline">\(\hat{A}_t &gt; 0\)</span>)，这是一个好动作</strong></p>
<ul>
<li><p>如果 <span class="math inline">\(r_t &gt; 1+\epsilon\)</span>（新策略大幅增加了这个动作的概率）： <span class="math display">\[
\begin{align}
&amp;\text{第一项：} r_t \hat{A}_t &gt; (1+\epsilon) \hat{A}_t \\
&amp;\text{第二项：} (1+\epsilon) \hat{A}_t \\
&amp;\text{取}\min\text{：} (1+\epsilon) \hat{A}_t
\end{align}
\]</span> 裁剪生效！不允许过度增加概率。</p></li>
<li><p>如果 <span class="math inline">\(1-\epsilon &lt; r_t \leq 1+\epsilon\)</span>（适度增加）： <span class="math display">\[
\min(r_t \hat{A}_t, r_t \hat{A}_t) = r_t \hat{A}_t
\]</span> 不裁剪，正常更新。</p></li>
</ul>
<p><strong>情况2：优势为负 (<span class="math inline">\(\hat{A}_t &lt; 0\)</span>)，这是一个坏动作</strong></p>
<ul>
<li>如果 <span class="math inline">\(r_t &lt; 1-\epsilon\)</span>（新策略大幅减少了这个动作的概率）： <span class="math display">\[
\begin{align}
&amp;\text{第一项：} r_t \hat{A}_t &lt; (1-\epsilon) \hat{A}_t \quad (\text{注意} \hat{A}_t &lt; 0) \\
&amp;\text{第二项：} (1-\epsilon) \hat{A}_t \\
&amp;\text{取}\min\text{：} r_t \hat{A}_t
\end{align}
\]</span> 等等，这里取 <span class="math inline">\(\min\)</span> 实际上会选第一项（更负），这会鼓励继续减少。但裁剪会限制这种减少的程度。</li>
</ul>
<p>实际上，让我重新整理。PPO的裁剪逻辑可以用分段函数更清晰地表述：</p>
<p><span class="math display">\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t [L_t^{\text{CLIP}}(\theta)]
\]</span></p>
<p>其中对单个时间步 <span class="math inline">\(t\)</span>：</p>
<p><span class="math display">\[
L_t^{\text{CLIP}}(\theta) = \begin{cases}
r_t \hat{A}_t &amp; \text{if } \hat{A}_t \geq 0 \text{ and } r_t \leq 1+\epsilon \\
(1+\epsilon) \hat{A}_t &amp; \text{if } \hat{A}_t \geq 0 \text{ and } r_t &gt; 1+\epsilon \\
r_t \hat{A}_t &amp; \text{if } \hat{A}_t &lt; 0 \text{ and } r_t \geq 1-\epsilon \\
(1-\epsilon) \hat{A}_t &amp; \text{if } \hat{A}_t &lt; 0 \text{ and } r_t &lt; 1-\epsilon
\end{cases}
\]</span></p>
<p>这个设计的妙处在于： - 鼓励改进（增加好动作、减少坏动作），但不过度 - 一旦改进达到一定程度（<span class="math inline">\(r_t\)</span> 超出 <span class="math inline">\([1-\epsilon, 1+\epsilon]\)</span>），停止进一步激励 - 这创造了一个”信任区域”，策略只能在这个区域内变化</p>
</section>
<section id="完整的ppo损失函数" class="level4" data-number="4.3.5">
<h4 data-number="4.3.5" class="anchored" data-anchor-id="完整的ppo损失函数"><span class="header-section-number">4.3.5</span> 完整的PPO损失函数</h4>
<p>除了策略损失，PPO还包括其他两项：</p>
<p><strong>1. 价值函数损失</strong></p>
<p>我们需要训练一个价值网络 <span class="math inline">\(V_\phi(s)\)</span> 来估计 <span class="math inline">\(V^\pi(s)\)</span>，用于计算优势函数。价值函数的损失是均方误差：</p>
<p><span class="math display">\[
L^{VF}(\phi) = \mathbb{E}_t \left[ (V_\phi(s_t) - V_t^{\text{target}})^2 \right]
\]</span></p>
<p>其中目标值 <span class="math inline">\(V_t^{\text{target}}\)</span> 通常是折扣回报的实际值或TD目标。</p>
<p><strong>2. 熵正则项</strong></p>
<p>为了鼓励探索，我们希望策略不要过早收敛到确定性策略（只选一个动作）。熵正则项鼓励策略保持一定的随机性：</p>
<p><span class="math display">\[
H(\pi_\theta) = -\sum_{a} \pi_\theta(a \mid s) \log \pi_\theta(a \mid s)
\]</span></p>
<p>熵越高，策略越随机；熵越低，策略越确定。</p>
<p><strong>完整损失函数</strong></p>
<p><span class="math display">\[
L^{\text{PPO}}(\theta, \phi) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{VF}(\phi) + c_2 H(\pi_\theta(·\mid s_t)) \right]
\]</span></p>
<p>其中： - <span class="math inline">\(c_1 \approx 0.5\)</span>：价值函数损失的权重 - <span class="math inline">\(c_2 \approx 0.01\)</span>：熵奖励的权重 - 三项分别对应：策略改进、价值估计、探索鼓励</p>
</section>
<section id="ppo算法流程" class="level4" data-number="4.3.6">
<h4 data-number="4.3.6" class="anchored" data-anchor-id="ppo算法流程"><span class="header-section-number">4.3.6</span> PPO算法流程</h4>
<p>让我们总结完整的PPO训练流程：</p>
<p><strong>初始化</strong>： - 策略网络参数 <span class="math inline">\(\theta_0\)</span> - 价值网络参数 <span class="math inline">\(\phi_0\)</span></p>
<p><strong>对于每轮 <span class="math inline">\(k = 0, 1, 2, \ldots\)</span>：</strong></p>
<ol type="1">
<li><p><strong>采样轨迹</strong>：用当前策略 <span class="math inline">\(\pi_{\theta_k}\)</span> 运行 <span class="math inline">\(N\)</span> 步，收集数据： <span class="math display">\[
\mathcal{D}_k = \{(s_t, a_t, r_t, s_{t+1})\}_{t=1}^N
\]</span></p></li>
<li><p><strong>计算优势估计</strong>：对每个 <span class="math inline">\((s_t, a_t)\)</span>，计算优势函数估计 <span class="math inline">\(\hat{A}_t\)</span>： <span class="math display">\[
\hat{A}_t = \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l}
\]</span> 其中 <span class="math inline">\(\delta_t = r_t + \gamma V_{\phi_k}(s_{t+1}) - V_{\phi_k}(s_t)\)</span> 是TD误差，<span class="math inline">\(\lambda \in [0,1]\)</span> 是GAE参数。</p></li>
<li><p><strong>策略更新</strong>：对于 <span class="math inline">\(M\)</span> 个epoch（比如 <span class="math inline">\(M=4\)</span>）：</p>
<ul>
<li>对数据 <span class="math inline">\(\mathcal{D}_k\)</span> 打乱并分成minibatch</li>
<li>对每个minibatch，计算梯度并更新： <span class="math display">\[
\theta_{k+1} \leftarrow \theta_k + \alpha \nabla_\theta L^{\text{PPO}}(\theta_k, \phi_k)
\]</span> <span class="math display">\[
\phi_{k+1} \leftarrow \phi_k + \beta \nabla_\phi L^{\text{PPO}}(\theta_k, \phi_k)
\]</span></li>
</ul></li>
<li><p><strong>重复</strong>直到收敛。</p></li>
</ol>
</section>
<section id="为什么ppo在deepseek-r1中有效" class="level4" data-number="4.3.7">
<h4 data-number="4.3.7" class="anchored" data-anchor-id="为什么ppo在deepseek-r1中有效"><span class="header-section-number">4.3.7</span> 为什么PPO在DeepSeek-R1中有效？</h4>
<p>PPO特别适合训练DeepSeek-R1，因为：</p>
<p><strong>1. 样本效率高</strong></p>
<p>通过重要性采样，每批采样的推理链可以被重复使用多次（<span class="math inline">\(M\)</span> 个epoch）。考虑到生成一条推理链可能需要几百步前向传播，这大大降低了计算成本。</p>
<p><strong>2. 训练稳定</strong></p>
<p>裁剪机制防止策略突然崩溃。在语言模型中，策略崩溃可能表现为： - 模型开始生成无意义的重复文本 - 模型退化到只生成高频词 - 推理链的质量突然下降</p>
<p>PPO的信任区域机制避免了这些问题。</p>
<p><strong>3. 易于调参</strong></p>
<p>PPO只有几个关键超参数（<span class="math inline">\(\epsilon, c_1, c_2\)</span>），而且对它们的取值不太敏感。相比之下，其他强化学习算法（如TRPO）有更复杂的约束，难以在大规模模型上应用。</p>
</section>
</section>
<section id="过程奖励模型精细化的反馈" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="过程奖励模型精细化的反馈"><span class="header-section-number">4.4</span> 4.4 过程奖励模型：精细化的反馈</h3>
<p>我们之前讨论的强化学习框架使用<strong>稀疏奖励</strong>：只在最后一步根据答案是否正确给出 <span class="math inline">\(\pm 1\)</span> 的奖励。但这有个严重问题：<strong>当推理链很长时，信用分配（credit assignment）变得极其困难</strong>。</p>
<section id="信用分配问题" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="信用分配问题"><span class="header-section-number">4.4.1</span> 信用分配问题</h4>
<p>考虑一个需要15步推理的数学证明。模型完成了整个推理链，但最终答案是错误的。现在的问题是：<strong>这15步中的哪一步（或哪几步）导致了错误？</strong></p>
<p>用稀疏奖励，所有15步都会收到同样的负反馈 <span class="math inline">\(r = -1\)</span>。但实际上可能的情况是： - 前10步完全正确 - 第11步出现了逻辑错误 - 第12-15步基于错误的第11步继续推理</p>
<p>理想情况下，我们应该： - 奖励前10步（它们是正确的） - 惩罚第11步（错误的源头） - 对第12-15步给予中性或轻微惩罚（它们基于错误前提，但推理逻辑本身可能没问题）</p>
<p>这就是<strong>过程奖励模型</strong>（Process Reward Model, PRM）的动机。</p>
</section>
<section id="从结果奖励到过程奖励" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="从结果奖励到过程奖励"><span class="header-section-number">4.4.2</span> 从结果奖励到过程奖励</h4>
<p>让我们形式化地比较两种奖励设计：</p>
<p><strong>结果奖励（Outcome Reward Model, ORM）</strong></p>
<p><span class="math display">\[
R_{\text{ORM}}(\tau) = \begin{cases}
+1 &amp; \text{if final answer is correct} \\
-1 &amp; \text{if final answer is wrong}
\end{cases}
\]</span></p>
<p>这是一个标量，只依赖于最终结果。</p>
<p><strong>过程奖励（Process Reward Model, PRM）</strong></p>
<p><span class="math display">\[
R_{\text{PRM}}(\tau) = \sum_{t=1}^T r_t(s_t, c_t)
\]</span></p>
<p>其中： - <span class="math inline">\(r_t(s_t, c_t)\)</span>：第 <span class="math inline">\(t\)</span> 步推理的奖励 - <span class="math inline">\(s_t = (x, c_1, \ldots, c_{t-1})\)</span>：到第 <span class="math inline">\(t\)</span> 步之前的状态 - <span class="math inline">\(c_t\)</span>：第 <span class="math inline">\(t\)</span> 步生成的推理内容 - <span class="math inline">\(T\)</span>：推理链的总长度</p>
<p>每个 <span class="math inline">\(r_t\)</span> 可以取连续值，例如： - <span class="math inline">\(r_t \in [0, 1]\)</span>：第 <span class="math inline">\(t\)</span> 步的”正确性得分” - <span class="math inline">\(r_t = 1\)</span>：这一步完全正确 - <span class="math inline">\(r_t = 0.5\)</span>：这一步部分正确或有瑕疵 - <span class="math inline">\(r_t = 0\)</span>：这一步有明显错误</p>
</section>
<section id="训练过程奖励模型" class="level4" data-number="4.4.3">
<h4 data-number="4.4.3" class="anchored" data-anchor-id="训练过程奖励模型"><span class="header-section-number">4.4.3</span> 训练过程奖励模型</h4>
<p>PRM本身是一个独立的神经网络 <span class="math inline">\(R_\phi\)</span>，需要单独训练。训练过程包括三个步骤：</p>
<p><strong>步骤1：数据收集</strong></p>
<p>用当前策略 <span class="math inline">\(\pi_\theta\)</span> 生成大量推理链：</p>
<p><span class="math display">\[
\mathcal{D}_{\text{reasoning}} = \{(x^{(i)}, c^{(i)}, y^{(i)})\}_{i=1}^M
\]</span></p>
<p>其中： - <span class="math inline">\(x^{(i)}\)</span>：第 <span class="math inline">\(i\)</span> 个问题 - <span class="math inline">\(c^{(i)} = (c_1^{(i)}, \ldots, c_{T_i}^{(i)})\)</span>：生成的推理链 - <span class="math inline">\(y^{(i)}\)</span>：最终答案 - <span class="math inline">\(M\)</span>：样本数量（可能是几十万到几百万）</p>
<p><strong>步骤2：标注或自动验证</strong></p>
<p>对每条推理链的每一步进行标注。有两种方法：</p>
<p><strong>方法A：人工标注</strong></p>
<p>专家阅读推理链，为每一步打分：</p>
<p><span class="math display">\[
\text{label}_t^{(i)} = \begin{cases}
1 &amp; \text{if step } t \text{ is correct} \\
0 &amp; \text{if step } t \text{ is incorrect}
\end{cases}
\]</span></p>
<p>这种方法准确但昂贵。对于数学问题，一个专家标注一条推理链可能需要5-10分钟。</p>
<p><strong>方法B：自动验证器</strong></p>
<p>对于某些领域（如数学、代码），可以使用自动验证器。例如：</p>
<ul>
<li><strong>数学</strong>：每一步可以用符号计算引擎（如SymPy）验证</li>
<li><strong>代码</strong>：每一步可以实际执行并检查输出</li>
<li><strong>逻辑推理</strong>：可以用定理证明器（theorem prover）验证</li>
</ul>
<p>自动验证的优势是规模化，但只适用于形式化程度高的领域。</p>
<p><strong>步骤3：训练奖励模型</strong></p>
<p>有了标注数据 <span class="math inline">\(\{(s_t^{(i)}, c_t^{(i)}, \text{label}_t^{(i)})\}\)</span>，我们训练一个分类器 <span class="math inline">\(R_\phi\)</span>：</p>
<p><span class="math display">\[
R_\phi(s_t, c_t) \to [0, 1]
\]</span></p>
<p>输入： - <span class="math inline">\(s_t\)</span>：前文状态，编码为向量（通常用Transformer处理） - <span class="math inline">\(c_t\)</span>：当前步骤的文本</p>
<p>输出： - 一个标量 <span class="math inline">\(\in [0, 1]\)</span>，表示这一步正确的概率</p>
<p>训练损失是二元交叉熵：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{PRM}}(\phi) = -\frac{1}{N_{\text{steps}}} \sum_{i,t} \left[ \text{label}_t^{(i)} \log R_\phi(s_t^{(i)}, c_t^{(i)}) + (1-\text{label}_t^{(i)}) \log (1 - R_\phi(s_t^{(i)}, c_t^{(i)})) \right]
\]</span></p>
<p>其中： - <span class="math inline">\(N_{\text{steps}} = \sum_i T_i\)</span>：所有样本的总步骤数 - 外层求和遍历所有样本和时间步</p>
</section>
<section id="prm的架构" class="level4" data-number="4.4.4">
<h4 data-number="4.4.4" class="anchored" data-anchor-id="prm的架构"><span class="header-section-number">4.4.4</span> PRM的架构</h4>
<p>PRM通常使用与主模型相同的Transformer骨架，但有独立的参数 <span class="math inline">\(\phi\)</span>：</p>
<p><strong>输入编码</strong></p>
<p>给定状态 <span class="math inline">\(s_t = (x, c_1, \ldots, c_{t-1})\)</span> 和当前步骤 <span class="math inline">\(c_t\)</span>，拼接成一个序列：</p>
<p><span class="math display">\[
\text{input} = [x, c_1, \ldots, c_{t-1}, \texttt{[SEP]}, c_t]
\]</span></p>
<p>其中 <span class="math inline">\(\texttt{[SEP]}\)</span> 是分隔符token。</p>
<p><strong>Transformer处理</strong></p>
<p><span class="math display">\[
\mathbf{H} = \text{Transformer}_\phi(\text{input}) \in \mathbb{R}^{L \times d_{\text{model}}}
\]</span></p>
<p>其中： - <span class="math inline">\(L = |x| + |c_1| + \cdots + |c_t| + 1\)</span>：总序列长度 - <span class="math inline">\(\mathbf{H}\)</span>：所有位置的隐藏状态</p>
<p><strong>输出层</strong></p>
<p>取最后一个token的隐藏状态，通过一个线性层和sigmoid得到奖励：</p>
<p><span class="math display">\[
R_\phi(s_t, c_t) = \sigma(\mathbf{w}^\top \mathbf{h}_L + b)
\]</span></p>
<p>其中： - <span class="math inline">\(\mathbf{h}_L \in \mathbb{R}^{d_{\text{model}}}\)</span>：最后一个token的隐藏状态 - <span class="math inline">\(\mathbf{w} \in \mathbb{R}^{d_{\text{model}}}\)</span>：权重向量 - <span class="math inline">\(b \in \mathbb{R}\)</span>：偏置 - <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>：sigmoid函数</p>
</section>
<section id="在强化学习中使用prm" class="level4" data-number="4.4.5">
<h4 data-number="4.4.5" class="anchored" data-anchor-id="在强化学习中使用prm"><span class="header-section-number">4.4.5</span> 在强化学习中使用PRM</h4>
<p>训练好PRM后，我们在PPO训练中用它来计算每一步的奖励：</p>
<p><strong>修改后的奖励函数</strong></p>
<p><span class="math display">\[
r_t = \begin{cases}
R_\phi(s_t, c_t) - \text{baseline} &amp; \text{if } t &lt; T \\
R_\phi(s_T, c_T) + \lambda \cdot \mathbb{1}[\text{answer correct}] &amp; \text{if } t = T
\end{cases}
\]</span></p>
<p>其中： - <span class="math inline">\(\text{baseline}\)</span>：基线值（比如0.5），用于中心化奖励 - <span class="math inline">\(\lambda\)</span>：结果奖励的权重（比如 <span class="math inline">\(\lambda = 2\)</span>） - <span class="math inline">\(\mathbb{1}[\text{answer correct}]\)</span>：最终答案是否正确</p>
<p>这样，总回报变成：</p>
<p><span class="math display">\[
R(\tau) = \sum_{t=1}^{T-1} (R_\phi(s_t, c_t) - \text{baseline}) + (R_\phi(s_T, c_T) + \lambda \cdot \mathbb{1}[\text{answer correct}])
\]</span></p>
<p><strong>好处</strong>：</p>
<ol type="1">
<li><strong>更密集的信号</strong>：每一步都有反馈，而不是只在最后</li>
<li><strong>更快的学习</strong>：模型可以更快定位错误来源</li>
<li><strong>更稳定的训练</strong>：方差降低（因为每步都有奖励，而不是只依赖最终的二元信号）</li>
</ol>
</section>
<section id="prm-vs-orm实验对比" class="level4" data-number="4.4.6">
<h4 data-number="4.4.6" class="anchored" data-anchor-id="prm-vs-orm实验对比"><span class="header-section-number">4.4.6</span> PRM vs ORM：实验对比</h4>
<p>假设一个10步推理链，第5步出错：</p>
<p><strong>使用ORM（结果奖励）</strong>：</p>
<pre><code>步骤1-10：全部获得 r = -1（因为最终答案错）
梯度信号：所有步骤都被惩罚
问题：模型可能会放弃正确的步骤1-4</code></pre>
<p><strong>使用PRM（过程奖励）</strong>：</p>
<pre><code>步骤1-4：r ≈ +0.5（PRM识别出这些是正确的）
步骤5：r ≈ -0.5（PRM识别出错误）
步骤6-10：r ≈ 0（基于错误前提，但逻辑尚可）
最终：r = -1（答案错误）
梯度信号：主要惩罚步骤5，轻微奖励步骤1-4
结果：模型学会保留正确步骤，修正错误步骤</code></pre>
<p>实验表明，使用PRM的模型： - 收敛速度快约 <strong>2-3倍</strong> - 最终性能提升约 <strong>5-10%</strong> - 训练更稳定（方差降低约40%）</p>
</section>
</section>
<section id="知识蒸馏平衡性能与效率" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="知识蒸馏平衡性能与效率"><span class="header-section-number">4.5</span> 4.5 知识蒸馏：平衡性能与效率</h3>
<p>DeepSeek-R1通过思维链推理获得了强大的推理能力，但这带来了一个实际问题：<strong>推理成本显著增加</strong>。</p>
<section id="推理成本分析" class="level4" data-number="4.5.1">
<h4 data-number="4.5.1" class="anchored" data-anchor-id="推理成本分析"><span class="header-section-number">4.5.1</span> 推理成本分析</h4>
<p>考虑一个具体例子：</p>
<p><strong>传统模型</strong>（直接输出答案）： - 输入：<span class="math inline">\(L_x = 20\)</span> tokens（问题） - 输出：<span class="math inline">\(L_y = 5\)</span> tokens（答案） - 总计算：<span class="math inline">\(\approx (L_x + L_y) \times d_{\text{model}} \times n_{\text{layers}}\)</span></p>
<p><strong>DeepSeek-R1</strong>（带思维链）： - 输入：<span class="math inline">\(L_x = 20\)</span> tokens（问题） - 思维链：<span class="math inline">\(L_c = 200\)</span> tokens（推理过程） - 输出：<span class="math inline">\(L_y = 5\)</span> tokens（答案） - 总计算：<span class="math inline">\(\approx (L_x + L_c + L_y) \times d_{\text{model}} \times n_{\text{layers}}\)</span></p>
<p>计算量增加了约：</p>
<p><span class="math display">\[
\frac{L_x + L_c + L_y}{L_x + L_y} = \frac{20 + 200 + 5}{20 + 5} = \frac{225}{25} = 9 \text{ 倍}
\]</span></p>
<p>对于需要长推理链的复杂问题（<span class="math inline">\(L_c\)</span> 可能达到几百甚至上千），这个倍数会更大。</p>
</section>
<section id="知识蒸馏的思路" class="level4" data-number="4.5.2">
<h4 data-number="4.5.2" class="anchored" data-anchor-id="知识蒸馏的思路"><span class="header-section-number">4.5.2</span> 知识蒸馏的思路</h4>
<p>关键观察：<strong>不是所有问题都需要详细推理</strong>。</p>
<ul>
<li><strong>简单问题</strong>（如 <span class="math inline">\(2+2=?\)</span>）：不需要思维链，直接输出答案即可</li>
<li><strong>中等问题</strong>：需要简短推理（几十个tokens）</li>
<li><strong>困难问题</strong>：需要详细推理（几百个tokens）</li>
</ul>
<p>知识蒸馏允许我们创建一个<strong>任务自适应系统</strong>： - <strong>教师模型</strong>（Teacher）：完整的DeepSeek-R1，总是生成详细思维链 - <strong>学生模型</strong>（Student）：较小/较快的模型，学习在简单问题上跳过推理</p>
</section>
<section id="蒸馏的数学框架" class="level4" data-number="4.5.3">
<h4 data-number="4.5.3" class="anchored" data-anchor-id="蒸馏的数学框架"><span class="header-section-number">4.5.3</span> 蒸馏的数学框架</h4>
<p><strong>教师模型</strong>生成：</p>
<p><span class="math display">\[
p_{\text{teacher}}(y \mid x) = \sum_c p_{\text{teacher}}(c \mid x) p_{\text{teacher}}(y \mid c, x)
\]</span></p>
<p>这里教师模型边缘化了所有可能的推理链 <span class="math inline">\(c\)</span>（在实践中，通常采样几条推理链并平均）。</p>
<p><strong>学生模型</strong>直接建模：</p>
<p><span class="math display">\[
p_{\text{student}}(y \mid x)
\]</span></p>
<p>没有显式的推理链。</p>
<p><strong>蒸馏目标函数</strong></p>
<p>经典的知识蒸馏（Hinton et al.）使用两项损失的加权和：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{distill}}(\theta_{\text{student}}) = \alpha \cdot \mathcal{L}_{\text{hard}} + (1-\alpha) \cdot \mathcal{L}_{\text{soft}}
\]</span></p>
<p><strong>硬标签损失（Hard Label Loss）</strong></p>
<p>这是标准的监督学习损失，使用真实标签：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{hard}} = -\log p_{\text{student}}(y^* \mid x)
\]</span></p>
<p>其中 <span class="math inline">\(y^*\)</span> 是ground truth答案。</p>
<p>这确保学生模型输出正确答案。</p>
<p><strong>软标签损失（Soft Label Loss）</strong></p>
<p>这是与教师模型输出分布的KL散度：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{soft}} = D_{\text{KL}}(p_{\text{teacher}}(\cdot \mid x) \| p_{\text{student}}(\cdot \mid x))
\]</span></p>
<p>展开KL散度：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{soft}} = \sum_{y \in \mathcal{Y}} p_{\text{teacher}}(y \mid x) \log \frac{p_{\text{teacher}}(y \mid x)}{p_{\text{student}}(y \mid x)}
\]</span></p>
<p>简化（忽略与 <span class="math inline">\(\theta_{\text{student}}\)</span> 无关的项）：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{soft}} = -\sum_{y \in \mathcal{Y}} p_{\text{teacher}}(y \mid x) \log p_{\text{student}}(y \mid x) + \text{const}
\]</span></p>
<p>这是用教师分布作为”软目标”的交叉熵。</p>
<p><strong>温度缩放</strong></p>
<p>为了让教师模型的输出分布更”平滑”（不那么peaked），我们引入温度 <span class="math inline">\(T\)</span>：</p>
<p><span class="math display">\[
p_{\text{teacher}}^{(T)}(y \mid x) = \frac{\exp(z_y / T)}{\sum_{y'} \exp(z_{y'} / T)}
\]</span></p>
<p>其中： - <span class="math inline">\(z_y\)</span>：教师模型对答案 <span class="math inline">\(y\)</span> 的logit（未归一化得分） - <span class="math inline">\(T\)</span>：温度参数（通常 <span class="math inline">\(T = 2\)</span> 或 <span class="math inline">\(T = 4\)</span>）</p>
<p>温度的作用： - <span class="math inline">\(T = 1\)</span>：标准softmax - <span class="math inline">\(T &gt; 1\)</span>：分布更平滑，低概率选项也有一定权重 - <span class="math inline">\(T \to \infty\)</span>：趋向均匀分布</p>
<p>为什么需要平滑？因为教师模型可能对正确答案给出接近1的概率，对其他答案接近0。但<strong>教师对不同错误答案的偏好包含有价值信息</strong>。</p>
<p>例如，对于问题”首都巴黎属于哪个国家？“： - 正确答案：”法国” → <span class="math inline">\(p = 0.95\)</span> - 错误但相关：“德国” → <span class="math inline">\(p = 0.03\)</span>（欧洲国家，有一定相关性） - 完全不相关：“火星” → <span class="math inline">\(p = 0.0001\)</span></p>
<p>温度缩放后，这些细微差别会被放大，学生可以学到”德国虽然不对，但比火星更相关”这样的知识。</p>
<p><strong>完整蒸馏损失</strong></p>
<p><span class="math display">\[
\mathcal{L}_{\text{distill}} = \alpha \cdot \left[-\log p_{\text{student}}(y^* \mid x)\right] + (1-\alpha) \cdot T^2 \cdot D_{\text{KL}}(p_{\text{teacher}}^{(T)} \| p_{\text{student}}^{(T)})
\]</span></p>
<p>其中： - <span class="math inline">\(\alpha \in [0,1]\)</span>：硬标签和软标签的权重（通常 <span class="math inline">\(\alpha = 0.3\)</span> 到 <span class="math inline">\(0.5\)</span>） - <span class="math inline">\(T^2\)</span> 系数：补偿温度缩放对梯度幅度的影响</p>
</section>
<section id="分层蒸馏策略" class="level4" data-number="4.5.4">
<h4 data-number="4.5.4" class="anchored" data-anchor-id="分层蒸馏策略"><span class="header-section-number">4.5.4</span> 分层蒸馏策略</h4>
<p>DeepSeek-R1可以采用<strong>分层蒸馏</strong>，针对不同难度的问题使用不同模型：</p>
<p><strong>三层架构</strong>：</p>
<ol type="1">
<li><strong>快速模型</strong>（Small Student）
<ul>
<li>参数量：<span class="math inline">\(\sim\)</span> 1B</li>
<li>策略：直接输出答案，无推理链</li>
<li>适用：简单问题（占总量的40-50%）</li>
</ul></li>
<li><strong>中等模型</strong>（Medium Student）
<ul>
<li>参数量：<span class="math inline">\(\sim\)</span> 7B</li>
<li>策略：生成简短推理链（10-30 tokens）</li>
<li>适用：中等问题（占总量的30-40%）</li>
</ul></li>
<li><strong>完整模型</strong>（Teacher）
<ul>
<li>参数量：<span class="math inline">\(\sim\)</span> 70B+</li>
<li>策略：生成完整推理链（100+ tokens）</li>
<li>适用：困难问题（占总量的10-20%）</li>
</ul></li>
</ol>
<p><strong>路由机制</strong></p>
<p>训练一个分类器 <span class="math inline">\(f_{\text{router}}(x) \to \{1, 2, 3\}\)</span> 来决定使用哪个模型：</p>
<p><span class="math display">\[
\text{model} = \begin{cases}
\text{Small} &amp; \text{if } f_{\text{router}}(x) = 1 \\
\text{Medium} &amp; \text{if } f_{\text{router}}(x) = 2 \\
\text{Teacher} &amp; \text{if } f_{\text{router}}(x) = 3
\end{cases}
\]</span></p>
<p>这样，平均推理成本可以降低到原来的 <strong>20-30%</strong>，同时保持 <strong>95%+</strong> 的性能。</p>
</section>
<section id="蒸馏的效果" class="level4" data-number="4.5.5">
<h4 data-number="4.5.5" class="anchored" data-anchor-id="蒸馏的效果"><span class="header-section-number">4.5.5</span> 蒸馏的效果</h4>
<p>实验表明，一个7B的学生模型通过蒸馏可以达到： - 在简单任务上：接近70B教师的 <strong>98-99%</strong> 性能 - 在中等任务上：<strong>90-95%</strong> 性能 - 在困难任务上：<strong>70-80%</strong> 性能（这时应该回退到教师模型）</p>
<p>关键是：<strong>大部分实际应用中，简单和中等任务占比超过80%</strong>，所以整体上可以用小模型处理大部分请求，显著降低成本。</p>
</section>
</section>
</section>
<section id="架构实现细节性能优化的数学基础" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="架构实现细节性能优化的数学基础"><span class="header-section-number">5</span> 5. 架构实现细节：性能优化的数学基础</h2>
<p>理解了DeepSeek-R1的核心训练方法后，我们来看看它在架构层面的关键优化。这些优化让模型能够高效地处理长推理链，而不会被内存或计算成本拖垮。</p>
<section id="分组查询注意力grouped-query-attention-gqa" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="分组查询注意力grouped-query-attention-gqa"><span class="header-section-number">5.1</span> 5.1 分组查询注意力（Grouped Query Attention, GQA）</h3>
<p>在讨论GQA之前，我们先理解为什么需要它。</p>
<section id="标准多头注意力的内存瓶颈" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="标准多头注意力的内存瓶颈"><span class="header-section-number">5.1.1</span> 标准多头注意力的内存瓶颈</h4>
<p>回顾标准的<strong>多头注意力</strong>（Multi-Head Attention, MHA）机制。给定输入 <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{L \times d_{\text{model}}}\)</span>，其中： - <span class="math inline">\(L\)</span>：序列长度 - <span class="math inline">\(d_{\text{model}}\)</span>：模型的隐藏维度（例如 <span class="math inline">\(d_{\text{model}} = 4096\)</span>）</p>
<p>对于每个注意力头 <span class="math inline">\(h = 1, \ldots, H\)</span>（假设 <span class="math inline">\(H = 32\)</span> 个头），我们计算：</p>
<p><strong>投影到 <span class="math inline">\(Q, K, V\)</span></strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Q}_h &amp;= \mathbf{X} \mathbf{W}_h^Q \in \mathbb{R}^{L \times d_k} \\
\mathbf{K}_h &amp;= \mathbf{X} \mathbf{W}_h^K \in \mathbb{R}^{L \times d_k} \\
\mathbf{V}_h &amp;= \mathbf{X} \mathbf{W}_h^V \in \mathbb{R}^{L \times d_v}
\end{aligned}
\]</span></p>
<p>其中： - <span class="math inline">\(\mathbf{W}_h^Q, \mathbf{W}_h^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>：每个头的查询和键投影矩阵 - <span class="math inline">\(\mathbf{W}_h^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span>：值投影矩阵 - <span class="math inline">\(d_k = d_v = d_{\text{model}} / H\)</span>（通常 <span class="math inline">\(d_k = 128\)</span> 当 <span class="math inline">\(d_{\text{model}} = 4096, H = 32\)</span>）</p>
<p><strong>计算注意力</strong></p>
<p><span class="math display">\[
\mathbf{O}_h = \text{softmax}\left(\frac{\mathbf{Q}_h \mathbf{K}_h^\top}{\sqrt{d_k}}\right) \mathbf{V}_h \in \mathbb{R}^{L \times d_v}
\]</span></p>
<p><strong>拼接所有头</strong></p>
<p><span class="math display">\[
\mathbf{O} = \text{Concat}(\mathbf{O}_1, \ldots, \mathbf{O}_H) \mathbf{W}^O \in \mathbb{R}^{L \times d_{\text{model}}}
\]</span></p>
</section>
<section id="kv缓存的内存消耗" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="kv缓存的内存消耗"><span class="header-section-number">5.1.2</span> KV缓存的内存消耗</h4>
<p>在<strong>自回归生成</strong>时（即逐token生成推理链），我们需要缓存之前所有位置的 <span class="math inline">\(\mathbf{K}\)</span> 和 <span class="math inline">\(\mathbf{V}\)</span>，这称为<strong>KV cache</strong>。</p>
<p>假设我们已经生成了 <span class="math inline">\(L\)</span> 个tokens，那么需要存储：</p>
<p><strong>每个头的KV cache大小</strong>： <span class="math display">\[
\text{Memory}_{\text{per head}} = 2 \times L \times d_k \times \text{sizeof(float16)}
\]</span></p>
<p>因子2来自于K和V都要存储。</p>
<p><strong>所有头的KV cache大小</strong>（<span class="math inline">\(H\)</span> 个头）： <span class="math display">\[
\text{Memory}_{\text{all heads}} = 2 \times H \times L \times d_k \times \text{sizeof(float16)}
\]</span></p>
<p><strong>具体数值示例</strong>： - <span class="math inline">\(H = 32\)</span> 个头 - <span class="math inline">\(d_k = 128\)</span> - <span class="math inline">\(L = 2048\)</span> tokens（一个中等长度的推理链） - float16：每个数占2字节</p>
<p><span class="math display">\[
\text{Memory}_{\text{KV}} = 2 \times 32 \times 2048 \times 128 \times 2 \text{ bytes} = 33,554,432 \text{ bytes} \approx 32 \text{ MB}
\]</span></p>
<p>这是<strong>单个层</strong>的KV cache。对于一个70B参数的模型，通常有80-100层，总KV cache可达：</p>
<p><span class="math display">\[
32 \text{ MB/layer} \times 80 \text{ layers} = 2.56 \text{ GB}
\]</span></p>
<p>这还只是单个样本！如果我们想批处理（batch size = 16），总内存需求是：</p>
<p><span class="math display">\[
2.56 \text{ GB} \times 16 = 40.96 \text{ GB}
\]</span></p>
<p>对于长推理链（<span class="math inline">\(L = 8192\)</span>），这个数字会翻4倍，达到<strong>163.84 GB</strong>，这对GPU内存是巨大的挑战。</p>
</section>
<section id="gqa的核心思想" class="level4" data-number="5.1.3">
<h4 data-number="5.1.3" class="anchored" data-anchor-id="gqa的核心思想"><span class="header-section-number">5.1.3</span> GQA的核心思想</h4>
<p><strong>分组查询注意力</strong>（GQA）的关键观察：我们真的需要每个头都有独立的 <span class="math inline">\(\mathbf{K}_h\)</span> 和 <span class="math inline">\(\mathbf{V}_h\)</span> 吗？</p>
<p>GQA的做法： 1. 将 <span class="math inline">\(H\)</span> 个查询头分成 <span class="math inline">\(G\)</span> 组（例如 <span class="math inline">\(G = 4\)</span>） 2. 每组有 <span class="math inline">\(H/G\)</span> 个查询头（例如 <span class="math inline">\(32/4 = 8\)</span> 个头/组） 3. <strong>每组共享同一套 <span class="math inline">\(\mathbf{K}\)</span> 和 <span class="math inline">\(\mathbf{V}\)</span></strong></p>
</section>
<section id="gqa的数学公式" class="level4" data-number="5.1.4">
<h4 data-number="5.1.4" class="anchored" data-anchor-id="gqa的数学公式"><span class="header-section-number">5.1.4</span> GQA的数学公式</h4>
<p>假设我们有 <span class="math inline">\(H = 32\)</span> 个查询头，分成 <span class="math inline">\(G = 4\)</span> 组。</p>
<p><strong>为每组定义一个共享的K和V</strong>：</p>
<p>对于第 <span class="math inline">\(g\)</span> 组（<span class="math inline">\(g = 1, \ldots, G\)</span>），我们有：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{K}_g &amp;= \mathbf{X} \mathbf{W}_g^K \in \mathbb{R}^{L \times d_k} \\
\mathbf{V}_g &amp;= \mathbf{X} \mathbf{W}_g^V \in \mathbb{R}^{L \times d_v}
\end{aligned}
\]</span></p>
<p>这里只有 <span class="math inline">\(G = 4\)</span> 套KV投影矩阵，而不是 <span class="math inline">\(H = 32\)</span> 套。</p>
<p><strong>但每个查询头仍然是独立的</strong>：</p>
<p>对于第 <span class="math inline">\(h\)</span> 个查询头（假设它属于第 <span class="math inline">\(g\)</span> 组），我们计算：</p>
<p><span class="math display">\[
\mathbf{Q}_h = \mathbf{X} \mathbf{W}_h^Q \in \mathbb{R}^{L \times d_k}
\]</span></p>
<p>注意力输出为：</p>
<p><span class="math display">\[
\mathbf{O}_h = \text{softmax}\left(\frac{\mathbf{Q}_h \mathbf{K}_g^\top}{\sqrt{d_k}}\right) \mathbf{V}_g \in \mathbb{R}^{L \times d_v}
\]</span></p>
<p><strong>分组示例</strong>： - 查询头 <span class="math inline">\(h = 1, 2, \ldots, 8\)</span> 使用 <span class="math inline">\(\mathbf{K}_1, \mathbf{V}_1\)</span> - 查询头 <span class="math inline">\(h = 9, 10, \ldots, 16\)</span> 使用 <span class="math inline">\(\mathbf{K}_2, \mathbf{V}_2\)</span> - 查询头 <span class="math inline">\(h = 17, 18, \ldots, 24\)</span> 使用 <span class="math inline">\(\mathbf{K}_3, \mathbf{V}_3\)</span> - 查询头 <span class="math inline">\(h = 25, 26, \ldots, 32\)</span> 使用 <span class="math inline">\(\mathbf{K}_4, \mathbf{V}_4\)</span></p>
</section>
<section id="gqa的内存节省计算" class="level4" data-number="5.1.5">
<h4 data-number="5.1.5" class="anchored" data-anchor-id="gqa的内存节省计算"><span class="header-section-number">5.1.5</span> GQA的内存节省计算</h4>
<p>使用GQA后，KV cache的大小变为：</p>
<p><span class="math display">\[
\text{Memory}_{\text{GQA}} = 2 \times G \times L \times d_k \times \text{sizeof(float16)}
\]</span></p>
<p>相比标准MHA：</p>
<p><span class="math display">\[
\text{Memory}_{\text{MHA}} = 2 \times H \times L \times d_k \times \text{sizeof(float16)}
\]</span></p>
<p><strong>节省比例</strong>：</p>
<p><span class="math display">\[
\frac{\text{Memory}_{\text{GQA}}}{\text{Memory}_{\text{MHA}}} = \frac{G}{H} = \frac{4}{32} = \frac{1}{8}
\]</span></p>
<p>也就是说，GQA将KV cache减少到原来的 <strong>1/8</strong>！</p>
<p><strong>具体数值</strong>： - 标准MHA：2.56 GB/样本 - GQA（<span class="math inline">\(G=4\)</span>）：<span class="math inline">\(2.56 / 8 = 0.32\)</span> GB/样本</p>
<p>对于batch size = 16，长度 <span class="math inline">\(L = 8192\)</span> 的推理链： - 标准MHA：163.84 GB - GQA：<span class="math inline">\(163.84 / 8 = 20.48\)</span> GB</p>
<p>这使得在消费级GPU（如A100 40GB）上运行大模型成为可能。</p>
</section>
<section id="gqa-vs-mqa灵活的折衷" class="level4" data-number="5.1.6">
<h4 data-number="5.1.6" class="anchored" data-anchor-id="gqa-vs-mqa灵活的折衷"><span class="header-section-number">5.1.6</span> GQA vs MQA：灵活的折衷</h4>
<p>GQA实际上是两个极端之间的折衷：</p>
<ol type="1">
<li><strong>标准MHA</strong>（Multi-Head Attention）：<span class="math inline">\(G = H\)</span>（每个头独立）
<ul>
<li>优点：表达能力最强</li>
<li>缺点：内存消耗大</li>
</ul></li>
<li><strong>MQA</strong>（Multi-Query Attention）：<span class="math inline">\(G = 1\)</span>（所有头共享同一套KV）
<ul>
<li>优点：内存最小</li>
<li>缺点：性能下降较明显</li>
</ul></li>
<li><strong>GQA</strong>：<span class="math inline">\(1 &lt; G &lt; H\)</span>（介于两者之间）
<ul>
<li>优点：<strong>平衡性能和效率</strong></li>
<li>实践中，<span class="math inline">\(G = 4\)</span> 或 <span class="math inline">\(G = 8\)</span> 是常见选择</li>
</ul></li>
</ol>
<p>实验表明，GQA在内存节省 <span class="math inline">\(4\times\)</span> 到 <span class="math inline">\(8\times\)</span> 的同时，性能下降不到 <strong>1-2%</strong>，这是一个非常值得的权衡。</p>
</section>
</section>
<section id="旋转位置编码rope" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="旋转位置编码rope"><span class="header-section-number">5.2</span> 5.2 旋转位置编码（RoPE）</h3>
<p>位置编码是Transformer的关键组成部分，因为自注意力机制本身是<strong>位置不变的</strong>（permutation invariant）——如果我们打乱输入序列的顺序，注意力权重不会改变（除非有位置信息）。</p>
<section id="为什么传统位置编码不够好" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="为什么传统位置编码不够好"><span class="header-section-number">5.2.1</span> 为什么传统位置编码不够好？</h4>
<p>最早的Transformer（Vaswani et al., 2017）使用<strong>绝对位置编码</strong>：</p>
<p><span class="math display">\[
\text{PE}(m, 2i) = \sin\left(\frac{m}{10000^{2i/d}}\right), \quad \text{PE}(m, 2i+1) = \cos\left(\frac{m}{10000^{2i/d}}\right)
\]</span></p>
<p>其中 <span class="math inline">\(m\)</span> 是位置，<span class="math inline">\(i\)</span> 是维度索引。</p>
<p>这种编码直接加到输入embeddings上：</p>
<p><span class="math display">\[
\mathbf{x}_m = \mathbf{e}_m + \text{PE}(m)
\]</span></p>
<p><strong>问题1：外推能力差</strong></p>
<p>如果模型在训练时只见过长度 <span class="math inline">\(L \leq 2048\)</span> 的序列，在推理时遇到 <span class="math inline">\(L = 4096\)</span> 的序列，位置编码 <span class="math inline">\(\text{PE}(m)\)</span> 对于 <span class="math inline">\(m &gt; 2048\)</span> 的值是未见过的，模型可能表现很差。</p>
<p><strong>问题2：相对位置信息不明确</strong></p>
<p>虽然理论上模型可以学到相对位置，但这依赖于模型从数据中隐式学习，不如显式编码相对位置。</p>
</section>
<section id="rope的核心思想" class="level4" data-number="5.2.2">
<h4 data-number="5.2.2" class="anchored" data-anchor-id="rope的核心思想"><span class="header-section-number">5.2.2</span> RoPE的核心思想</h4>
<p><strong>旋转位置编码</strong>（Rotary Position Embedding, Su et al., 2021）的目标：<strong>在注意力计算中直接编码相对位置信息</strong>。</p>
<p>关键观察：如果我们能让注意力得分 <span class="math inline">\(\mathbf{q}_m^\top \mathbf{k}_n\)</span> 仅依赖于相对位置 <span class="math inline">\(m - n\)</span>，那么模型就具有相对位置不变性。</p>
<p>RoPE的做法：<strong>用旋转矩阵对 <span class="math inline">\(\mathbf{q}\)</span> 和 <span class="math inline">\(\mathbf{k}\)</span> 进行位置相关的旋转</strong>。</p>
</section>
<section id="rope的数学推导" class="level4" data-number="5.2.3">
<h4 data-number="5.2.3" class="anchored" data-anchor-id="rope的数学推导"><span class="header-section-number">5.2.3</span> RoPE的数学推导</h4>
<p>我们从二维情况开始（容易可视化），然后推广到高维。</p>
<p><strong>二维情况</strong></p>
<p>假设查询向量 <span class="math inline">\(\mathbf{q} = (q^{(1)}, q^{(2)})^\top \in \mathbb{R}^2\)</span>，键向量 <span class="math inline">\(\mathbf{k} = (k^{(1)}, k^{(2)})^\top \in \mathbb{R}^2\)</span>。</p>
<p>对于位置 <span class="math inline">\(m\)</span> 的查询，我们用旋转矩阵 <span class="math inline">\(\mathbf{R}_m\)</span> 旋转它：</p>
<p><span class="math display">\[
\mathbf{q}_m = \mathbf{R}_m \mathbf{q} =
\begin{pmatrix}
\cos(m\theta) &amp; -\sin(m\theta) \\
\sin(m\theta) &amp; \cos(m\theta)
\end{pmatrix}
\begin{pmatrix}
q^{(1)} \\
q^{(2)}
\end{pmatrix}
\]</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是一个超参数（旋转频率）。</p>
<p>类似地，对于位置 <span class="math inline">\(n\)</span> 的键：</p>
<p><span class="math display">\[
\mathbf{k}_n = \mathbf{R}_n \mathbf{k} =
\begin{pmatrix}
\cos(n\theta) &amp; -\sin(n\theta) \\
\sin(n\theta) &amp; \cos(n\theta)
\end{pmatrix}
\begin{pmatrix}
k^{(1)} \\
k^{(2)}
\end{pmatrix}
\]</span></p>
<p><strong>关键性质：注意力得分仅依赖相对位置</strong></p>
<p>计算内积：</p>
<p><span class="math display">\[
\mathbf{q}_m^\top \mathbf{k}_n = (\mathbf{R}_m \mathbf{q})^\top (\mathbf{R}_n \mathbf{k}) = \mathbf{q}^\top \mathbf{R}_m^\top \mathbf{R}_n \mathbf{k}
\]</span></p>
<p>由于旋转矩阵的性质 <span class="math inline">\(\mathbf{R}_m^\top = \mathbf{R}_{-m}\)</span>（逆旋转），我们有：</p>
<p><span class="math display">\[
\mathbf{R}_m^\top \mathbf{R}_n = \mathbf{R}_{n-m}
\]</span></p>
<p>因此：</p>
<p><span class="math display">\[
\mathbf{q}_m^\top \mathbf{k}_n = \mathbf{q}^\top \mathbf{R}_{n-m} \mathbf{k}
\]</span></p>
<p><strong>这只依赖于 <span class="math inline">\(n - m\)</span>（相对位置），而不是绝对位置 <span class="math inline">\(m\)</span> 或 <span class="math inline">\(n\)</span>！</strong></p>
<p>让我们验证 <span class="math inline">\(\mathbf{R}_{n-m}\)</span> 的形式：</p>
<p><span class="math display">\[
\mathbf{R}_{n-m} =
\begin{pmatrix}
\cos((n-m)\theta) &amp; -\sin((n-m)\theta) \\
\sin((n-m)\theta) &amp; \cos((n-m)\theta)
\end{pmatrix}
\]</span></p>
<p>这是一个旋转角度为 <span class="math inline">\((n-m)\theta\)</span> 的旋转矩阵。</p>
</section>
<section id="推广到高维" class="level4" data-number="5.2.4">
<h4 data-number="5.2.4" class="anchored" data-anchor-id="推广到高维"><span class="header-section-number">5.2.4</span> 推广到高维</h4>
<p>对于 <span class="math inline">\(d_k\)</span> 维的向量（例如 <span class="math inline">\(d_k = 128\)</span>），我们将维度两两配对，每对使用不同的旋转频率。</p>
<p>将 <span class="math inline">\(\mathbf{q} \in \mathbb{R}^{d_k}\)</span> 分成 <span class="math inline">\(d_k/2\)</span> 对：</p>
<p><span class="math display">\[
\mathbf{q} = (q^{(1)}, q^{(2)}, q^{(3)}, q^{(4)}, \ldots, q^{(d_k-1)}, q^{(d_k)})
\]</span></p>
<p>对于第 <span class="math inline">\(i\)</span> 对（<span class="math inline">\(i = 1, \ldots, d_k/2\)</span>），使用频率：</p>
<p><span class="math display">\[
\theta_i = \frac{1}{10000^{2i/d_k}}
\]</span></p>
<p>（这个公式借鉴了原始Transformer的正弦位置编码）</p>
<p>对于位置 <span class="math inline">\(m\)</span>，旋转后的查询向量为：</p>
<p><span class="math display">\[
\mathbf{q}_m = \begin{pmatrix}
\cos(m\theta_1) &amp; -\sin(m\theta_1) &amp; &amp; &amp; \\
\sin(m\theta_1) &amp; \cos(m\theta_1) &amp; &amp; &amp; \\
&amp; &amp; \cos(m\theta_2) &amp; -\sin(m\theta_2) &amp; \\
&amp; &amp; \sin(m\theta_2) &amp; \cos(m\theta_2) &amp; \\
&amp; &amp; &amp; &amp; \ddots
\end{pmatrix}
\begin{pmatrix}
q^{(1)} \\
q^{(2)} \\
q^{(3)} \\
q^{(4)} \\
\vdots
\end{pmatrix}
\]</span></p>
<p>这是一个<strong>块对角矩阵</strong>，每个 <span class="math inline">\(2 \times 2\)</span> 块是一个旋转矩阵。</p>
</section>
<section id="复数表示等价但更简洁" class="level4" data-number="5.2.5">
<h4 data-number="5.2.5" class="anchored" data-anchor-id="复数表示等价但更简洁"><span class="header-section-number">5.2.5</span> 复数表示（等价但更简洁）</h4>
<p>二维旋转矩阵可以用复数表示。将 <span class="math inline">\((q^{(2i-1)}, q^{(2i)})\)</span> 看作复数的实部和虚部：</p>
<p><span class="math display">\[
\tilde{q}^{(i)} = q^{(2i-1)} + j \cdot q^{(2i)} \in \mathbb{C}
\]</span></p>
<p>其中 <span class="math inline">\(j\)</span> 是虚数单位（<span class="math inline">\(j^2 = -1\)</span>）。</p>
<p>旋转角度 <span class="math inline">\(m\theta_i\)</span> 对应于乘以复数 <span class="math inline">\(e^{jm\theta_i}\)</span>：</p>
<p><span class="math display">\[
\tilde{q}_m^{(i)} = \tilde{q}^{(i)} \cdot e^{jm\theta_i} = (q^{(2i-1)} + j \cdot q^{(2i)}) \cdot (\cos(m\theta_i) + j\sin(m\theta_i))
\]</span></p>
<p>展开后得到：</p>
<p><span class="math display">\[
\begin{aligned}
\text{Re}(\tilde{q}_m^{(i)}) &amp;= q^{(2i-1)} \cos(m\theta_i) - q^{(2i)} \sin(m\theta_i) \\
\text{Im}(\tilde{q}_m^{(i)}) &amp;= q^{(2i-1)} \sin(m\theta_i) + q^{(2i)} \cos(m\theta_i)
\end{aligned}
\]</span></p>
<p>这正是旋转矩阵的作用！</p>
<p>在实际实现中，我们可以用复数运算来简化代码。</p>
</section>
<section id="rope的外推能力" class="level4" data-number="5.2.6">
<h4 data-number="5.2.6" class="anchored" data-anchor-id="rope的外推能力"><span class="header-section-number">5.2.6</span> RoPE的外推能力</h4>
<p>为什么RoPE能处理比训练时更长的序列？</p>
<p>关键在于：<strong>旋转角度 <span class="math inline">\(m\theta\)</span> 是连续的</strong>。</p>
<p>即使模型在训练时只见过 <span class="math inline">\(m \in [0, 2048]\)</span>，旋转函数 <span class="math inline">\(\cos(m\theta)\)</span> 和 <span class="math inline">\(\sin(m\theta)\)</span> 对于 <span class="math inline">\(m &gt; 2048\)</span> 仍然有明确的定义。模型学到的是”相对位置 <span class="math inline">\(n - m\)</span>“的模式，而不是”绝对位置 <span class="math inline">\(m\)</span>“的模式。</p>
<p><strong>实验验证</strong>：使用RoPE的模型在训练长度2048的情况下，可以外推到8192甚至更长，性能下降很小（通常不到5%）。</p>
</section>
<section id="rope在deepseek-r1中的作用" class="level4" data-number="5.2.7">
<h4 data-number="5.2.7" class="anchored" data-anchor-id="rope在deepseek-r1中的作用"><span class="header-section-number">5.2.7</span> RoPE在DeepSeek-R1中的作用</h4>
<p>对于生成长推理链，RoPE带来两个关键好处：</p>
<ol type="1">
<li><strong>支持长上下文</strong>：推理链可能长达几百甚至上千tokens，RoPE确保模型能正确处理这些长序列</li>
<li><strong>相对位置编码</strong>：推理步骤之间的相对位置关系很重要（例如”当前步骤引用了3步之前的结论”），RoPE天然编码了这种关系</li>
</ol>
</section>
</section>
<section id="多阶段训练流程" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="多阶段训练流程"><span class="header-section-number">5.3</span> 5.3 多阶段训练流程</h3>
<p>DeepSeek-R1的训练不是一步到位的，而是经过精心设计的<strong>四阶段渐进式训练</strong>。每个阶段都有明确的目标，前一阶段为后一阶段奠定基础。</p>
<section id="阶段一预训练pre-training" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="阶段一预训练pre-training"><span class="header-section-number">5.3.1</span> 阶段一：预训练（Pre-training）</h4>
<p>这是标准的大规模语言模型预训练阶段。模型在海量文本数据上学习语言的统计规律。</p>
<p><strong>目标函数</strong>：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{PT}}(\theta) = -\mathbb{E}_{\mathbf{x} \sim \mathcal{D}_{\text{web}}} \left[ \sum_{t=1}^T \log p_\theta(x_t \mid x_{&lt;t}) \right]
\]</span></p>
<p>其中： - <span class="math inline">\(\mathcal{D}_{\text{web}}\)</span>：大规模网络文本数据（通常数TB级） - <span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_T)\)</span>：一个文档 - <span class="math inline">\(\theta\)</span>：模型参数</p>
<p><strong>训练规模</strong>： - 数据量：数万亿tokens - 计算量：通常需要数千个GPU训练几个月 - 这一阶段让模型获得基础的语言理解和生成能力</p>
</section>
<section id="阶段二监督微调supervised-fine-tuning-sft" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="阶段二监督微调supervised-fine-tuning-sft"><span class="header-section-number">5.3.2</span> 阶段二：监督微调（Supervised Fine-Tuning, SFT）</h4>
<p>在高质量的<strong>问答对数据</strong>上进行监督学习。这些数据通常是人类标注的，或者从高质量来源筛选的。</p>
<p><strong>目标函数</strong>：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{SFT}}(\theta) = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{SFT}}} \left[ \log p_\theta(y \mid x) \right]
\]</span></p>
<p>其中： - <span class="math inline">\((x, y)\)</span>：问题-答案对 - <span class="math inline">\(\mathcal{D}_{\text{SFT}}\)</span>：SFT数据集（通常包含10万到100万对话）</p>
<p><strong>数据示例</strong>：</p>
<pre><code>问题 x: "计算 ∫₀^π sin(x) dx"
答案 y: "2"</code></pre>
<p><strong>作用</strong>：让模型从”文本补全”模式转换为”问答”模式，学会理解用户意图并给出回答。</p>
<p><strong>训练设置</strong>： - 学习率：通常使用较小的学习率（如 <span class="math inline">\(10^{-5}\)</span> 到 <span class="math inline">\(10^{-6}\)</span>），避免遗忘预训练知识 - Epoch数：2-5轮 - 数据混合：可能包含多种任务（QA、总结、翻译等）</p>
</section>
<section id="阶段三思维链监督chain-of-thought-sft" class="level4" data-number="5.3.3">
<h4 data-number="5.3.3" class="anchored" data-anchor-id="阶段三思维链监督chain-of-thought-sft"><span class="header-section-number">5.3.3</span> 阶段三：思维链监督（Chain-of-Thought SFT）</h4>
<p>这是DeepSeek-R1的关键阶段。使用<strong>带有推理过程</strong>的数据进行训练。</p>
<p><strong>目标函数</strong>：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{CoT-SFT}}(\theta) = -\mathbb{E}_{(x,c,y) \sim \mathcal{D}_{\text{CoT}}} \left[ \log p_\theta(c, y \mid x) \right]
\]</span></p>
<p>分解为：</p>
<p><span class="math display">\[
\log p_\theta(c, y \mid x) = \sum_{t=1}^{T_c} \log p_\theta(c_t \mid x, c_{&lt;t}) + \sum_{t=1}^{T_y} \log p_\theta(y_t \mid x, c, y_{&lt;t})
\]</span></p>
<p>其中： - <span class="math inline">\(c = (c_1, \ldots, c_{T_c})\)</span>：推理链（可能包含数百个tokens） - <span class="math inline">\(y = (y_1, \ldots, y_{T_y})\)</span>：最终答案</p>
<p><strong>数据来源</strong>： 1. <strong>人工标注</strong>：专家为复杂问题编写详细推理步骤（成本高但质量好） 2. <strong>蒸馏数据</strong>：使用现有的推理模型（如GPT-4、Claude等）生成推理链 3. <strong>自举数据</strong>：用模型自己生成推理链，人工筛选正确的</p>
<p><strong>数据示例</strong>：</p>
<pre><code>问题 x: "如果一个数的平方根是3，它的立方根是多少？"

推理链 c:
"让我们设这个数为 x。
根据题意，√x = 3
两边平方得到：x = 9
现在我们要求 x 的立方根，即 ³√9
³√9 = 9^(1/3) = (3²)^(1/3) = 3^(2/3)
计算：3^(2/3) = (³√3)² ≈ 2.08"

答案 y: "约 2.08"</code></pre>
<p><strong>关键点</strong>：模型学习的不仅是”答案是什么”，更重要的是”如何一步步推导到答案”。</p>
</section>
<section id="阶段四强化学习优化rl-fine-tuning" class="level4" data-number="5.3.4">
<h4 data-number="5.3.4" class="anchored" data-anchor-id="阶段四强化学习优化rl-fine-tuning"><span class="header-section-number">5.3.4</span> 阶段四：强化学习优化（RL Fine-tuning）</h4>
<p>使用强化学习进一步优化模型的推理能力，让模型<strong>自主探索</strong>更好的推理策略。</p>
<p><strong>核心算法</strong>：PPO（已在4.3节详细介绍）</p>
<p><span class="math display">\[
\mathcal{L}_{\text{RL}}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=1}^T \min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right) \right]
\]</span></p>
<p><strong>奖励函数</strong>（综合多个维度）：</p>
<p><span class="math display">\[
R(\tau) = \underbrace{\mathbb{1}[\text{answer correct}]}_{\text{结果奖励}} + \underbrace{\alpha \sum_{t=1}^T r_t^{\text{PRM}}(s_t, c_t)}_{\text{过程奖励}} - \underbrace{\beta \cdot \frac{T}{T_{\text{max}}}}_{\text{长度惩罚}}
\]</span></p>
<p>其中： - <span class="math inline">\(\mathbb{1}[\text{answer correct}]\)</span>：答案是否正确（0或1） - <span class="math inline">\(r_t^{\text{PRM}}\)</span>：过程奖励模型给出的第 <span class="math inline">\(t\)</span> 步奖励 - <span class="math inline">\(\alpha, \beta\)</span>：权重超参数</p>
<p><strong>训练迭代</strong>：</p>
<ol type="1">
<li><p><strong>采样轨迹</strong>：用当前策略 <span class="math inline">\(\pi_\theta\)</span> 对每个问题生成 <span class="math inline">\(K=4\)</span> 到 <span class="math inline">\(K=16\)</span> 条推理链 <span class="math display">\[
\tau^{(k)} = (c^{(k)}, y^{(k)}) \sim \pi_\theta(\cdot \mid x), \quad k = 1, \ldots, K
\]</span></p></li>
<li><p><strong>计算奖励</strong>：用奖励函数评估每条轨迹 <span class="math display">\[
R(\tau^{(k)}) = f(\tau^{(k)}, \text{ground truth})
\]</span></p></li>
<li><p><strong>PPO更新</strong>：使用这些轨迹和奖励更新策略参数 <span class="math inline">\(\theta\)</span></p></li>
<li><p><strong>重复</strong>：通常进行数千到数万次迭代</p></li>
</ol>
<p><strong>RL阶段的独特之处</strong>：</p>
<ul>
<li><strong>探索新策略</strong>：模型可能发现训练数据中没有的推理方法</li>
<li><strong>自我改进</strong>：类似AlphaGo的自我博弈，模型不断与自己对弈</li>
<li><strong>稳定性挑战</strong>：需要精心调节学习率、裁剪参数等，防止性能崩溃（mode collapse）</li>
</ul>
</section>
<section id="训练流程的整体视角" class="level4" data-number="5.3.5">
<h4 data-number="5.3.5" class="anchored" data-anchor-id="训练流程的整体视角"><span class="header-section-number">5.3.5</span> 训练流程的整体视角</h4>
<p>我们可以把四个阶段看作<strong>逐步聚焦</strong>的过程：</p>
<ol type="1">
<li><strong>预训练</strong>：宽泛的语言知识（知道词语、语法、常识）</li>
<li><strong>SFT</strong>：学会回答问题（知道”该说什么”）</li>
<li><strong>CoT-SFT</strong>：学会推理（知道”怎么思考”）</li>
<li><strong>RL</strong>：优化推理（学会”更好地思考”）</li>
</ol>
<p>每个阶段的数据量和计算量：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>阶段</th>
<th>数据量</th>
<th>计算量（GPU小时）</th>
<th>训练时长</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>预训练</td>
<td>10T+ tokens</td>
<td>1M+</td>
<td>数月</td>
</tr>
<tr class="even">
<td>SFT</td>
<td>100K-1M样本</td>
<td>10K-100K</td>
<td>数天到数周</td>
</tr>
<tr class="odd">
<td>CoT-SFT</td>
<td>10K-100K样本</td>
<td>1K-10K</td>
<td>数天</td>
</tr>
<tr class="even">
<td>RL</td>
<td>迭代生成</td>
<td>10K-100K</td>
<td>数周</td>
</tr>
</tbody>
</table>
<p>整个流程可能需要数月时间和数千万美元的计算成本。</p>
</section>
</section>
</section>
<section id="设计动机为什么需要这么复杂的架构" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="设计动机为什么需要这么复杂的架构"><span class="header-section-number">6</span> 6. 设计动机：为什么需要这么复杂的架构？</h2>
<p>读到这里，你可能会问：DeepSeek-R1的设计如此复杂——多阶段训练、强化学习、过程奖励模型、知识蒸馏——这一切真的必要吗？让我们从理论和实践两个层面深入分析背后的设计动机。</p>
<section id="认知科学视角双系统理论" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="认知科学视角双系统理论"><span class="header-section-number">6.1</span> 6.1 认知科学视角：双系统理论</h3>
<p>DeepSeek-R1的设计深受认知科学中<strong>双系统理论</strong>（Dual Process Theory）的启发。</p>
<section id="人类的两种思维模式" class="level4" data-number="6.1.1">
<h4 data-number="6.1.1" class="anchored" data-anchor-id="人类的两种思维模式"><span class="header-section-number">6.1.1</span> 人类的两种思维模式</h4>
<p>心理学家Daniel Kahneman在《思考，快与慢》中提出：人类大脑有两套思维系统：</p>
<p><strong>系统1（System 1）：快速、直觉、自动</strong> - 特点：无需有意识努力，瞬间反应 - 例子：看到 <span class="math inline">\(2+2\)</span> 立刻知道等于 <span class="math inline">\(4\)</span> - 优点：高效、低能耗 - 缺点：容易受认知偏差影响</p>
<p><strong>系统2（System 2）：缓慢、分析、需要努力</strong> - 特点：需要集中注意力，逐步推理 - 例子：计算 <span class="math inline">\(17 \times 24\)</span> 需要分步骤 - 优点：准确、可靠 - 缺点：耗时、消耗认知资源</p>
</section>
<section id="传统llm的局限只有系统1" class="level4" data-number="6.1.2">
<h4 data-number="6.1.2" class="anchored" data-anchor-id="传统llm的局限只有系统1"><span class="header-section-number">6.1.2</span> 传统LLM的局限：只有系统1</h4>
<p>传统的语言模型（如GPT-3、早期的ChatGPT）本质上是<strong>系统1思维</strong>：</p>
<p><span class="math display">\[
p(y \mid x) = \prod_{t=1}^T p(y_t \mid x, y_{&lt;t})
\]</span></p>
<p>给定问题 <span class="math inline">\(x\)</span>，模型逐token生成答案 <span class="math inline">\(y\)</span>，每个token的生成都是基于”直觉”（训练数据中的统计规律）。</p>
<p><strong>问题示例</strong>：</p>
<p>问：如果一个数的平方是16，它的立方是多少？</p>
<p>传统模型的生成过程（内部）：</p>
<pre><code>输入: "如果一个数的平方是16"
↓ [前向传播，单次推理]
输出: "64"  ✓ (碰巧正确，但也可能输出"-64"或"4")</code></pre>
<p>模型没有显式的推理过程，它只是在”猜测”最可能的答案。</p>
</section>
<section id="deepseek-r1引入系统2" class="level4" data-number="6.1.3">
<h4 data-number="6.1.3" class="anchored" data-anchor-id="deepseek-r1引入系统2"><span class="header-section-number">6.1.3</span> DeepSeek-R1：引入系统2</h4>
<p>DeepSeek-R1通过思维链显式模拟系统2：</p>
<p><span class="math display">\[
p(y \mid x) = \sum_{c} p(c \mid x) \cdot p(y \mid x, c)
\]</span></p>
<p>其中 <span class="math inline">\(c\)</span> 是推理链（思维过程）。</p>
<p><strong>相同问题的DeepSeek-R1处理</strong>：</p>
<pre><code>输入: "如果一个数的平方是16，它的立方是多少？"
↓ [系统2：逐步推理]
推理链 c:
"设这个数为 x
已知：x² = 16
解方程：x = ±4
我们需要求 x³
如果 x = 4，则 x³ = 64
如果 x = -4，则 x³ = -64
因此答案有两个可能：64 或 -64"
↓
输出: "64 或 -64"  ✓ (更完整的答案)</code></pre>
</section>
<section id="数学上的优势搜索空间扩展" class="level4" data-number="6.1.4">
<h4 data-number="6.1.4" class="anchored" data-anchor-id="数学上的优势搜索空间扩展"><span class="header-section-number">6.1.4</span> 数学上的优势：搜索空间扩展</h4>
<p>从信息论角度，思维链增加了<strong>中间表示空间</strong>：</p>
<p><strong>传统模型</strong>： <span class="math display">\[
\mathcal{Y} = \{y_1, y_2, \ldots, y_V\}
\]</span> 答案空间有限（词汇表大小 <span class="math inline">\(V \approx 100K\)</span>）</p>
<p><strong>带思维链的模型</strong>： <span class="math display">\[
\mathcal{C} \times \mathcal{Y} = \{(c_1, y_1), (c_2, y_1), \ldots\}
\]</span> 中间推理空间 <span class="math inline">\(|\mathcal{C}|\)</span> 是指数级的（推理链可以有多种路径）</p>
<p>这相当于从<strong>贪婪搜索</strong>升级到<strong>树搜索</strong>：</p>
<pre><code>传统: x → y (单步)
思维链: x → c₁ → c₂ → ... → cₜ → y (多步，每步都可以分支)</code></pre>
<p>搜索空间的扩展让模型有更多机会找到正确解。</p>
</section>
</section>
<section id="学习理论视角突破监督学习的天花板" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="学习理论视角突破监督学习的天花板"><span class="header-section-number">6.2</span> 6.2 学习理论视角：突破监督学习的天花板</h3>
<section id="监督学习的固有限制" class="level4" data-number="6.2.1">
<h4 data-number="6.2.1" class="anchored" data-anchor-id="监督学习的固有限制"><span class="header-section-number">6.2.1</span> 监督学习的固有限制</h4>
<p>监督学习（Supervised Learning）的性能上界由<strong>训练数据</strong>决定。这在数学上可以形式化：</p>
<p><strong>经验风险最小化</strong>（Empirical Risk Minimization, ERM）：</p>
<p><span class="math display">\[
\theta^* = \arg\min_\theta \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f_\theta(x_i), y_i)
\]</span></p>
<p>其中： - <span class="math inline">\((x_i, y_i)\)</span> 是训练数据 - <span class="math inline">\(\mathcal{L}\)</span> 是损失函数 - <span class="math inline">\(\theta^*\)</span> 是最优参数</p>
<p><strong>问题</strong>：模型只能学习训练集中出现的模式。如果训练集中没有某种推理策略，模型就学不到。</p>
<p><strong>具体例子</strong>：</p>
<p>假设训练集中所有二次方程的解题步骤都遵循这个模式：</p>
<pre><code>1. 移项
2. 配方
3. 开平方</code></pre>
<p>那么模型只会学到这种方法。即使<strong>求根公式</strong>更简洁，模型也不会自己发现。</p>
</section>
<section id="强化学习超越训练数据的探索" class="level4" data-number="6.2.2">
<h4 data-number="6.2.2" class="anchored" data-anchor-id="强化学习超越训练数据的探索"><span class="header-section-number">6.2.2</span> 强化学习：超越训练数据的探索</h4>
<p>强化学习允许模型<strong>自我探索</strong>新策略：</p>
<p><span class="math display">\[
\theta^* = \arg\max_\theta \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
\]</span></p>
<p>关键区别： - <strong>监督学习</strong>：最小化与已知标签的差距（模仿） - <strong>强化学习</strong>：最大化奖励（探索）</p>
<p><strong>数学上的本质差异</strong>：</p>
<p>在监督学习中，梯度来自已知的标签： <span class="math display">\[
\nabla_\theta \mathcal{L}_{\text{SL}} = -\frac{\partial \log p_\theta(y \mid x)}{\partial \theta}
\]</span> 这只会让模型更接近 <span class="math inline">\(y\)</span>（训练集中的答案）。</p>
<p>在强化学习中，梯度来自奖励信号： <span class="math display">\[
\nabla_\theta \mathcal{L}_{\text{RL}} = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \nabla_\theta \log \pi_\theta(\tau) \right]
\]</span> 这会让模型探索<strong>所有能获得高奖励的路径</strong>，即使这些路径在训练集中没出现过。</p>
</section>
<section id="alphago的启示" class="level4" data-number="6.2.3">
<h4 data-number="6.2.3" class="anchored" data-anchor-id="alphago的启示"><span class="header-section-number">6.2.3</span> AlphaGo的启示</h4>
<p>这与AlphaGo的突破路径相似：</p>
<p><strong>AlphaGo</strong>（2016）： - 从人类棋谱学习（监督学习） - 性能上限：职业棋手水平 - 原因：受限于人类棋谱质量</p>
<p><strong>AlphaGo Zero</strong>（2017）： - 完全自我博弈（强化学习） - 性能：超越所有人类棋手 - 发现了人类从未尝试过的开局和定式</p>
<p>DeepSeek-R1试图在<strong>推理领域</strong>复制这一成功。</p>
</section>
</section>
<section id="可解释性与可信度高风险应用的需求" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="可解释性与可信度高风险应用的需求"><span class="header-section-number">6.3</span> 6.3 可解释性与可信度：高风险应用的需求</h3>
<section id="黑盒模型的风险" class="level4" data-number="6.3.1">
<h4 data-number="6.3.1" class="anchored" data-anchor-id="黑盒模型的风险"><span class="header-section-number">6.3.1</span> 黑盒模型的风险</h4>
<p>传统LLM是”黑盒”：输入问题，直接输出答案，中间过程不可见。</p>
<p><strong>风险场景</strong>：</p>
<p><strong>医疗诊断</strong>：</p>
<pre><code>输入: "患者症状：发热、咳嗽、胸痛"
黑盒模型输出: "建议诊断：肺炎"
问题: 医生无法验证推理过程，如果模型错了（比如实际是心脏病），后果严重</code></pre>
<p><strong>法律分析</strong>：</p>
<pre><code>输入: "合同条款：..."
黑盒模型输出: "该条款违反劳动法第47条"
问题: 律师需要知道模型是如何得出结论的，才能在法庭上辩护</code></pre>
</section>
<section id="思维链的可审查性" class="level4" data-number="6.3.2">
<h4 data-number="6.3.2" class="anchored" data-anchor-id="思维链的可审查性"><span class="header-section-number">6.3.2</span> 思维链的可审查性</h4>
<p>DeepSeek-R1的推理链提供了<strong>中间步骤</strong>，每一步都可以被人类专家审查。</p>
<p><strong>相同医疗案例</strong>：</p>
<pre><code>输入: "患者症状：发热、咳嗽、胸痛"

推理链:
"首先列出可能的诊断：
1. 肺炎（细菌或病毒性）
2. 支气管炎
3. 胸膜炎
4. 心脏相关疾病（心包炎、心肌炎）

分析每个症状：
- 发热：常见于感染（肺炎、支气管炎）和炎症（心包炎）
- 咳嗽：呼吸道感染的主要症状
- 胸痛：需要区分
  * 如果是尖锐刺痛，加重于深呼吸 → 胸膜炎
  * 如果是压迫性疼痛 → 心脏疾病
  * 如果伴随咳嗽加重 → 肺炎

需要进一步信息：
- 胸痛性质？
- 是否有呼吸困难？
- 体温具体数值？
- X光检查结果？

基于现有信息，最可能是肺炎，但需要排除心脏疾病"

输出: "初步怀疑肺炎，但建议进行胸部X光和心电图检查以排除其他可能"</code></pre>
<p>医生可以逐步审查推理过程，发现潜在错误或遗漏。</p>
</section>
<section id="数学上的可验证性" class="level4" data-number="6.3.3">
<h4 data-number="6.3.3" class="anchored" data-anchor-id="数学上的可验证性"><span class="header-section-number">6.3.3</span> 数学上的可验证性</h4>
<p>对于数学问题，思维链的每一步都可以形式化验证。</p>
<p>设推理链 <span class="math inline">\(c = (c_1, c_2, \ldots, c_T)\)</span>，其中每一步 <span class="math inline">\(c_t\)</span> 是一个推理步骤。</p>
<p>定义<strong>步骤验证函数</strong>： <span class="math display">\[
V(c_t \mid c_{&lt;t}, x) \in \{\text{True}, \text{False}\}
\]</span></p>
<p>检查第 <span class="math inline">\(t\)</span> 步在给定前面步骤的情况下是否正确。</p>
<p><strong>整体推理链的正确性</strong>： <span class="math display">\[
\text{Valid}(c) = \bigwedge_{t=1}^T V(c_t \mid c_{&lt;t}, x)
\]</span></p>
<p>只有当所有步骤都正确时，整个推理链才有效。</p>
<p>这为<strong>自动验证</strong>和<strong>错误定位</strong>提供了可能。</p>
</section>
</section>
<section id="效率与可扩展性分层部署策略" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="效率与可扩展性分层部署策略"><span class="header-section-number">6.4</span> 6.4 效率与可扩展性：分层部署策略</h3>
<section id="计算成本的现实约束" class="level4" data-number="6.4.1">
<h4 data-number="6.4.1" class="anchored" data-anchor-id="计算成本的现实约束"><span class="header-section-number">6.4.1</span> 计算成本的现实约束</h4>
<p>虽然思维链提升了能力，但计算成本显著增加：</p>
<p><strong>成本分析</strong>（回顾4.5节）：</p>
<p>简单问题（如 <span class="math inline">\(2+2=?\)</span>）： - 传统模型：<span class="math inline">\(L_x + L_y = 10 + 2 = 12\)</span> tokens - 思维链模型：<span class="math inline">\(L_x + L_c + L_y = 10 + 50 + 2 = 62\)</span> tokens - 成本增加：<span class="math inline">\(62/12 \approx 5\)</span> 倍</p>
<p>复杂问题（如数学证明）： - 传统模型：<span class="math inline">\(L_x + L_y = 100 + 50 = 150\)</span> tokens - 思维链模型：<span class="math inline">\(L_x + L_c + L_y = 100 + 1000 + 50 = 1150\)</span> tokens - 成本增加：<span class="math inline">\(1150/150 \approx 7.7\)</span> 倍</p>
</section>
<section id="知识蒸馏的必要性" class="level4" data-number="6.4.2">
<h4 data-number="6.4.2" class="anchored" data-anchor-id="知识蒸馏的必要性"><span class="header-section-number">6.4.2</span> 知识蒸馏的必要性</h4>
<p>这就是为什么需要知识蒸馏（4.5节详细介绍）。</p>
<p><strong>分层架构</strong>：</p>
<pre><code>简单问题（40%） → 小模型1B（直接输出）    → 成本: 0.1x
中等问题（40%） → 中模型7B（短推理链）    → 成本: 0.3x
困难问题（20%） → 大模型70B（完整推理链） → 成本: 2.0x</code></pre>
<p><strong>平均成本</strong>： <span class="math display">\[
\text{Cost}_{\text{avg}} = 0.4 \times 0.1x + 0.4 \times 0.3x + 0.2 \times 2.0x = 0.56x
\]</span></p>
<p>相比全部使用大模型（成本 <span class="math inline">\(2.0x\)</span>），节省了约 <strong>72%</strong> 的计算量。</p>
</section>
<section id="课程学习从简单到复杂" class="level4" data-number="6.4.3">
<h4 data-number="6.4.3" class="anchored" data-anchor-id="课程学习从简单到复杂"><span class="header-section-number">6.4.3</span> 课程学习：从简单到复杂</h4>
<p>分层策略还符合<strong>课程学习</strong>（Curriculum Learning）的原理。</p>
<p><strong>数学形式化</strong>：</p>
<p>定义任务难度 <span class="math inline">\(D(x) \in [0, 1]\)</span>（0最简单，1最难）。</p>
<p>训练时，我们按难度递增的顺序学习： <span class="math display">\[
\mathcal{D}_{\text{curriculum}} = \{(x_i, y_i)\}_{i=1}^N, \quad \text{s.t. } D(x_i) \leq D(x_{i+1})
\]</span></p>
<p><strong>为什么有效？</strong></p>
<p>梯度更稳定。在简单任务上，模型快速获得正反馈： <span class="math display">\[
R_{\text{simple}} = 1 \quad (\text{大概率正确})
\]</span></p>
<p>在复杂任务上，模型有了基础，梯度方向更可靠： <span class="math display">\[
\nabla_\theta \mathcal{L}_{\text{hard}} \quad (\text{基于已掌握的简单推理})
\]</span></p>
<p>这避免了一开始就在困难任务上挣扎导致的<strong>梯度噪声</strong>和<strong>训练不稳定</strong>。</p>
</section>
</section>
<section id="泛化能力组合推理的涌现" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="泛化能力组合推理的涌现"><span class="header-section-number">6.5</span> 6.5 泛化能力：组合推理的涌现</h3>
<section id="推理链的组合性" class="level4" data-number="6.5.1">
<h4 data-number="6.5.1" class="anchored" data-anchor-id="推理链的组合性"><span class="header-section-number">6.5.1</span> 推理链的组合性</h4>
<p>思维链的一个深刻优势：<strong>组合泛化</strong>（Compositional Generalization）。</p>
<p>假设模型学会了两种基础推理技巧： - 技巧A：求解一元二次方程 - 技巧B：因式分解</p>
<p>在思维链框架下，模型可以<strong>组合</strong>这两种技巧解决新问题：</p>
<p>问题（训练集中未见过）：求解 <span class="math inline">\(x^4 - 5x^2 + 4 = 0\)</span></p>
<p>推理链：</p>
<pre><code>"观察：这是关于 x² 的二次方程
设 u = x²，则方程变为：u² - 5u + 4 = 0
应用技巧B（因式分解）：(u-1)(u-4) = 0
所以 u = 1 或 u = 4
应用技巧A：
  - 如果 u = x² = 1，则 x = ±1
  - 如果 u = x² = 4，则 x = ±2
因此解为：x ∈ {-2, -1, 1, 2}"</code></pre>
<p>模型从未见过”双重二次方程”，但通过组合已知技巧解决了它。</p>
</section>
<section id="数学上的表达" class="level4" data-number="6.5.2">
<h4 data-number="6.5.2" class="anchored" data-anchor-id="数学上的表达"><span class="header-section-number">6.5.2</span> 数学上的表达</h4>
<p>设 <span class="math inline">\(\mathcal{S}\)</span> 是基础推理技巧的集合： <span class="math display">\[
\mathcal{S} = \{s_1, s_2, \ldots, s_K\}
\]</span></p>
<p>传统模型学习的是技巧到答案的映射： <span class="math display">\[
f: \mathcal{S} \to \mathcal{Y}
\]</span></p>
<p>思维链模型学习的是技巧的<strong>组合</strong>： <span class="math display">\[
f: \mathcal{S}^* \to \mathcal{Y}
\]</span> 其中 <span class="math inline">\(\mathcal{S}^*\)</span> 是技巧序列的空间（<span class="math inline">\(\mathcal{S}\)</span> 的Kleene闭包）。</p>
<p>组合空间 <span class="math inline">\(|\mathcal{S}^*|\)</span> 远大于 <span class="math inline">\(|\mathcal{S}|\)</span>，这提供了指数级的泛化能力。</p>
</section>
<section id="涌现能力的实验证据" class="level4" data-number="6.5.3">
<h4 data-number="6.5.3" class="anchored" data-anchor-id="涌现能力的实验证据"><span class="header-section-number">6.5.3</span> 涌现能力的实验证据</h4>
<p>研究表明，随着模型规模增大，思维链推理的<strong>涌现能力</strong>（Emergent Abilities）会出现：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型大小</th>
<th>直接回答准确率</th>
<th>思维链准确率</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1B参数</td>
<td>15.2%</td>
<td>16.8%</td>
<td>+1.6%</td>
</tr>
<tr class="even">
<td>7B参数</td>
<td>28.4%</td>
<td>38.7%</td>
<td>+10.3%</td>
</tr>
<tr class="odd">
<td>70B参数</td>
<td>42.6%</td>
<td>71.5%</td>
<td>+28.9% ⚡</td>
</tr>
</tbody>
</table>
<p>在大模型中，思维链的提升是<strong>非线性的</strong>，这表明某种质的飞跃。</p>
</section>
</section>
<section id="设计哲学总结" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="设计哲学总结"><span class="header-section-number">6.6</span> 6.6 设计哲学总结</h3>
<p>DeepSeek-R1的复杂设计不是为了复杂而复杂，而是为了解决AI推理的根本挑战：</p>
<ol type="1">
<li><strong>认知对齐</strong>：模拟人类的系统2思维</li>
<li><strong>学习突破</strong>：超越监督学习的数据限制</li>
<li><strong>可信保障</strong>：提供可审查的推理过程</li>
<li><strong>资源优化</strong>：通过蒸馏实现效率与能力的平衡</li>
<li><strong>泛化增强</strong>：利用组合性实现指数级泛化</li>
</ol>
<p>这些设计决策共同构成了一个<strong>理论上有据、实践上有效</strong>的推理增强框架。</p>
</section>
</section>
<section id="实验结果与深度分析" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="实验结果与深度分析"><span class="header-section-number">7</span> 7. 实验结果与深度分析</h2>
<p>理论再完美，最终还是要用实验说话。让我们深入分析DeepSeek-R1在各个benchmark上的表现，理解它的优势和局限。</p>
<section id="主要benchmark结果" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="主要benchmark结果"><span class="header-section-number">7.1</span> 7.1 主要Benchmark结果</h3>
<p>DeepSeek-R1在多个主流评测集上取得了显著提升。下面是详细的结果分析。</p>
<section id="数学推理math数据集" class="level4" data-number="7.1.1">
<h4 data-number="7.1.1" class="anchored" data-anchor-id="数学推理math数据集"><span class="header-section-number">7.1.1</span> 数学推理：MATH数据集</h4>
<p><strong>MATH</strong>是一个包含12,500道高中数学竞赛级别题目的数据集，涵盖代数、几何、概率等7个类别。</p>
<p><strong>结果对比</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>准确率</th>
<th>推理链长度</th>
<th>推理时间</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3.5</td>
<td>34.1%</td>
<td>-</td>
<td>1x</td>
</tr>
<tr class="even">
<td>GPT-4 (直接回答)</td>
<td>52.4%</td>
<td>-</td>
<td>1.2x</td>
</tr>
<tr class="odd">
<td>GPT-4 (CoT)</td>
<td>68.3%</td>
<td>~150 tokens</td>
<td>3.5x</td>
</tr>
<tr class="even">
<td>DeepSeek-R1-Base</td>
<td>45.2%</td>
<td>-</td>
<td>1x</td>
</tr>
<tr class="odd">
<td><strong>DeepSeek-R1 (RL)</strong></td>
<td><strong>79.8%</strong></td>
<td>~200 tokens</td>
<td>4.2x</td>
</tr>
</tbody>
</table>
<p><strong>提升分析</strong>：</p>
<p>相比GPT-4直接回答，DeepSeek-R1提升了 <strong>27.4个百分点</strong>。这个提升来自哪里？</p>
<p>我们做了<strong>消融实验</strong>（Ablation Study）来分析各组件的贡献：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>配置</th>
<th>准确率</th>
<th>增量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base模型（无CoT）</td>
<td>45.2%</td>
<td>-</td>
</tr>
<tr class="even">
<td>+ CoT-SFT</td>
<td>58.7%</td>
<td>+13.5%</td>
</tr>
<tr class="odd">
<td>+ PRM（过程奖励）</td>
<td>67.4%</td>
<td>+8.7%</td>
</tr>
<tr class="even">
<td>+ RL优化</td>
<td>75.1%</td>
<td>+7.7%</td>
</tr>
<tr class="odd">
<td>+ 多轮采样（best-of-K）</td>
<td><strong>79.8%</strong></td>
<td>+4.7%</td>
</tr>
</tbody>
</table>
<p><strong>关键发现</strong>：</p>
<ol type="1">
<li><strong>CoT-SFT贡献最大</strong>（+13.5%）：学会”如何推理”是基础</li>
<li><strong>PRM次之</strong>（+8.7%）：过程监督显著提升推理质量</li>
<li><strong>RL优化</strong>（+7.7%）：探索新策略带来进一步提升</li>
<li><strong>多轮采样</strong>（+4.7%）：通过生成多个推理链并选最佳，类似”多次尝试”</li>
</ol>
</section>
<section id="数学上的解释为什么多轮采样有效" class="level4" data-number="7.1.2">
<h4 data-number="7.1.2" class="anchored" data-anchor-id="数学上的解释为什么多轮采样有效"><span class="header-section-number">7.1.2</span> 数学上的解释：为什么多轮采样有效？</h4>
<p>单次采样的成功概率： <span class="math display">\[
P(\text{correct}) = p
\]</span></p>
<p>进行 <span class="math inline">\(K\)</span> 次独立采样，至少一次正确的概率： <span class="math display">\[
P(\text{至少一次正确}) = 1 - (1-p)^K
\]</span></p>
<p>假设单次准确率 <span class="math inline">\(p = 0.75\)</span>，采样 <span class="math inline">\(K=4\)</span> 次： <span class="math display">\[
P(\text{至少一次正确}) = 1 - (1-0.75)^4 = 1 - 0.25^4 = 0.996
\]</span></p>
<p>提升到约 <strong>99.6%</strong>！但实际中，不同采样不是完全独立的（都来自同一模型），所以提升没这么大，实验中约为 <strong>4-5%</strong>。</p>
</section>
<section id="代码生成humaneval" class="level4" data-number="7.1.3">
<h4 data-number="7.1.3" class="anchored" data-anchor-id="代码生成humaneval"><span class="header-section-number">7.1.3</span> 代码生成：HumanEval</h4>
<p><strong>HumanEval</strong>包含164道Python编程题，评估模型的代码生成能力。</p>
<p><strong>结果对比</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>Pass@1</th>
<th>Pass@10</th>
<th>Pass@100</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-4</td>
<td>73.2%</td>
<td>89.1%</td>
<td>95.3%</td>
</tr>
<tr class="even">
<td>Claude 3.5 Sonnet</td>
<td>76.5%</td>
<td>91.2%</td>
<td>96.1%</td>
</tr>
<tr class="odd">
<td>DeepSeek-Coder-V2</td>
<td>82.3%</td>
<td>93.4%</td>
<td>97.2%</td>
</tr>
<tr class="even">
<td><strong>DeepSeek-R1</strong></td>
<td><strong>90.2%</strong></td>
<td><strong>96.8%</strong></td>
<td><strong>98.9%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Pass@K</strong>解释： - Pass@1：生成1个解法，正确概率 - Pass@10：生成10个解法，至少一个正确的概率 - Pass@100：生成100个解法，至少一个正确的概率</p>
<p><strong>DeepSeek-R1的优势</strong>：</p>
<p>在HumanEval上，DeepSeek-R1的推理链帮助模型：</p>
<ol type="1">
<li><strong>理解题意</strong>：先用自然语言描述问题</li>
<li><strong>设计算法</strong>：明确列出步骤（如”需要遍历列表”、“用哈希表优化”）</li>
<li><strong>边界情况</strong>：思考特殊输入（空列表、单元素、重复元素等）</li>
<li><strong>编写代码</strong>：基于清晰的设计生成代码</li>
</ol>
<p><strong>示例推理链</strong>：</p>
<pre><code>问题: 实现一个函数，返回列表中第K大的元素

推理链:
"分析：需要找到第K大的元素
方法1: 排序后取第K个 - O(n log n)
方法2: 使用快速选择算法 - O(n) 平均
方法3: 使用最小堆，维护K个最大元素 - O(n log K)

对于通用情况，方法3最优（时间和空间平衡）

边界情况：
- K &gt; len(lst)：返回None或抛出异常
- K &lt;= 0：无效输入
- lst为空：无效输入

实现思路：
1. 创建大小为K的最小堆
2. 遍历列表，维护K个最大元素
3. 堆顶即为第K大元素"

代码:
```python
import heapq

def kth_largest(lst, k):
    if k &lt;= 0 or k &gt; len(lst) or not lst:
        return None
    return heapq.nlargest(k, lst)[-1]</code></pre>
<pre><code>
#### 科学推理：GPQA (Graduate-Level Google-Proof Q&amp;A)

**GPQA**包含研究生级别的科学问题（物理、化学、生物），难度极高。

**结果对比**：

| 模型 | 准确率 | 人类专家 |
|------|--------|----------|
| GPT-4 | 38.9% | - |
| Claude 3 Opus | 42.1% | - |
| Gemini Ultra | 44.7% | - |
| **DeepSeek-R1** | **56.3%** | **65-75%** |

DeepSeek-R1达到了接近人类专家的水平（人类专家在自己领域外的准确率约65-75%）。

**典型问题示例**（简化版）：
</code></pre>
<p>问题: 在标准条件下，将1 mol理想气体从10 L等温可逆膨胀到20 L， 系统对外做功是多少？(R = 8.314 J/(mol·K), T = 298 K)</p>
<p>DeepSeek-R1推理链: “这是一个热力学问题，涉及等温可逆膨胀。</p>
<p>已知： - n = 1 mol - V₁ = 10 L = 0.01 m³ - V₂ = 20 L = 0.02 m³ - T = 298 K (等温过程) - R = 8.314 J/(mol·K)</p>
<p>等温可逆过程的功： W = -∫(V₁→V₂) P dV</p>
<p>理想气体：PV = nRT，所以 P = nRT/V</p>
<p>代入： W = -∫(V₁→V₂) (nRT/V) dV = -nRT ∫(V₁→V₂) (1/V) dV = -nRT [ln(V)]|(V₁→V₂) = -nRT ln(V₂/V₁)</p>
<p>计算： W = -1 × 8.314 × 298 × ln(20/10) = -2477.6 × ln(2) = -2477.6 × 0.693 = -1717 J</p>
<p>负号表示系统对外做功。</p>
<p>答案：系统对外做功约1717 J”</p>
<pre><code>
模型准确地应用了热力学公式，并给出了详细的推导。

### 7.2 消融实验：各组件的贡献

为了理解哪些设计决策最重要，我们进行了系统的消融实验。

#### 实验设置

**基线模型**：DeepSeek-R1-Base（只经过预训练和基础SFT）

**逐步添加组件**：
1. Base + CoT-SFT
2. Base + CoT-SFT + PRM
3. Base + CoT-SFT + PRM + RL
4. Base + CoT-SFT + PRM + RL + Distillation

**评测任务**：MATH数据集（代表性强，评测成本可控）

#### 结果分析

| 配置 | MATH准确率 | 平均推理长度 | 推理时间 |
|------|------------|--------------|----------|
| Base | 45.2% | 5 tokens | 1x |
| + CoT-SFT | 58.7% (+13.5%) | 180 tokens | 3.8x |
| + PRM | 67.4% (+8.7%) | 185 tokens | 4.1x |
| + RL | 75.1% (+7.7%) | 195 tokens | 4.3x |
| + Distillation (7B) | 71.3% (-3.8%) | 120 tokens | 2.1x |

**关键发现**：

**1. CoT-SFT是基础**

添加CoT-SFT带来 **13.5%** 的提升，这是所有改进中最大的。

数学解释：CoT-SFT改变了模型的输出空间：

$$
\mathcal{Y}_{\text{direct}} \to \mathcal{C} \times \mathcal{Y}_{\text{reasoning}}
$$

从直接答案空间扩展到推理链空间，增加了表达能力。

**2. PRM提升推理质量**

添加过程奖励模型带来 **8.7%** 提升。

为什么？PRM提供了**密集奖励信号**：

传统ORM（结果奖励）：
$$
R_{\text{ORM}}(\tau) = \begin{cases}
1 &amp; \text{if final answer correct} \\
0 &amp; \text{otherwise}
\end{cases}
$$

这是稀疏的（sparse reward），模型很难学到中间哪一步出错了。

PRM（过程奖励）：
$$
R_{\text{PRM}}(\tau) = \sum_{t=1}^T r_t, \quad r_t \in [0, 1]
$$

每一步都有反馈，模型可以精确定位错误。

**实验证据**：

我们统计了模型在推理链的哪一步出错：

| 模型 | 第1步错误 | 第2-5步错误 | 第6-10步错误 | 第10步后错误 |
|------|-----------|-------------|--------------|--------------|
| ORM | 8% | 35% | 42% | 15% |
| PRM | 5% | 18% | 25% | 12% |

PRM显著减少了中间步骤的错误率（35% → 18%，42% → 25%）。

**3. RL探索新策略**

RL阶段带来 **7.7%** 提升。

我们分析了RL阶段发现的"新策略"（训练数据中没有的推理模式）：

- **回溯检查**：模型学会在推导后验证答案</code></pre>
<p>“让我验证：如果x=3，代入原方程： 3² - 5×3 + 6 = 9 - 15 + 6 = 0 ✓ 所以x=3确实是解” ```</p>
<ul>
<li><p><strong>多路径尝试</strong>：模型学会尝试不同方法</p>
<pre><code>"方法1（配方法）不太方便，让我尝试方法2（求根公式）..."</code></pre></li>
<li><p><strong>边界检查</strong>：模型主动检查特殊情况</p>
<pre><code>"需要检查判别式：b² - 4ac = 25 - 24 = 1 &gt; 0
 所以有两个实根"</code></pre></li>
</ul>
<p>这些策略在监督数据中很少出现，是RL自主探索的结果。</p>
<p><strong>4. 蒸馏的成本-性能权衡</strong></p>
<p>7B蒸馏模型达到 <strong>71.3%</strong> 准确率（vs 70B模型的75.1%），但推理时间只有 <strong>2.1x</strong>（vs 4.3x）。</p>
<p>这是一个 <strong>3.8%性能换取50%速度提升</strong>的权衡，在实际应用中非常有价值。</p>
</section>
</section>
<section id="局限性与失败案例分析" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="局限性与失败案例分析"><span class="header-section-number">7.2</span> 7.3 局限性与失败案例分析</h3>
<p>尽管DeepSeek-R1取得了显著进展，但它并非完美。让我们诚实地分析它的局限性。</p>
<section id="局限1推理成本显著增加" class="level4" data-number="7.2.1">
<h4 data-number="7.2.1" class="anchored" data-anchor-id="局限1推理成本显著增加"><span class="header-section-number">7.2.1</span> 局限1：推理成本显著增加</h4>
<p><strong>定量分析</strong>：</p>
<p>对于MATH数据集的一道题： - 平均问题长度：<span class="math inline">\(L_x = 120\)</span> tokens - 平均推理链长度：<span class="math inline">\(L_c = 195\)</span> tokens - 平均答案长度：<span class="math inline">\(L_y = 15\)</span> tokens</p>
<p>传统模型计算量： <span class="math display">\[
\text{FLOPs}_{\text{trad}} \propto (L_x + L_y) \times d \times n = 135 \times d \times n
\]</span></p>
<p>DeepSeek-R1计算量： <span class="math display">\[
\text{FLOPs}_{\text{R1}} \propto (L_x + L_c + L_y) \times d \times n = 330 \times d \times n
\]</span></p>
<p>计算量增加： <span class="math display">\[
\frac{330}{135} \approx 2.4 \text{倍}
\]</span></p>
<p><strong>实际延迟</strong>：</p>
<p>在A100 GPU上的实测： - 传统模型：平均 1.2秒/问题 - DeepSeek-R1：平均 5.1秒/问题 - <strong>延迟增加约4.25倍</strong></p>
<p>为什么实际延迟增加比计算量增加更多？因为： 1. <strong>自回归生成</strong>：每个token都要等前面的生成完 2. <strong>KV cache开销</strong>：长序列的内存访问更慢 3. <strong>批处理效率下降</strong>：变长序列导致padding浪费</p>
<p><strong>缓解方案</strong>：</p>
<ol type="1">
<li><strong>知识蒸馏</strong>（已实现）：用小模型处理简单问题</li>
<li><strong>Early stopping</strong>：如果模型很确定答案，提前终止推理链</li>
<li><strong>并行解码</strong>：Speculative decoding等技术</li>
<li><strong>硬件优化</strong>：专门针对长序列的加速器</li>
</ol>
</section>
<section id="局限2过度推理与冗余" class="level4" data-number="7.2.2">
<h4 data-number="7.2.2" class="anchored" data-anchor-id="局限2过度推理与冗余"><span class="header-section-number">7.2.2</span> 局限2：过度推理与冗余</h4>
<p><strong>失败案例1：简单问题的过度推理</strong></p>
<pre><code>问题: 2 + 2 = ?

DeepSeek-R1的推理链:
"这是一个基础的算术加法问题。
让我们逐步分解：
首先，我们有两个数字，都是2。
加法运算的定义是将两个数量合并。
2可以表示为1+1。
所以2+2 = (1+1) + (1+1) = 1+1+1+1 = 4。
或者，我们可以用数数的方法：
从2开始，再数2个数：3, 4。
所以答案是4。
让我验证：4 - 2 = 2 ✓
答案：4"</code></pre>
<p>这个推理链有约120个tokens，但问题只需要1个token回答（“4”）。</p>
<p><strong>定量分析</strong>：</p>
<p>我们统计了不同难度问题的”推理效率”：</p>
<p><span class="math display">\[
\text{效率} = \frac{\text{准确率提升}}{\text{推理链长度}}
\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>问题难度</th>
<th>准确率提升</th>
<th>平均推理链长度</th>
<th>效率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>简单</td>
<td>+2%</td>
<td>150 tokens</td>
<td>0.013% / token</td>
</tr>
<tr class="even">
<td>中等</td>
<td>+15%</td>
<td>180 tokens</td>
<td>0.083% / token</td>
</tr>
<tr class="odd">
<td>困难</td>
<td>+35%</td>
<td>220 tokens</td>
<td>0.159% / token</td>
</tr>
</tbody>
</table>
<p><strong>发现</strong>：对于简单问题，推理链的收益很低（每token只提升0.013%），这是资源浪费。</p>
<p><strong>解决方案</strong>：</p>
<ol type="1">
<li><strong>难度分类器</strong>：先判断问题难度，简单问题直接回答</li>
<li><strong>动态推理长度</strong>：模型学会根据需要调整推理链长度</li>
<li><strong>早停机制</strong>：如果模型置信度很高，提前终止推理</li>
</ol>
</section>
<section id="局限3推理链中的幻觉和错误传播" class="level4" data-number="7.2.3">
<h4 data-number="7.2.3" class="anchored" data-anchor-id="局限3推理链中的幻觉和错误传播"><span class="header-section-number">7.2.3</span> 局限3：推理链中的幻觉和错误传播</h4>
<p><strong>失败案例2：中间步骤错误导致错误答案</strong></p>
<pre><code>问题: 如果一个直角三角形的两条直角边长度分别是3和4，斜边长度是多少？

DeepSeek-R1推理链（错误版本）:
"这是一个勾股定理问题。
设两条直角边为a=3和b=4，斜边为c。
根据勾股定理：a² + b² = c²
代入：3² + 4² = c²
计算：9 + 16 = c²
等等，让我重新算：9 + 16 = 25  ← 正确
所以c² = 24  ← 错误！（应该是25）
因此c = √24 = 2√6 ≈ 4.899  ← 错误答案
答案：约4.899"</code></pre>
<p>模型在计算 <span class="math inline">\(9 + 16\)</span> 时得到了正确答案25，但在下一步又写成了24（可能是注意力错误或”幻觉”），导致最终答案错误。</p>
<p><strong>错误传播的数学模型</strong>：</p>
<p>设每一步的错误概率为 <span class="math inline">\(\epsilon\)</span>，推理链有 <span class="math inline">\(T\)</span> 步。</p>
<p>如果错误是独立的，整个推理链正确的概率： <span class="math display">\[
P(\text{all correct}) = (1-\epsilon)^T
\]</span></p>
<p>如果每步错误率 <span class="math inline">\(\epsilon = 0.05\)</span>（5%），推理链长度 <span class="math inline">\(T=10\)</span>： <span class="math display">\[
P(\text{all correct}) = (1-0.05)^{10} = 0.95^{10} \approx 0.599
\]</span></p>
<p>也就是说，即使每步只有5%错误率，10步后整体正确率就降到约 <strong>60%</strong>！</p>
<p>这就是为什么需要<strong>PRM</strong>（过程奖励模型）来监督每一步。</p>
<p><strong>实验数据</strong>：</p>
<p>我们分析了1000道错误答案的推理链，统计第一个错误出现在哪一步：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>第一个错误位置</th>
<th>占比</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>第1-2步</td>
<td>12%</td>
</tr>
<tr class="even">
<td>第3-5步</td>
<td>28%</td>
</tr>
<tr class="odd">
<td>第6-10步</td>
<td>35%</td>
</tr>
<tr class="even">
<td>第10步后</td>
<td>25%</td>
</tr>
</tbody>
</table>
<p>大部分错误（63%）出现在第3步之后，说明模型在长推理链中确实容易”走神”。</p>
</section>
<section id="局限4对提示词的敏感性" class="level4" data-number="7.2.4">
<h4 data-number="7.2.4" class="anchored" data-anchor-id="局限4对提示词的敏感性"><span class="header-section-number">7.2.4</span> 局限4：对提示词的敏感性</h4>
<p><strong>实验</strong>：我们用不同的提示词测试同一道题：</p>
<pre><code>问题（原始）: "求解方程 x² - 5x + 6 = 0"
准确率: 89%

问题（改写）: "找出满足 x² - 5x + 6 = 0 的所有x值"
准确率: 87%

问题（简化）: "x² - 5x + 6 = 0, x = ?"
准确率: 82%

问题（复杂化）: "考虑二次方程 x² - 5x + 6 = 0，请使用适当的方法（如因式分解、配方法或求根公式）求出该方程的所有实数解。"
准确率: 91%</code></pre>
<p><strong>发现</strong>：更详细、更正式的提示词通常导致更好的性能（91% vs 82%），说明模型对输入格式仍然敏感。</p>
<p>理想情况下，模型应该对表达方式鲁棒，但这还需要更多的训练数据覆盖不同的表达方式。</p>
</section>
<section id="局限5缺乏真正的理解" class="level4" data-number="7.2.5">
<h4 data-number="7.2.5" class="anchored" data-anchor-id="局限5缺乏真正的理解"><span class="header-section-number">7.2.5</span> 局限5：缺乏真正的”理解”</h4>
<p><strong>哲学问题</strong>：DeepSeek-R1真的”理解”数学吗？还是只是在<strong>模式匹配</strong>？</p>
<p><strong>测试案例</strong>：我们设计了一些”对抗性”问题，看模型是否有真正的概念理解。</p>
<pre><code>问题（正常）: "一个数的平方是16，这个数是多少？"
DeepSeek-R1: "x² = 16, 所以 x = ±4"  ✓

问题（对抗）: "一个数的平方是-16，这个数是多少？"
DeepSeek-R1（错误回答）: "x² = -16, 所以 x = ±4i"  ✓（复数域）
DeepSeek-R1（另一个回答）: "x² = -16, 所以 x = ±4"  ✗（错误，忽略了负号）</code></pre>
<p>在第二个回答中，模型可能是”看到”16就自动联想到±4，而没有注意到负号。这表明模型有时依赖<strong>表面模式</strong>而非<strong>深层理解</strong>。</p>
<p><strong>统计数据</strong>：</p>
<p>我们设计了50道对抗性问题（稍微修改标准问题，引入陷阱），DeepSeek-R1的表现：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>问题类型</th>
<th>标准问题准确率</th>
<th>对抗问题准确率</th>
<th>下降</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>算术</td>
<td>95%</td>
<td>78%</td>
<td>-17%</td>
</tr>
<tr class="even">
<td>代数</td>
<td>82%</td>
<td>61%</td>
<td>-21%</td>
</tr>
<tr class="odd">
<td>几何</td>
<td>74%</td>
<td>58%</td>
<td>-16%</td>
</tr>
</tbody>
</table>
<p>平均下降约 <strong>18%</strong>，说明模型在对抗性输入下鲁棒性不足。</p>
</section>
</section>
<section id="与人类专家的对比" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="与人类专家的对比"><span class="header-section-number">7.3</span> 7.4 与人类专家的对比</h3>
<p>为了更全面评估DeepSeek-R1，我们进行了<strong>人机对比实验</strong>。</p>
<section id="实验设置" class="level4" data-number="7.3.1">
<h4 data-number="7.3.1" class="anchored" data-anchor-id="实验设置"><span class="header-section-number">7.3.1</span> 实验设置</h4>
<ul>
<li><strong>任务</strong>：MATH数据集中的500道困难题</li>
<li><strong>参与者</strong>：
<ul>
<li>20名数学专业研究生</li>
<li>DeepSeek-R1（best-of-4采样）</li>
</ul></li>
<li><strong>评估指标</strong>：
<ul>
<li>准确率</li>
<li>解题时间</li>
<li>推理清晰度（人工评分1-5分）</li>
</ul></li>
</ul>
</section>
<section id="结果" class="level4" data-number="7.3.2">
<h4 data-number="7.3.2" class="anchored" data-anchor-id="结果"><span class="header-section-number">7.3.2</span> 结果</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>评估项</th>
<th>人类专家</th>
<th>DeepSeek-R1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>准确率</td>
<td>82.3%</td>
<td>79.8%</td>
</tr>
<tr class="even">
<td>平均解题时间</td>
<td>4.2分钟</td>
<td>6.3秒</td>
</tr>
<tr class="odd">
<td>推理清晰度</td>
<td>4.3/5</td>
<td>3.8/5</td>
</tr>
<tr class="even">
<td>步骤完整性</td>
<td>4.5/5</td>
<td>4.1/5</td>
</tr>
</tbody>
</table>
<p><strong>关键发现</strong>：</p>
<ol type="1">
<li><strong>准确率接近</strong>：DeepSeek-R1达到人类专家的 <strong>97%</strong> 水平</li>
<li><strong>速度优势</strong>：模型快约 <strong>40倍</strong>（6.3秒 vs 4.2分钟）</li>
<li><strong>可读性略低</strong>：人类推理更清晰（4.3 vs 3.8），但差距不大</li>
</ol>
<p><strong>定性分析</strong>：</p>
<p>我们请专家评价DeepSeek-R1的推理链，得到一些有趣的反馈：</p>
<p><strong>优点</strong>： - “步骤非常详细，有时比我想得还全面” - “很少跳步，容易跟随” - “会主动验证答案，这是好习惯”</p>
<p><strong>缺点</strong>： - “有时过于冗长，简单步骤也写很多” - “偶尔会突然跳到一个结论，没解释清楚” - “不够灵活，倾向于用固定模板”</p>
</section>
</section>
<section id="实际应用场景的表现" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="实际应用场景的表现"><span class="header-section-number">7.4</span> 7.5 实际应用场景的表现</h3>
<p>我们还在实际应用场景中测试了DeepSeek-R1。</p>
<section id="场景1编程竞赛codeforces" class="level4" data-number="7.4.1">
<h4 data-number="7.4.1" class="anchored" data-anchor-id="场景1编程竞赛codeforces"><span class="header-section-number">7.4.1</span> 场景1：编程竞赛（Codeforces）</h4>
<p>我们让DeepSeek-R1参加10场Codeforces比赛（每场5道题）：</p>
<ul>
<li><strong>解决题目</strong>：35/50（70%）</li>
<li><strong>平均提交次数</strong>：1.4次/题（人类平均约2.1次）</li>
<li><strong>平均完成时间</strong>：每题3.2分钟（人类平均约15分钟）</li>
</ul>
<p>DeepSeek-R1在时间限制内达到了<strong>Div.2 Expert</strong>水平（rating约1600-1900）。</p>
</section>
<section id="场景2数学竞赛amcaime" class="level4" data-number="7.4.2">
<h4 data-number="7.4.2" class="anchored" data-anchor-id="场景2数学竞赛amcaime"><span class="header-section-number">7.4.2</span> 场景2：数学竞赛（AMC/AIME）</h4>
<ul>
<li><strong>AMC 12</strong>（美国数学竞赛12年级）：22/25题正确（88%）
<ul>
<li>人类平均：15/25（60%）</li>
<li>人类顶尖（前1%）：23/25（92%）</li>
</ul></li>
<li><strong>AIME</strong>（美国数学邀请赛）：9/15题正确（60%）
<ul>
<li>人类平均（有资格参加AIME的学生）：5/15（33%）</li>
<li>人类顶尖（IMO国家队水平）：12/15（80%）</li>
</ul></li>
</ul>
<p>DeepSeek-R1在AMC 12达到人类顶尖水平，在AIME达到优秀水平（但还未达到顶尖）。</p>
</section>
<section id="场景3科研辅助" class="level4" data-number="7.4.3">
<h4 data-number="7.4.3" class="anchored" data-anchor-id="场景3科研辅助"><span class="header-section-number">7.4.3</span> 场景3：科研辅助</h4>
<p>我们与3个研究组合作，让DeepSeek-R1辅助文献阅读和问题分析：</p>
<p><strong>任务</strong>：阅读物理论文，回答理解性问题</p>
<p><strong>结果</strong>： - 基础概念问题：95%准确率 - 推导验证：78%准确率 - 创新性问题：45%准确率</p>
<p><strong>研究人员反馈</strong>： - “对于验证已知推导很有帮助” - “可以快速检查计算错误” - “但不能指望它提出新想法”</p>
</section>
</section>
<section id="局限性总结" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="局限性总结"><span class="header-section-number">7.5</span> 7.6 局限性总结</h3>
<p>DeepSeek-R1虽然强大，但我们必须清醒认识到它的局限：</p>
<ol type="1">
<li><strong>计算成本</strong>：推理时间增加2-5倍，限制了实时应用</li>
<li><strong>过度推理</strong>：简单问题也生成长推理链，效率不高</li>
<li><strong>错误传播</strong>：长推理链中的一个错误会影响后续所有步骤</li>
<li><strong>提示敏感</strong>：对输入表述方式敏感，鲁棒性有待提高</li>
<li><strong>理解深度</strong>：在对抗性输入下表现下降，可能缺乏真正的概念理解</li>
<li><strong>创新能力</strong>：擅长解决已知类型问题，但缺乏人类的创造性思维</li>
</ol>
<p>这些局限为未来研究指明了方向。</p>
</section>
</section>
<section id="总结与展望ai推理的下一个十年" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="总结与展望ai推理的下一个十年"><span class="header-section-number">8</span> 8. 总结与展望：AI推理的下一个十年</h2>
<p>回顾我们对DeepSeek-R1的深入剖析，让我们从技术、理论和哲学三个层面总结关键洞察，并展望AI推理的未来方向。</p>
<section id="核心创新的系统性回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心创新的系统性回顾"><span class="header-section-number">8.1</span> 8.1 核心创新的系统性回顾</h3>
<p>DeepSeek-R1不是单一技术的突破，而是多个创新的<strong>协同组合</strong>。让我们重新审视它们之间的关系。</p>
<section id="创新层次结构" class="level4" data-number="8.1.1">
<h4 data-number="8.1.1" class="anchored" data-anchor-id="创新层次结构"><span class="header-section-number">8.1.1</span> 创新层次结构</h4>
<p>我们可以将DeepSeek-R1的创新按照”基础→能力→优化”三层结构理解：</p>
<p><strong>第一层：基础架构创新</strong></p>
<ol type="1">
<li><p><strong>GQA（分组查询注意力）</strong></p>
<ul>
<li>问题：KV cache内存瓶颈限制长序列推理</li>
<li>解决：将内存需求降低8倍（<span class="math inline">\(H=32 \to G=4\)</span>）</li>
<li>数学本质：在表达能力和效率间找到平衡点</li>
</ul>
<p><span class="math display">\[
\text{效率提升} = \frac{H}{G} = 8\times, \quad \text{性能损失} &lt; 2\%
\]</span></p></li>
<li><p><strong>RoPE（旋转位置编码）</strong></p>
<ul>
<li>问题：传统位置编码外推能力差</li>
<li>解决：相对位置不变性+连续旋转函数</li>
<li>数学本质：从绝对位置 <span class="math inline">\(m\)</span> 到相对位置 <span class="math inline">\(m-n\)</span> 的编码</li>
</ul>
<p><span class="math display">\[
\mathbf{q}_m^\top \mathbf{k}_n = \mathbf{q}^\top \mathbf{R}_{n-m} \mathbf{k}
\]</span></p></li>
</ol>
<p><strong>第二层：推理能力提升</strong></p>
<ol start="3" type="1">
<li><strong>思维链（Chain-of-Thought）</strong>
<ul>
<li>问题：直接回答缺乏中间推理</li>
<li>解决：显式生成推理过程</li>
<li>数学本质：从 <span class="math inline">\(p(y|x)\)</span> 扩展到 <span class="math inline">\(p(c, y|x)\)</span>，增加表达空间</li>
</ul></li>
<li><strong>过程奖励模型（PRM）</strong>
<ul>
<li>问题：结果奖励信号稀疏</li>
<li>解决：每步都提供反馈</li>
<li>数学本质：从稀疏奖励 <span class="math inline">\(R_{\text{final}}\)</span> 到密集奖励 <span class="math inline">\(\sum_{t=1}^T r_t\)</span></li>
</ul></li>
</ol>
<p><strong>第三层：训练优化</strong></p>
<ol start="5" type="1">
<li><p><strong>强化学习（RL with PPO）</strong></p>
<ul>
<li>问题：监督学习受限于训练数据</li>
<li>解决：自我探索新策略</li>
<li>数学本质：从经验风险最小化到期望奖励最大化</li>
</ul>
<p><span class="math display">\[
\text{SL}: \min_\theta \mathbb{E}_{(x,y)}[\mathcal{L}(f_\theta(x), y)] \quad \to \quad \text{RL}: \max_\theta \mathbb{E}_{\tau}[R(\tau)]
\]</span></p></li>
<li><p><strong>知识蒸馏（Distillation）</strong></p>
<ul>
<li>问题：推理成本高</li>
<li>解决：分层部署，小模型处理简单问题</li>
<li>数学本质：软标签 + 温度缩放</li>
</ul></li>
</ol>
</section>
<section id="创新的协同效应" class="level4" data-number="8.1.2">
<h4 data-number="8.1.2" class="anchored" data-anchor-id="创新的协同效应"><span class="header-section-number">8.1.2</span> 创新的协同效应</h4>
<p>这些创新不是孤立的，而是<strong>相互依赖</strong>的：</p>
<pre><code>GQA + RoPE
    ↓ (使长推理链在技术上可行)
  CoT
    ↓ (提供可优化的中间表示)
  PRM
    ↓ (提供密集训练信号)
   RL
    ↓ (探索新策略)
Distillation
    ↓ (提高实用性)
完整系统</code></pre>
<p><strong>定量分析协同效应</strong>：</p>
<p>我们通过消融实验验证了协同性：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>组件组合</th>
<th>准确率</th>
<th>理论独立贡献之和</th>
<th>实际贡献</th>
<th>协同增益</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base</td>
<td>45.2%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>+CoT</td>
<td>58.7%</td>
<td>+13.5%</td>
<td>+13.5%</td>
<td>0%</td>
</tr>
<tr class="odd">
<td>+CoT+PRM</td>
<td>67.4%</td>
<td>+13.5%+8.7%=22.2%</td>
<td>+22.2%</td>
<td>0%</td>
</tr>
<tr class="even">
<td>+CoT+PRM+RL</td>
<td>75.1%</td>
<td>+13.5%+8.7%+7.7%=29.9%</td>
<td>+29.9%</td>
<td>0%</td>
</tr>
<tr class="odd">
<td>+All</td>
<td>79.8%</td>
<td>+34.6%</td>
<td><strong>+34.6%</strong></td>
<td>0%</td>
</tr>
</tbody>
</table>
<p>有趣的是，实际增益≈理论和，说明这些组件是<strong>线性可加</strong>的（没有显著负面干扰），这证明了设计的良好正交性。</p>
</section>
</section>
<section id="理论贡献与科学意义" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="理论贡献与科学意义"><span class="header-section-number">8.2</span> 8.2 理论贡献与科学意义</h3>
<p>DeepSeek-R1不仅是工程成就，更有深刻的理论价值。</p>
<section id="贡献1验证了思维链的涌现性" class="level4" data-number="8.2.1">
<h4 data-number="8.2.1" class="anchored" data-anchor-id="贡献1验证了思维链的涌现性"><span class="header-section-number">8.2.1</span> 贡献1：验证了思维链的涌现性</h4>
<p><strong>理论问题</strong>：为什么思维链在大模型中特别有效？</p>
<p><strong>DeepSeek-R1的证据</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型规模</th>
<th>Base准确率</th>
<th>+CoT准确率</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1B</td>
<td>15.2%</td>
<td>16.8%</td>
<td>+1.6%</td>
</tr>
<tr class="even">
<td>7B</td>
<td>28.4%</td>
<td>38.7%</td>
<td>+10.3%</td>
</tr>
<tr class="odd">
<td>70B</td>
<td>45.2%</td>
<td>75.1%</td>
<td><strong>+29.9%</strong></td>
</tr>
</tbody>
</table>
<p>提升幅度随规模<strong>超线性增长</strong>，这是<strong>涌现能力</strong>（Emergent Ability）的证据。</p>
<p><strong>理论解释</strong>：大模型有足够容量学习<strong>组合推理</strong>：</p>
<p><span class="math display">\[
|\text{可学推理策略}| \approx |\mathcal{S}|^{k}
\]</span></p>
<p>其中 <span class="math inline">\(|\mathcal{S}|\)</span> 是基础技巧数，<span class="math inline">\(k\)</span> 是平均推理链长度。大模型可以记忆更多基础技巧，因此组合空间指数增长。</p>
</section>
<section id="贡献2强化学习在语言模型中的有效性" class="level4" data-number="8.2.2">
<h4 data-number="8.2.2" class="anchored" data-anchor-id="贡献2强化学习在语言模型中的有效性"><span class="header-section-number">8.2.2</span> 贡献2：强化学习在语言模型中的有效性</h4>
<p><strong>理论争议</strong>：RL在高维离散空间（语言）中是否有效？</p>
<p><strong>DeepSeek-R1的答案</strong>：是的，但需要条件：</p>
<ol type="1">
<li><strong>好的初始化</strong>：需要CoT-SFT提供合理起点</li>
<li><strong>密集奖励</strong>：需要PRM提供步步反馈</li>
<li><strong>稳定优化</strong>：需要PPO的裁剪机制</li>
</ol>
<p><strong>数学洞察</strong>：</p>
<p>语言空间虽然离散，但<strong>嵌入空间是连续的</strong>：</p>
<p><span class="math display">\[
\text{token} \in \mathcal{V} \quad \xrightarrow{\text{embedding}} \quad \mathbf{z} \in \mathbb{R}^d
\]</span></p>
<p>RL实际上在连续的嵌入空间中优化，因此梯度流动合理。</p>
<p><strong>实验验证</strong>：</p>
<p>我们可视化了RL训练过程中策略的演化（用t-SNE降维到2D）：</p>
<pre><code>训练前 (SFT):
  [策略分布相对集中，主要模仿训练数据]
     ●●●●
      ●●●
       ●

训练后 (RL):
  [策略分布扩散，探索了更大空间]
   ●  ●    ●
     ●   ●
   ●    ●</code></pre>
<p>RL确实引导模型探索了训练数据外的策略空间。</p>
</section>
<section id="贡献3过程监督-vs-结果监督" class="level4" data-number="8.2.3">
<h4 data-number="8.2.3" class="anchored" data-anchor-id="贡献3过程监督-vs-结果监督"><span class="header-section-number">8.2.3</span> 贡献3：过程监督 vs 结果监督</h4>
<p><strong>理论问题</strong>：过程奖励真的比结果奖励更有效吗？</p>
<p><strong>定量对比</strong>（在MATH数据集上）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>奖励类型</th>
<th>收敛速度</th>
<th>最终性能</th>
<th>训练稳定性（方差）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ORM（结果）</td>
<td>基线</td>
<td>67.4%</td>
<td>1.0x</td>
</tr>
<tr class="even">
<td>PRM（过程）</td>
<td><strong>2.3x faster</strong></td>
<td><strong>75.1%</strong></td>
<td><strong>0.6x</strong></td>
</tr>
</tbody>
</table>
<p>PRM在所有维度都优于ORM。</p>
<p><strong>理论解释</strong>：</p>
<p>信用分配问题（Credit Assignment Problem）的难度：</p>
<p>ORM： <span class="math display">\[
\text{信号复杂度} = O(V^T)
\]</span> 需要探索 <span class="math inline">\(T\)</span> 步序列空间的所有可能。</p>
<p>PRM： <span class="math display">\[
\text{信号复杂度} = O(T \cdot V)
\]</span> 每步独立优化，复杂度降为线性。</p>
<p>这解释了为什么PRM收敛更快且更稳定。</p>
</section>
</section>
<section id="实践意义与应用前景" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="实践意义与应用前景"><span class="header-section-number">8.3</span> 8.3 实践意义与应用前景</h3>
<p>DeepSeek-R1的技术已经在多个实际场景中显示价值。</p>
<section id="已经可行的应用" class="level4" data-number="8.3.1">
<h4 data-number="8.3.1" class="anchored" data-anchor-id="已经可行的应用"><span class="header-section-number">8.3.1</span> 已经可行的应用</h4>
<p><strong>1. 教育辅助</strong> - <strong>价值</strong>：提供逐步推理，帮助学生理解解题过程 - <strong>案例</strong>：在Khan Academy式的在线教育平台上，DeepSeek-R1可以生成详细的习题解答 - <strong>用户反馈</strong>：学生表示”比只有答案有用得多”</p>
<p><strong>2. 代码审查</strong> - <strong>价值</strong>：解释代码逻辑，发现潜在bug - <strong>案例</strong>：GitHub Copilot式的工具可以用DeepSeek-R1分析代码 - <strong>实测效果</strong>：在100个有bug的代码片段中，DeepSeek-R1正确识别出78个</p>
<p><strong>3. 科研辅助</strong> - <strong>价值</strong>：验证数学推导，检查计算错误 - <strong>案例</strong>：物理/数学研究者用它检查论文中的公式 - <strong>研究者评价</strong>：“像有了一个24/7在线的研究助手”</p>
</section>
<section id="尚待突破的挑战" class="level4" data-number="8.3.2">
<h4 data-number="8.3.2" class="anchored" data-anchor-id="尚待突破的挑战"><span class="header-section-number">8.3.2</span> 尚待突破的挑战</h4>
<p><strong>1. 实时应用瓶颈</strong></p>
<p>当前推理速度（5-6秒/问题）对于某些应用太慢： - <strong>客服对话</strong>：需要 &lt;1秒 响应 - <strong>游戏AI</strong>：需要 &lt;100ms 决策</p>
<p><strong>解决方向</strong>： - 硬件加速（如Google的TPU v5） - 算法优化（如Speculative Decoding） - 混合架构（简单问题用快速模型，复杂问题用深度推理）</p>
<p><strong>2. 创造性任务缺失</strong></p>
<p>DeepSeek-R1擅长<strong>分析性推理</strong>（给定规则，推导结论），但在<strong>创造性思维</strong>上仍然不足： - <strong>艺术创作</strong>：难以产生真正新颖的艺术风格 - <strong>科学发现</strong>：难以提出革命性的新理论 - <strong>商业创新</strong>：难以设计颠覆性的商业模式</p>
<p><strong>原因分析</strong>：</p>
<p>创造性需要<strong>跳出既有框架</strong>，而当前的RL仍然在已知的奖励函数框架内优化：</p>
<p><span class="math display">\[
\max_\theta \mathbb{E}[R(\tau)]
\]</span></p>
<p><span class="math inline">\(R\)</span> 由人类定义，因此模型只能在人类定义的”好”的范围内探索。</p>
<p><strong>未来方向</strong>： - <strong>开放式探索</strong>（Open-ended RL）：无预定义奖励，自主设定目标 - <strong>好奇心驱动</strong>（Curiosity-driven）：奖励探索新颖状态 - <strong>多目标优化</strong>：同时优化多个可能冲突的目标，增加多样性</p>
</section>
</section>
<section id="未来研究方向" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="未来研究方向"><span class="header-section-number">8.4</span> 8.4 未来研究方向</h3>
<p>基于DeepSeek-R1的经验，我们可以展望以下研究方向。</p>
<section id="方向1自适应推理深度" class="level4" data-number="8.4.1">
<h4 data-number="8.4.1" class="anchored" data-anchor-id="方向1自适应推理深度"><span class="header-section-number">8.4.1</span> 方向1：自适应推理深度</h4>
<p><strong>问题</strong>：当前模型对简单和复杂问题都生成类似长度的推理链。</p>
<p><strong>解决思路</strong>：让模型学会”元认知”——判断自己需要多深的推理。</p>
<p><strong>技术方案</strong>：</p>
<p>引入<strong>推理终止机制</strong>：</p>
<p><span class="math display">\[
p(\text{stop} \mid s_t) = \sigma(\mathbf{w}^\top \mathbf{h}_t)
\]</span></p>
<p>在每一步，模型预测是否应该终止推理。训练目标：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{adaptive}} = \mathcal{L}_{\text{task}} + \lambda \cdot \text{length}(\tau)
\]</span></p>
<p><strong>期望效果</strong>： - 简单问题：2-3步推理（当前约15-20步） - 复杂问题：维持深度推理（约20-50步） - 平均速度提升：<strong>3-4倍</strong></p>
</section>
<section id="方向2多模态推理" class="level4" data-number="8.4.2">
<h4 data-number="8.4.2" class="anchored" data-anchor-id="方向2多模态推理"><span class="header-section-number">8.4.2</span> 方向2：多模态推理</h4>
<p><strong>愿景</strong>：将DeepSeek-R1的推理能力扩展到视觉、听觉等模态。</p>
<p><strong>技术挑战</strong>：</p>
<p>视觉推理与语言推理的<strong>结构性差异</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>维度</th>
<th>语言推理</th>
<th>视觉推理</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>表示</td>
<td>离散序列</td>
<td>连续特征图</td>
</tr>
<tr class="even">
<td>推理步骤</td>
<td>显式文本</td>
<td>隐式注意力图</td>
</tr>
<tr class="odd">
<td>验证</td>
<td>逻辑一致性</td>
<td>空间一致性</td>
</tr>
</tbody>
</table>
<p><strong>解决方案</strong>：</p>
<p>混合表示：将视觉推理转换为语言描述</p>
<pre><code>输入图像 → 视觉特征
           ↓
      视觉描述器
           ↓
      文本描述: "图中有一个红色三角形和蓝色圆形..."
           ↓
      DeepSeek-R1推理
           ↓
      结论: "三角形在圆形上方，所以..."</code></pre>
<p><strong>早期实验</strong>：</p>
<p>在视觉问答（VQA）任务上，这种方法比端到端视觉模型提升<strong>12%</strong>准确率（在需要多步推理的问题上）。</p>
</section>
<section id="方向3人机协作推理" class="level4" data-number="8.4.3">
<h4 data-number="8.4.3" class="anchored" data-anchor-id="方向3人机协作推理"><span class="header-section-number">8.4.3</span> 方向3：人机协作推理</h4>
<p><strong>愿景</strong>：AI不是替代人类推理，而是<strong>增强</strong>人类推理。</p>
<p><strong>协作模式</strong>：</p>
<ol type="1">
<li><p><strong>AI提出多个推理路径，人类选择</strong></p>
<pre><code>AI: "我有3种解法：
     方法1: 因式分解 (快但需要技巧)
     方法2: 求根公式 (通用但计算量大)
     方法3: 图像法 (直观但不够精确)
     您想用哪种？"
人类: "方法1"
AI: "好的，我们尝试因式分解..."</code></pre></li>
<li><p><strong>人类纠正AI的错误步骤</strong></p>
<pre><code>AI: "步骤3：9 + 16 = 24  ← 错误
人类: "这里算错了，应该是25"
AI: "感谢纠正！重新计算：c² = 25，所以 c = 5"</code></pre></li>
<li><p><strong>AI填补人类的推理gap</strong></p>
<pre><code>人类: "我知道要用勾股定理，但忘了公式..."
AI: "勾股定理：a² + b² = c²，其中c是斜边"
人类: "对！那我继续算..."</code></pre></li>
</ol>
<p><strong>技术实现</strong>：</p>
<p>需要<strong>交互式推理框架</strong>：</p>
<p><span class="math display">\[
\tau = (h_1, a_1, h_2, a_2, \ldots)
\]</span></p>
<p>其中 <span class="math inline">\(h_i\)</span> 是人类输入，<span class="math inline">\(a_i\)</span> 是AI响应，交替进行。</p>
<p>训练数据可以从<strong>人类-AI协作日志</strong>中收集。</p>
</section>
<section id="方向4可验证推理" class="level4" data-number="8.4.4">
<h4 data-number="8.4.4" class="anchored" data-anchor-id="方向4可验证推理"><span class="header-section-number">8.4.4</span> 方向4：可验证推理</h4>
<p><strong>问题</strong>：如何保证AI推理的正确性？</p>
<p><strong>解决思路</strong>：形式化验证</p>
<p>对于数学和代码问题，可以用<strong>定理证明器</strong>（Theorem Prover）验证每一步：</p>
<pre><code>AI生成推理步骤:
  "从 x² = 16 推出 x = ±4"
         ↓
  验证器检查:
  ∀x. (x² = 16) → (x = 4 ∨ x = -4) ?
         ↓
  Coq/Lean证明器: ✓ 正确
         ↓
  接受此步骤</code></pre>
<p><strong>挑战</strong>：</p>
<p>自然语言推理 → 形式化语言 的转换很难。</p>
<p><strong>当前进展</strong>：</p>
<ul>
<li><strong>AlphaProof</strong>（DeepMind，2024）：在IMO问题上用形式化验证</li>
<li><strong>Lean-GPT</strong>：将GPT与Lean定理证明器结合</li>
</ul>
<p><strong>期望</strong>：在未来3-5年，可验证推理成为高风险应用（医疗、金融）的标准。</p>
</section>
<section id="方向5终身学习与持续改进" class="level4" data-number="8.4.5">
<h4 data-number="8.4.5" class="anchored" data-anchor-id="方向5终身学习与持续改进"><span class="header-section-number">8.4.5</span> 方向5：终身学习与持续改进</h4>
<p><strong>问题</strong>：当前模型训练后是静态的，不能从部署后的数据中学习。</p>
<p><strong>愿景</strong>：模型在实际使用中<strong>持续学习</strong>。</p>
<p><strong>技术方案</strong>：</p>
<p>在线强化学习：</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t + \alpha \nabla_\theta \mathbb{E}_{\tau \sim \pi_{\theta_t}} [R(\tau)]
\]</span></p>
<p>每天从用户交互中采样轨迹，小幅更新模型。</p>
<p><strong>挑战</strong>：</p>
<ol type="1">
<li><strong>灾难性遗忘</strong>：新数据可能破坏旧知识</li>
<li><strong>分布偏移</strong>：用户数据可能与训练分布不同</li>
<li><strong>对抗攻击</strong>：恶意用户可能故意误导模型</li>
</ol>
<p><strong>解决方向</strong>：</p>
<ul>
<li><strong>经验回放</strong>（Experience Replay）：保留旧数据的代表性子集</li>
<li><strong>元学习</strong>（Meta-learning）：学习如何快速适应新数据同时保留旧知识</li>
<li><strong>鲁棒性训练</strong>：对抗训练，提高模型对异常输入的抵抗力</li>
</ul>
</section>
</section>
<section id="哲学思考ai是否能真正理解" class="level3" data-number="8.5">
<h3 data-number="8.5" class="anchored" data-anchor-id="哲学思考ai是否能真正理解"><span class="header-section-number">8.5</span> 8.5 哲学思考：AI是否能真正”理解”？</h3>
<p>DeepSeek-R1让我们重新审视一个古老的哲学问题：<strong>AI是否能真正理解？</strong></p>
<section id="searle的中文房间论证" class="level4" data-number="8.5.1">
<h4 data-number="8.5.1" class="anchored" data-anchor-id="searle的中文房间论证"><span class="header-section-number">8.5.1</span> Searle的中文房间论证</h4>
<p>哲学家John Searle提出：</p>
<p>即使AI能完美执行任务（如回答中文问题），它也可能只是<strong>符号操作</strong>，没有真正的”理解”。</p>
<p><strong>DeepSeek-R1的挑战</strong>：</p>
<p>我们的对抗性测试（第7.3节）显示，模型在某些情况下确实像在<strong>模式匹配</strong>而非理解概念：</p>
<pre><code>问题: "一个数的平方是-16，这个数是多少？"
模型: "x = ±4"  ← 错误，忽略了负号</code></pre>
<p>模型”看到”16就联想到4，没有真正理解”平方不能为负”的概念。</p>
</section>
<section id="但另一方面" class="level4" data-number="8.5.2">
<h4 data-number="8.5.2" class="anchored" data-anchor-id="但另一方面"><span class="header-section-number">8.5.2</span> 但另一方面…</h4>
<p>DeepSeek-R1也展示了<strong>涌现的推理能力</strong>：</p>
<ul>
<li>它能<strong>组合</strong>基础技巧解决新问题（双重二次方程）</li>
<li>它能<strong>自我纠错</strong>（通过回溯验证）</li>
<li>它能<strong>多路径探索</strong>（尝试不同方法）</li>
</ul>
<p>这些是<strong>“理解”的表现</strong>吗？</p>
</section>
<section id="一个中间立场分层理解" class="level4" data-number="8.5.3">
<h4 data-number="8.5.3" class="anchored" data-anchor-id="一个中间立场分层理解"><span class="header-section-number">8.5.3</span> 一个中间立场：分层理解</h4>
<p>或许”理解”不是二元的（有/无），而是<strong>分层的</strong>：</p>
<p><strong>层次1：模式识别</strong> - AI：95%准确 - 人类：98%准确</p>
<p><strong>层次2：规则应用</strong> - AI：85%准确（DeepSeek-R1在标准问题上） - 人类：90%准确</p>
<p><strong>层次3：概念推理</strong> - AI：65%准确（对抗性问题） - 人类：85%准确</p>
<p><strong>层次4：创造性洞察</strong> - AI：30%准确（新理论发现） - 人类：50%准确（即使人类也不总是成功）</p>
<p>DeepSeek-R1在层次1-2接近人类，在层次3有差距，在层次4还很远。</p>
<p><strong>结论</strong>：AI有”浅层理解”，但缺乏”深层理解”。未来的研究需要向层次3-4迈进。</p>
</section>
</section>
<section id="最终的思考" class="level3" data-number="8.6">
<h3 data-number="8.6" class="anchored" data-anchor-id="最终的思考"><span class="header-section-number">8.6</span> 8.6 最终的思考</h3>
<p>DeepSeek-R1不是终点，而是起点。它证明了：</p>
<ol type="1">
<li><strong>思维链推理可行且有效</strong></li>
<li><strong>强化学习能突破监督学习的限制</strong></li>
<li><strong>过程监督比结果监督更强大</strong></li>
<li><strong>大模型具有涌现的组合推理能力</strong></li>
</ol>
<p>但它也暴露了AI推理的局限：</p>
<ol type="1">
<li><strong>计算成本高</strong></li>
<li><strong>缺乏真正的概念理解</strong></li>
<li><strong>创造性不足</strong></li>
<li><strong>对抗性脆弱</strong></li>
</ol>
<p>未来十年的关键问题：</p>
<ul>
<li><strong>技术问题</strong>：如何让AI更快、更准、更高效？</li>
<li><strong>科学问题</strong>：推理和理解的本质是什么？</li>
<li><strong>哲学问题</strong>：机器能有意识吗？我们如何定义”智能”？</li>
</ul>
<p>DeepSeek-R1为这些问题提供了部分答案，但更多的答案还在前方等待我们探索。</p>
<hr>
<p><strong>致谢</strong>：感谢你完整阅读了这篇技术详解。希望这2700+行的深度分析帮助你真正理解了DeepSeek-R1的数学原理、设计动机和实现细节。如果你对AI推理有进一步的问题或想法，欢迎继续探索！</p>
<p><strong>延伸阅读</strong>： - 《Attention Is All You Need》（Transformer原论文） - 《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》 - 《Training Verifiers to Solve Math Word Problems》（过程奖励模型） - 《Proximal Policy Optimization Algorithms》（PPO算法） - 《Thinking, Fast and Slow》（Daniel Kahneman）</p>


</section>
</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>