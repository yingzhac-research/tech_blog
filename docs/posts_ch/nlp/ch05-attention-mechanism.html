<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-25">
<meta name="description" content="Attention机制的诞生：Bahdanau如何让解码器学会’回头看’，打破Seq2Seq的信息瓶颈，以及注意力权重的可解释性。">

<title>第5章：注意力机制的诞生 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#问题的精确定义" id="toc-问题的精确定义" class="nav-link" data-scroll-target="#问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</a></li>
  <li><a href="#之前的尝试为何失败" id="toc-之前的尝试为何失败" class="nav-link" data-scroll-target="#之前的尝试为何失败"><span class="header-section-number">2.2</span> 之前的尝试为何失败？</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#关键洞察动态的基于内容的寻址" id="toc-关键洞察动态的基于内容的寻址" class="nav-link" data-scroll-target="#关键洞察动态的基于内容的寻址"><span class="header-section-number">3.1</span> 关键洞察：动态的、基于内容的寻址</a></li>
  <li><a href="#直觉解释聚光灯与图书馆" id="toc-直觉解释聚光灯与图书馆" class="nav-link" data-scroll-target="#直觉解释聚光灯与图书馆"><span class="header-section-number">3.2</span> 直觉解释：聚光灯与图书馆</a></li>
  <li><a href="#另一个类比加权投票" id="toc-另一个类比加权投票" class="nav-link" data-scroll-target="#另一个类比加权投票"><span class="header-section-number">3.3</span> 另一个类比：加权投票</a></li>
  <li><a href="#设计动机为什么选择软注意力" id="toc-设计动机为什么选择软注意力" class="nav-link" data-scroll-target="#设计动机为什么选择软注意力"><span class="header-section-number">3.4</span> 设计动机：为什么选择软注意力？</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#bahdanau-attention加性注意力" id="toc-bahdanau-attention加性注意力" class="nav-link" data-scroll-target="#bahdanau-attention加性注意力"><span class="header-section-number">4.1</span> Bahdanau Attention：加性注意力</a></li>
  <li><a href="#注意力权重的计算" id="toc-注意力权重的计算" class="nav-link" data-scroll-target="#注意力权重的计算"><span class="header-section-number">4.2</span> 注意力权重的计算</a></li>
  <li><a href="#完整数值示例attention计算" id="toc-完整数值示例attention计算" class="nav-link" data-scroll-target="#完整数值示例attention计算"><span class="header-section-number">4.3</span> 完整数值示例：Attention计算</a></li>
  <li><a href="#解码器的完整流程" id="toc-解码器的完整流程" class="nav-link" data-scroll-target="#解码器的完整流程"><span class="header-section-number">4.4</span> 解码器的完整流程</a></li>
  <li><a href="#复杂度分析" id="toc-复杂度分析" class="nav-link" data-scroll-target="#复杂度分析"><span class="header-section-number">4.5</span> 复杂度分析</a></li>
  </ul></li>
  <li><a href="#注意力可视化模型在看什么" id="toc-注意力可视化模型在看什么" class="nav-link" data-scroll-target="#注意力可视化模型在看什么"><span class="header-section-number">5</span> 注意力可视化：模型在”看”什么？</a>
  <ul class="collapse">
  <li><a href="#对齐矩阵" id="toc-对齐矩阵" class="nav-link" data-scroll-target="#对齐矩阵"><span class="header-section-number">5.1</span> 对齐矩阵</a></li>
  <li><a href="#对齐模式的语言学意义" id="toc-对齐模式的语言学意义" class="nav-link" data-scroll-target="#对齐模式的语言学意义"><span class="header-section-number">5.2</span> 对齐模式的语言学意义</a></li>
  <li><a href="#可视化的局限性" id="toc-可视化的局限性" class="nav-link" data-scroll-target="#可视化的局限性"><span class="header-section-number">5.3</span> 可视化的局限性</a></li>
  </ul></li>
  <li><a href="#工程实践带attention的seq2seq" id="toc-工程实践带attention的seq2seq" class="nav-link" data-scroll-target="#工程实践带attention的seq2seq"><span class="header-section-number">6</span> 工程实践：带Attention的Seq2Seq</a>
  <ul class="collapse">
  <li><a href="#编码器" id="toc-编码器" class="nav-link" data-scroll-target="#编码器"><span class="header-section-number">6.1</span> 编码器</a></li>
  <li><a href="#attention层" id="toc-attention层" class="nav-link" data-scroll-target="#attention层"><span class="header-section-number">6.2</span> Attention层</a></li>
  <li><a href="#解码器" id="toc-解码器" class="nav-link" data-scroll-target="#解码器"><span class="header-section-number">6.3</span> 解码器</a></li>
  <li><a href="#完整的seq2seq模型" id="toc-完整的seq2seq模型" class="nav-link" data-scroll-target="#完整的seq2seq模型"><span class="header-section-number">6.4</span> 完整的Seq2Seq模型</a></li>
  <li><a href="#关键实现细节" id="toc-关键实现细节" class="nav-link" data-scroll-target="#关键实现细节"><span class="header-section-number">6.5</span> 关键实现细节</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">7</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么attention有效理论视角" id="toc-为什么attention有效理论视角" class="nav-link" data-scroll-target="#为什么attention有效理论视角"><span class="header-section-number">7.1</span> 为什么Attention有效？——理论视角</a></li>
  <li><a href="#边界条件与失效模式" id="toc-边界条件与失效模式" class="nav-link" data-scroll-target="#边界条件与失效模式"><span class="header-section-number">7.2</span> 边界条件与失效模式</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">7.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与展望" id="toc-局限性与展望" class="nav-link" data-scroll-target="#局限性与展望"><span class="header-section-number">8</span> 局限性与展望</a>
  <ul class="collapse">
  <li><a href="#本章方法的核心局限" id="toc-本章方法的核心局限" class="nav-link" data-scroll-target="#本章方法的核心局限"><span class="header-section-number">8.1</span> 本章方法的核心局限</a></li>
  <li><a href="#这些局限指向什么" id="toc-这些局限指向什么" class="nav-link" data-scroll-target="#这些局限指向什么"><span class="header-section-number">8.2</span> 这些局限指向什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">9</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">9.1</span> 关键公式速查</a></li>
  </ul></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">10</span> 思考题</a></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">11</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">11.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">11.3</span> 后续发展</a></li>
  <li><a href="#对attention可解释性的讨论" id="toc-对attention可解释性的讨论" class="nav-link" data-scroll-target="#对attention可解释性的讨论"><span class="header-section-number">11.4</span> 对Attention可解释性的讨论</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">12</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第5章：注意力机制的诞生</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">打破信息瓶颈：让解码器学会’回头看’</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Attention</div>
    <div class="quarto-category">Seq2Seq</div>
    <div class="quarto-category">机器翻译</div>
    <div class="quarto-category">Bahdanau</div>
  </div>
  </div>

<div>
  <div class="description">
    Attention机制的诞生：Bahdanau如何让解码器学会’回头看’，打破Seq2Seq的信息瓶颈，以及注意力权重的可解释性。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何让解码器在生成每个词时，能够访问输入序列的不同部分，而不是只依赖一个压缩后的向量？</p>
<p><strong>历史坐标</strong>：2014-2015 | Bahdanau, Cho, Bengio | 神经机器翻译的突破</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了RNN的辉煌与困境。LSTM和GRU通过门控机制解决了梯度消失问题，Seq2Seq架构让神经网络能够处理翻译、摘要等序列到序列的任务。</p>
<p>但Seq2Seq有一个致命的设计缺陷：<strong>信息瓶颈</strong>。</p>
<p>回顾Seq2Seq的工作方式：编码器读取整个输入序列，将所有信息压缩到一个固定长度的上下文向量 <span class="math inline">\(\mathbf{c}\)</span> 中；解码器仅凭这个向量，逐词生成输出。这意味着，无论输入是5个词还是50个词，所有信息都要塞进同一个维度的向量。</p>
<p>Sutskever等人(2014)的实验清楚地展示了这个问题：当输入句子超过20个词时，翻译质量急剧下降。更长的句子包含更多信息，而固定大小的向量无法承载。</p>
<p>让我们用一个具体的例子感受这个问题。考虑翻译任务：</p>
<blockquote class="blockquote">
<p><strong>英语</strong>：The agreement on the European Economic Area was signed in August 1992.</p>
<p><strong>法语</strong>：L’accord sur la zone économique européenne a été signé en août 1992.</p>
</blockquote>
<p>当解码器生成”août”（八月）时，它需要知道原文中的”August”。但在标准Seq2Seq中，“August”这个词首先被编码进隐藏状态，然后与其他所有词的信息混合在一起，最终压缩成上下文向量 <span class="math inline">\(\mathbf{c}\)</span>。解码器要从这个压缩后的向量中”挖出”August的信息——这就像从一锅汤里找回原来的食材。</p>
<p>更糟糕的是，句子中的某些词对当前生成的词更重要。翻译”August”时，模型最需要关注的是原文中的”August”，而不是”The”或”was”。但标准Seq2Seq对所有输入位置一视同仁——它们都被同等地压缩进了 <span class="math inline">\(\mathbf{c}\)</span>。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：解码器在生成每个词时，应该能够<strong>有选择地关注</strong>输入序列的不同位置。不同的输出词需要关注不同的输入词——这就是”注意力”的本质。Attention机制让解码器在每一步都能访问编码器的所有隐藏状态，并根据当前任务动态计算它们的重要性权重。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="问题的精确定义" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</h3>
<p>让我们形式化地描述Seq2Seq的信息瓶颈问题。</p>
<p>在标准Seq2Seq中，编码器产生一系列隐藏状态 <span class="math inline">\(\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, \ldots, \mathbf{h}_T^{enc}\)</span>，但只有最后一个状态 <span class="math inline">\(\mathbf{h}_T^{enc}\)</span> 被传递给解码器作为上下文向量：</p>
<p><span class="math display">\[
\mathbf{c} = \mathbf{h}_T^{enc}
\]</span></p>
<p>解码器的每一步都使用这同一个 <span class="math inline">\(\mathbf{c}\)</span>：</p>
<p><span class="math display">\[
\mathbf{h}_t^{dec} = f(\mathbf{h}_{t-1}^{dec}, y_{t-1}, \mathbf{c})
\]</span></p>
<p>问题在于：<span class="math inline">\(\mathbf{c}\)</span> 是一个<strong>静态</strong>的、<strong>全局</strong>的表示。它在解码的每一步都保持不变，无法根据当前生成的词动态调整。</p>
<p>从信息论的角度看，如果输入序列 <span class="math inline">\(\mathbf{x}\)</span> 的信息熵是 <span class="math inline">\(H(\mathbf{x})\)</span>，而 <span class="math inline">\(\mathbf{c}\)</span> 的维度是 <span class="math inline">\(d\)</span>，那么 <span class="math inline">\(\mathbf{c}\)</span> 最多能携带 <span class="math inline">\(O(d)\)</span> 的信息量。当 <span class="math inline">\(H(\mathbf{x}) &gt; O(d)\)</span> 时，信息丢失是不可避免的。</p>
</section>
<section id="之前的尝试为何失败" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="之前的尝试为何失败"><span class="header-section-number">2.2</span> 之前的尝试为何失败？</h3>
<p>在Attention出现之前，研究者尝试过一些缓解信息瓶颈的方法：</p>
<p><strong>增加上下文向量维度</strong>：直觉上，更大的 <span class="math inline">\(\mathbf{c}\)</span> 可以携带更多信息。但这只是延缓问题，而非解决问题。而且更大的向量意味着更多参数，更容易过拟合。</p>
<p><strong>使用双向RNN</strong>：让编码器同时从左到右和从右到左读取输入，然后拼接两个方向的最终隐藏状态。这确实能捕获更多上下文，但仍然是压缩到一个固定向量——只是这个向量稍微大了一点。</p>
<p><strong>输入反转</strong>：Sutskever等人发现，将输入序列反转后再输入编码器，翻译效果更好。这是因为输入的最后几个词（反转后变成最先输入的词）与输出的最先几个词往往有更强的对应关系。但这只是一个启发式技巧，不能从根本上解决问题。</p>
<p>这些方法都没有触及问题的核心：<strong>解码器只能看到一个固定的、全局的表示，无法动态地访问输入的不同部分</strong>。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>理想的解决方案应该具备以下特性：</p>
<ol type="1">
<li><strong>动态性</strong>：解码器在生成不同词时，应该能够关注输入的不同位置</li>
<li><strong>软选择</strong>：不是硬性地选择某一个位置，而是对所有位置计算一个重要性分布</li>
<li><strong>端到端可训练</strong>：整个机制应该可以通过反向传播优化</li>
<li><strong>可解释性</strong>：模型关注哪些位置应该是可以观察和理解的</li>
</ol>
<p>这些特性正是Attention机制所提供的。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="关键洞察动态的基于内容的寻址" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="关键洞察动态的基于内容的寻址"><span class="header-section-number">3.1</span> 关键洞察：动态的、基于内容的寻址</h3>
<p>Attention的核心洞察可以用一句话概括：</p>
<blockquote class="blockquote">
<p><strong>让解码器在每一步都能”回头看”编码器的所有位置，并根据当前需要动态决定关注哪些位置。</strong></p>
</blockquote>
<p>这个想法听起来简单，但它彻底改变了序列到序列学习的范式。</p>
</section>
<section id="直觉解释聚光灯与图书馆" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="直觉解释聚光灯与图书馆"><span class="header-section-number">3.2</span> 直觉解释：聚光灯与图书馆</h3>
<p>想象你在一个黑暗的图书馆里找书。传统Seq2Seq就像是：你先用手电筒快速扫过所有书架，然后关掉手电筒，仅凭记忆去取书。你对整个图书馆有一个模糊的整体印象，但细节很容易遗忘。</p>
<p>Attention机制则像是：你手里有一个<strong>可调节的聚光灯</strong>。当你需要找某本书时，你可以把聚光灯照向相关的书架，仔细查看那里的书名。不同的查询需求会让你把光照向不同的位置。</p>
<p>更具体地说，当解码器生成”août”（八月）这个词时，Attention机制会：</p>
<ol type="1">
<li>查看编码器的所有隐藏状态（图书馆的所有书架）</li>
<li>计算每个位置与当前任务的相关性（判断每个书架是否可能有你要的书）</li>
<li>把”聚光灯”主要照向相关的位置（“August”对应的编码器状态）</li>
<li>从这些位置汇总信息，辅助生成当前词</li>
</ol>
</section>
<section id="另一个类比加权投票" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="另一个类比加权投票"><span class="header-section-number">3.3</span> 另一个类比：加权投票</h3>
<p>你也可以把Attention理解为一种加权投票机制。</p>
<p>想象解码器是一个领导，需要做一个决定（生成下一个词）。它有一个顾问团队（编码器的各个隐藏状态），每个顾问掌握输入序列不同部分的信息。</p>
<p>传统Seq2Seq：只听一个”总顾问”的意见（上下文向量 <span class="math inline">\(\mathbf{c}\)</span>），这个总顾问要综合所有人的信息。</p>
<p>Attention机制：直接征询每个顾问的意见，然后根据议题相关性给不同顾问的意见赋予不同权重，加权求和得出最终决定。</p>
</section>
<section id="设计动机为什么选择软注意力" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="设计动机为什么选择软注意力"><span class="header-section-number">3.4</span> 设计动机：为什么选择软注意力？</h3>
<p>Attention机制有两种变体：</p>
<ul>
<li><strong>软注意力（Soft Attention）</strong>：对所有位置计算概率分布，加权求和</li>
<li><strong>硬注意力（Hard Attention）</strong>：选择一个位置，只看那里的信息</li>
</ul>
<p>Bahdanau等人选择了软注意力，原因是：</p>
<ol type="1">
<li><strong>可微分</strong>：软注意力的加权求和是可微的，可以用标准的反向传播训练</li>
<li><strong>稳定</strong>：硬注意力需要采样或强化学习方法训练，方差大，不稳定</li>
<li><strong>信息更丰富</strong>：软注意力可以同时利用多个位置的信息，而不是非此即彼</li>
</ol>
<p>硬注意力也有其优势（计算更高效，更稀疏），但在实践中，软注意力因其简单和有效成为了主流。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="bahdanau-attention加性注意力" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="bahdanau-attention加性注意力"><span class="header-section-number">4.1</span> Bahdanau Attention：加性注意力</h3>
<p>2014年，Bahdanau、Cho和Bengio提出了第一个成功的注意力机制用于机器翻译。让我们详细看看它是如何工作的。</p>
<p>首先，编码器使用<strong>双向RNN</strong>，在每个位置 <span class="math inline">\(j\)</span> 产生一个隐藏状态：</p>
<p><span class="math display">\[
\mathbf{h}_j = [\overrightarrow{\mathbf{h}}_j; \overleftarrow{\mathbf{h}}_j]
\]</span></p>
<p>其中 <span class="math inline">\(\overrightarrow{\mathbf{h}}_j\)</span> 是前向RNN的隐藏状态，<span class="math inline">\(\overleftarrow{\mathbf{h}}_j\)</span> 是后向RNN的隐藏状态。拼接后，<span class="math inline">\(\mathbf{h}_j\)</span> 同时包含了位置 <span class="math inline">\(j\)</span> 的左侧和右侧上下文。</p>
<p>在解码的第 <span class="math inline">\(i\)</span> 步，我们计算一个<strong>动态的上下文向量</strong> <span class="math inline">\(\mathbf{c}_i\)</span>（注意：不再是固定的 <span class="math inline">\(\mathbf{c}\)</span>，而是每一步都不同的 <span class="math inline">\(\mathbf{c}_i\)</span>）：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
\]</span></p>
<p>其中 <span class="math inline">\(\alpha_{ij}\)</span> 是第 <span class="math inline">\(i\)</span> 步解码时，对输入位置 <span class="math inline">\(j\)</span> 的注意力权重。</p>
</section>
<section id="注意力权重的计算" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="注意力权重的计算"><span class="header-section-number">4.2</span> 注意力权重的计算</h3>
<p>那么 <span class="math inline">\(\alpha_{ij}\)</span> 是怎么计算的呢？这是Attention机制的核心。</p>
<p>首先，计算一个<strong>对齐分数（alignment score）</strong> <span class="math inline">\(e_{ij}\)</span>，衡量解码器当前状态与编码器位置 <span class="math inline">\(j\)</span> 的相关性：</p>
<p><span class="math display">\[
e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{s}_{i-1}\)</span> 是解码器在第 <span class="math inline">\(i-1\)</span> 步的隐藏状态，<span class="math inline">\(a\)</span> 是一个<strong>对齐模型（alignment model）</strong>。</p>
<p>Bahdanau使用了一个单层前馈网络作为对齐模型：</p>
<p><span class="math display">\[
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
\]</span></p>
<p>这被称为<strong>加性注意力（additive attention）</strong>，因为 <span class="math inline">\(\mathbf{s}_{i-1}\)</span> 和 <span class="math inline">\(\mathbf{h}_j\)</span> 是通过加法结合的。</p>
<p>然后，对所有位置的分数做softmax归一化，得到注意力权重：</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
\]</span></p>
<p>softmax确保了：</p>
<ul>
<li>所有权重都是正数：<span class="math inline">\(\alpha_{ij} &gt; 0\)</span></li>
<li>权重之和为1：<span class="math inline">\(\sum_j \alpha_{ij} = 1\)</span></li>
</ul>
<p>这样，<span class="math inline">\(\alpha_{ij}\)</span> 可以解释为一个概率分布——解码器在第 <span class="math inline">\(i\)</span> 步”关注”输入位置 <span class="math inline">\(j\)</span> 的概率。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Bahdanau Attention (Bahdanau et al., 2015)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Bahdanau (加性) 注意力机制</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        s_prev: 解码器上一步的隐藏状态 [batch, dec_hidden]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        W_a, U_a, v_a: 可学习参数</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        context: 上下文向量 [batch, enc_hidden]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_weights: 注意力权重 [batch, src_len]</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 计算对齐分数</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># s_prev 广播到所有源位置</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> v_a <span class="op">@</span> tanh(W_a <span class="op">@</span> s_prev <span class="op">+</span> U_a <span class="op">@</span> encoder_outputs)  <span class="co"># [batch, src_len]</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Softmax 归一化</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 加权求和</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> attention_weights <span class="op">@</span> encoder_outputs  <span class="co"># [batch, enc_hidden]</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Bahdanau, Cho, &amp; Bengio (2015) “Neural Machine Translation by Jointly Learning to Align and Translate”, ICLR 2015. <a href="https://arxiv.org/abs/1409.0473">arXiv:1409.0473</a></em></p>
</div>
</div>
<p>下图展示了带Bahdanau Attention的RNN Encoder-Decoder架构：</p>
<div id="fig-attention-mechanism" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-mechanism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig-bahdanau-attention-d2l.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-mechanism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 带Bahdanau Attention的RNN Encoder-Decoder架构。编码器（底部）使用双向RNN处理输入序列，产生隐藏状态序列。解码器（顶部）在每一步通过Attention机制动态计算上下文向量：将当前解码器状态与所有编码器状态比较，得到注意力权重，加权求和得到上下文向量<span class="math inline">\(c_t\)</span>，辅助生成下一个词。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Dive into Deep Learning, Figure 11.4.2. <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">d2l.ai</a></em></p>
</div>
</section>
<section id="完整数值示例attention计算" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="完整数值示例attention计算"><span class="header-section-number">4.3</span> 完整数值示例：Attention计算</h3>
<p>让我们用一个小例子走一遍完整的Attention计算过程。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>输入序列：3个词（“I love NLP”），编码后得到3个隐藏状态</li>
<li>解码器隐藏状态维度：<span class="math inline">\(d_s = 4\)</span></li>
<li>编码器隐藏状态维度：<span class="math inline">\(d_h = 4\)</span></li>
<li>注意力中间维度：<span class="math inline">\(d_a = 3\)</span></li>
</ul>
<p><strong>编码器输出</strong>（假设已经计算好）：</p>
<p><span class="math display">\[
\mathbf{h}_1 = [0.2, 0.5, -0.3, 0.8]^\top \quad \text{("I")}
\]</span></p>
<p><span class="math display">\[
\mathbf{h}_2 = [0.7, -0.2, 0.4, 0.1]^\top \quad \text{("love")}
\]</span></p>
<p><span class="math display">\[
\mathbf{h}_3 = [-0.1, 0.6, 0.5, -0.4]^\top \quad \text{("NLP")}
\]</span></p>
<p><strong>解码器当前状态</strong>（正在生成第一个目标词）：</p>
<p><span class="math display">\[
\mathbf{s}_0 = [0.1, -0.3, 0.4, 0.2]^\top
\]</span></p>
<p><strong>参数</strong>（简化的随机值）：</p>
<p><span class="math display">\[
\mathbf{W}_a = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.1 \\ 0.2 &amp; 0.1 &amp; -0.1 &amp; 0.2 \\ -0.1 &amp; 0.3 &amp; 0.2 &amp; -0.2 \end{bmatrix}, \quad
\mathbf{U}_a = \begin{bmatrix} 0.2 &amp; 0.1 &amp; -0.2 &amp; 0.3 \\ -0.1 &amp; 0.2 &amp; 0.1 &amp; 0.1 \\ 0.3 &amp; -0.1 &amp; 0.2 &amp; -0.1 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{v}_a = [0.5, -0.3, 0.4]^\top
\]</span></p>
<p><strong>Step 1：计算 <span class="math inline">\(\mathbf{W}_a \mathbf{s}_0\)</span></strong></p>
<p><span class="math display">\[
\mathbf{W}_a \mathbf{s}_0 = \begin{bmatrix} 0.1 \cdot 0.1 + (-0.2) \cdot (-0.3) + 0.3 \cdot 0.4 + 0.1 \cdot 0.2 \\ \vdots \end{bmatrix} = \begin{bmatrix} 0.21 \\ 0.03 \\ 0.04 \end{bmatrix}
\]</span></p>
<p><strong>Step 2：对每个编码器状态计算 <span class="math inline">\(\mathbf{U}_a \mathbf{h}_j\)</span></strong></p>
<p><span class="math display">\[
\mathbf{U}_a \mathbf{h}_1 = [0.33, 0.14, -0.05]^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{U}_a \mathbf{h}_2 = [0.13, 0.06, 0.29]^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{U}_a \mathbf{h}_3 = [-0.15, 0.17, 0.05]^\top
\]</span></p>
<p><strong>Step 3：计算对齐分数 <span class="math inline">\(e_{1j}\)</span></strong></p>
<p><span class="math display">\[
e_{11} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_0 + \mathbf{U}_a \mathbf{h}_1) = \mathbf{v}_a^\top \tanh([0.54, 0.17, -0.01]^\top)
\]</span></p>
<p><span class="math display">\[
= [0.5, -0.3, 0.4] \cdot [\tanh(0.54), \tanh(0.17), \tanh(-0.01)]^\top
\]</span></p>
<p><span class="math display">\[
= [0.5, -0.3, 0.4] \cdot [0.49, 0.17, -0.01]^\top = 0.24 - 0.05 - 0.004 \approx 0.19
\]</span></p>
<p>类似地计算 <span class="math inline">\(e_{12}\)</span> 和 <span class="math inline">\(e_{13}\)</span>：</p>
<p><span class="math display">\[
e_{12} \approx 0.25, \quad e_{13} \approx 0.08
\]</span></p>
<p><strong>Step 4：Softmax归一化</strong></p>
<p><span class="math display">\[
\alpha_{11} = \frac{\exp(0.19)}{\exp(0.19) + \exp(0.25) + \exp(0.08)} = \frac{1.21}{1.21 + 1.28 + 1.08} = \frac{1.21}{3.57} \approx 0.34
\]</span></p>
<p><span class="math display">\[
\alpha_{12} = \frac{1.28}{3.57} \approx 0.36, \quad \alpha_{13} = \frac{1.08}{3.57} \approx 0.30
\]</span></p>
<p><strong>Step 5：计算上下文向量</strong></p>
<p><span class="math display">\[
\mathbf{c}_1 = \alpha_{11} \mathbf{h}_1 + \alpha_{12} \mathbf{h}_2 + \alpha_{13} \mathbf{h}_3
\]</span></p>
<p><span class="math display">\[
= 0.34 \cdot [0.2, 0.5, -0.3, 0.8]^\top + 0.36 \cdot [0.7, -0.2, 0.4, 0.1]^\top + 0.30 \cdot [-0.1, 0.6, 0.5, -0.4]^\top
\]</span></p>
<p><span class="math display">\[
\approx [0.29, 0.18, 0.09, 0.19]^\top
\]</span></p>
<p><strong>解读</strong>：在这个例子中，模型对”love”的关注最多（0.36），其次是”I”（0.34）和”NLP”（0.30）。注意力权重相对均匀，这可能是因为我们用的是随机参数。在训练后的真实模型中，权重分布会更加尖锐——模型会学会在需要时聚焦于特定位置。</p>
</section>
<section id="解码器的完整流程" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="解码器的完整流程"><span class="header-section-number">4.4</span> 解码器的完整流程</h3>
<p>有了Attention机制，解码器的每一步工作流程变为：</p>
<ol type="1">
<li><strong>计算注意力权重</strong> <span class="math inline">\(\alpha_{ij}\)</span>：基于当前解码器状态和所有编码器状态</li>
<li><strong>计算上下文向量</strong> <span class="math inline">\(\mathbf{c}_i\)</span>：对编码器状态加权求和</li>
<li><strong>更新解码器状态</strong>：结合上下文向量、前一步输出、前一步状态</li>
</ol>
<p><span class="math display">\[
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)
\]</span></p>
<ol start="4" type="1">
<li><strong>生成输出</strong>：基于新的解码器状态</li>
</ol>
<p><span class="math display">\[
P(y_i | y_{&lt;i}, \mathbf{x}) = g(\mathbf{s}_i, y_{i-1}, \mathbf{c}_i)
\]</span></p>
<p>关键区别是：<strong>每一步都有一个不同的上下文向量 <span class="math inline">\(\mathbf{c}_i\)</span></strong>，它是根据当前任务动态计算的。</p>
</section>
<section id="复杂度分析" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="复杂度分析"><span class="header-section-number">4.5</span> 复杂度分析</h3>
<p><strong>时间复杂度</strong>：</p>
<ul>
<li>计算所有对齐分数：<span class="math inline">\(O(T_x \cdot T_y \cdot d)\)</span></li>
<li>其中 <span class="math inline">\(T_x\)</span> 是源序列长度，<span class="math inline">\(T_y\)</span> 是目标序列长度，<span class="math inline">\(d\)</span> 是隐藏维度</li>
</ul>
<p>与标准Seq2Seq相比，Attention增加了 <span class="math inline">\(O(T_x \cdot T_y)\)</span> 的计算量。对于长序列，这个开销是显著的。</p>
<p><strong>空间复杂度</strong>：</p>
<ul>
<li>需要存储所有编码器隐藏状态：<span class="math inline">\(O(T_x \cdot d)\)</span></li>
<li>标准Seq2Seq只需要存储最终状态：<span class="math inline">\(O(d)\)</span></li>
</ul>
<p>这是用空间换取性能的典型例子。</p>
<hr>
</section>
</section>
<section id="注意力可视化模型在看什么" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="注意力可视化模型在看什么"><span class="header-section-number">5</span> 注意力可视化：模型在”看”什么？</h2>
<section id="对齐矩阵" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="对齐矩阵"><span class="header-section-number">5.1</span> 对齐矩阵</h3>
<p>Attention机制的一个美妙特性是<strong>可解释性</strong>。注意力权重 <span class="math inline">\(\alpha_{ij}\)</span> 直接告诉我们：在生成第 <span class="math inline">\(i\)</span> 个目标词时，模型关注了哪些源词。</p>
<p>我们可以把所有的注意力权重排列成一个矩阵，横轴是源序列，纵轴是目标序列。这个矩阵被称为<strong>对齐矩阵（alignment matrix）</strong>。</p>
<div id="fig-alignment-visualization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alignment-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig3-alignment-visualization.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alignment-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: 对齐可视化：四个英法翻译例子的注意力权重热力图。横轴是英语源句子，纵轴是法语目标句子。白色表示高注意力权重，黑色表示低权重。注意对角线模式（单词一一对应）和偏离对角线的区域（语序调整）。例如(a)中”August”对应”août”，“European Economic Area”对应”zone économique européenne”。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Bahdanau, Cho, &amp; Bengio (2015) “Neural Machine Translation by Jointly Learning to Align and Translate”, Figure 3. <a href="https://arxiv.org/abs/1409.0473">arXiv:1409.0473</a></em></p>
</div>
</section>
<section id="对齐模式的语言学意义" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="对齐模式的语言学意义"><span class="header-section-number">5.2</span> 对齐模式的语言学意义</h3>
<p>通过观察对齐矩阵，我们可以发现一些有趣的语言学模式：</p>
<p><strong>1. 单调对齐</strong></p>
<p>对于语序相似的语言对（如英语到德语的某些结构），对齐矩阵接近对角线——第1个源词对应第1个目标词，第2个对应第2个，依此类推。</p>
<p><strong>2. 语序调整</strong></p>
<p>当源语言和目标语言的词序不同时，对齐矩阵会偏离对角线。例如，英语的”red car”翻译成法语是”voiture rouge”（车 红），对齐矩阵会显示交叉模式。</p>
<p><strong>3. 一对多和多对一</strong></p>
<p>某些词没有直接对应，或一个词对应多个词。例如，英语的”going to”可能对应法语的单个词”va”。</p>
<p><strong>4. 空对齐</strong></p>
<p>某些目标词（如冠词）可能没有明确的源词对应，它们的注意力权重会分散在多个位置。</p>
</section>
<section id="可视化的局限性" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="可视化的局限性"><span class="header-section-number">5.3</span> 可视化的局限性</h3>
<p>虽然注意力可视化很吸引人，但我们要谨慎解读：</p>
<ol type="1">
<li><strong>注意力不等于解释</strong>：高注意力权重不一定意味着模型”理解”了那个位置的内容</li>
<li><strong>可能有多重因素</strong>：模型可能通过其他机制（如位置信息）做出决定</li>
<li><strong>训练目标的影响</strong>：注意力权重是为了最小化翻译损失而学习的，不一定反映人类的对齐直觉</li>
</ol>
<p>后来的研究（如Jain &amp; Wallace, 2019）对注意力的可解释性提出了质疑。但作为一个诊断工具，注意力可视化仍然非常有价值。</p>
<hr>
</section>
</section>
<section id="工程实践带attention的seq2seq" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="工程实践带attention的seq2seq"><span class="header-section-number">6</span> 工程实践：带Attention的Seq2Seq</h2>
<p>让我们用PyTorch实现一个带Attention的Seq2Seq模型。</p>
<section id="编码器" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="编码器"><span class="header-section-number">6.1</span> 编码器</h3>
<div id="d766c2b5" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, hidden_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            embed_dim, hidden_dim,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,  <span class="co"># 双向</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 将双向的隐藏状态压缩到单向维度</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim <span class="op">*</span> <span class="dv">2</span>, hidden_dim)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_lengths<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src: [batch_size, src_len]</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(src))  <span class="co"># [batch, src_len, embed_dim]</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> src_lengths <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            packed <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                embedded, src_lengths.cpu(), batch_first<span class="op">=</span><span class="va">True</span>, enforce_sorted<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            packed_outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(packed)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            outputs, _ <span class="op">=</span> nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(embedded)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs: [batch, src_len, hidden_dim * 2] (双向拼接)</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [num_layers * 2, batch, hidden_dim]</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并前向和后向的最终隐藏状态</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-2] 是最后一层前向，hidden[-1] 是最后一层后向</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> torch.tanh(<span class="va">self</span>.fc(torch.cat([hidden[<span class="op">-</span><span class="dv">2</span>], hidden[<span class="op">-</span><span class="dv">1</span>]], dim<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, hidden_dim]</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, hidden</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="attention层" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="attention层"><span class="header-section-number">6.2</span> Attention层</h3>
<div id="5d037926" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BahdanauAttention(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, attention_dim):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加性注意力的参数</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(dec_hidden_dim, attention_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.U_a <span class="op">=</span> nn.Linear(enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>, attention_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># 双向编码器</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_a <span class="op">=</span> nn.Linear(attention_dim, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_hidden: [batch, dec_hidden]</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, src_len], True表示需要mask的位置（padding）</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decoder_hidden 扩展到所有源位置</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, dec_hidden] -&gt; [batch, src_len, dec_hidden]</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        decoder_hidden <span class="op">=</span> decoder_hidden.unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, src_len, <span class="dv">1</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算对齐分数</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, attention_dim]</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        energy <span class="op">=</span> torch.tanh(<span class="va">self</span>.W_a(decoder_hidden) <span class="op">+</span> <span class="va">self</span>.U_a(encoder_outputs))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, 1] -&gt; [batch, src_len]</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> <span class="va">self</span>.v_a(energy).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用mask（将padding位置的分数设为很小的负数）</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            attention_scores <span class="op">=</span> attention_scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax归一化</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(attention_scores, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算上下文向量</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -&gt; [batch, 1, enc_hidden*2]</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(attention_weights.unsqueeze(<span class="dv">1</span>), encoder_outputs)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> context.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, enc_hidden * 2]</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="解码器" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="解码器"><span class="header-section-number">6.3</span> 解码器</h3>
<div id="ded5da62" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionDecoder(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                 attention_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># GRU输入是：embedded + context</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            embed_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>,  <span class="co"># 双向编码器</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            dec_hidden_dim,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(dec_hidden_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> embed_dim, vocab_size)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_token, hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">        单步解码</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co">        input_token: [batch] - 上一步的输出token</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden: [1, batch, dec_hidden] - 上一步的隐藏状态</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Embedding</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(input_token))  <span class="co"># [batch, embed_dim]</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-1] 取最后一层，[batch, dec_hidden]</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        context, attention_weights <span class="op">=</span> <span class="va">self</span>.attention(hidden[<span class="op">-</span><span class="dv">1</span>], encoder_outputs, mask)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接embedded和context作为RNN输入</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        rnn_input <span class="op">=</span> torch.cat([embedded, context], dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch, 1, embed+ctx]</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RNN</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        output, hidden <span class="op">=</span> <span class="va">self</span>.rnn(rnn_input, hidden)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, dec_hidden]</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="va">self</span>.fc(torch.cat([output, context, embedded], dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction, hidden, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="完整的seq2seq模型" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="完整的seq2seq模型"><span class="header-section-number">6.4</span> 完整的Seq2Seq模型</h3>
<div id="e372dfb6" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2SeqAttention(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, device):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg, teacher_forcing_ratio<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">        src: [batch, src_len]</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">        trg: [batch, trg_len]</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> src.shape[<span class="dv">0</span>]</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        trg_vocab_size <span class="op">=</span> <span class="va">self</span>.decoder.vocab_size</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 存储输出</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> torch.zeros(batch_size, trg_len, trg_vocab_size).to(<span class="va">self</span>.device)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        attentions <span class="op">=</span> []</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 编码</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        encoder_outputs, hidden <span class="op">=</span> <span class="va">self</span>.encoder(src)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, dec_hidden] -&gt; [1, batch, dec_hidden]</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> hidden.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 第一个解码输入是 &lt;sos&gt; token</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        input_token <span class="op">=</span> trg[:, <span class="dv">0</span>]</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, trg_len):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            prediction, hidden, attention <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>                input_token, hidden, encoder_outputs</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            outputs[:, t] <span class="op">=</span> prediction</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>            attentions.append(attention)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Teacher forcing</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>            teacher_force <span class="op">=</span> torch.rand(<span class="dv">1</span>).item() <span class="op">&lt;</span> teacher_forcing_ratio</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>            top1 <span class="op">=</span> prediction.argmax(<span class="dv">1</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>            input_token <span class="op">=</span> trg[:, t] <span class="cf">if</span> teacher_force <span class="cf">else</span> top1</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, torch.stack(attentions, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建模型示例</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>, hidden_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> AttentionDecoder(</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    enc_hidden_dim<span class="op">=</span><span class="dv">512</span>, dec_hidden_dim<span class="op">=</span><span class="dv">512</span>, attention_dim<span class="op">=</span><span class="dv">256</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2SeqAttention(encoder, decoder, device).to(device)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"编码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> encoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"解码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"总参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>编码器参数: 5,450,240
解码器参数: 23,639,056
总参数: 29,089,296</code></pre>
</div>
</div>
</section>
<section id="关键实现细节" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="关键实现细节"><span class="header-section-number">6.5</span> 关键实现细节</h3>
<p><strong>1. Mask处理</strong></p>
<p>在实际应用中，batch中的序列长度不同，需要padding。计算注意力时，padding位置不应该获得任何权重。我们通过mask将这些位置的分数设为很大的负数，softmax后它们的权重趋近于0。</p>
<p><strong>2. Teacher Forcing</strong></p>
<p>训练时，解码器的输入可以是真实的上一个词（teacher forcing）或模型预测的词。<code>teacher_forcing_ratio</code> 控制两者的混合比例。较高的比例加速训练，但可能导致exposure bias。</p>
<p><strong>3. 双向编码器</strong></p>
<p>我们使用双向GRU，编码器输出的维度是 <code>hidden_dim * 2</code>。这让每个位置都包含完整的上下文信息。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">7</span> 深入理解</h2>
<section id="为什么attention有效理论视角" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="为什么attention有效理论视角"><span class="header-section-number">7.1</span> 为什么Attention有效？——理论视角</h3>
<p><strong>1. 信息论视角</strong></p>
<p>标准Seq2Seq的上下文向量 <span class="math inline">\(\mathbf{c}\)</span> 是输入 <span class="math inline">\(\mathbf{x}\)</span> 的一个<strong>充分统计量（sufficient statistic）</strong>——如果 <span class="math inline">\(\mathbf{c}\)</span> 完美，它应该包含关于 <span class="math inline">\(\mathbf{y}\)</span> 的所有必要信息。但在实践中，有限维度的 <span class="math inline">\(\mathbf{c}\)</span> 无法做到这一点。</p>
<p>Attention通过让解码器访问所有的 <span class="math inline">\(\mathbf{h}_j\)</span>，实际上是在说：<strong>不要求一个充分统计量，而是让模型在需要时直接查询原始信息</strong>。这绕过了信息瓶颈。</p>
<p><strong>2. 记忆寻址视角</strong></p>
<p>可以把编码器的隐藏状态看作一个<strong>外部记忆（external memory）</strong>，每个 <span class="math inline">\(\mathbf{h}_j\)</span> 是一个记忆槽。Attention机制实现了<strong>基于内容的软寻址（content-based soft addressing）</strong>——根据当前查询（解码器状态）检索相关的记忆。</p>
<p>这个视角后来被显式化为Memory Networks和Neural Turing Machine。</p>
<p><strong>3. 梯度流视角</strong></p>
<p>从优化角度，Attention提供了一条从解码器到编码器特定位置的<strong>直接路径</strong>。在标准Seq2Seq中，梯度要从解码器流回编码器，必须经过 <span class="math inline">\(\mathbf{c}\)</span>，再经过整个编码过程。Attention创造了”捷径”——梯度可以通过注意力权重直接传到相关的编码器位置。</p>
</section>
<section id="边界条件与失效模式" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="边界条件与失效模式"><span class="header-section-number">7.2</span> 边界条件与失效模式</h3>
<p><strong>1. 单调对齐假设</strong></p>
<p>Bahdanau Attention隐含假设源和目标之间存在某种对齐关系。对于翻译任务这通常成立，但对于某些任务（如摘要），这个假设可能不成立——摘要可能需要整合分散在各处的信息，而不是”对齐”到特定位置。</p>
<p><strong>2. 复杂度限制</strong></p>
<p>当源序列很长时（如文档级翻译），计算所有位置的注意力权重变得昂贵。<span class="math inline">\(O(T_x \cdot T_y)\)</span> 的复杂度在 <span class="math inline">\(T_x = 10000\)</span> 时是不可接受的。</p>
<p><strong>3. 分布偏移</strong></p>
<p>训练时，解码器看到的上下文向量分布与推理时可能不同（因为teacher forcing）。这可能导致注意力权重在推理时不够准确。</p>
</section>
<section id="开放研究问题" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">7.3</span> 开放研究问题</h3>
<ol type="1">
<li><p><strong>最优对齐</strong>：Attention学到的对齐与语言学家标注的对齐有什么关系？是否可以用语言学知识改进Attention？</p></li>
<li><p><strong>稀疏注意力</strong>：能否学习更稀疏的注意力分布，只关注少数关键位置，而不是soft地分布到所有位置？</p></li>
<li><p><strong>层次化注意力</strong>：对于长文档，能否设计层次化的Attention——先关注段落，再关注句子，最后关注词？</p></li>
</ol>
<hr>
</section>
</section>
<section id="局限性与展望" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="局限性与展望"><span class="header-section-number">8</span> 局限性与展望</h2>
<section id="本章方法的核心局限" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="本章方法的核心局限"><span class="header-section-number">8.1</span> 本章方法的核心局限</h3>
<p><strong>1. 仍然依赖RNN</strong></p>
<p>Bahdanau Attention是Seq2Seq的”补丁”——它增强了解码器访问信息的能力，但底层仍然是RNN。这意味着：</p>
<ul>
<li>仍然是顺序计算，无法并行</li>
<li>仍然受限于RNN的长距离依赖问题（虽然因为Attention有所缓解）</li>
</ul>
<p><strong>2. 注意力计算与序列长度平方相关</strong></p>
<p>每个解码步需要计算对所有编码位置的注意力，总复杂度是 <span class="math inline">\(O(T_x \cdot T_y)\)</span>。对于长序列，这是显著的开销。</p>
<p><strong>3. 没有位置感知</strong></p>
<p>Attention是基于内容的，它不直接考虑位置信息。虽然双向RNN隐式编码了位置，但Attention本身对位置是”盲目”的。</p>
</section>
<section id="这些局限指向什么" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="这些局限指向什么"><span class="header-section-number">8.2</span> 这些局限指向什么？</h3>
<p>Attention的成功引发了一个自然的问题：<strong>如果Attention如此强大，我们还需要RNN吗？</strong></p>
<p>下一章将探讨Attention的各种变体，包括Luong提出的乘性注意力。更重要的是，这些探索最终导向了一个革命性的结论：<strong>我们可以完全用Attention取代RNN</strong>。</p>
<p>这就是第8章Transformer的核心思想——“Attention Is All You Need”。在那里，Self-Attention让序列中的每个位置都能直接与其他位置交互，完全抛弃了循环结构，实现了真正的并行计算。</p>
<blockquote class="blockquote">
<p>从Bahdanau Attention到Transformer，Attention从一个”辅助机制”演变为”核心架构”。这是深度学习历史上最重要的范式转变之一。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">9</span> 本章小结</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>核心要点
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>问题</strong>：Seq2Seq的信息瓶颈——所有输入信息压缩到一个固定向量，导致长序列信息丢失</li>
<li><strong>洞察</strong>：解码器应该能够动态地、有选择地关注输入的不同位置</li>
<li><strong>方法</strong>：Attention机制计算解码器状态与每个编码器状态的相关性，生成注意力权重，加权求和得到动态上下文向量</li>
<li><strong>意义</strong>：打破了固定向量的限制，大幅提升了长序列翻译质量，为后续的Transformer奠定了基础</li>
</ul>
</div>
</div>
<section id="关键公式速查" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">9.1</span> 关键公式速查</h3>
<p><strong>对齐分数（Bahdanau加性注意力）</strong>：</p>
<p><span class="math display">\[
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
\]</span></p>
<p><strong>注意力权重</strong>：</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
\]</span></p>
<p><strong>上下文向量</strong>：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
\]</span></p>
<hr>
</section>
</section>
<section id="思考题" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="思考题"><span class="header-section-number">10</span> 思考题</h2>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么说Attention实现了”软寻址”？它与计算机内存的硬寻址有什么本质区别？这种软寻址的优势和劣势是什么？</p></li>
<li><p><strong>[数学推导]</strong> 证明：当注意力权重集中在单一位置时（即 <span class="math inline">\(\alpha_{ij} \to 1\)</span> 对某个 <span class="math inline">\(j\)</span>，其他为0），上下文向量就退化为那个位置的编码器状态。这与硬注意力有什么关系？</p></li>
<li><p><strong>[工程实践]</strong> 在实现Attention时，为什么要对padding位置应用mask？如果不做mask会有什么后果？如何正确实现mask（考虑数值稳定性）？</p></li>
<li><p><strong>[批判思考]</strong> Attention的可视化经常被用来”解释”模型的决策。但这种解释是否可靠？设计一个实验来检验：注意力权重高的位置是否真的对模型输出有重要影响。</p></li>
<li><p><strong>[开放问题]</strong> Bahdanau Attention需要为每个解码步计算对所有编码位置的注意力，复杂度是 <span class="math inline">\(O(T_x \cdot T_y)\)</span>。有哪些方法可以降低这个复杂度？（提示：考虑稀疏化、局部化、或近似方法）</p></li>
</ol>
<hr>
</section>
<section id="延伸阅读" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">11</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate</strong>
<ul>
<li>Attention机制在NMT中的开创性工作</li>
<li>重点读：Section 3（模型架构）、Section 5（可视化分析）</li>
<li>arXiv: <a href="https://arxiv.org/abs/1409.0473">1409.0473</a></li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">11.2</span> 理论基础</h3>
<ul>
<li><strong>[Graves et al., 2014] Neural Turing Machines</strong>
<ul>
<li>提出了基于内容的软寻址，是Attention的理论先驱</li>
<li>重点读：Section 3.1（Attention机制）</li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">11.3</span> 后续发展</h3>
<ul>
<li><strong>[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation</strong>
<ul>
<li>提出乘性注意力，对比不同注意力变体</li>
<li>这是下一章的核心内容</li>
<li>arXiv: <a href="https://arxiv.org/abs/1508.04025">1508.04025</a></li>
</ul></li>
<li><strong>[Vaswani et al., 2017] Attention Is All You Need</strong>
<ul>
<li>Transformer：完全用Attention取代RNN</li>
<li>这是第8章的核心内容</li>
<li>arXiv: <a href="https://arxiv.org/abs/1706.03762">1706.03762</a></li>
</ul></li>
</ul>
</section>
<section id="对attention可解释性的讨论" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="对attention可解释性的讨论"><span class="header-section-number">11.4</span> 对Attention可解释性的讨论</h3>
<ul>
<li><strong>[Jain &amp; Wallace, 2019] Attention is not Explanation</strong>
<ul>
<li>质疑Attention权重作为解释的可靠性</li>
<li>arXiv: <a href="https://arxiv.org/abs/1902.10186">1902.10186</a></li>
</ul></li>
<li><strong>[Wiegreffe &amp; Pinter, 2019] Attention is not not Explanation</strong>
<ul>
<li>对上述论文的回应，更细致地讨论Attention的解释性</li>
<li>arXiv: <a href="https://arxiv.org/abs/1908.04626">1908.04626</a></li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">12</span> 历史注脚</h2>
<p>Attention机制的灵感部分来自人类视觉系统。当我们看一幅复杂的图像时，我们不会同时处理所有像素，而是会”聚焦”在感兴趣的区域。这种选择性注意（selective attention）是认知科学研究的经典课题。</p>
<p>Bahdanau在2014年将这个思想引入神经机器翻译时，并没有预料到它会成为深度学习最核心的组件之一。在论文中，他们谦虚地称之为”对齐模型”（alignment model），而不是”注意力”。“Attention”这个术语是后来被社区广泛采用的。</p>
<p>有趣的是，Bahdanau Attention的成功让研究者开始思考：如果Attention这么有效，我们是否需要RNN？两年后，Vaswani等人给出了答案——“Attention Is All You Need”。这篇论文不仅在技术上革新了序列建模，其标题本身也成为了深度学习历史上最具影响力的金句之一。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第5章：注意力机制的诞生"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "打破信息瓶颈：让解码器学会'回头看'"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-25"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Attention, Seq2Seq, 机器翻译, Bahdanau]</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [注意力机制, 加性注意力, 对齐模型, 上下文向量, 神经机器翻译]</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Attention机制的诞生：Bahdanau如何让解码器学会'回头看'，打破Seq2Seq的信息瓶颈，以及注意力权重的可解释性。"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何让解码器在生成每个词时，能够访问输入序列的不同部分，而不是只依赖一个压缩后的向量？</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2014-2015 </span><span class="pp">|</span><span class="at"> Bahdanau, Cho, Bengio </span><span class="pp">|</span><span class="at"> 神经机器翻译的突破</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>上一章我们见证了RNN的辉煌与困境。LSTM和GRU通过门控机制解决了梯度消失问题，Seq2Seq架构让神经网络能够处理翻译、摘要等序列到序列的任务。</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>但Seq2Seq有一个致命的设计缺陷：**信息瓶颈**。</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>回顾Seq2Seq的工作方式：编码器读取整个输入序列，将所有信息压缩到一个固定长度的上下文向量 $\mathbf{c}$ 中；解码器仅凭这个向量，逐词生成输出。这意味着，无论输入是5个词还是50个词，所有信息都要塞进同一个维度的向量。</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>Sutskever等人(2014)的实验清楚地展示了这个问题：当输入句子超过20个词时，翻译质量急剧下降。更长的句子包含更多信息，而固定大小的向量无法承载。</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的例子感受这个问题。考虑翻译任务：</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **英语**：The agreement on the European Economic Area was signed in August 1992.</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **法语**：L'accord sur la zone économique européenne a été signé en août 1992.</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>当解码器生成"août"（八月）时，它需要知道原文中的"August"。但在标准Seq2Seq中，"August"这个词首先被编码进隐藏状态，然后与其他所有词的信息混合在一起，最终压缩成上下文向量 $\mathbf{c}$。解码器要从这个压缩后的向量中"挖出"August的信息——这就像从一锅汤里找回原来的食材。</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>更糟糕的是，句子中的某些词对当前生成的词更重要。翻译"August"时，模型最需要关注的是原文中的"August"，而不是"The"或"was"。但标准Seq2Seq对所有输入位置一视同仁——它们都被同等地压缩进了 $\mathbf{c}$。</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：解码器在生成每个词时，应该能够**有选择地关注**输入序列的不同位置。不同的输出词需要关注不同的输入词——这就是"注意力"的本质。Attention机制让解码器在每一步都能访问编码器的所有隐藏状态，并根据当前任务动态计算它们的重要性权重。</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的精确定义</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>让我们形式化地描述Seq2Seq的信息瓶颈问题。</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>在标准Seq2Seq中，编码器产生一系列隐藏状态 $\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, \ldots, \mathbf{h}_T^{enc}$，但只有最后一个状态 $\mathbf{h}_T^{enc}$ 被传递给解码器作为上下文向量：</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>\mathbf{c} = \mathbf{h}_T^{enc}</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>解码器的每一步都使用这同一个 $\mathbf{c}$：</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t^{dec} = f(\mathbf{h}_{t-1}^{dec}, y_{t-1}, \mathbf{c})</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>问题在于：$\mathbf{c}$ 是一个**静态**的、**全局**的表示。它在解码的每一步都保持不变，无法根据当前生成的词动态调整。</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>从信息论的角度看，如果输入序列 $\mathbf{x}$ 的信息熵是 $H(\mathbf{x})$，而 $\mathbf{c}$ 的维度是 $d$，那么 $\mathbf{c}$ 最多能携带 $O(d)$ 的信息量。当 $H(\mathbf{x}) &gt; O(d)$ 时，信息丢失是不可避免的。</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### 之前的尝试为何失败？</span></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>在Attention出现之前，研究者尝试过一些缓解信息瓶颈的方法：</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>**增加上下文向量维度**：直觉上，更大的 $\mathbf{c}$ 可以携带更多信息。但这只是延缓问题，而非解决问题。而且更大的向量意味着更多参数，更容易过拟合。</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>**使用双向RNN**：让编码器同时从左到右和从右到左读取输入，然后拼接两个方向的最终隐藏状态。这确实能捕获更多上下文，但仍然是压缩到一个固定向量——只是这个向量稍微大了一点。</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>**输入反转**：Sutskever等人发现，将输入序列反转后再输入编码器，翻译效果更好。这是因为输入的最后几个词（反转后变成最先输入的词）与输出的最先几个词往往有更强的对应关系。但这只是一个启发式技巧，不能从根本上解决问题。</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>这些方法都没有触及问题的核心：**解码器只能看到一个固定的、全局的表示，无法动态地访问输入的不同部分**。</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>理想的解决方案应该具备以下特性：</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**动态性**：解码器在生成不同词时，应该能够关注输入的不同位置</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**软选择**：不是硬性地选择某一个位置，而是对所有位置计算一个重要性分布</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**端到端可训练**：整个机制应该可以通过反向传播优化</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**可解释性**：模型关注哪些位置应该是可以观察和理解的</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>这些特性正是Attention机制所提供的。</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键洞察：动态的、基于内容的寻址</span></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>Attention的核心洞察可以用一句话概括：</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **让解码器在每一步都能"回头看"编码器的所有位置，并根据当前需要动态决定关注哪些位置。**</span></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>这个想法听起来简单，但它彻底改变了序列到序列学习的范式。</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a><span class="fu">### 直觉解释：聚光灯与图书馆</span></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>想象你在一个黑暗的图书馆里找书。传统Seq2Seq就像是：你先用手电筒快速扫过所有书架，然后关掉手电筒，仅凭记忆去取书。你对整个图书馆有一个模糊的整体印象，但细节很容易遗忘。</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>Attention机制则像是：你手里有一个**可调节的聚光灯**。当你需要找某本书时，你可以把聚光灯照向相关的书架，仔细查看那里的书名。不同的查询需求会让你把光照向不同的位置。</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>更具体地说，当解码器生成"août"（八月）这个词时，Attention机制会：</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>查看编码器的所有隐藏状态（图书馆的所有书架）</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>计算每个位置与当前任务的相关性（判断每个书架是否可能有你要的书）</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>把"聚光灯"主要照向相关的位置（"August"对应的编码器状态）</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>从这些位置汇总信息，辅助生成当前词</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### 另一个类比：加权投票</span></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>你也可以把Attention理解为一种加权投票机制。</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>想象解码器是一个领导，需要做一个决定（生成下一个词）。它有一个顾问团队（编码器的各个隐藏状态），每个顾问掌握输入序列不同部分的信息。</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>传统Seq2Seq：只听一个"总顾问"的意见（上下文向量 $\mathbf{c}$），这个总顾问要综合所有人的信息。</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>Attention机制：直接征询每个顾问的意见，然后根据议题相关性给不同顾问的意见赋予不同权重，加权求和得出最终决定。</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="fu">### 设计动机：为什么选择软注意力？</span></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>Attention机制有两种变体：</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**软注意力（Soft Attention）**：对所有位置计算概率分布，加权求和</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**硬注意力（Hard Attention）**：选择一个位置，只看那里的信息</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>Bahdanau等人选择了软注意力，原因是：</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**可微分**：软注意力的加权求和是可微的，可以用标准的反向传播训练</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**稳定**：硬注意力需要采样或强化学习方法训练，方差大，不稳定</span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**信息更丰富**：软注意力可以同时利用多个位置的信息，而不是非此即彼</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>硬注意力也有其优势（计算更高效，更稀疏），但在实践中，软注意力因其简单和有效成为了主流。</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bahdanau Attention：加性注意力</span></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>2014年，Bahdanau、Cho和Bengio提出了第一个成功的注意力机制用于机器翻译。让我们详细看看它是如何工作的。</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>首先，编码器使用**双向RNN**，在每个位置 $j$ 产生一个隐藏状态：</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_j = <span class="co">[</span><span class="ot">\overrightarrow{\mathbf{h}}_j; \overleftarrow{\mathbf{h}}_j</span><span class="co">]</span></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>其中 $\overrightarrow{\mathbf{h}}_j$ 是前向RNN的隐藏状态，$\overleftarrow{\mathbf{h}}_j$ 是后向RNN的隐藏状态。拼接后，$\mathbf{h}_j$ 同时包含了位置 $j$ 的左侧和右侧上下文。</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a>在解码的第 $i$ 步，我们计算一个**动态的上下文向量** $\mathbf{c}_i$（注意：不再是固定的 $\mathbf{c}$，而是每一步都不同的 $\mathbf{c}_i$）：</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>其中 $\alpha_{ij}$ 是第 $i$ 步解码时，对输入位置 $j$ 的注意力权重。</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a><span class="fu">### 注意力权重的计算</span></span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a>那么 $\alpha_{ij}$ 是怎么计算的呢？这是Attention机制的核心。</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>首先，计算一个**对齐分数（alignment score）** $e_{ij}$，衡量解码器当前状态与编码器位置 $j$ 的相关性：</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{s}_{i-1}$ 是解码器在第 $i-1$ 步的隐藏状态，$a$ 是一个**对齐模型（alignment model）**。</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>Bahdanau使用了一个单层前馈网络作为对齐模型：</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>这被称为**加性注意力（additive attention）**，因为 $\mathbf{s}_{i-1}$ 和 $\mathbf{h}_j$ 是通过加法结合的。</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>然后，对所有位置的分数做softmax归一化，得到注意力权重：</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>softmax确保了：</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>所有权重都是正数：$\alpha_{ij} &gt; 0$</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>权重之和为1：$\sum_j \alpha_{ij} = 1$</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>这样，$\alpha_{ij}$ 可以解释为一个概率分布——解码器在第 $i$ 步"关注"输入位置 $j$ 的概率。</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Bahdanau Attention (Bahdanau et al., 2015)</span></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a><span class="co">    Bahdanau (加性) 注意力机制</span></span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a><span class="co">        s_prev: 解码器上一步的隐藏状态 [batch, dec_hidden]</span></span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]</span></span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a><span class="co">        W_a, U_a, v_a: 可学习参数</span></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a><span class="co">        context: 上下文向量 [batch, enc_hidden]</span></span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_weights: 注意力权重 [batch, src_len]</span></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 计算对齐分数</span></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>    <span class="co"># s_prev 广播到所有源位置</span></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> v_a <span class="op">@</span> tanh(W_a <span class="op">@</span> s_prev <span class="op">+</span> U_a <span class="op">@</span> encoder_outputs)  <span class="co"># [batch, src_len]</span></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Softmax 归一化</span></span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 加权求和</span></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> attention_weights <span class="op">@</span> encoder_outputs  <span class="co"># [batch, enc_hidden]</span></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, attention_weights</span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>*Source: Bahdanau, Cho, &amp; Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", ICLR 2015. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*</span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a>下图展示了带Bahdanau Attention的RNN Encoder-Decoder架构：</span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a><span class="al">![带Bahdanau Attention的RNN Encoder-Decoder架构。编码器（底部）使用双向RNN处理输入序列，产生隐藏状态序列。解码器（顶部）在每一步通过Attention机制动态计算上下文向量：将当前解码器状态与所有编码器状态比较，得到注意力权重，加权求和得到上下文向量$c_t$，辅助生成下一个词。](figures/chapter-5/original/fig-bahdanau-attention-d2l.svg)</span>{#fig-attention-mechanism width=70%}</span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>*Source: Dive into Deep Learning, Figure 11.4.2. [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)*</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：Attention计算</span></span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a>让我们用一个小例子走一遍完整的Attention计算过程。</span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入序列：3个词（"I love NLP"），编码后得到3个隐藏状态</span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>解码器隐藏状态维度：$d_s = 4$</span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>编码器隐藏状态维度：$d_h = 4$</span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>注意力中间维度：$d_a = 3$</span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a>**编码器输出**（假设已经计算好）：</span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_1 = <span class="co">[</span><span class="ot">0.2, 0.5, -0.3, 0.8</span><span class="co">]</span>^\top \quad \text{("I")}</span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_2 = <span class="co">[</span><span class="ot">0.7, -0.2, 0.4, 0.1</span><span class="co">]</span>^\top \quad \text{("love")}</span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_3 = <span class="co">[</span><span class="ot">-0.1, 0.6, 0.5, -0.4</span><span class="co">]</span>^\top \quad \text{("NLP")}</span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a>**解码器当前状态**（正在生成第一个目标词）：</span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a>\mathbf{s}_0 = <span class="co">[</span><span class="ot">0.1, -0.3, 0.4, 0.2</span><span class="co">]</span>^\top</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a>**参数**（简化的随机值）：</span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_a = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.1 <span class="sc">\\</span> 0.2 &amp; 0.1 &amp; -0.1 &amp; 0.2 <span class="sc">\\</span> -0.1 &amp; 0.3 &amp; 0.2 &amp; -0.2 \end{bmatrix}, \quad</span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a = \begin{bmatrix} 0.2 &amp; 0.1 &amp; -0.2 &amp; 0.3 <span class="sc">\\</span> -0.1 &amp; 0.2 &amp; 0.1 &amp; 0.1 <span class="sc">\\</span> 0.3 &amp; -0.1 &amp; 0.2 &amp; -0.1 \end{bmatrix}</span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_a = <span class="co">[</span><span class="ot">0.5, -0.3, 0.4</span><span class="co">]</span>^\top</span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a>**Step 1：计算 $\mathbf{W}_a \mathbf{s}_0$**</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_a \mathbf{s}_0 = \begin{bmatrix} 0.1 \cdot 0.1 + (-0.2) \cdot (-0.3) + 0.3 \cdot 0.4 + 0.1 \cdot 0.2 <span class="sc">\\</span> \vdots \end{bmatrix} = \begin{bmatrix} 0.21 <span class="sc">\\</span> 0.03 <span class="sc">\\</span> 0.04 \end{bmatrix}</span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a>**Step 2：对每个编码器状态计算 $\mathbf{U}_a \mathbf{h}_j$**</span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a \mathbf{h}_1 = <span class="co">[</span><span class="ot">0.33, 0.14, -0.05</span><span class="co">]</span>^\top</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a \mathbf{h}_2 = <span class="co">[</span><span class="ot">0.13, 0.06, 0.29</span><span class="co">]</span>^\top</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a \mathbf{h}_3 = <span class="co">[</span><span class="ot">-0.15, 0.17, 0.05</span><span class="co">]</span>^\top</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>**Step 3：计算对齐分数 $e_{1j}$**</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>e_{11} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_0 + \mathbf{U}_a \mathbf{h}_1) = \mathbf{v}_a^\top \tanh(<span class="co">[</span><span class="ot">0.54, 0.17, -0.01</span><span class="co">]</span>^\top)</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.5, -0.3, 0.4</span><span class="co">]</span> \cdot <span class="co">[</span><span class="ot">\tanh(0.54), \tanh(0.17), \tanh(-0.01)</span><span class="co">]</span>^\top</span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.5, -0.3, 0.4</span><span class="co">]</span> \cdot <span class="co">[</span><span class="ot">0.49, 0.17, -0.01</span><span class="co">]</span>^\top = 0.24 - 0.05 - 0.004 \approx 0.19</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a>类似地计算 $e_{12}$ 和 $e_{13}$：</span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a>e_{12} \approx 0.25, \quad e_{13} \approx 0.08</span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a>**Step 4：Softmax归一化**</span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a>\alpha_{11} = \frac{\exp(0.19)}{\exp(0.19) + \exp(0.25) + \exp(0.08)} = \frac{1.21}{1.21 + 1.28 + 1.08} = \frac{1.21}{3.57} \approx 0.34</span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a>\alpha_{12} = \frac{1.28}{3.57} \approx 0.36, \quad \alpha_{13} = \frac{1.08}{3.57} \approx 0.30</span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a>**Step 5：计算上下文向量**</span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_1 = \alpha_{11} \mathbf{h}_1 + \alpha_{12} \mathbf{h}_2 + \alpha_{13} \mathbf{h}_3</span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>= 0.34 \cdot <span class="co">[</span><span class="ot">0.2, 0.5, -0.3, 0.8</span><span class="co">]</span>^\top + 0.36 \cdot <span class="co">[</span><span class="ot">0.7, -0.2, 0.4, 0.1</span><span class="co">]</span>^\top + 0.30 \cdot <span class="co">[</span><span class="ot">-0.1, 0.6, 0.5, -0.4</span><span class="co">]</span>^\top</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>\approx <span class="co">[</span><span class="ot">0.29, 0.18, 0.09, 0.19</span><span class="co">]</span>^\top</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a>**解读**：在这个例子中，模型对"love"的关注最多（0.36），其次是"I"（0.34）和"NLP"（0.30）。注意力权重相对均匀，这可能是因为我们用的是随机参数。在训练后的真实模型中，权重分布会更加尖锐——模型会学会在需要时聚焦于特定位置。</span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### 解码器的完整流程</span></span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a>有了Attention机制，解码器的每一步工作流程变为：</span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**计算注意力权重** $\alpha_{ij}$：基于当前解码器状态和所有编码器状态</span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**计算上下文向量** $\mathbf{c}_i$：对编码器状态加权求和</span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**更新解码器状态**：结合上下文向量、前一步输出、前一步状态</span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**生成输出**：基于新的解码器状态</span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a>P(y_i | y_{&lt;i}, \mathbf{x}) = g(\mathbf{s}_i, y_{i-1}, \mathbf{c}_i)</span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a>关键区别是：**每一步都有一个不同的上下文向量 $\mathbf{c}_i$**，它是根据当前任务动态计算的。</span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度分析</span></span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a>**时间复杂度**：</span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算所有对齐分数：$O(T_x \cdot T_y \cdot d)$</span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>其中 $T_x$ 是源序列长度，$T_y$ 是目标序列长度，$d$ 是隐藏维度</span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a>与标准Seq2Seq相比，Attention增加了 $O(T_x \cdot T_y)$ 的计算量。对于长序列，这个开销是显著的。</span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a>**空间复杂度**：</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>需要存储所有编码器隐藏状态：$O(T_x \cdot d)$</span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>标准Seq2Seq只需要存储最终状态：$O(d)$</span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a>这是用空间换取性能的典型例子。</span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a><span class="fu">## 注意力可视化：模型在"看"什么？</span></span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对齐矩阵</span></span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a>Attention机制的一个美妙特性是**可解释性**。注意力权重 $\alpha_{ij}$ 直接告诉我们：在生成第 $i$ 个目标词时，模型关注了哪些源词。</span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a>我们可以把所有的注意力权重排列成一个矩阵，横轴是源序列，纵轴是目标序列。这个矩阵被称为**对齐矩阵（alignment matrix）**。</span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a><span class="al">![对齐可视化：四个英法翻译例子的注意力权重热力图。横轴是英语源句子，纵轴是法语目标句子。白色表示高注意力权重，黑色表示低权重。注意对角线模式（单词一一对应）和偏离对角线的区域（语序调整）。例如(a)中"August"对应"août"，"European Economic Area"对应"zone économique européenne"。](figures/chapter-5/original/fig3-alignment-visualization.png)</span>{#fig-alignment-visualization width=95%}</span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>*Source: Bahdanau, Cho, &amp; Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", Figure 3. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*</span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对齐模式的语言学意义</span></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>通过观察对齐矩阵，我们可以发现一些有趣的语言学模式：</span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a>**1. 单调对齐**</span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a>对于语序相似的语言对（如英语到德语的某些结构），对齐矩阵接近对角线——第1个源词对应第1个目标词，第2个对应第2个，依此类推。</span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a>**2. 语序调整**</span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a>当源语言和目标语言的词序不同时，对齐矩阵会偏离对角线。例如，英语的"red car"翻译成法语是"voiture rouge"（车 红），对齐矩阵会显示交叉模式。</span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a>**3. 一对多和多对一**</span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>某些词没有直接对应，或一个词对应多个词。例如，英语的"going to"可能对应法语的单个词"va"。</span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a>**4. 空对齐**</span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a>某些目标词（如冠词）可能没有明确的源词对应，它们的注意力权重会分散在多个位置。</span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a><span class="fu">### 可视化的局限性</span></span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a>虽然注意力可视化很吸引人，但我们要谨慎解读：</span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**注意力不等于解释**：高注意力权重不一定意味着模型"理解"了那个位置的内容</span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**可能有多重因素**：模型可能通过其他机制（如位置信息）做出决定</span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**训练目标的影响**：注意力权重是为了最小化翻译损失而学习的，不一定反映人类的对齐直觉</span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a>后来的研究（如Jain &amp; Wallace, 2019）对注意力的可解释性提出了质疑。但作为一个诊断工具，注意力可视化仍然非常有价值。</span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：带Attention的Seq2Seq</span></span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a>让我们用PyTorch实现一个带Attention的Seq2Seq模型。</span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a><span class="fu">### 编码器</span></span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, hidden_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a>            embed_dim, hidden_dim,</span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,  <span class="co"># 双向</span></span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 将双向的隐藏状态压缩到单向维度</span></span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim <span class="op">*</span> <span class="dv">2</span>, hidden_dim)</span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_lengths<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src: [batch_size, src_len]</span></span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(src))  <span class="co"># [batch, src_len, embed_dim]</span></span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> src_lengths <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a>            packed <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a>                embedded, src_lengths.cpu(), batch_first<span class="op">=</span><span class="va">True</span>, enforce_sorted<span class="op">=</span><span class="va">False</span></span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>            packed_outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(packed)</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a>            outputs, _ <span class="op">=</span> nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a>            outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(embedded)</span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs: [batch, src_len, hidden_dim * 2] (双向拼接)</span></span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [num_layers * 2, batch, hidden_dim]</span></span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并前向和后向的最终隐藏状态</span></span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-2] 是最后一层前向，hidden[-1] 是最后一层后向</span></span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> torch.tanh(<span class="va">self</span>.fc(torch.cat([hidden[<span class="op">-</span><span class="dv">2</span>], hidden[<span class="op">-</span><span class="dv">1</span>]], dim<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, hidden_dim]</span></span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, hidden</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a><span class="fu">### Attention层</span></span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BahdanauAttention(nn.Module):</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, attention_dim):</span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加性注意力的参数</span></span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(dec_hidden_dim, attention_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.U_a <span class="op">=</span> nn.Linear(enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>, attention_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># 双向编码器</span></span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_a <span class="op">=</span> nn.Linear(attention_dim, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_hidden: [batch, dec_hidden]</span></span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, src_len], True表示需要mask的位置（padding）</span></span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decoder_hidden 扩展到所有源位置</span></span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, dec_hidden] -&gt; [batch, src_len, dec_hidden]</span></span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a>        decoder_hidden <span class="op">=</span> decoder_hidden.unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, src_len, <span class="dv">1</span>)</span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算对齐分数</span></span>
<span id="cb7-515"><a href="#cb7-515" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, attention_dim]</span></span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a>        energy <span class="op">=</span> torch.tanh(<span class="va">self</span>.W_a(decoder_hidden) <span class="op">+</span> <span class="va">self</span>.U_a(encoder_outputs))</span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, 1] -&gt; [batch, src_len]</span></span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> <span class="va">self</span>.v_a(energy).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用mask（将padding位置的分数设为很小的负数）</span></span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a>            attention_scores <span class="op">=</span> attention_scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax归一化</span></span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(attention_scores, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算上下文向量</span></span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -&gt; [batch, 1, enc_hidden*2]</span></span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(attention_weights.unsqueeze(<span class="dv">1</span>), encoder_outputs)</span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> context.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, enc_hidden * 2]</span></span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights</span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a><span class="fu">### 解码器</span></span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionDecoder(nn.Module):</span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,</span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a>                 attention_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)</span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a>        <span class="co"># GRU输入是：embedded + context</span></span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a>            embed_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>,  <span class="co"># 双向编码器</span></span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a>            dec_hidden_dim,</span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(dec_hidden_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> embed_dim, vocab_size)</span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_token, hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a><span class="co">        单步解码</span></span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a><span class="co">        input_token: [batch] - 上一步的输出token</span></span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden: [1, batch, dec_hidden] - 上一步的隐藏状态</span></span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Embedding</span></span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(input_token))  <span class="co"># [batch, embed_dim]</span></span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention</span></span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-1] 取最后一层，[batch, dec_hidden]</span></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a>        context, attention_weights <span class="op">=</span> <span class="va">self</span>.attention(hidden[<span class="op">-</span><span class="dv">1</span>], encoder_outputs, mask)</span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接embedded和context作为RNN输入</span></span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a>        rnn_input <span class="op">=</span> torch.cat([embedded, context], dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch, 1, embed+ctx]</span></span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RNN</span></span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a>        output, hidden <span class="op">=</span> <span class="va">self</span>.rnn(rnn_input, hidden)</span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, dec_hidden]</span></span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="va">self</span>.fc(torch.cat([output, context, embedded], dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction, hidden, attention_weights</span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整的Seq2Seq模型</span></span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2SeqAttention(nn.Module):</span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, device):</span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg, teacher_forcing_ratio<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a><span class="co">        src: [batch, src_len]</span></span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a><span class="co">        trg: [batch, trg_len]</span></span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> src.shape[<span class="dv">0</span>]</span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a>        trg_vocab_size <span class="op">=</span> <span class="va">self</span>.decoder.vocab_size</span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 存储输出</span></span>
<span id="cb7-613"><a href="#cb7-613" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> torch.zeros(batch_size, trg_len, trg_vocab_size).to(<span class="va">self</span>.device)</span>
<span id="cb7-614"><a href="#cb7-614" aria-hidden="true" tabindex="-1"></a>        attentions <span class="op">=</span> []</span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 编码</span></span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a>        encoder_outputs, hidden <span class="op">=</span> <span class="va">self</span>.encoder(src)</span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, dec_hidden] -&gt; [1, batch, dec_hidden]</span></span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> hidden.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 第一个解码输入是 &lt;sos&gt; token</span></span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a>        input_token <span class="op">=</span> trg[:, <span class="dv">0</span>]</span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, trg_len):</span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a>            prediction, hidden, attention <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a>                input_token, hidden, encoder_outputs</span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a>            outputs[:, t] <span class="op">=</span> prediction</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a>            attentions.append(attention)</span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Teacher forcing</span></span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a>            teacher_force <span class="op">=</span> torch.rand(<span class="dv">1</span>).item() <span class="op">&lt;</span> teacher_forcing_ratio</span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a>            top1 <span class="op">=</span> prediction.argmax(<span class="dv">1</span>)</span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a>            input_token <span class="op">=</span> trg[:, t] <span class="cf">if</span> teacher_force <span class="cf">else</span> top1</span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, torch.stack(attentions, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建模型示例</span></span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>, hidden_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> AttentionDecoder(</span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a>    enc_hidden_dim<span class="op">=</span><span class="dv">512</span>, dec_hidden_dim<span class="op">=</span><span class="dv">512</span>, attention_dim<span class="op">=</span><span class="dv">256</span></span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2SeqAttention(encoder, decoder, device).to(device)</span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"编码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> encoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"解码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"总参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键实现细节</span></span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a>**1. Mask处理**</span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a>在实际应用中，batch中的序列长度不同，需要padding。计算注意力时，padding位置不应该获得任何权重。我们通过mask将这些位置的分数设为很大的负数，softmax后它们的权重趋近于0。</span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a>**2. Teacher Forcing**</span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a>训练时，解码器的输入可以是真实的上一个词（teacher forcing）或模型预测的词。<span class="in">`teacher_forcing_ratio`</span> 控制两者的混合比例。较高的比例加速训练，但可能导致exposure bias。</span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a>**3. 双向编码器**</span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a>我们使用双向GRU，编码器输出的维度是 <span class="in">`hidden_dim * 2`</span>。这让每个位置都包含完整的上下文信息。</span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么Attention有效？——理论视角</span></span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a>**1. 信息论视角**</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a>标准Seq2Seq的上下文向量 $\mathbf{c}$ 是输入 $\mathbf{x}$ 的一个**充分统计量（sufficient statistic）**——如果 $\mathbf{c}$ 完美，它应该包含关于 $\mathbf{y}$ 的所有必要信息。但在实践中，有限维度的 $\mathbf{c}$ 无法做到这一点。</span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a>Attention通过让解码器访问所有的 $\mathbf{h}_j$，实际上是在说：**不要求一个充分统计量，而是让模型在需要时直接查询原始信息**。这绕过了信息瓶颈。</span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a>**2. 记忆寻址视角**</span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a>可以把编码器的隐藏状态看作一个**外部记忆（external memory）**，每个 $\mathbf{h}_j$ 是一个记忆槽。Attention机制实现了**基于内容的软寻址（content-based soft addressing）**——根据当前查询（解码器状态）检索相关的记忆。</span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a>这个视角后来被显式化为Memory Networks和Neural Turing Machine。</span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a>**3. 梯度流视角**</span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a>从优化角度，Attention提供了一条从解码器到编码器特定位置的**直接路径**。在标准Seq2Seq中，梯度要从解码器流回编码器，必须经过 $\mathbf{c}$，再经过整个编码过程。Attention创造了"捷径"——梯度可以通过注意力权重直接传到相关的编码器位置。</span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a><span class="fu">### 边界条件与失效模式</span></span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a>**1. 单调对齐假设**</span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a>Bahdanau Attention隐含假设源和目标之间存在某种对齐关系。对于翻译任务这通常成立，但对于某些任务（如摘要），这个假设可能不成立——摘要可能需要整合分散在各处的信息，而不是"对齐"到特定位置。</span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a>**2. 复杂度限制**</span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a>当源序列很长时（如文档级翻译），计算所有位置的注意力权重变得昂贵。$O(T_x \cdot T_y)$ 的复杂度在 $T_x = 10000$ 时是不可接受的。</span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a>**3. 分布偏移**</span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a>训练时，解码器看到的上下文向量分布与推理时可能不同（因为teacher forcing）。这可能导致注意力权重在推理时不够准确。</span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**最优对齐**：Attention学到的对齐与语言学家标注的对齐有什么关系？是否可以用语言学知识改进Attention？</span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**稀疏注意力**：能否学习更稀疏的注意力分布，只关注少数关键位置，而不是soft地分布到所有位置？</span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**层次化注意力**：对于长文档，能否设计层次化的Attention——先关注段落，再关注句子，最后关注词？</span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与展望</span></span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本章方法的核心局限</span></span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a>**1. 仍然依赖RNN**</span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a>Bahdanau Attention是Seq2Seq的"补丁"——它增强了解码器访问信息的能力，但底层仍然是RNN。这意味着：</span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>仍然是顺序计算，无法并行</span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>仍然受限于RNN的长距离依赖问题（虽然因为Attention有所缓解）</span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a>**2. 注意力计算与序列长度平方相关**</span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a>每个解码步需要计算对所有编码位置的注意力，总复杂度是 $O(T_x \cdot T_y)$。对于长序列，这是显著的开销。</span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a>**3. 没有位置感知**</span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>Attention是基于内容的，它不直接考虑位置信息。虽然双向RNN隐式编码了位置，但Attention本身对位置是"盲目"的。</span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限指向什么？</span></span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a>Attention的成功引发了一个自然的问题：**如果Attention如此强大，我们还需要RNN吗？**</span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-736"><a href="#cb7-736" aria-hidden="true" tabindex="-1"></a>下一章将探讨Attention的各种变体，包括Luong提出的乘性注意力。更重要的是，这些探索最终导向了一个革命性的结论：**我们可以完全用Attention取代RNN**。</span>
<span id="cb7-737"><a href="#cb7-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-738"><a href="#cb7-738" aria-hidden="true" tabindex="-1"></a>这就是第8章Transformer的核心思想——"Attention Is All You Need"。在那里，Self-Attention让序列中的每个位置都能直接与其他位置交互，完全抛弃了循环结构，实现了真正的并行计算。</span>
<span id="cb7-739"><a href="#cb7-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-740"><a href="#cb7-740" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 从Bahdanau Attention到Transformer，Attention从一个"辅助机制"演变为"核心架构"。这是深度学习历史上最重要的范式转变之一。</span></span>
<span id="cb7-741"><a href="#cb7-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-742"><a href="#cb7-742" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-743"><a href="#cb7-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-744"><a href="#cb7-744" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb7-745"><a href="#cb7-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-746"><a href="#cb7-746" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb7-747"><a href="#cb7-747" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心要点</span></span>
<span id="cb7-748"><a href="#cb7-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-749"><a href="#cb7-749" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**问题**：Seq2Seq的信息瓶颈——所有输入信息压缩到一个固定向量，导致长序列信息丢失</span>
<span id="cb7-750"><a href="#cb7-750" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**洞察**：解码器应该能够动态地、有选择地关注输入的不同位置</span>
<span id="cb7-751"><a href="#cb7-751" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**方法**：Attention机制计算解码器状态与每个编码器状态的相关性，生成注意力权重，加权求和得到动态上下文向量</span>
<span id="cb7-752"><a href="#cb7-752" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**意义**：打破了固定向量的限制，大幅提升了长序列翻译质量，为后续的Transformer奠定了基础</span>
<span id="cb7-753"><a href="#cb7-753" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-754"><a href="#cb7-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-755"><a href="#cb7-755" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb7-756"><a href="#cb7-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-757"><a href="#cb7-757" aria-hidden="true" tabindex="-1"></a>**对齐分数（Bahdanau加性注意力）**：</span>
<span id="cb7-758"><a href="#cb7-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-759"><a href="#cb7-759" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-760"><a href="#cb7-760" aria-hidden="true" tabindex="-1"></a>e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)</span>
<span id="cb7-761"><a href="#cb7-761" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-762"><a href="#cb7-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-763"><a href="#cb7-763" aria-hidden="true" tabindex="-1"></a>**注意力权重**：</span>
<span id="cb7-764"><a href="#cb7-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-765"><a href="#cb7-765" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-766"><a href="#cb7-766" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}</span>
<span id="cb7-767"><a href="#cb7-767" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-768"><a href="#cb7-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-769"><a href="#cb7-769" aria-hidden="true" tabindex="-1"></a>**上下文向量**：</span>
<span id="cb7-770"><a href="#cb7-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-771"><a href="#cb7-771" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-772"><a href="#cb7-772" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j</span>
<span id="cb7-773"><a href="#cb7-773" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-774"><a href="#cb7-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-775"><a href="#cb7-775" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-776"><a href="#cb7-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-777"><a href="#cb7-777" aria-hidden="true" tabindex="-1"></a><span class="fu">## 思考题</span></span>
<span id="cb7-778"><a href="#cb7-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-779"><a href="#cb7-779" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么说Attention实现了"软寻址"？它与计算机内存的硬寻址有什么本质区别？这种软寻址的优势和劣势是什么？</span>
<span id="cb7-780"><a href="#cb7-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-781"><a href="#cb7-781" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明：当注意力权重集中在单一位置时（即 $\alpha_{ij} \to 1$ 对某个 $j$，其他为0），上下文向量就退化为那个位置的编码器状态。这与硬注意力有什么关系？</span>
<span id="cb7-782"><a href="#cb7-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-783"><a href="#cb7-783" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 在实现Attention时，为什么要对padding位置应用mask？如果不做mask会有什么后果？如何正确实现mask（考虑数值稳定性）？</span>
<span id="cb7-784"><a href="#cb7-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-785"><a href="#cb7-785" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[批判思考]** Attention的可视化经常被用来"解释"模型的决策。但这种解释是否可靠？设计一个实验来检验：注意力权重高的位置是否真的对模型输出有重要影响。</span>
<span id="cb7-786"><a href="#cb7-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-787"><a href="#cb7-787" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放问题]** Bahdanau Attention需要为每个解码步计算对所有编码位置的注意力，复杂度是 $O(T_x \cdot T_y)$。有哪些方法可以降低这个复杂度？（提示：考虑稀疏化、局部化、或近似方法）</span>
<span id="cb7-788"><a href="#cb7-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-789"><a href="#cb7-789" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-790"><a href="#cb7-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-791"><a href="#cb7-791" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb7-792"><a href="#cb7-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-793"><a href="#cb7-793" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb7-794"><a href="#cb7-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-795"><a href="#cb7-795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**</span>
<span id="cb7-796"><a href="#cb7-796" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Attention机制在NMT中的开创性工作</span>
<span id="cb7-797"><a href="#cb7-797" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（模型架构）、Section 5（可视化分析）</span>
<span id="cb7-798"><a href="#cb7-798" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1409.0473</span><span class="co">](https://arxiv.org/abs/1409.0473)</span></span>
<span id="cb7-799"><a href="#cb7-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-800"><a href="#cb7-800" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb7-801"><a href="#cb7-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-802"><a href="#cb7-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Graves et al., 2014] Neural Turing Machines**</span>
<span id="cb7-803"><a href="#cb7-803" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出了基于内容的软寻址，是Attention的理论先驱</span>
<span id="cb7-804"><a href="#cb7-804" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3.1（Attention机制）</span>
<span id="cb7-805"><a href="#cb7-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-806"><a href="#cb7-806" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb7-807"><a href="#cb7-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-808"><a href="#cb7-808" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**</span>
<span id="cb7-809"><a href="#cb7-809" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出乘性注意力，对比不同注意力变体</span>
<span id="cb7-810"><a href="#cb7-810" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>这是下一章的核心内容</span>
<span id="cb7-811"><a href="#cb7-811" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1508.04025</span><span class="co">](https://arxiv.org/abs/1508.04025)</span></span>
<span id="cb7-812"><a href="#cb7-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-813"><a href="#cb7-813" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Vaswani et al., 2017] Attention Is All You Need**</span>
<span id="cb7-814"><a href="#cb7-814" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Transformer：完全用Attention取代RNN</span>
<span id="cb7-815"><a href="#cb7-815" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>这是第8章的核心内容</span>
<span id="cb7-816"><a href="#cb7-816" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1706.03762</span><span class="co">](https://arxiv.org/abs/1706.03762)</span></span>
<span id="cb7-817"><a href="#cb7-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-818"><a href="#cb7-818" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对Attention可解释性的讨论</span></span>
<span id="cb7-819"><a href="#cb7-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-820"><a href="#cb7-820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Jain &amp; Wallace, 2019] Attention is not Explanation**</span>
<span id="cb7-821"><a href="#cb7-821" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>质疑Attention权重作为解释的可靠性</span>
<span id="cb7-822"><a href="#cb7-822" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1902.10186</span><span class="co">](https://arxiv.org/abs/1902.10186)</span></span>
<span id="cb7-823"><a href="#cb7-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-824"><a href="#cb7-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Wiegreffe &amp; Pinter, 2019] Attention is not not Explanation**</span>
<span id="cb7-825"><a href="#cb7-825" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>对上述论文的回应，更细致地讨论Attention的解释性</span>
<span id="cb7-826"><a href="#cb7-826" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1908.04626</span><span class="co">](https://arxiv.org/abs/1908.04626)</span></span>
<span id="cb7-827"><a href="#cb7-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-828"><a href="#cb7-828" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-829"><a href="#cb7-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-830"><a href="#cb7-830" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb7-831"><a href="#cb7-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-832"><a href="#cb7-832" aria-hidden="true" tabindex="-1"></a>Attention机制的灵感部分来自人类视觉系统。当我们看一幅复杂的图像时，我们不会同时处理所有像素，而是会"聚焦"在感兴趣的区域。这种选择性注意（selective attention）是认知科学研究的经典课题。</span>
<span id="cb7-833"><a href="#cb7-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-834"><a href="#cb7-834" aria-hidden="true" tabindex="-1"></a>Bahdanau在2014年将这个思想引入神经机器翻译时，并没有预料到它会成为深度学习最核心的组件之一。在论文中，他们谦虚地称之为"对齐模型"（alignment model），而不是"注意力"。"Attention"这个术语是后来被社区广泛采用的。</span>
<span id="cb7-835"><a href="#cb7-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-836"><a href="#cb7-836" aria-hidden="true" tabindex="-1"></a>有趣的是，Bahdanau Attention的成功让研究者开始思考：如果Attention这么有效，我们是否需要RNN？两年后，Vaswani等人给出了答案——"Attention Is All You Need"。这篇论文不仅在技术上革新了序列建模，其标题本身也成为了深度学习历史上最具影响力的金句之一。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>