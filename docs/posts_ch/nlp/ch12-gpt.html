<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-26">
<meta name="description" content="GPT：第一个将Transformer用于大规模语言模型预训练的工作。通过自回归语言建模预训练Transformer Decoder，再在下游任务上微调全部参数，确立了’预训练+微调’的现代NLP范式。">

<title>第12章：GPT——自回归预训练路线 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#如何最大化预训练知识的迁移" id="toc-如何最大化预训练知识的迁移" class="nav-link" data-scroll-target="#如何最大化预训练知识的迁移"><span class="header-section-number">2.1</span> 如何最大化预训练知识的迁移？</a></li>
  <li><a href="#之前的尝试有什么不足" id="toc-之前的尝试有什么不足" class="nav-link" data-scroll-target="#之前的尝试有什么不足"><span class="header-section-number">2.2</span> 之前的尝试有什么不足？</a></li>
  <li><a href="#为什么选择transformer-decoder" id="toc-为什么选择transformer-decoder" class="nav-link" data-scroll-target="#为什么选择transformer-decoder"><span class="header-section-number">2.3</span> 为什么选择Transformer Decoder？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#关键洞察从顾问到全职员工" id="toc-关键洞察从顾问到全职员工" class="nav-link" data-scroll-target="#关键洞察从顾问到全职员工"><span class="header-section-number">3.1</span> 关键洞察：从”顾问”到”全职员工”</a></li>
  <li><a href="#两阶段框架预训练-微调" id="toc-两阶段框架预训练-微调" class="nav-link" data-scroll-target="#两阶段框架预训练-微调"><span class="header-section-number">3.2</span> 两阶段框架：预训练 + 微调</a></li>
  <li><a href="#输入变换一个模型适配所有任务" id="toc-输入变换一个模型适配所有任务" class="nav-link" data-scroll-target="#输入变换一个模型适配所有任务"><span class="header-section-number">3.3</span> 输入变换：一个模型适配所有任务</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#预训练因果语言建模" id="toc-预训练因果语言建模" class="nav-link" data-scroll-target="#预训练因果语言建模"><span class="header-section-number">4.1</span> 预训练：因果语言建模</a></li>
  <li><a href="#模型架构" id="toc-模型架构" class="nav-link" data-scroll-target="#模型架构"><span class="header-section-number">4.2</span> 模型架构</a></li>
  <li><a href="#微调从语言模型到任务模型" id="toc-微调从语言模型到任务模型" class="nav-link" data-scroll-target="#微调从语言模型到任务模型"><span class="header-section-number">4.3</span> 微调：从语言模型到任务模型</a></li>
  <li><a href="#输入变换的具体实现" id="toc-输入变换的具体实现" class="nav-link" data-scroll-target="#输入变换的具体实现"><span class="header-section-number">4.4</span> 输入变换的具体实现</a></li>
  <li><a href="#训练细节" id="toc-训练细节" class="nav-link" data-scroll-target="#训练细节"><span class="header-section-number">4.5</span> 训练细节</a></li>
  <li><a href="#完整数值示例因果注意力掩码" id="toc-完整数值示例因果注意力掩码" class="nav-link" data-scroll-target="#完整数值示例因果注意力掩码"><span class="header-section-number">4.6</span> 完整数值示例：因果注意力掩码</a></li>
  <li><a href="#复杂度分析" id="toc-复杂度分析" class="nav-link" data-scroll-target="#复杂度分析"><span class="header-section-number">4.7</span> 复杂度分析</a></li>
  <li><a href="#与其他方法的对比" id="toc-与其他方法的对比" class="nav-link" data-scroll-target="#与其他方法的对比"><span class="header-section-number">4.8</span> 与其他方法的对比</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#使用hugging-face微调gpt" id="toc-使用hugging-face微调gpt" class="nav-link" data-scroll-target="#使用hugging-face微调gpt"><span class="header-section-number">5.1</span> 使用Hugging Face微调GPT</a></li>
  <li><a href="#从零实现gpt的因果注意力" id="toc-从零实现gpt的因果注意力" class="nav-link" data-scroll-target="#从零实现gpt的因果注意力"><span class="header-section-number">5.2</span> 从零实现GPT的因果注意力</a></li>
  <li><a href="#复现论文的关键细节" id="toc-复现论文的关键细节" class="nav-link" data-scroll-target="#复现论文的关键细节"><span class="header-section-number">5.3</span> 复现论文的关键细节</a></li>
  <li><a href="#实验结果" id="toc-实验结果" class="nav-link" data-scroll-target="#实验结果"><span class="header-section-number">5.4</span> 实验结果</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么生成式预训练对判别式任务有效" id="toc-为什么生成式预训练对判别式任务有效" class="nav-link" data-scroll-target="#为什么生成式预训练对判别式任务有效"><span class="header-section-number">6.1</span> 为什么生成式预训练对判别式任务有效？</a></li>
  <li><a href="#微调-vs-特征提取理论与实证" id="toc-微调-vs-特征提取理论与实证" class="nav-link" data-scroll-target="#微调-vs-特征提取理论与实证"><span class="header-section-number">6.2</span> 微调 vs 特征提取：理论与实证</a></li>
  <li><a href="#辅助lm损失的作用分析" id="toc-辅助lm损失的作用分析" class="nav-link" data-scroll-target="#辅助lm损失的作用分析"><span class="header-section-number">6.3</span> 辅助LM损失的作用分析</a></li>
  <li><a href="#gpt在预训练技术谱系中的位置" id="toc-gpt在预训练技术谱系中的位置" class="nav-link" data-scroll-target="#gpt在预训练技术谱系中的位置"><span class="header-section-number">6.4</span> GPT在预训练技术谱系中的位置</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.5</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题2018年视角" id="toc-开放研究问题2018年视角" class="nav-link" data-scroll-target="#开放研究问题2018年视角"><span class="header-section-number">6.6</span> 开放研究问题（2018年视角）</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#单向注意力的根本限制" id="toc-单向注意力的根本限制" class="nav-link" data-scroll-target="#单向注意力的根本限制"><span class="header-section-number">7.1</span> 单向注意力的根本限制</a></li>
  <li><a href="#模型规模的局限" id="toc-模型规模的局限" class="nav-link" data-scroll-target="#模型规模的局限"><span class="header-section-number">7.2</span> 模型规模的局限</a></li>
  <li><a href="#微调范式本身的限制" id="toc-微调范式本身的限制" class="nav-link" data-scroll-target="#微调范式本身的限制"><span class="header-section-number">7.3</span> 微调范式本身的限制</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.4</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#前驱工作" id="toc-前驱工作" class="nav-link" data-scroll-target="#前驱工作"><span class="header-section-number">9.2</span> 前驱工作</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.3</span> 后续发展</a></li>
  <li><a href="#理论分析" id="toc-理论分析" class="nav-link" data-scroll-target="#理论分析"><span class="header-section-number">9.4</span> 理论分析</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第12章：GPT——自回归预训练路线</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Improving Language Understanding by Generative Pre-Training</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Pre-training</div>
    <div class="quarto-category">GPT</div>
    <div class="quarto-category">Transformer</div>
  </div>
  </div>

<div>
  <div class="description">
    GPT：第一个将Transformer用于大规模语言模型预训练的工作。通过自回归语言建模预训练Transformer Decoder，再在下游任务上微调全部参数，确立了’预训练+微调’的现代NLP范式。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：能否用Transformer替代LSTM进行语言模型预训练，并通过微调整个模型（而非仅提取特征）来高效迁移到下游任务？</p>
<p><strong>历史坐标</strong>：2018年6月 | Radford et al.&nbsp;“Improving Language Understanding by Generative Pre-Training” | OpenAI</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Radford et al.&nbsp;(2018)</strong> “Improving Language Understanding by Generative Pre-Training” — 参考了 Section 3（框架描述）、Section 4（实验）、Table 1-5、Figure 1-2</li>
<li><strong>Peters et al.&nbsp;(2018)</strong> “Deep contextualized word representations” (ELMo) — 参考了与GPT的对比分析</li>
<li><strong>Howard &amp; Ruder (2018)</strong> “Universal Language Model Fine-tuning for Text Classification” (ULMFiT) — 参考了微调策略的对比</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Section 15.8-15.10 (BERT) — 参考了ELMo/GPT/BERT对比图和教学组织方式</li>
<li><strong>SLP3</strong> Chapter 10-11 — 参考了预训练范式的讲解框架</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 9 (2025) “Pretraining” — 参考了GPT架构讲解和预训练范式对比</li>
<li><strong>CMU 11-711</strong> ANLP (2024) — 参考了autoregressive LM的理论分析</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们详细介绍了ELMo——第一个成功的上下文词向量预训练模型。ELMo用深层双向LSTM语言模型为每个词生成上下文相关的表示，然后将这些表示作为”特征”拼接到下游模型的输入中。在6个NLP基准任务上，ELMo带来了显著的性能提升，证明了深层预训练不仅在理论上合理，在实践中也确实有效。</p>
<p>然而，上一章结尾我们也揭示了ELMo的三个根本局限。</p>
<p>第一个局限是<strong>分离式双向</strong>。ELMo的前向和后向LSTM独立运行，两者的表示通过简单拼接组合，从未在模型内部进行深度融合。这意味着模型无法在推理过程中让左右两侧的信息互相”对话”。</p>
<p>第二个局限是<strong>特征提取范式</strong>。ELMo冻结预训练模型的参数，只学习几个标量权重来混合不同层的表示。预训练模型永远不会为下游任务做出任何调整——就像雇了一个能力很强的顾问，却规定他只能提供意见而不能亲自动手。更重要的是，使用ELMo时，你仍然需要为每个下游任务从零设计一个完整的模型架构。</p>
<p>第三个局限是<strong>LSTM的可扩展性瓶颈</strong>。LSTM必须顺序处理序列，无法在序列维度上并行化。在第8章我们已经看到，Transformer通过Self-Attention实现了完全并行的计算，在速度和规模上远超LSTM。</p>
<p>2018年6月，也就是ELMo发表仅4个月之后，OpenAI的Alec Radford等人发布了一项工作，一举解决了后两个问题。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：用Transformer Decoder替代LSTM作为预训练骨架，用全模型微调替代特征提取作为迁移方式。这两个看似简单的替换，不仅带来了更好的性能，更确立了”预训练 + 微调”的现代NLP范式——一个预训练好的模型，只需要在顶部加一个简单的分类头，就能适配几乎任何下游任务。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="如何最大化预训练知识的迁移" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="如何最大化预训练知识的迁移"><span class="header-section-number">2.1</span> 如何最大化预训练知识的迁移？</h3>
<p>ELMo证明了预训练是有效的，但它也暴露了一个更深层的问题：<strong>预训练知识到底应该以什么方式迁移到下游任务？</strong></p>
<p>这个问题在2018年上半年有两个截然不同的答案。</p>
<p>第一种是ELMo的<strong>特征提取路线</strong>：预训练一个语言模型，冻结它的参数，把它的中间表示作为”特征”输入到一个独立的下游模型中。这种方式的好处是简单——预训练模型只需要运行一次前向传播就能提供特征，不需要反向传播。但代价也很明显：预训练模型的知识被”锁死”了，无法根据下游任务的需要进行任何调整。</p>
<p>第二种是ULMFiT（上一章简要提到的Howard &amp; Ruder, 2018）率先在文本分类上验证的<strong>微调路线</strong>：预训练一个语言模型，然后在下游任务上<strong>微调整个模型的参数</strong>。ULMFiT证明了微调在文本分类上的巨大优势，但它使用的是三层LSTM——一个相对简单的架构。一个自然的问题是：如果用更强大的Transformer替代LSTM，再配合全模型微调，效果会怎样？</p>
<p>GPT正是对这个问题的回答。</p>
</section>
<section id="之前的尝试有什么不足" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="之前的尝试有什么不足"><span class="header-section-number">2.2</span> 之前的尝试有什么不足？</h3>
<p>让我们更精确地分析ELMo特征提取范式的问题。</p>
<p>假设预训练模型学到了关于”bank”的两种语境——金融语境和地理语境——的丰富表示。在下游的金融情感分析任务中，我们希望”bank”的表示更偏向金融含义，且携带更多情感相关的信息。但ELMo的参数被冻结了，它在金融文本和地理文本中生成的”bank”表示完全取决于预训练阶段学到的模式，无法根据”这是一个情感分析任务”的信号做出任何调整。</p>
<p>微调则完全不同。如果允许整个模型的参数在下游任务上继续更新，那么模型可以逐渐将自己”特化”到目标任务：Attention头可以重新分配权重以关注情感相关的信号，FFN层可以调整内部的知识激活模式，甚至词嵌入本身都可以微妙地适配目标领域。</p>
</section>
<section id="为什么选择transformer-decoder" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="为什么选择transformer-decoder"><span class="header-section-number">2.3</span> 为什么选择Transformer Decoder？</h3>
<p>GPT面临的第一个关键设计决策是：预训练应该用什么架构？</p>
<p>2018年中期，可选的架构主要有三种：</p>
<p><strong>LSTM</strong>。ELMo已经证明了LSTM作为预训练骨架的可行性，但LSTM有顺序计算瓶颈，难以扩展到更大的规模。</p>
<p><strong>Transformer Encoder</strong>。第8章介绍的Transformer有完整的编码器和解码器。编码器使用双向Self-Attention（每个位置可以看到所有其他位置），非常适合理解任务。但有一个问题：如果用编码器做语言建模，每个位置都能”看到”要预测的目标词，这会导致信息泄漏——模型可以直接抄答案而不需要真正理解语言。</p>
<p><strong>Transformer Decoder</strong>。解码器使用因果（causal）Self-Attention——每个位置只能看到它左侧的位置。这天然地匹配自回归语言建模的需求：预测位置<span class="math inline">\(k\)</span>的词时，模型只能利用<span class="math inline">\(t_1, \ldots, t_{k-1}\)</span>的信息，不会泄漏未来信息。</p>
<p>Radford等人选择了Transformer Decoder。这个选择有一个深层的逻辑：<strong>自回归语言建模是最”自然”的无监督预训练目标</strong>——给定前文，预测下一个词。而Transformer Decoder的因果注意力掩码恰好实现了这一点。相比之下，ELMo为了实现双向性，不得不将前向和后向分成两个独立的模型——因为如果LSTM能同时看到左右两侧，语言模型训练就毫无意义了。GPT干脆接受了单向的限制，用一个统一的模型优雅地完成语言建模。</p>
<p>当然，选择单向意味着放弃了右侧上下文的信息——这是一个真实的代价，我们将在本章末尾的”局限性”一节中详细讨论。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="关键洞察从顾问到全职员工" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="关键洞察从顾问到全职员工"><span class="header-section-number">3.1</span> 关键洞察：从”顾问”到”全职员工”</h3>
<p>理解GPT与ELMo的核心区别，可以用一个直觉的类比。</p>
<p>ELMo就像一个<strong>外部顾问</strong>。你的公司（下游任务）遇到了一个难题，于是请了一位语言理解方面的顾问（预训练的biLM）。顾问来了，看了看你的问题，给出了一份书面报告（ELMo特征向量），然后就离开了。你的团队拿着这份报告，自己做决策。顾问的专业知识确实有帮助，但有几个问题：顾问给的是通用建议，不了解你公司的具体业务（没有针对下游任务调整）；你的团队还是得自己从零搭建决策框架（还需要设计下游模型架构）；顾问和你的团队之间缺乏深度合作（特征提取是单向的、一次性的）。</p>
<p>GPT则像<strong>直接雇佣这位顾问成为全职员工</strong>。他加入你的团队（预训练模型成为下游模型的一部分），深入了解你的业务（在下游数据上微调），逐渐调整自己的工作方式来适应你的需求（参数更新）。你只需要在他的基础上加一个简单的决策层（分类头），就能直接产出结果。他带来了全部的通用知识（预训练权重），又能针对你的具体问题做出调整（微调）。</p>
</section>
<section id="两阶段框架预训练-微调" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="两阶段框架预训练-微调"><span class="header-section-number">3.2</span> 两阶段框架：预训练 + 微调</h3>
<p>GPT的训练过程分为清晰的两个阶段。</p>
<p><strong>第一阶段：无监督预训练</strong>。在大规模无标注文本上，训练一个Transformer Decoder来做自回归语言建模——给定前面的词，预测下一个词。这个阶段的目标是让模型学习通用的语言知识：语法规则、语义关系、世界知识、推理模式。训练完成后，模型拥有了”理解语言”的基础能力。</p>
<p><strong>第二阶段：有监督微调</strong>。在特定的下游任务（如文本分类、自然语言推理、问答等）上，保留预训练的模型参数作为初始化，在模型顶部添加一个简单的线性分类头，然后用标注数据微调<strong>整个模型</strong>（包括预训练的Transformer参数和新加的分类头）。</p>
<p>这个框架的优雅之处在于它的<strong>简洁性</strong>。预训练阶段不需要任何标注数据，只需要大量的原始文本。微调阶段不需要设计复杂的下游模型架构——预训练的Transformer本身就是下游模型的主体，只需要在顶部加一个最简单的线性层就够了。</p>
</section>
<section id="输入变换一个模型适配所有任务" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="输入变换一个模型适配所有任务"><span class="header-section-number">3.3</span> 输入变换：一个模型适配所有任务</h3>
<p>GPT面临的一个实际挑战是：不同的下游任务有不同的输入格式。文本分类只需要一段文本，自然语言推理需要一对前提-假设，问答需要文章-问题-答案三元组。如何让同一个预训练模型适配这些不同的格式？</p>
<p>GPT的解决方案非常巧妙：<strong>将所有任务的输入都转换为一个连续的token序列</strong>，用特殊分隔符连接不同的部分。</p>
<p>对于<strong>文本分类</strong>，输入就是文本本身：<code>[Start] text [Extract]</code>，取最后一个token的表示做分类。</p>
<p>对于<strong>自然语言推理</strong>，将前提和假设用分隔符连接：<code>[Start] premise [Delim] hypothesis [Extract]</code>。</p>
<p>对于<strong>问答</strong>，将文章和问题连接：<code>[Start] document [Delim] question [Delim] answer [Extract]</code>。</p>
<p>对于<strong>语义相似度</strong>（两个句子顺序不重要的任务），分别构造两种顺序的输入，将两者的表示相加后做分类。</p>
<div id="fig-input-transform" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-input-transform-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-12/original/fig1b-input-transformations.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-input-transform-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: GPT的输入变换：将四种不同结构的下游任务统一转换为线性token序列。文本分类直接输入文本；文本蕴含用分隔符连接前提和假设；语义相似度构造两种顺序并相加；多选问答为每个候选答案构造独立序列。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Radford et al.&nbsp;(2018) “Improving Language Understanding by Generative Pre-Training”, Figure 1 (right)</em></p>
</div>
<p>这种”把结构化输入拍平成序列”的思路在当时看来只是一种工程上的权宜之计，但它实际上蕴含了一个深远的洞察：<strong>语言模型本身具有足够的能力从文本序列中理解任务结构</strong>。只要把信息以恰当的方式放入序列，模型就能学会处理它。这个思路后来在GPT-2和GPT-3中被推到极致——连分类头都不需要了，直接用自然语言”提示”模型生成答案。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="预训练因果语言建模" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="预训练因果语言建模"><span class="header-section-number">4.1</span> 预训练：因果语言建模</h3>
<p>GPT的预训练目标是标准的<strong>因果语言建模（Causal Language Modeling, CLM）</strong>：给定前面的词，最大化下一个词的条件概率。</p>
<p>给定一个无标注的token序列<span class="math inline">\(\mathcal{U} = \{u_1, u_2, \ldots, u_n\}\)</span>，目标是最大化以下似然函数：</p>
<p><span class="math display">\[
L_1(\mathcal{U}) = \sum_{i} \log P(u_i \mid u_{i-k}, \ldots, u_{i-1}; \Theta)
\]</span></p>
<p>其中<span class="math inline">\(k\)</span>是上下文窗口的大小（GPT中<span class="math inline">\(k = 512\)</span>），<span class="math inline">\(\Theta\)</span>是模型的全部参数。</p>
<p>这个目标与ELMo的biLM目标有什么区别？ELMo同时训练前向和后向两个方向的语言模型，但它们是<strong>两个独立的模型</strong>，参数不共享（除了嵌入层和softmax层）。GPT则只训练一个方向——从左到右——但使用的是<strong>单一的、统一的</strong>Transformer模型。</p>
<p>你可能会问：只看一个方向，不是比双向少了一半的信息吗？这个质疑是合理的，我们将在”局限性”一节中正面讨论。但GPT的立场是：<strong>单向建模在架构上更简洁</strong>——不需要将两个独立模型的输出拼接在一起，也不需要处理前向和后向信息如何融合的问题。整个模型是一个完整的端到端系统。</p>
</section>
<section id="模型架构" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="模型架构"><span class="header-section-number">4.2</span> 模型架构</h3>
<p>GPT使用的是12层Transformer Decoder，具体参数如下：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>超参数</th>
<th>值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>层数（<span class="math inline">\(L\)</span>）</td>
<td>12</td>
</tr>
<tr class="even">
<td>隐藏维度（<span class="math inline">\(d_{model}\)</span>）</td>
<td>768</td>
</tr>
<tr class="odd">
<td>注意力头数（<span class="math inline">\(h\)</span>）</td>
<td>12</td>
</tr>
<tr class="even">
<td>每头维度（<span class="math inline">\(d_k\)</span>）</td>
<td>64</td>
</tr>
<tr class="odd">
<td>FFN内部维度</td>
<td>3072</td>
</tr>
<tr class="even">
<td>上下文窗口</td>
<td>512 tokens</td>
</tr>
<tr class="odd">
<td>总参数量</td>
<td>~117M</td>
</tr>
<tr class="even">
<td>激活函数</td>
<td>GELU</td>
</tr>
</tbody>
</table>
<p>对比ELMo（2层biLSTM，93.6M参数）和同年稍后的BERT-Base（12层Transformer Encoder，110M参数），GPT在参数量上与它们处于同一数量级，但架构选择截然不同。</p>
<div id="fig-gpt-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-12/original/fig1a-gpt-architecture.png" class="img-fluid figure-img" style="width:45.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpt-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: GPT模型架构：12层Transformer Decoder堆叠。每层包含Masked Multi Self Attention和Feed Forward两个子模块，均配有Layer Norm和残差连接。底部是词嵌入与位置嵌入的相加，顶部同时输出Text Prediction（预训练）和Task Classifier（微调）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Radford et al.&nbsp;(2018) “Improving Language Understanding by Generative Pre-Training”, Figure 1 (left)</em></p>
</div>
<p>模型的前向传播过程可以形式化为：</p>
<p><span class="math display">\[
h_0 = UW_e + W_p
\]</span></p>
<p><span class="math display">\[
h_l = \text{transformer\_block}(h_{l-1}) \quad \forall l \in [1, L]
\]</span></p>
<p><span class="math display">\[
P(u) = \text{softmax}(h_L W_e^T)
\]</span></p>
<p>其中<span class="math inline">\(U = (u_{i-k}, \ldots, u_{i-1})\)</span>是上下文token的独热编码，<span class="math inline">\(W_e\)</span>是token嵌入矩阵，<span class="math inline">\(W_p\)</span>是位置嵌入矩阵。</p>
<p>值得注意的是，GPT使用了<strong>可学习的位置嵌入</strong>（learnable positional embeddings）而非第8章Transformer原文中的正弦位置编码。Radford等人发现可学习的位置嵌入在实践中效果与正弦编码相当，但更加简单。在最后的输出层，GPT<strong>重用了嵌入矩阵</strong> <span class="math inline">\(W_e^T\)</span>（即 weight tying），这不仅减少了参数量，还在输入和输出空间之间建立了一致的语义映射。</p>
<p>每个Transformer block内部的结构与第8章的标准Decoder层一致，但去掉了交叉注意力（Cross-Attention）模块（因为GPT没有编码器），只保留了因果Self-Attention和FFN：</p>
<p><span class="math display">\[
a_l = \text{LayerNorm}(h_{l-1} + \text{MaskedMultiHead}(h_{l-1}))
\]</span></p>
<p><span class="math display">\[
h_l = \text{LayerNorm}(a_l + \text{FFN}(a_l))
\]</span></p>
<p>另一个值得注意的细节是激活函数的选择：GPT使用了<strong>GELU（Gaussian Error Linear Unit）</strong>而非Transformer原文中的ReLU。GELU可以看作ReLU的平滑近似——ReLU在零点有一个硬拐角，GELU则是光滑的。这个选择后来被BERT和几乎所有后续的Transformer模型所采用，成为事实上的标准。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>GELU激活函数
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\text{GELU}(x) = x \cdot \Phi(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{2/\pi}\left(x + 0.044715x^3\right)\right]\right)
\]</span></p>
<p>其中<span class="math inline">\(\Phi(x)\)</span>是标准正态分布的累积分布函数。直觉上，GELU根据输入值的大小以一定概率”通过”或”抑制”输入，而不是像ReLU那样在零点做硬切割。</p>
<p><em>Source: Hendrycks &amp; Gimpel (2016) “Gaussian Error Linear Units (GELUs)”. <a href="https://arxiv.org/abs/1606.08415">arXiv:1606.08415</a></em></p>
</div>
</div>
</section>
<section id="微调从语言模型到任务模型" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="微调从语言模型到任务模型"><span class="header-section-number">4.3</span> 微调：从语言模型到任务模型</h3>
<p>预训练完成后，GPT在有标注的下游数据集上进行微调。假设标注数据集<span class="math inline">\(\mathcal{C}\)</span>中的每个样本由输入token序列<span class="math inline">\(x^1, \ldots, x^m\)</span>和标签<span class="math inline">\(y\)</span>组成。输入经过预训练的Transformer，取最后一个token的最终层表示<span class="math inline">\(h_L^m\)</span>，然后通过一个新增的线性层进行预测：</p>
<p><span class="math display">\[
P(y \mid x^1, \ldots, x^m) = \text{softmax}(h_L^m \cdot W_y)
\]</span></p>
<p>微调的目标是最大化：</p>
<p><span class="math display">\[
L_2(\mathcal{C}) = \sum_{(x, y)} \log P(y \mid x^1, \ldots, x^m)
\]</span></p>
<p>Radford等人发现了一个重要的技巧：<strong>在微调时保留语言模型目标作为辅助损失</strong>。最终的微调损失为：</p>
<p><span class="math display">\[
L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C})
\]</span></p>
<p>其中<span class="math inline">\(\lambda\)</span>是一个权重超参数（论文中<span class="math inline">\(\lambda = 0.5\)</span>）。</p>
<p>为什么辅助语言模型损失有帮助？有两个原因。第一，它起到了<strong>正则化</strong>的作用——防止模型在微调过程中过度”遗忘”预训练学到的通用语言知识。这类似于机器学习中的L2正则化，但更有意义：它不是简单地限制参数的大小，而是要求模型在适配下游任务的同时保持”理解语言”的基本能力。第二，它有助于<strong>加速收敛</strong>——语言模型目标为模型的低层提供了持续的、有意义的梯度信号，而不是仅依赖分类损失从顶层传下来的梯度。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: GPT Pre-training and Fine-tuning（改编自 Radford et al., 2018）
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpt_pretrain(corpus, n_layers<span class="op">=</span><span class="dv">12</span>, d_model<span class="op">=</span><span class="dv">768</span>, n_heads<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                 context_len<span class="op">=</span><span class="dv">512</span>, lr<span class="op">=</span><span class="fl">2.5e-4</span>, warmup_steps<span class="op">=</span><span class="dv">2000</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">    阶段一：无监督预训练</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    架构: 12层 Transformer Decoder (Masked Self-Attention + FFN)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    数据: BooksCorpus (~7000本书, ~8亿词)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化模型</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> TransformerDecoder(n_layers, d_model, n_heads)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> corpus:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> batch  <span class="co"># (u_1, u_2, ..., u_n), 最长 512 tokens</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 嵌入 = 词嵌入 + 位置嵌入</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        h_0 <span class="op">=</span> model.token_embed(tokens) <span class="op">+</span> model.pos_embed(positions)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 12层 Transformer Decoder</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h_0</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> layer(h)  <span class="co"># Masked Self-Attention + FFN + LayerNorm</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 因果语言建模损失</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 用位置 i 的输出预测位置 i+1 的词</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> h <span class="op">@</span> model.token_embed.weight.T  <span class="co"># Weight tying</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> cross_entropy(logits[:, :<span class="op">-</span><span class="dv">1</span>], tokens[:, <span class="dv">1</span>:])</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        optimizer.step()  <span class="co"># Adam, cosine lr schedule</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpt_finetune(pretrained_model, labeled_data, num_classes,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>                 lambda_lm<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co">    阶段二：有监督微调</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co">    在预训练模型顶部加一个线性分类头，微调所有参数</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 新增分类头</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> Linear(d_model, num_classes)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (tokens, label) <span class="kw">in</span> labeled_data:</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 前向传播（整个预训练模型）</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        h_L <span class="op">=</span> pretrained_model(tokens)  <span class="co"># [seq_len, d_model]</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 分类损失 — 取最后一个 token 的表示</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> classifier(h_L[<span class="op">-</span><span class="dv">1</span>])     <span class="co"># [num_classes]</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        loss_cls <span class="op">=</span> cross_entropy(logits, label)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 辅助语言模型损失 — 正则化</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        lm_logits <span class="op">=</span> h_L <span class="op">@</span> pretrained_model.token_embed.weight.T</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        loss_lm <span class="op">=</span> cross_entropy(lm_logits[:, :<span class="op">-</span><span class="dv">1</span>], tokens[:, <span class="dv">1</span>:])</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 联合损失</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_cls <span class="op">+</span> lambda_lm <span class="op">*</span> loss_lm</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        loss.backward()  <span class="co"># 更新所有参数（包括预训练的）</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>改编自 Radford et al.&nbsp;(2018) “Improving Language Understanding by Generative Pre-Training”, Section 3.1-3.2</em></p>
</div>
</div>
</section>
<section id="输入变换的具体实现" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="输入变换的具体实现"><span class="header-section-number">4.4</span> 输入变换的具体实现</h3>
<p>不同任务的输入结构不同，GPT通过<strong>特殊token和分隔符</strong>将它们统一为线性序列。</p>
<p><strong>文本分类</strong>：</p>
<p><span class="math display">\[
[\text{Start}]\; \text{text tokens}\; [\text{Extract}] \xrightarrow{h_L^{[\text{Extract}]}} W_y \rightarrow \text{softmax} \rightarrow P(y)
\]</span></p>
<p>直接将文本送入模型，取 <span class="math inline">\([\text{Extract}]\)</span> token的最终层表示进行分类。</p>
<p><strong>文本蕴含（Natural Language Inference）</strong>：</p>
<p><span class="math display">\[
[\text{Start}]\; \text{premise}\; [\text{Delim}]\; \text{hypothesis}\; [\text{Extract}]
\]</span></p>
<p>前提和假设用 <span class="math inline">\([\text{Delim}]\)</span> 分隔符连接。模型在处理这个序列时，通过因果注意力，hypothesis部分的每个token都能看到premise的全部内容。</p>
<p><strong>语义相似度</strong>（对称任务）：</p>
<p><span class="math display">\[
\text{输入1:}\; [\text{Start}]\; A\; [\text{Delim}]\; B\; [\text{Extract}] \qquad
\text{输入2:}\; [\text{Start}]\; B\; [\text{Delim}]\; A\; [\text{Extract}]
\]</span></p>
<p>由于语义相似度是对称的（A与B的相似度 = B与A的相似度），GPT构造两种顺序的输入，分别提取表示，然后逐元素相加后做分类。</p>
<p><strong>多选问答</strong>：</p>
<p><span class="math display">\[
[\text{Start}]\; \text{context}\; [\text{Delim}]\; \text{question}\; [\text{Delim}]\; \text{answer}_k\; [\text{Extract}] \quad k = 1, \ldots, K
\]</span></p>
<p>对每个候选答案构造一个输入序列，分别提取表示并通过分类头得到分数，然后softmax选择最高分的答案。</p>
</section>
<section id="训练细节" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="训练细节"><span class="header-section-number">4.5</span> 训练细节</h3>
<p><strong>预训练数据</strong>。GPT在BooksCorpus数据集上预训练。这个数据集包含超过7000本未出版的书籍，涵盖冒险、奇幻、言情等多种体裁，总计约8亿词。选择书籍而非网页文本的原因是书籍包含大段连贯的叙述——这让模型能够学习长距离的语言结构和叙事逻辑，而网页文本往往是碎片化的。</p>
<p><strong>分词器</strong>。GPT使用了BPE（Byte Pair Encoding，第3章讨论的子词分词方法），词汇表大小约40,000个token。</p>
<p><strong>优化器</strong>。使用Adam优化器，最大学习率<span class="math inline">\(2.5 \times 10^{-4}\)</span>。学习率在前2000步线性预热（warmup），然后按余弦退火（cosine annealing）降为0。</p>
<p><strong>微调超参数</strong>。微调阶段使用比预训练更保守的设置：学习率<span class="math inline">\(6.25 \times 10^{-5}\)</span>（比预训练低4倍），batch size 32，训练3个epoch。这些保守的设置是为了避免”灾难性遗忘”——在大量微调更新中丢失预训练学到的通用知识。</p>
</section>
<section id="完整数值示例因果注意力掩码" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="完整数值示例因果注意力掩码"><span class="header-section-number">4.6</span> 完整数值示例：因果注意力掩码</h3>
<div id="fig-causal-mask" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-causal-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-12/fig-causal-attention-mask.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-causal-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 因果注意力掩码的可视化。左图：掩码模式——下三角为允许的注意力连接（✓），上三角为被遮蔽的连接（✗），确保每个位置只能看到自己和左侧的词。右图：经过Softmax后的注意力权重——“The”只能关注自己（1.00），“cat”分配注意力给”The”和自己，“mat”能看到所有前面的词。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>作者绘制。基于 GPT 因果注意力机制的教学示意图。</em></p>
</div>
<p>为了具体理解GPT中因果注意力的工作方式，让我们通过一个小规模的数值例子来追踪信息的流动。</p>
<p><strong>设定</strong>：句子 “The cat sat”，<span class="math inline">\(d_{model} = 4\)</span>，单头注意力（<span class="math inline">\(h = 1\)</span>）。</p>
<p><strong>Step 1: 嵌入</strong></p>
<p>假设词嵌入和位置嵌入之和为：</p>
<p><span class="math display">\[
h_0 = \begin{bmatrix} \text{The:} &amp; 1.0 &amp; 0.5 &amp; 0.2 &amp; 0.1 \\ \text{cat:} &amp; 0.3 &amp; 1.2 &amp; 0.8 &amp; 0.4 \\ \text{sat:} &amp; 0.7 &amp; 0.1 &amp; 1.1 &amp; 0.9 \end{bmatrix}
\]</span></p>
<p><strong>Step 2: 计算 Q, K, V</strong></p>
<p>为简化，假设 <span class="math inline">\(W_Q = W_K = W_V = I\)</span>（单位矩阵），则 <span class="math inline">\(Q = K = V = h_0\)</span>。</p>
<p><strong>Step 3: 注意力分数 <span class="math inline">\(QK^T\)</span></strong></p>
<p><span class="math display">\[
QK^T = \begin{bmatrix}
1.30 &amp; 1.12 &amp; 1.03 \\
1.12 &amp; 2.33 &amp; 1.43 \\
1.03 &amp; 1.43 &amp; 2.52
\end{bmatrix}
\]</span></p>
<p><strong>Step 4: 因果掩码（关键步骤！）</strong></p>
<p>与标准Transformer Encoder不同，GPT的Decoder使用因果掩码：上三角部分设为<span class="math inline">\(-\infty\)</span>，确保每个位置只能看到自己和左侧的位置。</p>
<p><span class="math display">\[
\text{Masked Scores} = \begin{bmatrix}
1.30 &amp; -\infty &amp; -\infty \\
1.12 &amp; 2.33 &amp; -\infty \\
1.03 &amp; 1.43 &amp; 2.52
\end{bmatrix}
\]</span></p>
<p>“The”只能看到自己，“cat”能看到”The”和自己，“sat”能看到所有三个词。</p>
<p><strong>Step 5: 缩放 + Softmax</strong></p>
<p>除以<span class="math inline">\(\sqrt{d_k} = \sqrt{4} = 2\)</span>后做softmax：</p>
<p><span class="math display">\[
\text{Scaled} = \begin{bmatrix}
0.65 &amp; -\infty &amp; -\infty \\
0.56 &amp; 1.17 &amp; -\infty \\
0.52 &amp; 0.72 &amp; 1.26
\end{bmatrix}
\]</span></p>
<p>Softmax（对每行）：</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1.00 &amp; 0.00 &amp; 0.00 \\
0.35 &amp; 0.65 &amp; 0.00 \\
0.24 &amp; 0.29 &amp; 0.47
\end{bmatrix}
\]</span></p>
<p><strong>解读</strong>：</p>
<ul>
<li>“The”（第1行）：注意力权重<span class="math inline">\([1.00, 0.00, 0.00]\)</span>——只能关注自己</li>
<li>“cat”（第2行）：注意力权重<span class="math inline">\([0.35, 0.65, 0.00]\)</span>——关注自己更多（0.65），也参考了”The”（0.35），但完全看不到”sat”</li>
<li>“sat”（第3行）：注意力权重<span class="math inline">\([0.24, 0.29, 0.47]\)</span>——主要关注自己（0.47），也综合了”cat”（0.29）和”The”（0.24）</li>
</ul>
<p><strong>Step 6: 加权输出 <span class="math inline">\(AV\)</span></strong></p>
<p><span class="math display">\[
\text{Output} = AV = \begin{bmatrix}
1.00 &amp; 0.50 &amp; 0.20 &amp; 0.10 \\
0.55 &amp; 0.96 &amp; 0.59 &amp; 0.30 \\
0.69 &amp; 0.51 &amp; 0.78 &amp; 0.56
\end{bmatrix}
\]</span></p>
<p>“cat”位置的输出<span class="math inline">\([0.55, 0.96, 0.59, 0.30]\)</span>融合了”The”和自身的信息，但完全不包含”sat”的信息。这正是因果模型的特点——<strong>在生成”cat”之后的下一个词时，模型只能依赖”The”和”cat”，不能”偷看”后面的内容</strong>。</p>
<p>与ELMo的对比非常清晰：ELMo的前向LSTM在”cat”位置也只能看到左侧上下文，但它是通过顺序递归实现的（<span class="math inline">\(h_t = f(h_{t-1}, x_t)\)</span>），而GPT通过因果掩码在一步注意力计算中实现了同样的约束，同时保持了并行性。</p>
</section>
<section id="复杂度分析" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="复杂度分析"><span class="header-section-number">4.7</span> 复杂度分析</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 29%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>GPT</th>
<th>ELMo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>预训练参数</td>
<td>~117M</td>
<td>~93.6M</td>
</tr>
<tr class="even">
<td>预训练复杂度</td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span>（Self-Attention）</td>
<td><span class="math inline">\(O(n \cdot d^2)\)</span>（LSTM）</td>
</tr>
<tr class="odd">
<td>预训练并行性</td>
<td>✅ 完全并行</td>
<td>❌ 顺序计算</td>
</tr>
<tr class="even">
<td>微调新增参数</td>
<td><span class="math inline">\(d_{model} \times C\)</span>（分类头）</td>
<td><span class="math inline">\(L + 2\)</span>个标量</td>
</tr>
<tr class="odd">
<td>微调计算量</td>
<td>反向传播整个模型</td>
<td>仅前向传播+少量参数更新</td>
</tr>
</tbody>
</table>
<p>GPT在预训练阶段的计算复杂度中，<span class="math inline">\(n\)</span>是序列长度，<span class="math inline">\(d\)</span>是隐藏维度。Self-Attention的<span class="math inline">\(O(n^2 \cdot d)\)</span>复杂度在<span class="math inline">\(n = 512\)</span>时完全可以接受。与LSTM的<span class="math inline">\(O(n \cdot d^2)\)</span>相比，当<span class="math inline">\(n &lt; d\)</span>时Self-Attention更高效，当<span class="math inline">\(n &gt; d\)</span>时LSTM更高效——但Self-Attention的关键优势不在渐近复杂度，而在<strong>并行性</strong>。<span class="math inline">\(O(n^2 \cdot d)\)</span>的计算可以在GPU上并行完成，而<span class="math inline">\(O(n \cdot d^2)\)</span>的LSTM必须顺序执行。</p>
</section>
<section id="与其他方法的对比" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="与其他方法的对比"><span class="header-section-number">4.8</span> 与其他方法的对比</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 16%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>ELMo</th>
<th>GPT</th>
<th>BERT（下一章）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>预训练架构</td>
<td>双向LSTM</td>
<td>单向Transformer Decoder</td>
<td>双向Transformer Encoder</td>
</tr>
<tr class="even">
<td>预训练任务</td>
<td>双向语言建模</td>
<td>因果语言建模</td>
<td>掩码语言建模 (MLM)</td>
</tr>
<tr class="odd">
<td>方向性</td>
<td>分离式双向</td>
<td>单向（左→右）</td>
<td>融合式双向</td>
</tr>
<tr class="even">
<td>迁移方式</td>
<td>特征提取（冻结）</td>
<td>全模型微调</td>
<td>全模型微调</td>
</tr>
<tr class="odd">
<td>下游模型</td>
<td>需要独立设计</td>
<td>预训练模型+分类头</td>
<td>预训练模型+分类头</td>
</tr>
<tr class="even">
<td>参数量</td>
<td>~93.6M</td>
<td>~117M</td>
<td>~110M (Base)</td>
</tr>
<tr class="odd">
<td>并行性</td>
<td>❌</td>
<td>✅</td>
<td>✅</td>
</tr>
</tbody>
</table>
<div id="fig-three-pretrain" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-three-pretrain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-12/original/fig-elmo-gpt-bert-d2l.svg" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-three-pretrain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: ELMo、GPT 和 BERT 三种预训练架构的对比。ELMo 使用双向 LSTM 并通过特征拼接迁移（左）；GPT 使用单向 Transformer Decoder 并通过微调迁移（中）；BERT 使用双向 Transformer Encoder 并通过微调迁移（右）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Dive into Deep Learning (d2l.ai), Figure 15.8.3. License: CC BY-SA 4.0</em></p>
</div>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="使用hugging-face微调gpt" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="使用hugging-face微调gpt"><span class="header-section-number">5.1</span> 使用Hugging Face微调GPT</h3>
<p>虽然原始的GPT-1模型已经被更强大的后续版本取代，但通过Hugging Face Transformers库可以方便地体验GPT的预训练+微调流程。以下代码展示了在情感分析任务上微调GPT的标准工作流：</p>
<div id="3fb344e2" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> OpenAIGPTTokenizer, OpenAIGPTForSequenceClassification</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer, TrainingArguments</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的 GPT-1 模型和分词器</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> OpenAIGPTTokenizer.from_pretrained(<span class="st">"openai-gpt"</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> OpenAIGPTForSequenceClassification.from_pretrained(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"openai-gpt"</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    num_labels<span class="op">=</span><span class="dv">2</span>  <span class="co"># 二分类：正面/负面</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 分词器设置</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>model.config.pad_token_id <span class="op">=</span> tokenizer.eos_token_id</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备数据</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">"This movie is fantastic!"</span>, <span class="st">"Terrible waste of time."</span>]</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 正面, 负面</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>encodings <span class="op">=</span> tokenizer(</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    texts,</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 Dataset</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentDataset(torch.utils.data.Dataset):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> {k: v[idx] <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        item[<span class="st">"labels"</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> SentimentDataset(encodings, labels)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># 微调配置</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：学习率比预训练时低很多（6.25e-5 vs 2.5e-4）</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./gpt-sentiment"</span>,</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">6.25e-5</span>,     <span class="co"># GPT 微调推荐的学习率</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset,</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="从零实现gpt的因果注意力" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="从零实现gpt的因果注意力"><span class="header-section-number">5.2</span> 从零实现GPT的因果注意力</h3>
<p>为了深入理解GPT的核心机制，下面是因果Self-Attention的简化实现：</p>
<div id="295c60e9" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""GPT 的因果自注意力机制"""</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads, context_len):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> n_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_heads <span class="op">=</span> n_heads</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> n_heads</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q, K, V 投影（合并为一个矩阵以提高效率）</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qkv_proj <span class="op">=</span> nn.Linear(d_model, <span class="dv">3</span> <span class="op">*</span> d_model)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 因果掩码：下三角矩阵（关键！）</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 注册为 buffer，不参与梯度计算</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> torch.tril(torch.ones(context_len, context_len))</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"mask"</span>, mask.view(<span class="dv">1</span>, <span class="dv">1</span>, context_len, context_len))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> x.shape  <span class="co"># batch, seq_len, d_model</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算 Q, K, V</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv_proj(x)  <span class="co"># [B, T, 3*C]</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> qkv.chunk(<span class="dv">3</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 重塑为多头: [B, n_heads, T, d_k]</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 注意力分数</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.d_k)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 因果掩码：上三角设为 -inf</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.masked_fill(<span class="va">self</span>.mask[:, :, :T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax + 加权求和</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> attn <span class="op">@</span> v  <span class="co"># [B, n_heads, T, d_k]</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并多头</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out_proj(out)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPTBlock(nn.Module):</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""GPT 的单个 Transformer Block"""</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads, context_len):</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(d_model, n_heads, context_len)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>            nn.Linear(d_model, <span class="dv">4</span> <span class="op">*</span> d_model),</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> d_model, d_model),</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-Norm 残差连接</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln1(x))</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.ln2(x))</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="复现论文的关键细节" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="复现论文的关键细节"><span class="header-section-number">5.3</span> 复现论文的关键细节</h3>
<p>如果你要复现GPT的原始实验，以下几个细节容易被忽略。</p>
<p><strong>Weight Tying</strong>。GPT的输出层重用了嵌入矩阵的转置<span class="math inline">\(W_e^T\)</span>，而不是使用一个独立的线性层。这将参数量减少了<span class="math inline">\(V \times d_{model}\)</span>（<span class="math inline">\(V\)</span>是词汇表大小），同时在语义上保持了输入和输出空间的一致性：如果一个词的嵌入向量与某个输出向量很接近，那么模型就更可能在该位置预测这个词。</p>
<p><strong>辅助LM损失的实现</strong>。微调时的辅助语言模型损失<span class="math inline">\(\lambda \cdot L_1\)</span>需要注意一个细节：它应该只在输入序列的token上计算，不应该包含特殊token（<span class="math inline">\([\text{Start}]\)</span>、<span class="math inline">\([\text{Delim}]\)</span>、<span class="math inline">\([\text{Extract}]\)</span>）。在实践中，<span class="math inline">\(\lambda = 0.5\)</span>效果最好，但这个值在不同任务上可能需要微调。</p>
<p><strong>微调时的学习率选择</strong>。GPT原论文使用<span class="math inline">\(6.25 \times 10^{-5}\)</span>作为微调学习率，这比预训练学习率低了4倍。这个保守的设置是为了在适配下游任务的同时尽量保留预训练知识。如果学习率太高，模型会迅速”遗忘”预训练学到的语言知识，退化为从随机初始化训练。</p>
</section>
<section id="实验结果" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="实验结果"><span class="header-section-number">5.4</span> 实验结果</h3>
<p>GPT在12个NLP基准任务上进行了评测，在其中9个上达到了当时的最佳水平：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>任务</th>
<th>类型</th>
<th>之前SOTA</th>
<th>GPT</th>
<th>绝对提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stories Cloze (Acc)</td>
<td>常识推理</td>
<td>77.6%</td>
<td>86.5%</td>
<td>+8.9%</td>
</tr>
<tr class="even">
<td>RACE-m (Acc)</td>
<td>阅读理解</td>
<td>62.6%</td>
<td>68.3%</td>
<td>+5.7%</td>
</tr>
<tr class="odd">
<td>RACE-h (Acc)</td>
<td>阅读理解</td>
<td>57.4%</td>
<td>63.1%</td>
<td>+5.7%</td>
</tr>
<tr class="even">
<td>MultiNLI (Acc)</td>
<td>文本蕴含</td>
<td>82.1%</td>
<td>82.1%</td>
<td>0.0%</td>
</tr>
<tr class="odd">
<td>QNLI (Acc)</td>
<td>问答蕴含</td>
<td>—</td>
<td>88.1%</td>
<td>—</td>
</tr>
<tr class="even">
<td>QQP (F1)</td>
<td>释义检测</td>
<td>—</td>
<td>70.3%</td>
<td>—</td>
</tr>
<tr class="odd">
<td>SST-2 (Acc)</td>
<td>情感分析</td>
<td>93.2%</td>
<td>91.3%</td>
<td>-1.9%</td>
</tr>
<tr class="even">
<td>CoLA (Mc)</td>
<td>语言可接受性</td>
<td>35.0%</td>
<td>45.4%</td>
<td>+10.4%</td>
</tr>
<tr class="odd">
<td>MRPC (F1)</td>
<td>释义检测</td>
<td>—</td>
<td>82.3%</td>
<td>—</td>
</tr>
</tbody>
</table>
<p>值得特别关注的是GPT在常识推理（Stories Cloze Test，+8.9%）和语言可接受性判断（CoLA，+10.4%）上的巨大提升。这说明预训练确实让模型学到了深层的世界知识和语法知识，而这些知识很难从小规模标注数据中直接学到。</p>
<p>Radford等人还做了一个重要的消融实验：用单层2048维LSTM替换12层Transformer，在相同的框架下测试。结果LSTM版本的平均分数下降了5.6个百分点，且仅在一个数据集（MRPC）上优于Transformer。这直接证明了Transformer作为预训练骨架相比LSTM的显著优势。</p>
<p>另一个重要的消融是层迁移分析。下图展示了将不同数量的预训练层迁移到下游任务时的性能变化：</p>
<div id="fig-layer-transfer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layer-transfer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-12/original/fig2a-layer-transfer.png" class="img-fluid figure-img" style="width:55.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layer-transfer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 层迁移分析：横轴为迁移的预训练层数（0表示不使用任何预训练层），纵轴为RACE和MultiNLI的准确率。随着迁移层数增加，性能稳步提升，说明预训练模型的每一层都包含有用的信息。Dev曲线与Train曲线的差距在高层数时缩小，表明更多预训练层有助于泛化。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Radford et al.&nbsp;(2018) “Improving Language Understanding by Generative Pre-Training”, Figure 2 (left)</em></p>
</div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨GPT为什么有效、微调策略的分析、以及它在预训练技术谱系中的位置</p>
</blockquote>
<section id="为什么生成式预训练对判别式任务有效" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么生成式预训练对判别式任务有效"><span class="header-section-number">6.1</span> 为什么生成式预训练对判别式任务有效？</h3>
<p>GPT面临的一个核心理论问题是：预训练目标是<strong>生成式的</strong>（预测下一个词），但下游任务大多是<strong>判别式的</strong>（分类、匹配）。为什么在生成任务上学到的知识能迁移到判别任务？</p>
<p>一个直觉的解释来自于语言模型需要”理解”什么。要准确预测”The capital of France is <strong><em>“的下一个词，模型不仅需要知道”France”的含义，还需要知道”capital”和国家名称之间的关系，以及世界知识（法国的首都是巴黎）。要预测”After eating the poisonous mushroom, she felt </em></strong>”的续写，模型需要理解因果关系和常识推理。语言模型的训练迫使模型学习丰富的语言知识和世界知识——而这些知识恰恰是许多判别式任务所需要的。</p>
<p>Radford等人在论文中进行了一项有说服力的分析。他们跟踪了一系列任务在预训练过程中的<strong>零样本性能（zero-shot performance）</strong>变化——也就是在不做任何微调的情况下，仅靠预训练模型的语言建模能力来间接完成任务。结果发现，随着预训练的进行，这些任务的零样本性能稳步提升。这说明语言建模目标确实在隐式地学习与下游任务相关的能力。</p>
<div id="fig-zero-shot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-zero-shot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-12/original/fig2b-zero-shot-performance.png" class="img-fluid figure-img" style="width:55.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-zero-shot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: 零样本性能随预训练步数的变化。实线为Transformer，虚线为LSTM。随着预训练的推进，情感分析、Winograd Schema、语言可接受性、问答等任务的零样本性能持续提升，且Transformer始终优于LSTM。这直接证明了语言建模目标在隐式地学习下游任务相关的能力。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Radford et al.&nbsp;(2018) “Improving Language Understanding by Generative Pre-Training”, Figure 2 (right)</em></p>
</div>
<p>更正式地说，可以从信息论的角度理解这个现象。一个好的语言模型需要对文本的分布<span class="math inline">\(P(\text{text})\)</span>建立准确的模型。而许多判别式任务实质上是在问关于<span class="math inline">\(P(\text{text})\)</span>的某些条件概率。例如，情感分析本质上是估计<span class="math inline">\(P(\text{positive} \mid \text{review})\)</span>，文本蕴含是估计<span class="math inline">\(P(\text{entail} \mid \text{premise, hypothesis})\)</span>。如果模型对<span class="math inline">\(P(\text{text})\)</span>有了很好的建模，那么这些条件概率也应该能被有效地估计——微调所做的，就是教会模型如何从它对<span class="math inline">\(P(\text{text})\)</span>的知识中提取出特定任务需要的条件概率。</p>
</section>
<section id="微调-vs-特征提取理论与实证" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="微调-vs-特征提取理论与实证"><span class="header-section-number">6.2</span> 微调 vs 特征提取：理论与实证</h3>
<p>GPT的全模型微调与ELMo的特征提取，哪种迁移方式更好？Radford等人在论文中给出了直接的实验对比。</p>
<p>在GPT的框架下，他们对比了两种使用方式：一种是标准的全模型微调，另一种是冻结Transformer参数只训练分类头（即”特征提取”模式）。结果发现，全模型微调在几乎所有任务上都优于特征提取，优势在部分任务上超过10个百分点。</p>
<p>从优化的角度理解这个结果：微调允许预训练模型的<strong>每一层</strong>都根据下游任务的梯度信号进行调整。低层可以微调哪些特征更重要，中层可以调整语义组合的模式，高层可以直接对接任务目标。特征提取模式下，这些调整都无法进行——下游模型只能”被动地”使用预训练特征。</p>
<p>不过，特征提取也有它的适用场景。当下游数据量极小（如只有几十个样本）时，全模型微调可能导致过拟合——因为模型有1.17亿个参数却只有极少的训练信号。这时，冻结预训练参数、只训练分类头反而可能更稳定。</p>
</section>
<section id="辅助lm损失的作用分析" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="辅助lm损失的作用分析"><span class="header-section-number">6.3</span> 辅助LM损失的作用分析</h3>
<p>GPT在微调时保留语言模型目标作为辅助损失（<span class="math inline">\(\lambda = 0.5\)</span>），这个设计值得深入分析。</p>
<p>Radford等人的消融实验显示，辅助LM损失对<strong>大数据集</strong>上的效果几乎没有影响，但对<strong>小数据集</strong>上的效果有明显的帮助。原因在于：大数据集提供了足够的监督信号来指导微调，模型不容易遗忘预训练知识；小数据集的监督信号薄弱，模型更容易在微调过程中偏离预训练学到的语言知识分布，辅助LM损失起到了”锚定”的作用。</p>
<p>这个发现与后来的研究形成了有趣的呼应。BERT（下一章）在微调时<strong>不使用</strong>辅助预训练损失，直接只优化下游任务损失。这可能是因为BERT的双向注意力提供了更丰富的表示，使得模型在微调时更稳健；也可能只是因为BERT的作者没有尝试这个技巧。在更大规模的模型（如GPT-3）中，辅助损失变得不再必要，因为模型的知识已经足够丰富，不容易被小规模的微调所”覆盖”。</p>
</section>
<section id="gpt在预训练技术谱系中的位置" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="gpt在预训练技术谱系中的位置"><span class="header-section-number">6.4</span> GPT在预训练技术谱系中的位置</h3>
<p>站在更高的视角来看，GPT在预训练技术的演进中占据了一个关键的位置。</p>
<p><strong>从ULMFiT到GPT的进步</strong>。ULMFiT（Howard &amp; Ruder, 2018）率先证明了微调范式在NLP中的有效性，但它使用的是三层AWD-LSTM——一个相对浅层的架构。GPT将微调范式与Transformer结合，证明了更深的架构+更大的数据可以带来更好的性能。更重要的是，GPT提出了通用的输入变换方法，使得同一个预训练模型可以适配文本分类、文本蕴含、问答等多种不同格式的任务，而ULMFiT仅在文本分类上验证。</p>
<p><strong>从ELMo到GPT的两个替换</strong>。用Transformer替代LSTM（更好的可扩展性和并行性），用微调替代特征提取（更充分的知识迁移）。这两个替换各自独立地带来了性能提升，组合在一起则产生了更大的增益。</p>
<p><strong>从GPT到BERT的转变</strong>。GPT选择了单向（左→右）的因果语言建模，BERT则选择了双向的掩码语言建模。这两条路线在2018年同时出现，形成了预训练范式的两大阵营。短期内，BERT在大多数理解任务上占据了优势（得益于双向上下文），但长期来看，GPT的自回归路线在规模化和生成能力上展现了更大的潜力——这是后来GPT-2和GPT-3的故事。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.5</span> 方法的边界条件</h3>
<p><strong>假设一：序列内的信息足够</strong>。GPT假设一个512 token的上下文窗口足以捕获语言建模所需的信息。对于大多数句子和段落，这个假设成立。但对于需要更长上下文的任务（如长文档摘要、多轮对话），512 token的限制是一个实际的瓶颈。</p>
<p><strong>假设二：单向上下文对预训练足够</strong>。GPT只使用左侧上下文来预训练模型。这对于生成任务是自然的，但对于理解任务来说，放弃右侧上下文意味着丢失了可能很关键的信息。考虑句子”I went to the bank to fish”——如果模型在处理”bank”时只能看到左侧的”I went to the”，它很可能将”bank”理解为金融机构而非河岸。只有看到右侧的”to fish”，正确的语义才能确定。</p>
<p><strong>假设三：微调与预训练的分布差异可控</strong>。微调假设下游任务的数据分布与预训练数据的分布不会差异太大。如果下游任务涉及预训练数据中完全没有覆盖的领域（如医学专业术语、法律文本），微调的效果可能不如预期。</p>
</section>
<section id="开放研究问题2018年视角" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="开放研究问题2018年视角"><span class="header-section-number">6.6</span> 开放研究问题（2018年视角）</h3>
<p>站在2018年年中的时间节点，GPT提出后几个关键的研究方向浮现出来。</p>
<p><strong>单向 vs 双向的优劣</strong>。GPT选择单向，ELMo选择分离式双向。有没有一种方式可以在预训练时真正地利用双向上下文？单向模型在什么任务上确实不如双向？这个问题在几个月后被BERT以掩码语言模型（MLM）优雅地回答了。</p>
<p><strong>规模化的潜力</strong>。GPT-1只有117M参数，在7000本书上训练。如果把模型做大10倍、100倍，数据做大10倍、100倍，会发生什么？性能会线性提升还是会有”涌现”？这个问题催生了GPT-2（1.5B参数）和GPT-3（175B参数）的研究。</p>
<p><strong>微调的必要性</strong>。GPT需要为每个下游任务进行微调——能否找到一种方式让同一个预训练模型不经微调就能处理多种任务？GPT-1中观察到的零样本性能提升暗示了这个可能性，但在117M参数的规模下效果还不够好。GPT-3后来证明，足够大的模型确实可以通过In-Context Learning实现这一点。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="单向注意力的根本限制" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="单向注意力的根本限制"><span class="header-section-number">7.1</span> 单向注意力的根本限制</h3>
<p>GPT最显著的局限是它的<strong>单向性</strong>——每个位置只能看到左侧的上下文，无法利用右侧的信息。</p>
<p>这个限制在语言理解任务中尤为突出。以自然语言推理（NLI）为例，给定前提”A man is sleeping in a green room on a white bed”和假设”A man is asleep”。判断蕴含关系需要同时理解前提和假设的完整语义，然后进行推理。在GPT的因果注意力下，假设中的每个词可以看到前提的全部内容（因为前提在前面），但前提中的词<strong>无法看到假设的内容</strong>。这意味着前提的表示是在”不知道假设是什么”的情况下生成的——它无法根据假设的内容调整自己的注意力焦点。</p>
<p>对比ELMo的处理方式：ELMo虽然也是两个单向LSTM的拼接，但至少每个方向都独立地编码了完整的序列信息。而GPT在处理序列时，前面的token对后面的token一无所知。</p>
<p>对比即将出现的BERT：BERT使用双向Transformer Encoder，每个位置可以同时关注左右两侧的所有位置。在处理NLI时，前提中的”sleeping”可以直接关注假设中的”asleep”，建立起精确的语义对应。这种双向交互是GPT的因果模型无法实现的。</p>
<p>然而，单向性并非没有优势。因果模型天然地支持<strong>文本生成</strong>——按从左到右的顺序逐个生成token，每一步的注意力范围随着生成的推进而扩大。BERT的双向模型在生成方面则面临困难，因为它在训练时假设所有位置都可以互相看到，但在生成时不得不处理”还没有生成的位置”。</p>
</section>
<section id="模型规模的局限" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="模型规模的局限"><span class="header-section-number">7.2</span> 模型规模的局限</h3>
<p>GPT-1只有117M参数，在约8亿词的BooksCorpus上训练。这个规模在2018年已经相当可观，但从后来的视角看，它远未达到Transformer的潜力上限。GPT-2将参数量扩大到1.5B（13倍），GPT-3进一步扩大到175B（1500倍），每次规模的跳跃都带来了显著的能力提升。</p>
<p>更重要的是，GPT-1的训练数据只有BooksCorpus一个来源。书籍文本虽然连贯性好，但多样性有限——它不包含新闻、百科、代码、学术论文等领域的文本。GPT-2通过爬取Reddit链接指向的高质量网页（WebText数据集，约40GB），显著扩大了数据的多样性。</p>
</section>
<section id="微调范式本身的限制" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="微调范式本身的限制"><span class="header-section-number">7.3</span> 微调范式本身的限制</h3>
<p>虽然微调比特征提取更有效，但它也有自身的局限。</p>
<p>第一，<strong>每个任务需要单独微调一份模型</strong>。如果你有10个不同的下游任务，你需要存储10份完整的模型参数——对于117M参数的模型这还可以接受，但对于后来数十亿参数的模型来说，这就变成了一个严重的存储问题。</p>
<p>第二，<strong>微调需要标注数据</strong>。虽然微调所需的数据量远小于从零训练，但它仍然需要一定量的任务特定标注数据。对于资源匮乏的语言或领域，获取足够的标注数据仍然是一个挑战。</p>
<p>第三，<strong>灾难性遗忘的风险</strong>。如果微调不够谨慎（学习率太高、训练时间太长），模型可能”遗忘”预训练学到的通用语言知识，退化为一个仅在小规模标注数据上训练的模型。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.4</span> 这些局限导向了什么？</h3>
<p>GPT-1的局限精确地预示了接下来几年NLP的发展方向。</p>
<p>单向注意力的限制直接催生了<strong>BERT的掩码语言模型</strong>——通过随机遮蔽输入中的部分词，让模型根据所有未遮蔽的上下文（包括左右两侧）来预测被遮蔽的词。这是下一章的核心主题。</p>
<p>模型规模的局限推动了<strong>规模化的探索</strong>——GPT-2（2019）和GPT-3（2020）将参数量从117M推到175B，发现了模型能力随规模增长的”涌现”现象。这是第17章（Scaling Laws）的内容。</p>
<p>微调范式的局限引发了<strong>Prompt-based学习</strong>的研究——能否不微调，而是通过精心设计的”提示”让模型直接完成任务？GPT-2初步展示了这种可能性，GPT-3的In-Context Learning则将其推向了高潮。这是第20章的主题。</p>
<blockquote class="blockquote">
<p>下一章预告：第13章将介绍BERT——Google用Transformer Encoder进行双向预训练的突破性工作。BERT与GPT的核心差异在于：用掩码语言模型（MLM）替代因果语言建模，让模型在预训练时可以同时利用左右两侧的上下文。这个改变带来了在理解任务上的全面超越，但也埋下了生成能力受限的隐患。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>这一章我们详细介绍了GPT——第一个将Transformer用于大规模语言模型预训练并通过全模型微调迁移到下游任务的工作。</p>
<p>核心问题是如何最大化预训练知识的迁移效率。ELMo的特征提取范式冻结了预训练模型，无法根据下游任务做出任何调整，且仍需独立设计下游模型架构。</p>
<p>GPT的洞察是将这个问题简化为两个清晰的决策：用Transformer Decoder替代LSTM获得更好的可扩展性和并行性，用全模型微调替代特征提取获得更充分的知识迁移。</p>
<p>GPT的技术方案包含三个核心要素：因果语言建模作为预训练目标，12层Transformer Decoder作为架构，以及通过输入变换统一不同任务格式的微调框架。辅助语言模型损失作为微调时的正则化，有效防止了灾难性遗忘。</p>
<p>实验表明，GPT在12个NLP基准任务中的9个上达到了最佳水平，在常识推理（+8.9%）和语言可接受性（+10.4%）上的提升尤为突出。Transformer vs LSTM的消融实验证明了架构选择的重要性（平均+5.6%）。</p>
<p>然而，GPT的单向注意力在理解任务上是一个根本限制，这直接催生了BERT的双向预训练方案。</p>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(L_1(\mathcal{U}) = \sum_{i} \log P(u_i \mid u_{i-k}, \ldots, u_{i-1})\)</span></td>
<td>因果语言建模预训练目标</td>
</tr>
<tr class="even">
<td><span class="math inline">\(h_0 = UW_e + W_p\)</span></td>
<td>输入表示 = 词嵌入 + 位置嵌入</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P(y \mid x) = \text{softmax}(h_L^m \cdot W_y)\)</span></td>
<td>微调时的分类预测</td>
</tr>
<tr class="even">
<td><span class="math inline">\(L_3 = L_2 + \lambda \cdot L_1\)</span></td>
<td>微调时的联合损失（含辅助LM损失）</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> GPT选择Transformer Decoder而非Encoder的根本原因是什么？如果强行用Transformer Encoder做自回归语言建模，会遇到什么问题？能否通过某种掩码机制让Encoder也支持因果建模？</p></li>
<li><p><strong>[数学推导]</strong> 计算GPT-1的总参数量。12层Transformer Decoder，<span class="math inline">\(d_{model} = 768\)</span>，<span class="math inline">\(h = 12\)</span>，FFN内部维度3072，词汇表大小40000（使用weight tying）。提示：需要分别计算嵌入层、每层的注意力模块、每层的FFN模块、以及LayerNorm的参数。</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 在SST-2情感分析数据集上对比以下配置的性能：(a) 随机初始化的12层Transformer + 监督训练；(b) 预训练GPT + 微调（不含辅助LM损失）；(c) 预训练GPT + 微调（含辅助LM损失，<span class="math inline">\(\lambda = 0.5\)</span>）。分别在100、1000、10000条训练数据下对比，观察预训练的价值如何随数据量变化。</p></li>
<li><p><strong>[研究思考]</strong> GPT的输入变换将所有任务都”拍平”为线性序列。这意味着模型需要从纯文本序列中理解任务结构（如”哪部分是前提，哪部分是假设”）。你认为这种方式的上限在哪里？有没有一些任务结构是纯文本序列难以表达的？（提示：思考表格理解、图结构推理等场景。）</p></li>
<li><p><strong>[对比分析]</strong> GPT和ELMo代表了”微调”和”特征提取”两种迁移范式。从优化理论的角度分析，微调相当于用预训练权重作为良好的初始化点，而特征提取相当于将预训练表示视为固定的特征变换。在什么条件下，一个好的初始化比一个好的固定特征更有价值？反过来呢？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). “Improving Language Understanding by Generative Pre-Training”</strong>。GPT的原始论文。重点阅读：Section 3（框架描述，包括预训练和微调的完整细节）、Section 4.2（消融实验——辅助LM损失、层数影响、Transformer vs LSTM）。可快速浏览：Section 2的相关工作部分。<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI PDF</a></p>
</section>
<section id="前驱工作" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="前驱工作"><span class="header-section-number">9.2</span> 前驱工作</h3>
<p><strong>Howard, J. &amp; Ruder, S. (2018). “Universal Language Model Fine-tuning for Text Classification” (ULMFiT)</strong>。率先在NLP中系统地验证了微调范式的有效性。GPT可以看作是ULMFiT思想在更强架构（Transformer）上的实现。特别值得关注的是ULMFiT提出的三种微调技巧：判别性微调（不同层不同学习率）、斜三角学习率、逐层解冻。<a href="https://arxiv.org/abs/1801.06146">arXiv:1801.06146</a></p>
<p><strong>Peters, M.E. et al.&nbsp;(2018). “Deep contextualized word representations” (ELMo)</strong>。上一章的主题。与GPT对比阅读，可以清晰地看到”特征提取 vs 微调”和”LSTM vs Transformer”两个维度的影响。<a href="https://arxiv.org/abs/1802.05365">arXiv:1802.05365</a></p>
</section>
<section id="后续发展" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.3</span> 后续发展</h3>
<p><strong>Radford, A. et al.&nbsp;(2019). “Language Models are Unsupervised Multitask Learners” (GPT-2)</strong>。将GPT扩大到1.5B参数，在WebText上训练。关键发现：足够大的语言模型可以在无需微调的情况下完成多种任务（零样本迁移）。<a href="https://openai.com/research/better-language-models">OpenAI Blog</a></p>
<p><strong>Brown, T. et al.&nbsp;(2020). “Language Models are Few-Shot Learners” (GPT-3)</strong>。175B参数，展示了In-Context Learning的强大能力。标志着从”预训练+微调”到”预训练+提示”的范式转变。<a href="https://arxiv.org/abs/2005.14165">arXiv:2005.14165</a></p>
<p><strong>Devlin, J. et al.&nbsp;(2019). “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”</strong>。用双向Transformer Encoder和掩码语言模型对GPT发起”挑战”——下一章的主题。<a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></p>
</section>
<section id="理论分析" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="理论分析"><span class="header-section-number">9.4</span> 理论分析</h3>
<p><strong>Hendrycks, D. &amp; Gimpel, K. (2016). “Gaussian Error Linear Units (GELUs)”</strong>。GPT引入的GELU激活函数的原始论文。提供了GELU相对于ReLU在优化和正则化方面优势的理论分析。<a href="https://arxiv.org/abs/1606.08415">arXiv:1606.08415</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.5</span> 代码资源</h3>
<ul>
<li><strong>Hugging Face Transformers</strong>：<a href="https://huggingface.co/openai-gpt">huggingface.co/openai-gpt</a> — 预训练GPT-1模型，支持直接微调</li>
<li><strong>minGPT (Andrej Karpathy)</strong>：<a href="https://github.com/karpathy/minGPT">github.com/karpathy/minGPT</a> — 极简的GPT实现，约300行代码，适合学习GPT的核心架构</li>
<li><strong>nanoGPT (Andrej Karpathy)</strong>：<a href="https://github.com/karpathy/nanoGPT">github.com/karpathy/nanoGPT</a> — minGPT的后继，专注于训练效率</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>GPT的论文标题”Improving Language Understanding by Generative Pre-Training”非常朴素——它甚至没有为自己的模型起一个名字。“GPT”这个缩写最初并非来自论文本身，而是社区根据标题中的”Generative Pre-Training”自然形成的简称。与ELMo精心选择的《芝麻街》角色名、BERT明确打出的缩写不同，GPT的命名充满了偶然性——它的影响力让这个简称成为了AI领域最广为人知的品牌之一。</p>
<p>一个值得深思的时间线是：ELMo在2018年2月发表，GPT在2018年6月发布，BERT在2018年10月发表。三个团队在同一年几乎同时但独立地走向了上下文预训练，但选择了不同的路线——ELMo用LSTM+特征提取，GPT用Transformer Decoder+微调，BERT用Transformer Encoder+微调。这种”殊途同归”的现象说明，预训练范式的到来不是某个团队的灵感乍现，而是计算资源（大规模GPU集群）、数据条件（互联网文本的爆发）和方法论积累（Transformer、微调技术）三者汇合的必然结果。</p>
<p>更有趣的是，这三条路线在后来的命运截然不同。ELMo的LSTM+特征提取路线逐渐式微，BERT的双向Encoder路线在2019-2020年统治了理解任务的排行榜，而GPT的单向Decoder路线——在当时看起来似乎是三者中最”弱”的，因为它放弃了双向性——却在规模化的道路上走得最远，最终催生了ChatGPT和GPT-4，彻底改变了AI的面貌。</p>
<p>Radford等人在论文结尾写道：“We’ve demonstrated that achieving significant performance gains via generative pre-training […] is possible.” 这种谦逊的措辞很难让人预见到，仅仅两年之后，GPT-3就会以175B参数展示出近乎”智能”的In-Context Learning能力，引发AI领域前所未有的关注和讨论。回头看，GPT-1就像一颗种子——种在了正确的土壤（Transformer架构）和正确的方向（自回归语言建模 + 规模化）上，它的全部潜力需要等到参数量增长1000倍之后才真正显现。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第12章：GPT——自回归预训练路线"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Improving Language Understanding by Generative Pre-Training"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-26"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, Pre-training, GPT, Transformer]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [GPT, 自回归, Causal LM, 预训练, 微调, Transformer Decoder]</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "GPT：第一个将Transformer用于大规模语言模型预训练的工作。通过自回归语言建模预训练Transformer Decoder，再在下游任务上微调全部参数，确立了'预训练+微调'的现代NLP范式。"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-12/gpt-banner.png"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：能否用Transformer替代LSTM进行语言模型预训练，并通过微调整个模型（而非仅提取特征）来高效迁移到下游任务？</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2018年6月 </span><span class="pp">|</span><span class="at"> Radford et al. "Improving Language Understanding by Generative Pre-Training" </span><span class="pp">|</span><span class="at"> OpenAI</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radford et al. (2018)** "Improving Language Understanding by Generative Pre-Training" — 参考了 Section 3（框架描述）、Section 4（实验）、Table 1-5、Figure 1-2</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Peters et al. (2018)** "Deep contextualized word representations" (ELMo) — 参考了与GPT的对比分析</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Howard &amp; Ruder (2018)** "Universal Language Model Fine-tuning for Text Classification" (ULMFiT) — 参考了微调策略的对比</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Section 15.8-15.10 (BERT) — 参考了ELMo/GPT/BERT对比图和教学组织方式</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SLP3** Chapter 10-11 — 参考了预训练范式的讲解框架</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 9 (2025) "Pretraining" — 参考了GPT架构讲解和预训练范式对比</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CMU 11-711** ANLP (2024) — 参考了autoregressive LM的理论分析</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>上一章我们详细介绍了ELMo——第一个成功的上下文词向量预训练模型。ELMo用深层双向LSTM语言模型为每个词生成上下文相关的表示，然后将这些表示作为"特征"拼接到下游模型的输入中。在6个NLP基准任务上，ELMo带来了显著的性能提升，证明了深层预训练不仅在理论上合理，在实践中也确实有效。</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>然而，上一章结尾我们也揭示了ELMo的三个根本局限。</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>第一个局限是**分离式双向**。ELMo的前向和后向LSTM独立运行，两者的表示通过简单拼接组合，从未在模型内部进行深度融合。这意味着模型无法在推理过程中让左右两侧的信息互相"对话"。</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>第二个局限是**特征提取范式**。ELMo冻结预训练模型的参数，只学习几个标量权重来混合不同层的表示。预训练模型永远不会为下游任务做出任何调整——就像雇了一个能力很强的顾问，却规定他只能提供意见而不能亲自动手。更重要的是，使用ELMo时，你仍然需要为每个下游任务从零设计一个完整的模型架构。</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>第三个局限是**LSTM的可扩展性瓶颈**。LSTM必须顺序处理序列，无法在序列维度上并行化。在第8章我们已经看到，Transformer通过Self-Attention实现了完全并行的计算，在速度和规模上远超LSTM。</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>2018年6月，也就是ELMo发表仅4个月之后，OpenAI的Alec Radford等人发布了一项工作，一举解决了后两个问题。</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：用Transformer Decoder替代LSTM作为预训练骨架，用全模型微调替代特征提取作为迁移方式。这两个看似简单的替换，不仅带来了更好的性能，更确立了"预训练 + 微调"的现代NLP范式——一个预训练好的模型，只需要在顶部加一个简单的分类头，就能适配几乎任何下游任务。</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### 如何最大化预训练知识的迁移？</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>ELMo证明了预训练是有效的，但它也暴露了一个更深层的问题：**预训练知识到底应该以什么方式迁移到下游任务？**</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>这个问题在2018年上半年有两个截然不同的答案。</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>第一种是ELMo的**特征提取路线**：预训练一个语言模型，冻结它的参数，把它的中间表示作为"特征"输入到一个独立的下游模型中。这种方式的好处是简单——预训练模型只需要运行一次前向传播就能提供特征，不需要反向传播。但代价也很明显：预训练模型的知识被"锁死"了，无法根据下游任务的需要进行任何调整。</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>第二种是ULMFiT（上一章简要提到的Howard &amp; Ruder, 2018）率先在文本分类上验证的**微调路线**：预训练一个语言模型，然后在下游任务上**微调整个模型的参数**。ULMFiT证明了微调在文本分类上的巨大优势，但它使用的是三层LSTM——一个相对简单的架构。一个自然的问题是：如果用更强大的Transformer替代LSTM，再配合全模型微调，效果会怎样？</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>GPT正是对这个问题的回答。</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### 之前的尝试有什么不足？</span></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>让我们更精确地分析ELMo特征提取范式的问题。</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>假设预训练模型学到了关于"bank"的两种语境——金融语境和地理语境——的丰富表示。在下游的金融情感分析任务中，我们希望"bank"的表示更偏向金融含义，且携带更多情感相关的信息。但ELMo的参数被冻结了，它在金融文本和地理文本中生成的"bank"表示完全取决于预训练阶段学到的模式，无法根据"这是一个情感分析任务"的信号做出任何调整。</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>微调则完全不同。如果允许整个模型的参数在下游任务上继续更新，那么模型可以逐渐将自己"特化"到目标任务：Attention头可以重新分配权重以关注情感相关的信号，FFN层可以调整内部的知识激活模式，甚至词嵌入本身都可以微妙地适配目标领域。</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么选择Transformer Decoder？</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>GPT面临的第一个关键设计决策是：预训练应该用什么架构？</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>2018年中期，可选的架构主要有三种：</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>**LSTM**。ELMo已经证明了LSTM作为预训练骨架的可行性，但LSTM有顺序计算瓶颈，难以扩展到更大的规模。</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>**Transformer Encoder**。第8章介绍的Transformer有完整的编码器和解码器。编码器使用双向Self-Attention（每个位置可以看到所有其他位置），非常适合理解任务。但有一个问题：如果用编码器做语言建模，每个位置都能"看到"要预测的目标词，这会导致信息泄漏——模型可以直接抄答案而不需要真正理解语言。</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>**Transformer Decoder**。解码器使用因果（causal）Self-Attention——每个位置只能看到它左侧的位置。这天然地匹配自回归语言建模的需求：预测位置$k$的词时，模型只能利用$t_1, \ldots, t_{k-1}$的信息，不会泄漏未来信息。</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>Radford等人选择了Transformer Decoder。这个选择有一个深层的逻辑：**自回归语言建模是最"自然"的无监督预训练目标**——给定前文，预测下一个词。而Transformer Decoder的因果注意力掩码恰好实现了这一点。相比之下，ELMo为了实现双向性，不得不将前向和后向分成两个独立的模型——因为如果LSTM能同时看到左右两侧，语言模型训练就毫无意义了。GPT干脆接受了单向的限制，用一个统一的模型优雅地完成语言建模。</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>当然，选择单向意味着放弃了右侧上下文的信息——这是一个真实的代价，我们将在本章末尾的"局限性"一节中详细讨论。</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键洞察：从"顾问"到"全职员工"</span></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>理解GPT与ELMo的核心区别，可以用一个直觉的类比。</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>ELMo就像一个**外部顾问**。你的公司（下游任务）遇到了一个难题，于是请了一位语言理解方面的顾问（预训练的biLM）。顾问来了，看了看你的问题，给出了一份书面报告（ELMo特征向量），然后就离开了。你的团队拿着这份报告，自己做决策。顾问的专业知识确实有帮助，但有几个问题：顾问给的是通用建议，不了解你公司的具体业务（没有针对下游任务调整）；你的团队还是得自己从零搭建决策框架（还需要设计下游模型架构）；顾问和你的团队之间缺乏深度合作（特征提取是单向的、一次性的）。</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>GPT则像**直接雇佣这位顾问成为全职员工**。他加入你的团队（预训练模型成为下游模型的一部分），深入了解你的业务（在下游数据上微调），逐渐调整自己的工作方式来适应你的需求（参数更新）。你只需要在他的基础上加一个简单的决策层（分类头），就能直接产出结果。他带来了全部的通用知识（预训练权重），又能针对你的具体问题做出调整（微调）。</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a><span class="fu">### 两阶段框架：预训练 + 微调</span></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>GPT的训练过程分为清晰的两个阶段。</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>**第一阶段：无监督预训练**。在大规模无标注文本上，训练一个Transformer Decoder来做自回归语言建模——给定前面的词，预测下一个词。这个阶段的目标是让模型学习通用的语言知识：语法规则、语义关系、世界知识、推理模式。训练完成后，模型拥有了"理解语言"的基础能力。</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>**第二阶段：有监督微调**。在特定的下游任务（如文本分类、自然语言推理、问答等）上，保留预训练的模型参数作为初始化，在模型顶部添加一个简单的线性分类头，然后用标注数据微调**整个模型**（包括预训练的Transformer参数和新加的分类头）。</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>这个框架的优雅之处在于它的**简洁性**。预训练阶段不需要任何标注数据，只需要大量的原始文本。微调阶段不需要设计复杂的下游模型架构——预训练的Transformer本身就是下游模型的主体，只需要在顶部加一个最简单的线性层就够了。</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### 输入变换：一个模型适配所有任务</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>GPT面临的一个实际挑战是：不同的下游任务有不同的输入格式。文本分类只需要一段文本，自然语言推理需要一对前提-假设，问答需要文章-问题-答案三元组。如何让同一个预训练模型适配这些不同的格式？</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>GPT的解决方案非常巧妙：**将所有任务的输入都转换为一个连续的token序列**，用特殊分隔符连接不同的部分。</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>对于**文本分类**，输入就是文本本身：<span class="in">`[Start] text [Extract]`</span>，取最后一个token的表示做分类。</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>对于**自然语言推理**，将前提和假设用分隔符连接：<span class="in">`[Start] premise [Delim] hypothesis [Extract]`</span>。</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>对于**问答**，将文章和问题连接：<span class="in">`[Start] document [Delim] question [Delim] answer [Extract]`</span>。</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>对于**语义相似度**（两个句子顺序不重要的任务），分别构造两种顺序的输入，将两者的表示相加后做分类。</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a><span class="al">![GPT的输入变换：将四种不同结构的下游任务统一转换为线性token序列。文本分类直接输入文本；文本蕴含用分隔符连接前提和假设；语义相似度构造两种顺序并相加；多选问答为每个候选答案构造独立序列。](figures/chapter-12/original/fig1b-input-transformations.png)</span>{#fig-input-transform width=95%}</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>*Source: Radford et al. (2018) "Improving Language Understanding by Generative Pre-Training", Figure 1 (right)*</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>这种"把结构化输入拍平成序列"的思路在当时看来只是一种工程上的权宜之计，但它实际上蕴含了一个深远的洞察：**语言模型本身具有足够的能力从文本序列中理解任务结构**。只要把信息以恰当的方式放入序列，模型就能学会处理它。这个思路后来在GPT-2和GPT-3中被推到极致——连分类头都不需要了，直接用自然语言"提示"模型生成答案。</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练：因果语言建模</span></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>GPT的预训练目标是标准的**因果语言建模（Causal Language Modeling, CLM）**：给定前面的词，最大化下一个词的条件概率。</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>给定一个无标注的token序列$\mathcal{U} = <span class="sc">\{</span>u_1, u_2, \ldots, u_n<span class="sc">\}</span>$，目标是最大化以下似然函数：</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>L_1(\mathcal{U}) = \sum_{i} \log P(u_i \mid u_{i-k}, \ldots, u_{i-1}; \Theta)</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>其中$k$是上下文窗口的大小（GPT中$k = 512$），$\Theta$是模型的全部参数。</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>这个目标与ELMo的biLM目标有什么区别？ELMo同时训练前向和后向两个方向的语言模型，但它们是**两个独立的模型**，参数不共享（除了嵌入层和softmax层）。GPT则只训练一个方向——从左到右——但使用的是**单一的、统一的**Transformer模型。</span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>你可能会问：只看一个方向，不是比双向少了一半的信息吗？这个质疑是合理的，我们将在"局限性"一节中正面讨论。但GPT的立场是：**单向建模在架构上更简洁**——不需要将两个独立模型的输出拼接在一起，也不需要处理前向和后向信息如何融合的问题。整个模型是一个完整的端到端系统。</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型架构</span></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>GPT使用的是12层Transformer Decoder，具体参数如下：</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 超参数 <span class="pp">|</span> 值 <span class="pp">|</span></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----|</span></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 层数（$L$） <span class="pp">|</span> 12 <span class="pp">|</span></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 隐藏维度（$d_{model}$） <span class="pp">|</span> 768 <span class="pp">|</span></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 注意力头数（$h$） <span class="pp">|</span> 12 <span class="pp">|</span></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 每头维度（$d_k$） <span class="pp">|</span> 64 <span class="pp">|</span></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FFN内部维度 <span class="pp">|</span> 3072 <span class="pp">|</span></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 上下文窗口 <span class="pp">|</span> 512 tokens <span class="pp">|</span></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 总参数量 <span class="pp">|</span> ~117M <span class="pp">|</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 激活函数 <span class="pp">|</span> GELU <span class="pp">|</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>对比ELMo（2层biLSTM，93.6M参数）和同年稍后的BERT-Base（12层Transformer Encoder，110M参数），GPT在参数量上与它们处于同一数量级，但架构选择截然不同。</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a><span class="al">![GPT模型架构：12层Transformer Decoder堆叠。每层包含Masked Multi Self Attention和Feed Forward两个子模块，均配有Layer Norm和残差连接。底部是词嵌入与位置嵌入的相加，顶部同时输出Text Prediction（预训练）和Task Classifier（微调）。](figures/chapter-12/original/fig1a-gpt-architecture.png)</span>{#fig-gpt-arch width=45%}</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>*Source: Radford et al. (2018) "Improving Language Understanding by Generative Pre-Training", Figure 1 (left)*</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>模型的前向传播过程可以形式化为：</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>h_0 = UW_e + W_p</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>h_l = \text{transformer<span class="sc">\_</span>block}(h_{l-1}) \quad \forall l \in <span class="co">[</span><span class="ot">1, L</span><span class="co">]</span></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>P(u) = \text{softmax}(h_L W_e^T)</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>其中$U = (u_{i-k}, \ldots, u_{i-1})$是上下文token的独热编码，$W_e$是token嵌入矩阵，$W_p$是位置嵌入矩阵。</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>值得注意的是，GPT使用了**可学习的位置嵌入**（learnable positional embeddings）而非第8章Transformer原文中的正弦位置编码。Radford等人发现可学习的位置嵌入在实践中效果与正弦编码相当，但更加简单。在最后的输出层，GPT**重用了嵌入矩阵** $W_e^T$（即 weight tying），这不仅减少了参数量，还在输入和输出空间之间建立了一致的语义映射。</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>每个Transformer block内部的结构与第8章的标准Decoder层一致，但去掉了交叉注意力（Cross-Attention）模块（因为GPT没有编码器），只保留了因果Self-Attention和FFN：</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>a_l = \text{LayerNorm}(h_{l-1} + \text{MaskedMultiHead}(h_{l-1}))</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>h_l = \text{LayerNorm}(a_l + \text{FFN}(a_l))</span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>另一个值得注意的细节是激活函数的选择：GPT使用了**GELU（Gaussian Error Linear Unit）**而非Transformer原文中的ReLU。GELU可以看作ReLU的平滑近似——ReLU在零点有一个硬拐角，GELU则是光滑的。这个选择后来被BERT和几乎所有后续的Transformer模型所采用，成为事实上的标准。</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a><span class="fu">## GELU激活函数</span></span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>\text{GELU}(x) = x \cdot \Phi(x) \approx 0.5x\left(1 + \tanh\left<span class="co">[</span><span class="ot">\sqrt{2/\pi}\left(x + 0.044715x^3\right)\right</span><span class="co">]</span>\right)</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>其中$\Phi(x)$是标准正态分布的累积分布函数。直觉上，GELU根据输入值的大小以一定概率"通过"或"抑制"输入，而不是像ReLU那样在零点做硬切割。</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>*Source: Hendrycks &amp; Gimpel (2016) "Gaussian Error Linear Units (GELUs)". [arXiv:1606.08415](https://arxiv.org/abs/1606.08415)*</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### 微调：从语言模型到任务模型</span></span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>预训练完成后，GPT在有标注的下游数据集上进行微调。假设标注数据集$\mathcal{C}$中的每个样本由输入token序列$x^1, \ldots, x^m$和标签$y$组成。输入经过预训练的Transformer，取最后一个token的最终层表示$h_L^m$，然后通过一个新增的线性层进行预测：</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>P(y \mid x^1, \ldots, x^m) = \text{softmax}(h_L^m \cdot W_y)</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>微调的目标是最大化：</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>L_2(\mathcal{C}) = \sum_{(x, y)} \log P(y \mid x^1, \ldots, x^m)</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>Radford等人发现了一个重要的技巧：**在微调时保留语言模型目标作为辅助损失**。最终的微调损失为：</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C})</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>其中$\lambda$是一个权重超参数（论文中$\lambda = 0.5$）。</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>为什么辅助语言模型损失有帮助？有两个原因。第一，它起到了**正则化**的作用——防止模型在微调过程中过度"遗忘"预训练学到的通用语言知识。这类似于机器学习中的L2正则化，但更有意义：它不是简单地限制参数的大小，而是要求模型在适配下游任务的同时保持"理解语言"的基本能力。第二，它有助于**加速收敛**——语言模型目标为模型的低层提供了持续的、有意义的梯度信号，而不是仅依赖分类损失从顶层传下来的梯度。</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: GPT Pre-training and Fine-tuning（改编自 Radford et al., 2018）</span></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpt_pretrain(corpus, n_layers<span class="op">=</span><span class="dv">12</span>, d_model<span class="op">=</span><span class="dv">768</span>, n_heads<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>                 context_len<span class="op">=</span><span class="dv">512</span>, lr<span class="op">=</span><span class="fl">2.5e-4</span>, warmup_steps<span class="op">=</span><span class="dv">2000</span>):</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a><span class="co">    阶段一：无监督预训练</span></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a><span class="co">    架构: 12层 Transformer Decoder (Masked Self-Attention + FFN)</span></span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a><span class="co">    数据: BooksCorpus (~7000本书, ~8亿词)</span></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化模型</span></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> TransformerDecoder(n_layers, d_model, n_heads)</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> corpus:</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> batch  <span class="co"># (u_1, u_2, ..., u_n), 最长 512 tokens</span></span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 嵌入 = 词嵌入 + 位置嵌入</span></span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>        h_0 <span class="op">=</span> model.token_embed(tokens) <span class="op">+</span> model.pos_embed(positions)</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 12层 Transformer Decoder</span></span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h_0</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> layer(h)  <span class="co"># Masked Self-Attention + FFN + LayerNorm</span></span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 因果语言建模损失</span></span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 用位置 i 的输出预测位置 i+1 的词</span></span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> h <span class="op">@</span> model.token_embed.weight.T  <span class="co"># Weight tying</span></span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> cross_entropy(logits[:, :<span class="op">-</span><span class="dv">1</span>], tokens[:, <span class="dv">1</span>:])</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>        optimizer.step()  <span class="co"># Adam, cosine lr schedule</span></span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpt_finetune(pretrained_model, labeled_data, num_classes,</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>                 lambda_lm<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a><span class="co">    阶段二：有监督微调</span></span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a><span class="co">    在预训练模型顶部加一个线性分类头，微调所有参数</span></span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 新增分类头</span></span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> Linear(d_model, num_classes)</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (tokens, label) <span class="kw">in</span> labeled_data:</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 前向传播（整个预训练模型）</span></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>        h_L <span class="op">=</span> pretrained_model(tokens)  <span class="co"># [seq_len, d_model]</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 分类损失 — 取最后一个 token 的表示</span></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> classifier(h_L[<span class="op">-</span><span class="dv">1</span>])     <span class="co"># [num_classes]</span></span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>        loss_cls <span class="op">=</span> cross_entropy(logits, label)</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 辅助语言模型损失 — 正则化</span></span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>        lm_logits <span class="op">=</span> h_L <span class="op">@</span> pretrained_model.token_embed.weight.T</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>        loss_lm <span class="op">=</span> cross_entropy(lm_logits[:, :<span class="op">-</span><span class="dv">1</span>], tokens[:, <span class="dv">1</span>:])</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 联合损失</span></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_cls <span class="op">+</span> lambda_lm <span class="op">*</span> loss_lm</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>        loss.backward()  <span class="co"># 更新所有参数（包括预训练的）</span></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>*改编自 Radford et al. (2018) "Improving Language Understanding by Generative Pre-Training", Section 3.1-3.2*</span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a><span class="fu">### 输入变换的具体实现</span></span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>不同任务的输入结构不同，GPT通过**特殊token和分隔符**将它们统一为线性序列。</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>**文本分类**：</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">\text{Start}</span><span class="co">]</span>\; \text{text tokens}\; <span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span> \xrightarrow{h_L^{<span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span>}} W_y \rightarrow \text{softmax} \rightarrow P(y)</span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>直接将文本送入模型，取 $<span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span>$ token的最终层表示进行分类。</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>**文本蕴含（Natural Language Inference）**：</span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">\text{Start}</span><span class="co">]</span>\; \text{premise}\; <span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>\; \text{hypothesis}\; <span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>前提和假设用 $<span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>$ 分隔符连接。模型在处理这个序列时，通过因果注意力，hypothesis部分的每个token都能看到premise的全部内容。</span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>**语义相似度**（对称任务）：</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a>\text{输入1:}\; <span class="co">[</span><span class="ot">\text{Start}</span><span class="co">]</span>\; A\; <span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>\; B\; <span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span> \qquad</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a>\text{输入2:}\; <span class="co">[</span><span class="ot">\text{Start}</span><span class="co">]</span>\; B\; <span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>\; A\; <span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>由于语义相似度是对称的（A与B的相似度 = B与A的相似度），GPT构造两种顺序的输入，分别提取表示，然后逐元素相加后做分类。</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>**多选问答**：</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">\text{Start}</span><span class="co">]</span>\; \text{context}\; <span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>\; \text{question}\; <span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>\; \text{answer}_k\; <span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span> \quad k = 1, \ldots, K</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>对每个候选答案构造一个输入序列，分别提取表示并通过分类头得到分数，然后softmax选择最高分的答案。</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练细节</span></span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>**预训练数据**。GPT在BooksCorpus数据集上预训练。这个数据集包含超过7000本未出版的书籍，涵盖冒险、奇幻、言情等多种体裁，总计约8亿词。选择书籍而非网页文本的原因是书籍包含大段连贯的叙述——这让模型能够学习长距离的语言结构和叙事逻辑，而网页文本往往是碎片化的。</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a>**分词器**。GPT使用了BPE（Byte Pair Encoding，第3章讨论的子词分词方法），词汇表大小约40,000个token。</span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a>**优化器**。使用Adam优化器，最大学习率$2.5 \times 10^{-4}$。学习率在前2000步线性预热（warmup），然后按余弦退火（cosine annealing）降为0。</span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a>**微调超参数**。微调阶段使用比预训练更保守的设置：学习率$6.25 \times 10^{-5}$（比预训练低4倍），batch size 32，训练3个epoch。这些保守的设置是为了避免"灾难性遗忘"——在大量微调更新中丢失预训练学到的通用知识。</span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：因果注意力掩码</span></span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a><span class="al">![因果注意力掩码的可视化。左图：掩码模式——下三角为允许的注意力连接（✓），上三角为被遮蔽的连接（✗），确保每个位置只能看到自己和左侧的词。右图：经过Softmax后的注意力权重——"The"只能关注自己（1.00），"cat"分配注意力给"The"和自己，"mat"能看到所有前面的词。](figures/chapter-12/fig-causal-attention-mask.png)</span>{#fig-causal-mask width=95%}</span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a>*作者绘制。基于 GPT 因果注意力机制的教学示意图。*</span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>为了具体理解GPT中因果注意力的工作方式，让我们通过一个小规模的数值例子来追踪信息的流动。</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>**设定**：句子 "The cat sat"，$d_{model} = 4$，单头注意力（$h = 1$）。</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>**Step 1: 嵌入**</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>假设词嵌入和位置嵌入之和为：</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a>h_0 = \begin{bmatrix} \text{The:} &amp; 1.0 &amp; 0.5 &amp; 0.2 &amp; 0.1 <span class="sc">\\</span> \text{cat:} &amp; 0.3 &amp; 1.2 &amp; 0.8 &amp; 0.4 <span class="sc">\\</span> \text{sat:} &amp; 0.7 &amp; 0.1 &amp; 1.1 &amp; 0.9 \end{bmatrix}</span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>**Step 2: 计算 Q, K, V**</span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>为简化，假设 $W_Q = W_K = W_V = I$（单位矩阵），则 $Q = K = V = h_0$。</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>**Step 3: 注意力分数 $QK^T$**</span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>QK^T = \begin{bmatrix}</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>1.30 &amp; 1.12 &amp; 1.03 <span class="sc">\\</span></span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a>1.12 &amp; 2.33 &amp; 1.43 <span class="sc">\\</span></span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>1.03 &amp; 1.43 &amp; 2.52</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a>**Step 4: 因果掩码（关键步骤！）**</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>与标准Transformer Encoder不同，GPT的Decoder使用因果掩码：上三角部分设为$-\infty$，确保每个位置只能看到自己和左侧的位置。</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a>\text{Masked Scores} = \begin{bmatrix}</span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>1.30 &amp; -\infty &amp; -\infty <span class="sc">\\</span></span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>1.12 &amp; 2.33 &amp; -\infty <span class="sc">\\</span></span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>1.03 &amp; 1.43 &amp; 2.52</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>"The"只能看到自己，"cat"能看到"The"和自己，"sat"能看到所有三个词。</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>**Step 5: 缩放 + Softmax**</span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>除以$\sqrt{d_k} = \sqrt{4} = 2$后做softmax：</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a>\text{Scaled} = \begin{bmatrix}</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a>0.65 &amp; -\infty &amp; -\infty <span class="sc">\\</span></span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a>0.56 &amp; 1.17 &amp; -\infty <span class="sc">\\</span></span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>0.52 &amp; 0.72 &amp; 1.26</span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a>Softmax（对每行）：</span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>A = \begin{bmatrix}</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 0.00 &amp; 0.00 <span class="sc">\\</span></span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>0.35 &amp; 0.65 &amp; 0.00 <span class="sc">\\</span></span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a>0.24 &amp; 0.29 &amp; 0.47</span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>**解读**：</span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The"（第1行）：注意力权重$<span class="co">[</span><span class="ot">1.00, 0.00, 0.00</span><span class="co">]</span>$——只能关注自己</span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"cat"（第2行）：注意力权重$<span class="co">[</span><span class="ot">0.35, 0.65, 0.00</span><span class="co">]</span>$——关注自己更多（0.65），也参考了"The"（0.35），但完全看不到"sat"</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"sat"（第3行）：注意力权重$<span class="co">[</span><span class="ot">0.24, 0.29, 0.47</span><span class="co">]</span>$——主要关注自己（0.47），也综合了"cat"（0.29）和"The"（0.24）</span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a>**Step 6: 加权输出 $AV$**</span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a>\text{Output} = AV = \begin{bmatrix}</span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 0.50 &amp; 0.20 &amp; 0.10 <span class="sc">\\</span></span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a>0.55 &amp; 0.96 &amp; 0.59 &amp; 0.30 <span class="sc">\\</span></span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a>0.69 &amp; 0.51 &amp; 0.78 &amp; 0.56</span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a>"cat"位置的输出$<span class="co">[</span><span class="ot">0.55, 0.96, 0.59, 0.30</span><span class="co">]</span>$融合了"The"和自身的信息，但完全不包含"sat"的信息。这正是因果模型的特点——**在生成"cat"之后的下一个词时，模型只能依赖"The"和"cat"，不能"偷看"后面的内容**。</span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a>与ELMo的对比非常清晰：ELMo的前向LSTM在"cat"位置也只能看到左侧上下文，但它是通过顺序递归实现的（$h_t = f(h_{t-1}, x_t)$），而GPT通过因果掩码在一步注意力计算中实现了同样的约束，同时保持了并行性。</span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度分析</span></span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> GPT <span class="pp">|</span> ELMo <span class="pp">|</span></span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----|------|</span></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练参数 <span class="pp">|</span> ~117M <span class="pp">|</span> ~93.6M <span class="pp">|</span></span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练复杂度 <span class="pp">|</span> $O(n^2 \cdot d)$（Self-Attention） <span class="pp">|</span> $O(n \cdot d^2)$（LSTM） <span class="pp">|</span></span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练并行性 <span class="pp">|</span> ✅ 完全并行 <span class="pp">|</span> ❌ 顺序计算 <span class="pp">|</span></span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 微调新增参数 <span class="pp">|</span> $d_{model} \times C$（分类头） <span class="pp">|</span> $L + 2$个标量 <span class="pp">|</span></span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 微调计算量 <span class="pp">|</span> 反向传播整个模型 <span class="pp">|</span> 仅前向传播+少量参数更新 <span class="pp">|</span></span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>GPT在预训练阶段的计算复杂度中，$n$是序列长度，$d$是隐藏维度。Self-Attention的$O(n^2 \cdot d)$复杂度在$n = 512$时完全可以接受。与LSTM的$O(n \cdot d^2)$相比，当$n &lt; d$时Self-Attention更高效，当$n &gt; d$时LSTM更高效——但Self-Attention的关键优势不在渐近复杂度，而在**并行性**。$O(n^2 \cdot d)$的计算可以在GPU上并行完成，而$O(n \cdot d^2)$的LSTM必须顺序执行。</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a><span class="fu">### 与其他方法的对比</span></span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> ELMo <span class="pp">|</span> GPT <span class="pp">|</span> BERT（下一章） <span class="pp">|</span></span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|-----|-------------|</span></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练架构 <span class="pp">|</span> 双向LSTM <span class="pp">|</span> 单向Transformer Decoder <span class="pp">|</span> 双向Transformer Encoder <span class="pp">|</span></span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练任务 <span class="pp">|</span> 双向语言建模 <span class="pp">|</span> 因果语言建模 <span class="pp">|</span> 掩码语言建模 (MLM) <span class="pp">|</span></span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方向性 <span class="pp">|</span> 分离式双向 <span class="pp">|</span> 单向（左→右） <span class="pp">|</span> 融合式双向 <span class="pp">|</span></span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 迁移方式 <span class="pp">|</span> 特征提取（冻结） <span class="pp">|</span> 全模型微调 <span class="pp">|</span> 全模型微调 <span class="pp">|</span></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 下游模型 <span class="pp">|</span> 需要独立设计 <span class="pp">|</span> 预训练模型+分类头 <span class="pp">|</span> 预训练模型+分类头 <span class="pp">|</span></span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 参数量 <span class="pp">|</span> ~93.6M <span class="pp">|</span> ~117M <span class="pp">|</span> ~110M (Base) <span class="pp">|</span></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 并行性 <span class="pp">|</span> ❌ <span class="pp">|</span> ✅ <span class="pp">|</span> ✅ <span class="pp">|</span></span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a><span class="al">![ELMo、GPT 和 BERT 三种预训练架构的对比。ELMo 使用双向 LSTM 并通过特征拼接迁移（左）；GPT 使用单向 Transformer Decoder 并通过微调迁移（中）；BERT 使用双向 Transformer Encoder 并通过微调迁移（右）。](figures/chapter-12/original/fig-elmo-gpt-bert-d2l.svg)</span>{#fig-three-pretrain width=95%}</span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a>*Source: Dive into Deep Learning (d2l.ai), Figure 15.8.3. License: CC BY-SA 4.0*</span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Hugging Face微调GPT</span></span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a>虽然原始的GPT-1模型已经被更强大的后续版本取代，但通过Hugging Face Transformers库可以方便地体验GPT的预训练+微调流程。以下代码展示了在情感分析任务上微调GPT的标准工作流：</span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> OpenAIGPTTokenizer, OpenAIGPTForSequenceClassification</span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer, TrainingArguments</span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的 GPT-1 模型和分词器</span></span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> OpenAIGPTTokenizer.from_pretrained(<span class="st">"openai-gpt"</span>)</span>
<span id="cb4-507"><a href="#cb4-507" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> OpenAIGPTForSequenceClassification.from_pretrained(</span>
<span id="cb4-508"><a href="#cb4-508" aria-hidden="true" tabindex="-1"></a>    <span class="st">"openai-gpt"</span>,</span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a>    num_labels<span class="op">=</span><span class="dv">2</span>  <span class="co"># 二分类：正面/负面</span></span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a><span class="co"># 分词器设置</span></span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a>model.config.pad_token_id <span class="op">=</span> tokenizer.eos_token_id</span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备数据</span></span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">"This movie is fantastic!"</span>, <span class="st">"Terrible waste of time."</span>]</span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 正面, 负面</span></span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a>encodings <span class="op">=</span> tokenizer(</span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a>    texts,</span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-528"><a href="#cb4-528" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 Dataset</span></span>
<span id="cb4-529"><a href="#cb4-529" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentDataset(torch.utils.data.Dataset):</span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb4-535"><a href="#cb4-535" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> {k: v[idx] <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb4-536"><a href="#cb4-536" aria-hidden="true" tabindex="-1"></a>        item[<span class="st">"labels"</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb4-537"><a href="#cb4-537" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span>
<span id="cb4-538"><a href="#cb4-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-539"><a href="#cb4-539" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb4-540"><a href="#cb4-540" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb4-541"><a href="#cb4-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-542"><a href="#cb4-542" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> SentimentDataset(encodings, labels)</span>
<span id="cb4-543"><a href="#cb4-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-544"><a href="#cb4-544" aria-hidden="true" tabindex="-1"></a><span class="co"># 微调配置</span></span>
<span id="cb4-545"><a href="#cb4-545" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：学习率比预训练时低很多（6.25e-5 vs 2.5e-4）</span></span>
<span id="cb4-546"><a href="#cb4-546" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb4-547"><a href="#cb4-547" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./gpt-sentiment"</span>,</span>
<span id="cb4-548"><a href="#cb4-548" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb4-549"><a href="#cb4-549" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb4-550"><a href="#cb4-550" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">6.25e-5</span>,     <span class="co"># GPT 微调推荐的学习率</span></span>
<span id="cb4-551"><a href="#cb4-551" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb4-552"><a href="#cb4-552" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb4-553"><a href="#cb4-553" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb4-554"><a href="#cb4-554" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-555"><a href="#cb4-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-556"><a href="#cb4-556" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练</span></span>
<span id="cb4-557"><a href="#cb4-557" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb4-558"><a href="#cb4-558" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb4-559"><a href="#cb4-559" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb4-560"><a href="#cb4-560" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset,</span>
<span id="cb4-561"><a href="#cb4-561" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-562"><a href="#cb4-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-563"><a href="#cb4-563" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb4-564"><a href="#cb4-564" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-565"><a href="#cb4-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-566"><a href="#cb4-566" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现GPT的因果注意力</span></span>
<span id="cb4-567"><a href="#cb4-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-568"><a href="#cb4-568" aria-hidden="true" tabindex="-1"></a>为了深入理解GPT的核心机制，下面是因果Self-Attention的简化实现：</span>
<span id="cb4-569"><a href="#cb4-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-572"><a href="#cb4-572" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-573"><a href="#cb4-573" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb4-574"><a href="#cb4-574" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-575"><a href="#cb4-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-576"><a href="#cb4-576" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-577"><a href="#cb4-577" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-578"><a href="#cb4-578" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-579"><a href="#cb4-579" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-580"><a href="#cb4-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-581"><a href="#cb4-581" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb4-582"><a href="#cb4-582" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""GPT 的因果自注意力机制"""</span></span>
<span id="cb4-583"><a href="#cb4-583" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads, context_len):</span>
<span id="cb4-584"><a href="#cb4-584" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-585"><a href="#cb4-585" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> n_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb4-586"><a href="#cb4-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-587"><a href="#cb4-587" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_heads <span class="op">=</span> n_heads</span>
<span id="cb4-588"><a href="#cb4-588" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> n_heads</span>
<span id="cb4-589"><a href="#cb4-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-590"><a href="#cb4-590" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q, K, V 投影（合并为一个矩阵以提高效率）</span></span>
<span id="cb4-591"><a href="#cb4-591" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qkv_proj <span class="op">=</span> nn.Linear(d_model, <span class="dv">3</span> <span class="op">*</span> d_model)</span>
<span id="cb4-592"><a href="#cb4-592" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb4-593"><a href="#cb4-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-594"><a href="#cb4-594" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 因果掩码：下三角矩阵（关键！）</span></span>
<span id="cb4-595"><a href="#cb4-595" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 注册为 buffer，不参与梯度计算</span></span>
<span id="cb4-596"><a href="#cb4-596" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> torch.tril(torch.ones(context_len, context_len))</span>
<span id="cb4-597"><a href="#cb4-597" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"mask"</span>, mask.view(<span class="dv">1</span>, <span class="dv">1</span>, context_len, context_len))</span>
<span id="cb4-598"><a href="#cb4-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-599"><a href="#cb4-599" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-600"><a href="#cb4-600" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> x.shape  <span class="co"># batch, seq_len, d_model</span></span>
<span id="cb4-601"><a href="#cb4-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-602"><a href="#cb4-602" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算 Q, K, V</span></span>
<span id="cb4-603"><a href="#cb4-603" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv_proj(x)  <span class="co"># [B, T, 3*C]</span></span>
<span id="cb4-604"><a href="#cb4-604" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> qkv.chunk(<span class="dv">3</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-605"><a href="#cb4-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-606"><a href="#cb4-606" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 重塑为多头: [B, n_heads, T, d_k]</span></span>
<span id="cb4-607"><a href="#cb4-607" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-608"><a href="#cb4-608" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-609"><a href="#cb4-609" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-610"><a href="#cb4-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-611"><a href="#cb4-611" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 注意力分数</span></span>
<span id="cb4-612"><a href="#cb4-612" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.d_k)</span>
<span id="cb4-613"><a href="#cb4-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-614"><a href="#cb4-614" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 因果掩码：上三角设为 -inf</span></span>
<span id="cb4-615"><a href="#cb4-615" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.masked_fill(<span class="va">self</span>.mask[:, :, :T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb4-616"><a href="#cb4-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-617"><a href="#cb4-617" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax + 加权求和</span></span>
<span id="cb4-618"><a href="#cb4-618" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-619"><a href="#cb4-619" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> attn <span class="op">@</span> v  <span class="co"># [B, n_heads, T, d_k]</span></span>
<span id="cb4-620"><a href="#cb4-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-621"><a href="#cb4-621" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并多头</span></span>
<span id="cb4-622"><a href="#cb4-622" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C)</span>
<span id="cb4-623"><a href="#cb4-623" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out_proj(out)</span>
<span id="cb4-624"><a href="#cb4-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-625"><a href="#cb4-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-626"><a href="#cb4-626" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPTBlock(nn.Module):</span>
<span id="cb4-627"><a href="#cb4-627" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""GPT 的单个 Transformer Block"""</span></span>
<span id="cb4-628"><a href="#cb4-628" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads, context_len):</span>
<span id="cb4-629"><a href="#cb4-629" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-630"><a href="#cb4-630" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb4-631"><a href="#cb4-631" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(d_model, n_heads, context_len)</span>
<span id="cb4-632"><a href="#cb4-632" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb4-633"><a href="#cb4-633" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-634"><a href="#cb4-634" aria-hidden="true" tabindex="-1"></a>            nn.Linear(d_model, <span class="dv">4</span> <span class="op">*</span> d_model),</span>
<span id="cb4-635"><a href="#cb4-635" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb4-636"><a href="#cb4-636" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> d_model, d_model),</span>
<span id="cb4-637"><a href="#cb4-637" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-638"><a href="#cb4-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-639"><a href="#cb4-639" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-640"><a href="#cb4-640" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-Norm 残差连接</span></span>
<span id="cb4-641"><a href="#cb4-641" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln1(x))</span>
<span id="cb4-642"><a href="#cb4-642" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.ln2(x))</span>
<span id="cb4-643"><a href="#cb4-643" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb4-644"><a href="#cb4-644" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-645"><a href="#cb4-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-646"><a href="#cb4-646" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复现论文的关键细节</span></span>
<span id="cb4-647"><a href="#cb4-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-648"><a href="#cb4-648" aria-hidden="true" tabindex="-1"></a>如果你要复现GPT的原始实验，以下几个细节容易被忽略。</span>
<span id="cb4-649"><a href="#cb4-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-650"><a href="#cb4-650" aria-hidden="true" tabindex="-1"></a>**Weight Tying**。GPT的输出层重用了嵌入矩阵的转置$W_e^T$，而不是使用一个独立的线性层。这将参数量减少了$V \times d_{model}$（$V$是词汇表大小），同时在语义上保持了输入和输出空间的一致性：如果一个词的嵌入向量与某个输出向量很接近，那么模型就更可能在该位置预测这个词。</span>
<span id="cb4-651"><a href="#cb4-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-652"><a href="#cb4-652" aria-hidden="true" tabindex="-1"></a>**辅助LM损失的实现**。微调时的辅助语言模型损失$\lambda \cdot L_1$需要注意一个细节：它应该只在输入序列的token上计算，不应该包含特殊token（$<span class="co">[</span><span class="ot">\text{Start}</span><span class="co">]</span>$、$<span class="co">[</span><span class="ot">\text{Delim}</span><span class="co">]</span>$、$<span class="co">[</span><span class="ot">\text{Extract}</span><span class="co">]</span>$）。在实践中，$\lambda = 0.5$效果最好，但这个值在不同任务上可能需要微调。</span>
<span id="cb4-653"><a href="#cb4-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-654"><a href="#cb4-654" aria-hidden="true" tabindex="-1"></a>**微调时的学习率选择**。GPT原论文使用$6.25 \times 10^{-5}$作为微调学习率，这比预训练学习率低了4倍。这个保守的设置是为了在适配下游任务的同时尽量保留预训练知识。如果学习率太高，模型会迅速"遗忘"预训练学到的语言知识，退化为从随机初始化训练。</span>
<span id="cb4-655"><a href="#cb4-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-656"><a href="#cb4-656" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实验结果</span></span>
<span id="cb4-657"><a href="#cb4-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-658"><a href="#cb4-658" aria-hidden="true" tabindex="-1"></a>GPT在12个NLP基准任务上进行了评测，在其中9个上达到了当时的最佳水平：</span>
<span id="cb4-659"><a href="#cb4-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-660"><a href="#cb4-660" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> 类型 <span class="pp">|</span> 之前SOTA <span class="pp">|</span> GPT <span class="pp">|</span> 绝对提升 <span class="pp">|</span></span>
<span id="cb4-661"><a href="#cb4-661" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|---------|------|---------|</span></span>
<span id="cb4-662"><a href="#cb4-662" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Stories Cloze (Acc) <span class="pp">|</span> 常识推理 <span class="pp">|</span> 77.6% <span class="pp">|</span> 86.5% <span class="pp">|</span> +8.9% <span class="pp">|</span></span>
<span id="cb4-663"><a href="#cb4-663" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RACE-m (Acc) <span class="pp">|</span> 阅读理解 <span class="pp">|</span> 62.6% <span class="pp">|</span> 68.3% <span class="pp">|</span> +5.7% <span class="pp">|</span></span>
<span id="cb4-664"><a href="#cb4-664" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RACE-h (Acc) <span class="pp">|</span> 阅读理解 <span class="pp">|</span> 57.4% <span class="pp">|</span> 63.1% <span class="pp">|</span> +5.7% <span class="pp">|</span></span>
<span id="cb4-665"><a href="#cb4-665" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MultiNLI (Acc) <span class="pp">|</span> 文本蕴含 <span class="pp">|</span> 82.1% <span class="pp">|</span> 82.1% <span class="pp">|</span> 0.0% <span class="pp">|</span></span>
<span id="cb4-666"><a href="#cb4-666" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> QNLI (Acc) <span class="pp">|</span> 问答蕴含 <span class="pp">|</span> — <span class="pp">|</span> 88.1% <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb4-667"><a href="#cb4-667" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> QQP (F1) <span class="pp">|</span> 释义检测 <span class="pp">|</span> — <span class="pp">|</span> 70.3% <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb4-668"><a href="#cb4-668" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SST-2 (Acc) <span class="pp">|</span> 情感分析 <span class="pp">|</span> 93.2% <span class="pp">|</span> 91.3% <span class="pp">|</span> -1.9% <span class="pp">|</span></span>
<span id="cb4-669"><a href="#cb4-669" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CoLA (Mc) <span class="pp">|</span> 语言可接受性 <span class="pp">|</span> 35.0% <span class="pp">|</span> 45.4% <span class="pp">|</span> +10.4% <span class="pp">|</span></span>
<span id="cb4-670"><a href="#cb4-670" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MRPC (F1) <span class="pp">|</span> 释义检测 <span class="pp">|</span> — <span class="pp">|</span> 82.3% <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb4-671"><a href="#cb4-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-672"><a href="#cb4-672" aria-hidden="true" tabindex="-1"></a>值得特别关注的是GPT在常识推理（Stories Cloze Test，+8.9%）和语言可接受性判断（CoLA，+10.4%）上的巨大提升。这说明预训练确实让模型学到了深层的世界知识和语法知识，而这些知识很难从小规模标注数据中直接学到。</span>
<span id="cb4-673"><a href="#cb4-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-674"><a href="#cb4-674" aria-hidden="true" tabindex="-1"></a>Radford等人还做了一个重要的消融实验：用单层2048维LSTM替换12层Transformer，在相同的框架下测试。结果LSTM版本的平均分数下降了5.6个百分点，且仅在一个数据集（MRPC）上优于Transformer。这直接证明了Transformer作为预训练骨架相比LSTM的显著优势。</span>
<span id="cb4-675"><a href="#cb4-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-676"><a href="#cb4-676" aria-hidden="true" tabindex="-1"></a>另一个重要的消融是层迁移分析。下图展示了将不同数量的预训练层迁移到下游任务时的性能变化：</span>
<span id="cb4-677"><a href="#cb4-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-678"><a href="#cb4-678" aria-hidden="true" tabindex="-1"></a><span class="al">![层迁移分析：横轴为迁移的预训练层数（0表示不使用任何预训练层），纵轴为RACE和MultiNLI的准确率。随着迁移层数增加，性能稳步提升，说明预训练模型的每一层都包含有用的信息。Dev曲线与Train曲线的差距在高层数时缩小，表明更多预训练层有助于泛化。](figures/chapter-12/original/fig2a-layer-transfer.png)</span>{#fig-layer-transfer width=55%}</span>
<span id="cb4-679"><a href="#cb4-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-680"><a href="#cb4-680" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-681"><a href="#cb4-681" aria-hidden="true" tabindex="-1"></a>*Source: Radford et al. (2018) "Improving Language Understanding by Generative Pre-Training", Figure 2 (left)*</span>
<span id="cb4-682"><a href="#cb4-682" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-683"><a href="#cb4-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-684"><a href="#cb4-684" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-685"><a href="#cb4-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-686"><a href="#cb4-686" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb4-687"><a href="#cb4-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-688"><a href="#cb4-688" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨GPT为什么有效、微调策略的分析、以及它在预训练技术谱系中的位置</span></span>
<span id="cb4-689"><a href="#cb4-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-690"><a href="#cb4-690" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么生成式预训练对判别式任务有效？</span></span>
<span id="cb4-691"><a href="#cb4-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-692"><a href="#cb4-692" aria-hidden="true" tabindex="-1"></a>GPT面临的一个核心理论问题是：预训练目标是**生成式的**（预测下一个词），但下游任务大多是**判别式的**（分类、匹配）。为什么在生成任务上学到的知识能迁移到判别任务？</span>
<span id="cb4-693"><a href="#cb4-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-694"><a href="#cb4-694" aria-hidden="true" tabindex="-1"></a>一个直觉的解释来自于语言模型需要"理解"什么。要准确预测"The capital of France is ___"的下一个词，模型不仅需要知道"France"的含义，还需要知道"capital"和国家名称之间的关系，以及世界知识（法国的首都是巴黎）。要预测"After eating the poisonous mushroom, she felt ___"的续写，模型需要理解因果关系和常识推理。语言模型的训练迫使模型学习丰富的语言知识和世界知识——而这些知识恰恰是许多判别式任务所需要的。</span>
<span id="cb4-695"><a href="#cb4-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-696"><a href="#cb4-696" aria-hidden="true" tabindex="-1"></a>Radford等人在论文中进行了一项有说服力的分析。他们跟踪了一系列任务在预训练过程中的**零样本性能（zero-shot performance）**变化——也就是在不做任何微调的情况下，仅靠预训练模型的语言建模能力来间接完成任务。结果发现，随着预训练的进行，这些任务的零样本性能稳步提升。这说明语言建模目标确实在隐式地学习与下游任务相关的能力。</span>
<span id="cb4-697"><a href="#cb4-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-698"><a href="#cb4-698" aria-hidden="true" tabindex="-1"></a><span class="al">![零样本性能随预训练步数的变化。实线为Transformer，虚线为LSTM。随着预训练的推进，情感分析、Winograd Schema、语言可接受性、问答等任务的零样本性能持续提升，且Transformer始终优于LSTM。这直接证明了语言建模目标在隐式地学习下游任务相关的能力。](figures/chapter-12/original/fig2b-zero-shot-performance.png)</span>{#fig-zero-shot width=55%}</span>
<span id="cb4-699"><a href="#cb4-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-700"><a href="#cb4-700" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-701"><a href="#cb4-701" aria-hidden="true" tabindex="-1"></a>*Source: Radford et al. (2018) "Improving Language Understanding by Generative Pre-Training", Figure 2 (right)*</span>
<span id="cb4-702"><a href="#cb4-702" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-703"><a href="#cb4-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-704"><a href="#cb4-704" aria-hidden="true" tabindex="-1"></a>更正式地说，可以从信息论的角度理解这个现象。一个好的语言模型需要对文本的分布$P(\text{text})$建立准确的模型。而许多判别式任务实质上是在问关于$P(\text{text})$的某些条件概率。例如，情感分析本质上是估计$P(\text{positive} \mid \text{review})$，文本蕴含是估计$P(\text{entail} \mid \text{premise, hypothesis})$。如果模型对$P(\text{text})$有了很好的建模，那么这些条件概率也应该能被有效地估计——微调所做的，就是教会模型如何从它对$P(\text{text})$的知识中提取出特定任务需要的条件概率。</span>
<span id="cb4-705"><a href="#cb4-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-706"><a href="#cb4-706" aria-hidden="true" tabindex="-1"></a><span class="fu">### 微调 vs 特征提取：理论与实证</span></span>
<span id="cb4-707"><a href="#cb4-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-708"><a href="#cb4-708" aria-hidden="true" tabindex="-1"></a>GPT的全模型微调与ELMo的特征提取，哪种迁移方式更好？Radford等人在论文中给出了直接的实验对比。</span>
<span id="cb4-709"><a href="#cb4-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-710"><a href="#cb4-710" aria-hidden="true" tabindex="-1"></a>在GPT的框架下，他们对比了两种使用方式：一种是标准的全模型微调，另一种是冻结Transformer参数只训练分类头（即"特征提取"模式）。结果发现，全模型微调在几乎所有任务上都优于特征提取，优势在部分任务上超过10个百分点。</span>
<span id="cb4-711"><a href="#cb4-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-712"><a href="#cb4-712" aria-hidden="true" tabindex="-1"></a>从优化的角度理解这个结果：微调允许预训练模型的**每一层**都根据下游任务的梯度信号进行调整。低层可以微调哪些特征更重要，中层可以调整语义组合的模式，高层可以直接对接任务目标。特征提取模式下，这些调整都无法进行——下游模型只能"被动地"使用预训练特征。</span>
<span id="cb4-713"><a href="#cb4-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-714"><a href="#cb4-714" aria-hidden="true" tabindex="-1"></a>不过，特征提取也有它的适用场景。当下游数据量极小（如只有几十个样本）时，全模型微调可能导致过拟合——因为模型有1.17亿个参数却只有极少的训练信号。这时，冻结预训练参数、只训练分类头反而可能更稳定。</span>
<span id="cb4-715"><a href="#cb4-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-716"><a href="#cb4-716" aria-hidden="true" tabindex="-1"></a><span class="fu">### 辅助LM损失的作用分析</span></span>
<span id="cb4-717"><a href="#cb4-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-718"><a href="#cb4-718" aria-hidden="true" tabindex="-1"></a>GPT在微调时保留语言模型目标作为辅助损失（$\lambda = 0.5$），这个设计值得深入分析。</span>
<span id="cb4-719"><a href="#cb4-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-720"><a href="#cb4-720" aria-hidden="true" tabindex="-1"></a>Radford等人的消融实验显示，辅助LM损失对**大数据集**上的效果几乎没有影响，但对**小数据集**上的效果有明显的帮助。原因在于：大数据集提供了足够的监督信号来指导微调，模型不容易遗忘预训练知识；小数据集的监督信号薄弱，模型更容易在微调过程中偏离预训练学到的语言知识分布，辅助LM损失起到了"锚定"的作用。</span>
<span id="cb4-721"><a href="#cb4-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-722"><a href="#cb4-722" aria-hidden="true" tabindex="-1"></a>这个发现与后来的研究形成了有趣的呼应。BERT（下一章）在微调时**不使用**辅助预训练损失，直接只优化下游任务损失。这可能是因为BERT的双向注意力提供了更丰富的表示，使得模型在微调时更稳健；也可能只是因为BERT的作者没有尝试这个技巧。在更大规模的模型（如GPT-3）中，辅助损失变得不再必要，因为模型的知识已经足够丰富，不容易被小规模的微调所"覆盖"。</span>
<span id="cb4-723"><a href="#cb4-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-724"><a href="#cb4-724" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPT在预训练技术谱系中的位置</span></span>
<span id="cb4-725"><a href="#cb4-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-726"><a href="#cb4-726" aria-hidden="true" tabindex="-1"></a>站在更高的视角来看，GPT在预训练技术的演进中占据了一个关键的位置。</span>
<span id="cb4-727"><a href="#cb4-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-728"><a href="#cb4-728" aria-hidden="true" tabindex="-1"></a>**从ULMFiT到GPT的进步**。ULMFiT（Howard &amp; Ruder, 2018）率先证明了微调范式在NLP中的有效性，但它使用的是三层AWD-LSTM——一个相对浅层的架构。GPT将微调范式与Transformer结合，证明了更深的架构+更大的数据可以带来更好的性能。更重要的是，GPT提出了通用的输入变换方法，使得同一个预训练模型可以适配文本分类、文本蕴含、问答等多种不同格式的任务，而ULMFiT仅在文本分类上验证。</span>
<span id="cb4-729"><a href="#cb4-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-730"><a href="#cb4-730" aria-hidden="true" tabindex="-1"></a>**从ELMo到GPT的两个替换**。用Transformer替代LSTM（更好的可扩展性和并行性），用微调替代特征提取（更充分的知识迁移）。这两个替换各自独立地带来了性能提升，组合在一起则产生了更大的增益。</span>
<span id="cb4-731"><a href="#cb4-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-732"><a href="#cb4-732" aria-hidden="true" tabindex="-1"></a>**从GPT到BERT的转变**。GPT选择了单向（左→右）的因果语言建模，BERT则选择了双向的掩码语言建模。这两条路线在2018年同时出现，形成了预训练范式的两大阵营。短期内，BERT在大多数理解任务上占据了优势（得益于双向上下文），但长期来看，GPT的自回归路线在规模化和生成能力上展现了更大的潜力——这是后来GPT-2和GPT-3的故事。</span>
<span id="cb4-733"><a href="#cb4-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-734"><a href="#cb4-734" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb4-735"><a href="#cb4-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-736"><a href="#cb4-736" aria-hidden="true" tabindex="-1"></a>**假设一：序列内的信息足够**。GPT假设一个512 token的上下文窗口足以捕获语言建模所需的信息。对于大多数句子和段落，这个假设成立。但对于需要更长上下文的任务（如长文档摘要、多轮对话），512 token的限制是一个实际的瓶颈。</span>
<span id="cb4-737"><a href="#cb4-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-738"><a href="#cb4-738" aria-hidden="true" tabindex="-1"></a>**假设二：单向上下文对预训练足够**。GPT只使用左侧上下文来预训练模型。这对于生成任务是自然的，但对于理解任务来说，放弃右侧上下文意味着丢失了可能很关键的信息。考虑句子"I went to the bank to fish"——如果模型在处理"bank"时只能看到左侧的"I went to the"，它很可能将"bank"理解为金融机构而非河岸。只有看到右侧的"to fish"，正确的语义才能确定。</span>
<span id="cb4-739"><a href="#cb4-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-740"><a href="#cb4-740" aria-hidden="true" tabindex="-1"></a>**假设三：微调与预训练的分布差异可控**。微调假设下游任务的数据分布与预训练数据的分布不会差异太大。如果下游任务涉及预训练数据中完全没有覆盖的领域（如医学专业术语、法律文本），微调的效果可能不如预期。</span>
<span id="cb4-741"><a href="#cb4-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-742"><a href="#cb4-742" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题（2018年视角）</span></span>
<span id="cb4-743"><a href="#cb4-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-744"><a href="#cb4-744" aria-hidden="true" tabindex="-1"></a>站在2018年年中的时间节点，GPT提出后几个关键的研究方向浮现出来。</span>
<span id="cb4-745"><a href="#cb4-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-746"><a href="#cb4-746" aria-hidden="true" tabindex="-1"></a>**单向 vs 双向的优劣**。GPT选择单向，ELMo选择分离式双向。有没有一种方式可以在预训练时真正地利用双向上下文？单向模型在什么任务上确实不如双向？这个问题在几个月后被BERT以掩码语言模型（MLM）优雅地回答了。</span>
<span id="cb4-747"><a href="#cb4-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-748"><a href="#cb4-748" aria-hidden="true" tabindex="-1"></a>**规模化的潜力**。GPT-1只有117M参数，在7000本书上训练。如果把模型做大10倍、100倍，数据做大10倍、100倍，会发生什么？性能会线性提升还是会有"涌现"？这个问题催生了GPT-2（1.5B参数）和GPT-3（175B参数）的研究。</span>
<span id="cb4-749"><a href="#cb4-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-750"><a href="#cb4-750" aria-hidden="true" tabindex="-1"></a>**微调的必要性**。GPT需要为每个下游任务进行微调——能否找到一种方式让同一个预训练模型不经微调就能处理多种任务？GPT-1中观察到的零样本性能提升暗示了这个可能性，但在117M参数的规模下效果还不够好。GPT-3后来证明，足够大的模型确实可以通过In-Context Learning实现这一点。</span>
<span id="cb4-751"><a href="#cb4-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-752"><a href="#cb4-752" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-753"><a href="#cb4-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-754"><a href="#cb4-754" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb4-755"><a href="#cb4-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-756"><a href="#cb4-756" aria-hidden="true" tabindex="-1"></a><span class="fu">### 单向注意力的根本限制</span></span>
<span id="cb4-757"><a href="#cb4-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-758"><a href="#cb4-758" aria-hidden="true" tabindex="-1"></a>GPT最显著的局限是它的**单向性**——每个位置只能看到左侧的上下文，无法利用右侧的信息。</span>
<span id="cb4-759"><a href="#cb4-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-760"><a href="#cb4-760" aria-hidden="true" tabindex="-1"></a>这个限制在语言理解任务中尤为突出。以自然语言推理（NLI）为例，给定前提"A man is sleeping in a green room on a white bed"和假设"A man is asleep"。判断蕴含关系需要同时理解前提和假设的完整语义，然后进行推理。在GPT的因果注意力下，假设中的每个词可以看到前提的全部内容（因为前提在前面），但前提中的词**无法看到假设的内容**。这意味着前提的表示是在"不知道假设是什么"的情况下生成的——它无法根据假设的内容调整自己的注意力焦点。</span>
<span id="cb4-761"><a href="#cb4-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-762"><a href="#cb4-762" aria-hidden="true" tabindex="-1"></a>对比ELMo的处理方式：ELMo虽然也是两个单向LSTM的拼接，但至少每个方向都独立地编码了完整的序列信息。而GPT在处理序列时，前面的token对后面的token一无所知。</span>
<span id="cb4-763"><a href="#cb4-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-764"><a href="#cb4-764" aria-hidden="true" tabindex="-1"></a>对比即将出现的BERT：BERT使用双向Transformer Encoder，每个位置可以同时关注左右两侧的所有位置。在处理NLI时，前提中的"sleeping"可以直接关注假设中的"asleep"，建立起精确的语义对应。这种双向交互是GPT的因果模型无法实现的。</span>
<span id="cb4-765"><a href="#cb4-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-766"><a href="#cb4-766" aria-hidden="true" tabindex="-1"></a>然而，单向性并非没有优势。因果模型天然地支持**文本生成**——按从左到右的顺序逐个生成token，每一步的注意力范围随着生成的推进而扩大。BERT的双向模型在生成方面则面临困难，因为它在训练时假设所有位置都可以互相看到，但在生成时不得不处理"还没有生成的位置"。</span>
<span id="cb4-767"><a href="#cb4-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-768"><a href="#cb4-768" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型规模的局限</span></span>
<span id="cb4-769"><a href="#cb4-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-770"><a href="#cb4-770" aria-hidden="true" tabindex="-1"></a>GPT-1只有117M参数，在约8亿词的BooksCorpus上训练。这个规模在2018年已经相当可观，但从后来的视角看，它远未达到Transformer的潜力上限。GPT-2将参数量扩大到1.5B（13倍），GPT-3进一步扩大到175B（1500倍），每次规模的跳跃都带来了显著的能力提升。</span>
<span id="cb4-771"><a href="#cb4-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-772"><a href="#cb4-772" aria-hidden="true" tabindex="-1"></a>更重要的是，GPT-1的训练数据只有BooksCorpus一个来源。书籍文本虽然连贯性好，但多样性有限——它不包含新闻、百科、代码、学术论文等领域的文本。GPT-2通过爬取Reddit链接指向的高质量网页（WebText数据集，约40GB），显著扩大了数据的多样性。</span>
<span id="cb4-773"><a href="#cb4-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-774"><a href="#cb4-774" aria-hidden="true" tabindex="-1"></a><span class="fu">### 微调范式本身的限制</span></span>
<span id="cb4-775"><a href="#cb4-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-776"><a href="#cb4-776" aria-hidden="true" tabindex="-1"></a>虽然微调比特征提取更有效，但它也有自身的局限。</span>
<span id="cb4-777"><a href="#cb4-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-778"><a href="#cb4-778" aria-hidden="true" tabindex="-1"></a>第一，**每个任务需要单独微调一份模型**。如果你有10个不同的下游任务，你需要存储10份完整的模型参数——对于117M参数的模型这还可以接受，但对于后来数十亿参数的模型来说，这就变成了一个严重的存储问题。</span>
<span id="cb4-779"><a href="#cb4-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-780"><a href="#cb4-780" aria-hidden="true" tabindex="-1"></a>第二，**微调需要标注数据**。虽然微调所需的数据量远小于从零训练，但它仍然需要一定量的任务特定标注数据。对于资源匮乏的语言或领域，获取足够的标注数据仍然是一个挑战。</span>
<span id="cb4-781"><a href="#cb4-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-782"><a href="#cb4-782" aria-hidden="true" tabindex="-1"></a>第三，**灾难性遗忘的风险**。如果微调不够谨慎（学习率太高、训练时间太长），模型可能"遗忘"预训练学到的通用语言知识，退化为一个仅在小规模标注数据上训练的模型。</span>
<span id="cb4-783"><a href="#cb4-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-784"><a href="#cb4-784" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb4-785"><a href="#cb4-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-786"><a href="#cb4-786" aria-hidden="true" tabindex="-1"></a>GPT-1的局限精确地预示了接下来几年NLP的发展方向。</span>
<span id="cb4-787"><a href="#cb4-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-788"><a href="#cb4-788" aria-hidden="true" tabindex="-1"></a>单向注意力的限制直接催生了**BERT的掩码语言模型**——通过随机遮蔽输入中的部分词，让模型根据所有未遮蔽的上下文（包括左右两侧）来预测被遮蔽的词。这是下一章的核心主题。</span>
<span id="cb4-789"><a href="#cb4-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-790"><a href="#cb4-790" aria-hidden="true" tabindex="-1"></a>模型规模的局限推动了**规模化的探索**——GPT-2（2019）和GPT-3（2020）将参数量从117M推到175B，发现了模型能力随规模增长的"涌现"现象。这是第17章（Scaling Laws）的内容。</span>
<span id="cb4-791"><a href="#cb4-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-792"><a href="#cb4-792" aria-hidden="true" tabindex="-1"></a>微调范式的局限引发了**Prompt-based学习**的研究——能否不微调，而是通过精心设计的"提示"让模型直接完成任务？GPT-2初步展示了这种可能性，GPT-3的In-Context Learning则将其推向了高潮。这是第20章的主题。</span>
<span id="cb4-793"><a href="#cb4-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-794"><a href="#cb4-794" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第13章将介绍BERT——Google用Transformer Encoder进行双向预训练的突破性工作。BERT与GPT的核心差异在于：用掩码语言模型（MLM）替代因果语言建模，让模型在预训练时可以同时利用左右两侧的上下文。这个改变带来了在理解任务上的全面超越，但也埋下了生成能力受限的隐患。</span></span>
<span id="cb4-795"><a href="#cb4-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-796"><a href="#cb4-796" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-797"><a href="#cb4-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-798"><a href="#cb4-798" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb4-799"><a href="#cb4-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-800"><a href="#cb4-800" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb4-801"><a href="#cb4-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-802"><a href="#cb4-802" aria-hidden="true" tabindex="-1"></a>这一章我们详细介绍了GPT——第一个将Transformer用于大规模语言模型预训练并通过全模型微调迁移到下游任务的工作。</span>
<span id="cb4-803"><a href="#cb4-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-804"><a href="#cb4-804" aria-hidden="true" tabindex="-1"></a>核心问题是如何最大化预训练知识的迁移效率。ELMo的特征提取范式冻结了预训练模型，无法根据下游任务做出任何调整，且仍需独立设计下游模型架构。</span>
<span id="cb4-805"><a href="#cb4-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-806"><a href="#cb4-806" aria-hidden="true" tabindex="-1"></a>GPT的洞察是将这个问题简化为两个清晰的决策：用Transformer Decoder替代LSTM获得更好的可扩展性和并行性，用全模型微调替代特征提取获得更充分的知识迁移。</span>
<span id="cb4-807"><a href="#cb4-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-808"><a href="#cb4-808" aria-hidden="true" tabindex="-1"></a>GPT的技术方案包含三个核心要素：因果语言建模作为预训练目标，12层Transformer Decoder作为架构，以及通过输入变换统一不同任务格式的微调框架。辅助语言模型损失作为微调时的正则化，有效防止了灾难性遗忘。</span>
<span id="cb4-809"><a href="#cb4-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-810"><a href="#cb4-810" aria-hidden="true" tabindex="-1"></a>实验表明，GPT在12个NLP基准任务中的9个上达到了最佳水平，在常识推理（+8.9%）和语言可接受性（+10.4%）上的提升尤为突出。Transformer vs LSTM的消融实验证明了架构选择的重要性（平均+5.6%）。</span>
<span id="cb4-811"><a href="#cb4-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-812"><a href="#cb4-812" aria-hidden="true" tabindex="-1"></a>然而，GPT的单向注意力在理解任务上是一个根本限制，这直接催生了BERT的双向预训练方案。</span>
<span id="cb4-813"><a href="#cb4-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-814"><a href="#cb4-814" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb4-815"><a href="#cb4-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-816"><a href="#cb4-816" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 含义 <span class="pp">|</span></span>
<span id="cb4-817"><a href="#cb4-817" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb4-818"><a href="#cb4-818" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $L_1(\mathcal{U}) = \sum_{i} \log P(u_i \mid u_{i-k}, \ldots, u_{i-1})$ <span class="pp">|</span> 因果语言建模预训练目标 <span class="pp">|</span></span>
<span id="cb4-819"><a href="#cb4-819" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $h_0 = UW_e + W_p$ <span class="pp">|</span> 输入表示 = 词嵌入 + 位置嵌入 <span class="pp">|</span></span>
<span id="cb4-820"><a href="#cb4-820" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P(y \mid x) = \text{softmax}(h_L^m \cdot W_y)$ <span class="pp">|</span> 微调时的分类预测 <span class="pp">|</span></span>
<span id="cb4-821"><a href="#cb4-821" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $L_3 = L_2 + \lambda \cdot L_1$ <span class="pp">|</span> 微调时的联合损失（含辅助LM损失） <span class="pp">|</span></span>
<span id="cb4-822"><a href="#cb4-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-823"><a href="#cb4-823" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb4-824"><a href="#cb4-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-825"><a href="#cb4-825" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** GPT选择Transformer Decoder而非Encoder的根本原因是什么？如果强行用Transformer Encoder做自回归语言建模，会遇到什么问题？能否通过某种掩码机制让Encoder也支持因果建模？</span>
<span id="cb4-826"><a href="#cb4-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-827"><a href="#cb4-827" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 计算GPT-1的总参数量。12层Transformer Decoder，$d_{model} = 768$，$h = 12$，FFN内部维度3072，词汇表大小40000（使用weight tying）。提示：需要分别计算嵌入层、每层的注意力模块、每层的FFN模块、以及LayerNorm的参数。</span>
<span id="cb4-828"><a href="#cb4-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-829"><a href="#cb4-829" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 在SST-2情感分析数据集上对比以下配置的性能：(a) 随机初始化的12层Transformer + 监督训练；(b) 预训练GPT + 微调（不含辅助LM损失）；(c) 预训练GPT + 微调（含辅助LM损失，$\lambda = 0.5$）。分别在100、1000、10000条训练数据下对比，观察预训练的价值如何随数据量变化。</span>
<span id="cb4-830"><a href="#cb4-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-831"><a href="#cb4-831" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** GPT的输入变换将所有任务都"拍平"为线性序列。这意味着模型需要从纯文本序列中理解任务结构（如"哪部分是前提，哪部分是假设"）。你认为这种方式的上限在哪里？有没有一些任务结构是纯文本序列难以表达的？（提示：思考表格理解、图结构推理等场景。）</span>
<span id="cb4-832"><a href="#cb4-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-833"><a href="#cb4-833" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[对比分析]** GPT和ELMo代表了"微调"和"特征提取"两种迁移范式。从优化理论的角度分析，微调相当于用预训练权重作为良好的初始化点，而特征提取相当于将预训练表示视为固定的特征变换。在什么条件下，一个好的初始化比一个好的固定特征更有价值？反过来呢？</span>
<span id="cb4-834"><a href="#cb4-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-835"><a href="#cb4-835" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-836"><a href="#cb4-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-837"><a href="#cb4-837" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb4-838"><a href="#cb4-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-839"><a href="#cb4-839" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb4-840"><a href="#cb4-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-841"><a href="#cb4-841" aria-hidden="true" tabindex="-1"></a>**Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). "Improving Language Understanding by Generative Pre-Training"**。GPT的原始论文。重点阅读：Section 3（框架描述，包括预训练和微调的完整细节）、Section 4.2（消融实验——辅助LM损失、层数影响、Transformer vs LSTM）。可快速浏览：Section 2的相关工作部分。<span class="co">[</span><span class="ot">OpenAI PDF</span><span class="co">](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)</span></span>
<span id="cb4-842"><a href="#cb4-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-843"><a href="#cb4-843" aria-hidden="true" tabindex="-1"></a><span class="fu">### 前驱工作</span></span>
<span id="cb4-844"><a href="#cb4-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-845"><a href="#cb4-845" aria-hidden="true" tabindex="-1"></a>**Howard, J. &amp; Ruder, S. (2018). "Universal Language Model Fine-tuning for Text Classification" (ULMFiT)**。率先在NLP中系统地验证了微调范式的有效性。GPT可以看作是ULMFiT思想在更强架构（Transformer）上的实现。特别值得关注的是ULMFiT提出的三种微调技巧：判别性微调（不同层不同学习率）、斜三角学习率、逐层解冻。<span class="co">[</span><span class="ot">arXiv:1801.06146</span><span class="co">](https://arxiv.org/abs/1801.06146)</span></span>
<span id="cb4-846"><a href="#cb4-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-847"><a href="#cb4-847" aria-hidden="true" tabindex="-1"></a>**Peters, M.E. et al. (2018). "Deep contextualized word representations" (ELMo)**。上一章的主题。与GPT对比阅读，可以清晰地看到"特征提取 vs 微调"和"LSTM vs Transformer"两个维度的影响。<span class="co">[</span><span class="ot">arXiv:1802.05365</span><span class="co">](https://arxiv.org/abs/1802.05365)</span></span>
<span id="cb4-848"><a href="#cb4-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-849"><a href="#cb4-849" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb4-850"><a href="#cb4-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-851"><a href="#cb4-851" aria-hidden="true" tabindex="-1"></a>**Radford, A. et al. (2019). "Language Models are Unsupervised Multitask Learners" (GPT-2)**。将GPT扩大到1.5B参数，在WebText上训练。关键发现：足够大的语言模型可以在无需微调的情况下完成多种任务（零样本迁移）。<span class="co">[</span><span class="ot">OpenAI Blog</span><span class="co">](https://openai.com/research/better-language-models)</span></span>
<span id="cb4-852"><a href="#cb4-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-853"><a href="#cb4-853" aria-hidden="true" tabindex="-1"></a>**Brown, T. et al. (2020). "Language Models are Few-Shot Learners" (GPT-3)**。175B参数，展示了In-Context Learning的强大能力。标志着从"预训练+微调"到"预训练+提示"的范式转变。<span class="co">[</span><span class="ot">arXiv:2005.14165</span><span class="co">](https://arxiv.org/abs/2005.14165)</span></span>
<span id="cb4-854"><a href="#cb4-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-855"><a href="#cb4-855" aria-hidden="true" tabindex="-1"></a>**Devlin, J. et al. (2019). "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"**。用双向Transformer Encoder和掩码语言模型对GPT发起"挑战"——下一章的主题。<span class="co">[</span><span class="ot">arXiv:1810.04805</span><span class="co">](https://arxiv.org/abs/1810.04805)</span></span>
<span id="cb4-856"><a href="#cb4-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-857"><a href="#cb4-857" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论分析</span></span>
<span id="cb4-858"><a href="#cb4-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-859"><a href="#cb4-859" aria-hidden="true" tabindex="-1"></a>**Hendrycks, D. &amp; Gimpel, K. (2016). "Gaussian Error Linear Units (GELUs)"**。GPT引入的GELU激活函数的原始论文。提供了GELU相对于ReLU在优化和正则化方面优势的理论分析。<span class="co">[</span><span class="ot">arXiv:1606.08415</span><span class="co">](https://arxiv.org/abs/1606.08415)</span></span>
<span id="cb4-860"><a href="#cb4-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-861"><a href="#cb4-861" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb4-862"><a href="#cb4-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-863"><a href="#cb4-863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Transformers**：<span class="co">[</span><span class="ot">huggingface.co/openai-gpt</span><span class="co">](https://huggingface.co/openai-gpt)</span> — 预训练GPT-1模型，支持直接微调</span>
<span id="cb4-864"><a href="#cb4-864" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**minGPT (Andrej Karpathy)**：<span class="co">[</span><span class="ot">github.com/karpathy/minGPT</span><span class="co">](https://github.com/karpathy/minGPT)</span> — 极简的GPT实现，约300行代码，适合学习GPT的核心架构</span>
<span id="cb4-865"><a href="#cb4-865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**nanoGPT (Andrej Karpathy)**：<span class="co">[</span><span class="ot">github.com/karpathy/nanoGPT</span><span class="co">](https://github.com/karpathy/nanoGPT)</span> — minGPT的后继，专注于训练效率</span>
<span id="cb4-866"><a href="#cb4-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-867"><a href="#cb4-867" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-868"><a href="#cb4-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-869"><a href="#cb4-869" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb4-870"><a href="#cb4-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-871"><a href="#cb4-871" aria-hidden="true" tabindex="-1"></a>GPT的论文标题"Improving Language Understanding by Generative Pre-Training"非常朴素——它甚至没有为自己的模型起一个名字。"GPT"这个缩写最初并非来自论文本身，而是社区根据标题中的"Generative Pre-Training"自然形成的简称。与ELMo精心选择的《芝麻街》角色名、BERT明确打出的缩写不同，GPT的命名充满了偶然性——它的影响力让这个简称成为了AI领域最广为人知的品牌之一。</span>
<span id="cb4-872"><a href="#cb4-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-873"><a href="#cb4-873" aria-hidden="true" tabindex="-1"></a>一个值得深思的时间线是：ELMo在2018年2月发表，GPT在2018年6月发布，BERT在2018年10月发表。三个团队在同一年几乎同时但独立地走向了上下文预训练，但选择了不同的路线——ELMo用LSTM+特征提取，GPT用Transformer Decoder+微调，BERT用Transformer Encoder+微调。这种"殊途同归"的现象说明，预训练范式的到来不是某个团队的灵感乍现，而是计算资源（大规模GPU集群）、数据条件（互联网文本的爆发）和方法论积累（Transformer、微调技术）三者汇合的必然结果。</span>
<span id="cb4-874"><a href="#cb4-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-875"><a href="#cb4-875" aria-hidden="true" tabindex="-1"></a>更有趣的是，这三条路线在后来的命运截然不同。ELMo的LSTM+特征提取路线逐渐式微，BERT的双向Encoder路线在2019-2020年统治了理解任务的排行榜，而GPT的单向Decoder路线——在当时看起来似乎是三者中最"弱"的，因为它放弃了双向性——却在规模化的道路上走得最远，最终催生了ChatGPT和GPT-4，彻底改变了AI的面貌。</span>
<span id="cb4-876"><a href="#cb4-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-877"><a href="#cb4-877" aria-hidden="true" tabindex="-1"></a>Radford等人在论文结尾写道："We've demonstrated that achieving significant performance gains via generative pre-training <span class="co">[</span><span class="ot">...</span><span class="co">]</span> is possible." 这种谦逊的措辞很难让人预见到，仅仅两年之后，GPT-3就会以175B参数展示出近乎"智能"的In-Context Learning能力，引发AI领域前所未有的关注和讨论。回头看，GPT-1就像一颗种子——种在了正确的土壤（Transformer架构）和正确的方向（自回归语言建模 + 规模化）上，它的全部潜力需要等到参数量增长1000倍之后才真正显现。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>