<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-26">
<meta name="description" content="BERT：用掩码语言模型（MLM）实现真正的双向预训练。通过随机遮蔽输入token并让模型根据完整的左右上下文来预测被遮蔽的词，BERT在11个NLP基准任务上全面超越了GPT和ELMo，开启了预训练模型的’BERT时代’。">

<title>第13章：BERT——双向预训练路线 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#理解任务为什么需要双向上下文" id="toc-理解任务为什么需要双向上下文" class="nav-link" data-scroll-target="#理解任务为什么需要双向上下文"><span class="header-section-number">2.1</span> 理解任务为什么需要双向上下文？</a></li>
  <li><a href="#双向性与语言建模的矛盾" id="toc-双向性与语言建模的矛盾" class="nav-link" data-scroll-target="#双向性与语言建模的矛盾"><span class="header-section-number">2.2</span> 双向性与语言建模的矛盾</a></li>
  <li><a href="#从预测下一个到填空" id="toc-从预测下一个到填空" class="nav-link" data-scroll-target="#从预测下一个到填空"><span class="header-section-number">2.3</span> 从”预测下一个”到”填空”</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#关键洞察完形填空式的预训练" id="toc-关键洞察完形填空式的预训练" class="nav-link" data-scroll-target="#关键洞察完形填空式的预训练"><span class="header-section-number">3.1</span> 关键洞察：完形填空式的预训练</a></li>
  <li><a href="#两个预训练目标" id="toc-两个预训练目标" class="nav-link" data-scroll-target="#两个预训练目标"><span class="header-section-number">3.2</span> 两个预训练目标</a></li>
  <li><a href="#架构选择为什么是transformer-encoder" id="toc-架构选择为什么是transformer-encoder" class="nav-link" data-scroll-target="#架构选择为什么是transformer-encoder"><span class="header-section-number">3.3</span> 架构选择：为什么是Transformer Encoder？</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#模型架构" id="toc-模型架构" class="nav-link" data-scroll-target="#模型架构"><span class="header-section-number">4.1</span> 模型架构</a></li>
  <li><a href="#输入表示" id="toc-输入表示" class="nav-link" data-scroll-target="#输入表示"><span class="header-section-number">4.2</span> 输入表示</a></li>
  <li><a href="#预训练目标一掩码语言模型mlm" id="toc-预训练目标一掩码语言模型mlm" class="nav-link" data-scroll-target="#预训练目标一掩码语言模型mlm"><span class="header-section-number">4.3</span> 预训练目标一：掩码语言模型（MLM）</a></li>
  <li><a href="#预训练目标二下一句预测nsp" id="toc-预训练目标二下一句预测nsp" class="nav-link" data-scroll-target="#预训练目标二下一句预测nsp"><span class="header-section-number">4.4</span> 预训练目标二：下一句预测（NSP）</a></li>
  <li><a href="#预训练数据" id="toc-预训练数据" class="nav-link" data-scroll-target="#预训练数据"><span class="header-section-number">4.5</span> 预训练数据</a></li>
  <li><a href="#关键设计决策" id="toc-关键设计决策" class="nav-link" data-scroll-target="#关键设计决策"><span class="header-section-number">4.6</span> 关键设计决策</a></li>
  <li><a href="#完整数值示例mlm的工作过程" id="toc-完整数值示例mlm的工作过程" class="nav-link" data-scroll-target="#完整数值示例mlm的工作过程"><span class="header-section-number">4.7</span> 完整数值示例：MLM的工作过程</a></li>
  <li><a href="#微调一个模型多种任务" id="toc-微调一个模型多种任务" class="nav-link" data-scroll-target="#微调一个模型多种任务"><span class="header-section-number">4.8</span> 微调：一个模型，多种任务</a></li>
  <li><a href="#复杂度分析" id="toc-复杂度分析" class="nav-link" data-scroll-target="#复杂度分析"><span class="header-section-number">4.9</span> 复杂度分析</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#使用hugging-face微调bert" id="toc-使用hugging-face微调bert" class="nav-link" data-scroll-target="#使用hugging-face微调bert"><span class="header-section-number">5.1</span> 使用Hugging Face微调BERT</a></li>
  <li><a href="#从零实现bert的mlm预训练头" id="toc-从零实现bert的mlm预训练头" class="nav-link" data-scroll-target="#从零实现bert的mlm预训练头"><span class="header-section-number">5.2</span> 从零实现BERT的MLM预训练头</a></li>
  <li><a href="#复现论文的关键细节" id="toc-复现论文的关键细节" class="nav-link" data-scroll-target="#复现论文的关键细节"><span class="header-section-number">5.3</span> 复现论文的关键细节</a></li>
  <li><a href="#实验结果" id="toc-实验结果" class="nav-link" data-scroll-target="#实验结果"><span class="header-section-number">5.4</span> 实验结果</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么双向比单向更好消融实验的证据" id="toc-为什么双向比单向更好消融实验的证据" class="nav-link" data-scroll-target="#为什么双向比单向更好消融实验的证据"><span class="header-section-number">6.1</span> 为什么双向比单向更好？——消融实验的证据</a></li>
  <li><a href="#nsp的争议它真的有用吗" id="toc-nsp的争议它真的有用吗" class="nav-link" data-scroll-target="#nsp的争议它真的有用吗"><span class="header-section-number">6.2</span> NSP的争议：它真的有用吗？</a></li>
  <li><a href="#模型规模的影响" id="toc-模型规模的影响" class="nav-link" data-scroll-target="#模型规模的影响"><span class="header-section-number">6.3</span> 模型规模的影响</a></li>
  <li><a href="#微调-vs-特征提取" id="toc-微调-vs-特征提取" class="nav-link" data-scroll-target="#微调-vs-特征提取"><span class="header-section-number">6.4</span> 微调 vs 特征提取</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.5</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题2018-2019年视角" id="toc-开放研究问题2018-2019年视角" class="nav-link" data-scroll-target="#开放研究问题2018-2019年视角"><span class="header-section-number">6.6</span> 开放研究问题（2018-2019年视角）</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#mlm的内在效率问题" id="toc-mlm的内在效率问题" class="nav-link" data-scroll-target="#mlm的内在效率问题"><span class="header-section-number">7.1</span> MLM的内在效率问题</a></li>
  <li><a href="#预训练-微调不一致" id="toc-预训练-微调不一致" class="nav-link" data-scroll-target="#预训练-微调不一致"><span class="header-section-number">7.2</span> 预训练-微调不一致</a></li>
  <li><a href="#不适合文本生成" id="toc-不适合文本生成" class="nav-link" data-scroll-target="#不适合文本生成"><span class="header-section-number">7.3</span> 不适合文本生成</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.4</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#前驱工作" id="toc-前驱工作" class="nav-link" data-scroll-target="#前驱工作"><span class="header-section-number">9.2</span> 前驱工作</a></li>
  <li><a href="#后续改进" id="toc-后续改进" class="nav-link" data-scroll-target="#后续改进"><span class="header-section-number">9.3</span> 后续改进</a></li>
  <li><a href="#挑战质疑" id="toc-挑战质疑" class="nav-link" data-scroll-target="#挑战质疑"><span class="header-section-number">9.4</span> 挑战/质疑</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">9.5</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.6</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第13章：BERT——双向预训练路线</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Pre-Training of Deep Bidirectional Transformers for Language Understanding</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Pre-training</div>
    <div class="quarto-category">BERT</div>
    <div class="quarto-category">Transformer</div>
  </div>
  </div>

<div>
  <div class="description">
    BERT：用掩码语言模型（MLM）实现真正的双向预训练。通过随机遮蔽输入token并让模型根据完整的左右上下文来预测被遮蔽的词，BERT在11个NLP基准任务上全面超越了GPT和ELMo，开启了预训练模型的’BERT时代’。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何让预训练模型在每个位置同时利用左右两侧的上下文，获得真正的双向表示？</p>
<p><strong>历史坐标</strong>：2018年10月 | Devlin et al.&nbsp;“BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding” | Google AI Language</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Devlin et al.&nbsp;(2019)</strong> “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding” (arXiv:1810.04805) — 参考了 Section 3（BERT模型设计）、Section 4（实验）、Section 5（消融实验）；<strong>从论文PDF提取了4张原图</strong>：Figure 1（预训练与微调流程）、Figure 2（输入表示）、Figure 3（BERT/GPT/ELMo架构对比）、Figure 4（四类微调任务）；改编了2个算法伪代码框（MLM 80-10-10策略、预训练样本构造流程）</li>
<li><strong>Radford et al.&nbsp;(2018)</strong> “Improving Language Understanding by Generative Pre-Training” (GPT) — 参考了与BERT的架构对比</li>
<li><strong>Liu et al.&nbsp;(2019)</strong> “RoBERTa: A Robustly Optimized BERT Pretraining Approach” (arXiv:1907.11692) — 参考了对NSP的消融分析</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Section 15.8-15.10 — 参考了教学组织方式和代码实现思路</li>
<li><strong>SLP3</strong> Chapter 11 — 参考了BERT在NLP任务中的定位和讲解框架</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 9 (2025) “Pretraining” — 参考了BERT预训练目标的讲解角度</li>
<li><strong>Princeton COS 484</strong> (Danqi Chen) — 参考了BERT作者之一对预训练设计决策的解释</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们详细介绍了GPT——第一个将Transformer与全模型微调相结合的预训练方案。GPT用Transformer Decoder做自回归语言建模，然后在下游任务上微调整个模型参数，确立了”预训练 + 微调”的现代NLP范式。在12个基准任务中的9个上，GPT达到了当时的最佳水平，特别是在常识推理（+8.9%）和语言可接受性（+10.4%）上的提升令人印象深刻。</p>
<p>然而，上一章结尾我们也揭示了GPT的一个根本限制：<strong>单向注意力</strong>。</p>
<p>因为GPT使用因果掩码（causal mask），每个位置只能看到它左侧的token。回忆我们在上一章中构造的数值例子——在处理”The cat sat”时，“The”的注意力权重是<span class="math inline">\([1.00, 0.00, 0.00]\)</span>，它只能关注自己；“cat”的权重是<span class="math inline">\([0.35, 0.65, 0.00]\)</span>，能看到”The”和自己，但完全看不到”sat”。这种单向限制对于语言生成来说是自然的——你不能在写下一个词的时候”偷看”未来——但对于语言理解来说却是一个严重的障碍。</p>
<p>考虑句子”I went to the bank to deposit my check”。当模型处理”bank”这个词时，如果只能看到左侧的”I went to the”，它无法确定”bank”指的是金融机构还是河岸。只有看到右侧的”to deposit my check”，正确的语义才能被锁定。GPT的单向注意力让模型在处理”bank”时对右侧的关键线索一无所知。</p>
<p>ELMo试图解决这个问题，但它的方案是”分离式双向”——两个独立的单向LSTM分别从左到右和从右到左处理序列，然后将表示拼接在一起。这种拼接并不等于真正的双向：前向LSTM在编码”bank”时只看到了左侧上下文，后向LSTM在编码”bank”时只看到了右侧上下文，两者从未在模型内部进行深度交互。</p>
<p>2018年10月，也就是GPT发表仅4个月之后，Google AI Language团队的Jacob Devlin等人提出了一个优雅的解决方案。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：用<strong>掩码语言模型（Masked Language Model, MLM）</strong>替代因果语言建模，让Transformer Encoder在预训练时真正融合双向上下文——不是拼接两个单向模型的输出，而是在同一个深层网络的每一层都让信息双向流动。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="理解任务为什么需要双向上下文" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="理解任务为什么需要双向上下文"><span class="header-section-number">2.1</span> 理解任务为什么需要双向上下文？</h3>
<p>在深入BERT的技术细节之前，让我们先理解一个更基本的问题：为什么语言理解任务天然地需要双向信息？</p>
<p>自然语言的语义解析往往需要”全局视野”。一个词的含义不仅取决于它前面的内容，也取决于它后面的内容。语言学中有一个经典概念叫做<strong>消歧（disambiguation）</strong>——同一个词形在不同上下文中可能表达完全不同的语义。英语中约有7%的常用词是多义词，而在实际文本中，多义词出现的频率远高于7%，因为高频词往往有更多的义项。</p>
<p>更深层地说，语言理解不仅需要”逐词理解”，还需要”关系推理”。在自然语言推理（NLI）任务中，模型需要判断”A man is sleeping”是否可以从”A man is resting on a couch”推导出来。这需要”sleeping”和”resting”之间建立语义对应，“a man”在两个句子中建立共指关系。这些关系的建立需要模型在同一时刻看到两个句子的完整内容，并在它们之间建立交叉注意力。</p>
<p>那么，为什么不能简单地使用双向Transformer Encoder来做语言建模呢？这里有一个看似简单却非常深刻的技术障碍。</p>
</section>
<section id="双向性与语言建模的矛盾" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="双向性与语言建模的矛盾"><span class="header-section-number">2.2</span> 双向性与语言建模的矛盾</h3>
<p>标准的语言建模目标是给定前文预测下一个词：<span class="math inline">\(P(w_t \mid w_1, \ldots, w_{t-1})\)</span>。GPT使用因果掩码来实现这一点——每个位置只能看到左侧的token，因此预测<span class="math inline">\(w_t\)</span>时不会”偷看”到<span class="math inline">\(w_t\)</span>本身或它之后的内容。</p>
<p>但如果我们使用双向Transformer Encoder（没有因果掩码），每个位置可以看到所有其他位置——包括要预测的位置本身。在这种情况下，让模型”预测下一个词”变得毫无意义：模型可以直接通过注意力机制”看到”目标词，然后原样复制输出。这不是学习语言，而是作弊。</p>
<p>ELMo用一种保守的方式回避了这个矛盾：训练两个独立的单向模型，确保每个方向都不会泄漏信息，最后拼接。但这种方案的代价是两个方向的信息永远无法在模型内部深度融合。</p>
<p>GPT选择了另一种保守的方式：接受单向的限制，换取架构的简洁性和生成的能力。</p>
<p>那么，有没有一种方法可以<strong>既使用双向Transformer Encoder，又避免信息泄漏</strong>？</p>
<p>BERT的回答是：<strong>不要预测下一个词——预测被遮蔽的词</strong>。</p>
</section>
<section id="从预测下一个到填空" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="从预测下一个到填空"><span class="header-section-number">2.3</span> 从”预测下一个”到”填空”</h3>
<p>BERT的核心洞察可以用一个直觉来理解：把预训练任务从”续写作文”变成”完形填空”。</p>
<p>传统的语言建模就像写作文——给你一个开头”Once upon a time”，让你续写下一个词。续写天然地是从左到右的过程，因此模型只能看到左侧的上下文。</p>
<p>BERT的掩码语言模型则像完形填空——给你一个完整的句子”I went to the ___ to deposit my check”，让你猜被遮蔽的词。填空不要求特定的方向——你自然地会同时参考左侧的”I went to the”和右侧的”to deposit my check”来确定答案是”bank”。</p>
<p>这个简单的变换解决了双向性与语言建模之间的矛盾：被遮蔽的词不在输入中（或被替换为特殊标记），所以模型无法”作弊”直接复制；同时，所有未被遮蔽的位置都可以看到彼此，信息可以双向自由流动。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="关键洞察完形填空式的预训练" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="关键洞察完形填空式的预训练"><span class="header-section-number">3.1</span> 关键洞察：完形填空式的预训练</h3>
<p>BERT的全名是 <strong>Bidirectional Encoder Representations from Transformers</strong>——双向Transformer编码器表示。名字本身就在强调”双向”（Bidirectional），因为这正是它相对于GPT的核心创新。</p>
<p>如果把GPT比作一个只能从左往右阅读的读者，那么BERT就像一个可以纵览全文的读者。GPT在处理每个词时就像戴着一副”遮光眼镜”——只能看到左边，右边一片漆黑。BERT则摘掉了这副眼镜，让每个词都能同时感知左右两侧的完整上下文。</p>
<p>但BERT面临的挑战是：如果读者能看到全文，那让他”猜下一个词”就没有意义了（答案就在眼前）。BERT的解决方案极其巧妙——在全文中随机”涂掉”一些词，然后让读者根据剩余的内容来猜测被涂掉的是什么。这就是掩码语言模型（MLM）的本质。</p>
</section>
<section id="两个预训练目标" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="两个预训练目标"><span class="header-section-number">3.2</span> 两个预训练目标</h3>
<p>BERT使用两个预训练目标联合训练。</p>
<p><strong>目标一：掩码语言模型（Masked Language Model, MLM）</strong>。随机选择输入中15%的token进行”遮蔽”，然后让模型预测被遮蔽的原始token。这迫使模型学习深层的双向语言理解能力——要准确填空，模型必须同时理解左右两侧的上下文，以及词汇之间的语法和语义关系。</p>
<p><strong>目标二：下一句预测（Next Sentence Prediction, NSP）</strong>。给定两个句子A和B，模型判断B是否是A在原文中的真实下一句。这个目标旨在让模型学习句子级别的关系理解——许多重要的NLP任务（如问答、自然语言推理）需要理解两个句子之间的关系。</p>
</section>
<section id="架构选择为什么是transformer-encoder" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="架构选择为什么是transformer-encoder"><span class="header-section-number">3.3</span> 架构选择：为什么是Transformer Encoder？</h3>
<p>GPT选择了Transformer Decoder（因果注意力），BERT选择了Transformer Encoder（双向注意力）。这个选择背后有清晰的逻辑。</p>
<p>GPT的目标是因果语言建模——预测下一个词，因此需要因果掩码来防止信息泄漏。但BERT的目标是MLM——预测被遮蔽的词，遮蔽本身已经防止了信息泄漏（被遮蔽的词不在输入中），因此不需要因果掩码。没有了因果掩码的约束，Transformer Encoder的每个位置都可以自由地关注所有其他位置，实现真正的双向表示学习。</p>
<p>换句话说，GPT和BERT在”如何防止作弊”这个问题上选择了不同的策略。GPT通过<strong>限制视野</strong>来防止作弊——你看不到答案所在的位置。BERT通过<strong>隐藏答案</strong>来防止作弊——答案所在的位置被替换成了一个无意义的<code>[MASK]</code>标记，即使你能看到那个位置，也得不到任何有用的信息。</p>
<p>下图直观地展示了三种预训练架构的核心差异。BERT的双向Transformer让每个位置都能关注所有其他位置（箭头交叉连接）；OpenAI GPT的单向Transformer只允许从左到右的信息流动（箭头单向连接）；ELMo则使用两个独立的LSTM分别处理前向和后向，最后拼接表示。</p>
<div id="fig-architecture-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-13/original/fig3-architecture-comparison.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 三种预训练模型架构的对比。BERT使用双向Transformer，每个位置可以关注所有其他位置；OpenAI GPT使用单向Transformer（从左到右）；ELMo使用独立训练的前向和后向LSTM，最后拼接。三者中，只有BERT在模型内部实现了真正的深层双向交互。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Devlin et al.&nbsp;(2019) “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, Figure 3. <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></em></p>
</div>
<div id="fig-bert-pretraining" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-13/original/fig1-pretraining-finetuning.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: BERT的预训练与微调流程。左侧展示了预训练阶段的两个目标（NSP和Mask LM），右侧展示了微调阶段如何将同一模型适配到不同的下游任务（MNLI、NER、SQuAD等）。注意预训练和微调使用完全相同的架构，只是输出层不同。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Devlin et al.&nbsp;(2019) “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, Figure 1. <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></em></p>
</div>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="模型架构" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="模型架构"><span class="header-section-number">4.1</span> 模型架构</h3>
<p>BERT的架构是标准的Transformer Encoder——与第8章介绍的Transformer编码器完全一致。论文提供了两个规模的模型：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>超参数</th>
<th>BERT-Base</th>
<th>BERT-Large</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>层数（<span class="math inline">\(L\)</span>）</td>
<td>12</td>
<td>24</td>
</tr>
<tr class="even">
<td>隐藏维度（<span class="math inline">\(H\)</span>）</td>
<td>768</td>
<td>1024</td>
</tr>
<tr class="odd">
<td>注意力头数（<span class="math inline">\(A\)</span>）</td>
<td>12</td>
<td>16</td>
</tr>
<tr class="even">
<td>每头维度</td>
<td>64</td>
<td>64</td>
</tr>
<tr class="odd">
<td>FFN内部维度</td>
<td>3072</td>
<td>4096</td>
</tr>
<tr class="even">
<td>总参数量</td>
<td>110M</td>
<td>340M</td>
</tr>
</tbody>
</table>
<p>BERT-Base的参数量与GPT-1（117M）几乎相同，这是有意为之的——Devlin等人想要在公平的条件下对比双向Encoder与单向Decoder的优劣。BERT-Large则将参数量扩大到340M，探索更大规模带来的增益。</p>
<p>值得注意的是BERT使用了<strong>可学习的位置嵌入</strong>，最大序列长度为512 token，这与GPT的选择一致。分词器使用的是<strong>WordPiece</strong>（第3章讨论的子词分词方法之一），词汇表大小为30,522个token。</p>
</section>
<section id="输入表示" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="输入表示"><span class="header-section-number">4.2</span> 输入表示</h3>
<p>BERT的输入表示设计是它区别于GPT的一个重要方面，因为BERT需要处理<strong>单句和句对</strong>两种输入格式。</p>
<p>每个输入序列的第一个token总是特殊标记<code>[CLS]</code>（classification的缩写）。如果输入是一个句对，两个句子用特殊标记<code>[SEP]</code>分隔。最终的输入嵌入是三种嵌入的逐元素相加：</p>
<p><span class="math display">\[
\text{Input} = \text{Token Embedding} + \text{Segment Embedding} + \text{Position Embedding}
\]</span></p>
<p><strong>Token Embedding</strong> 是标准的WordPiece嵌入，将每个token映射到一个<span class="math inline">\(H\)</span>维的向量。</p>
<p><strong>Segment Embedding</strong> 用来区分两个句子。属于句子A的所有token共享一个可学习的嵌入<span class="math inline">\(E_A\)</span>，属于句子B的所有token共享另一个可学习的嵌入<span class="math inline">\(E_B\)</span>。如果输入只有一个句子，则所有token都使用<span class="math inline">\(E_A\)</span>。这种设计让模型可以从嵌入层面区分”这个词属于哪个句子”。</p>
<p><strong>Position Embedding</strong> 是可学习的位置嵌入，与GPT相同，最大支持512个位置。</p>
<p>以一个具体的例子来说明。假设输入是句对”I love NLP”和”It is fun”，经过WordPiece分词后，完整的输入序列是：</p>
<p><span class="math display">\[
[\text{CLS}] \;\; \text{I} \;\; \text{love} \;\; \text{NLP} \;\; [\text{SEP}] \;\; \text{It} \;\; \text{is} \;\; \text{fun} \;\; [\text{SEP}]
\]</span></p>
<p>对应的Segment ID是：</p>
<p><span class="math display">\[
A \;\; A \;\; A \;\; A \;\; A \;\; B \;\; B \;\; B \;\; B
\]</span></p>
<div id="fig-bert-input" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-input-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-13/original/fig2-input-representation.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-input-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: BERT的输入表示：由Token Embedding、Segment Embedding和Position Embedding三者逐元素相加而成。示例句对”my dog is cute”（句子A）和”he likes play ##ing”（句子B）的完整输入构造过程。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Devlin et al.&nbsp;(2019) “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, Figure 2. <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></em></p>
</div>
<p><code>[CLS]</code>的设计值得单独讨论。它始终位于序列的第一个位置，经过12层（或24层）双向注意力之后，<code>[CLS]</code>的最终表示<span class="math inline">\(h_{[\text{CLS}]}\)</span>已经融合了整个序列的信息——因为在每一层中，它都可以通过注意力机制”看到”所有其他token。因此，<span class="math inline">\(h_{[\text{CLS}]}\)</span>可以作为整个句子（或句对）的汇聚表示（pooled representation），直接用于句子级别的分类任务。这与GPT取序列最后一个token的表示来做分类是异曲同工的，但<code>[CLS]</code>的优势在于它从第一层开始就在”综合”全局信息，而不是像GPT那样只能利用单向累积的信息。</p>
</section>
<section id="预训练目标一掩码语言模型mlm" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="预训练目标一掩码语言模型mlm"><span class="header-section-number">4.3</span> 预训练目标一：掩码语言模型（MLM）</h3>
<p>MLM的核心思想很简单：随机选择输入中的一些token，将它们”遮蔽”掉，然后让模型预测被遮蔽的原始token。但细节中隐藏着许多精妙的设计决策。</p>
<p><strong>为什么遮蔽15%？</strong> Devlin等人选择遮蔽每个序列中15%的token。这个比例是一个权衡的结果。如果遮蔽太少（比如5%），每个序列能提供的训练信号就太少——模型需要处理很多次才能积累足够的监督信号，训练效率低下。如果遮蔽太多（比如50%），剩余的上下文就不够丰富，模型很难做出准确的预测——就像一道完形填空题如果挖掉一半的字，人类也很难填对。15%在”训练信号充足”和”上下文足够丰富”之间取得了一个经验上的平衡点。</p>
<p>这里有一个微妙但重要的效率问题。在标准的语言建模中（如GPT），模型对序列中的每一个位置都做预测（预测下一个词），因此每个token都贡献一个训练信号。但在MLM中，只有被遮蔽的15%的token贡献训练信号，其余85%的token不参与损失计算。这意味着<strong>BERT需要比GPT多看大约6-7倍的数据才能获得同等数量的训练信号</strong>。从训练效率的角度看，这是MLM相对于标准语言建模的一个内在代价。</p>
<p><strong>为什么不能全部替换为<code>[MASK]</code>？</strong> 一个直觉的实现方式是把所有被选中的token都替换成<code>[MASK]</code>标记。但这会造成一个严重的<strong>预训练-微调不一致</strong>问题：在预训练时，输入中充满了<code>[MASK]</code>标记；在微调时，输入是正常的自然文本，没有任何<code>[MASK]</code>。模型在预训练时学会了”看到<code>[MASK]</code>就预测原词”，但在微调时永远不会看到<code>[MASK]</code>，这让预训练学到的部分模式变得无用。</p>
<p><strong>80-10-10策略</strong>。为了缓解这个不一致，BERT对被选中的15%的token采用了三种不同的处理方式：</p>
<ul>
<li><strong>80%的概率</strong>：替换为<code>[MASK]</code>标记（标准遮蔽）</li>
<li><strong>10%的概率</strong>：替换为词汇表中的一个随机token</li>
<li><strong>10%的概率</strong>：保持不变</li>
</ul>
<p>这个设计的逻辑是什么？80%的<code>[MASK]</code>确保模型的主要训练信号来自于真正的”填空”——模型看到一个<code>[MASK]</code>，需要根据上下文推断原词。10%的随机替换迫使模型不能完全信任输入——即使看到一个正常的词，它也可能是错误的替换，模型需要有能力判断”这个词在上下文中是否合理”。10%的保持不变则向模型展示了正常输入的样子——这个词没有被动过，它的表示应该尽量接近真实含义。后两种策略共同缓解了预训练与微调之间的分布差异。</p>
<p>你可能会问：随机替换不会”污染”模型的表示学习吗？考虑到随机替换的概率只有<span class="math inline">\(15\% \times 10\% = 1.5\%\)</span>，平均每100个token中只有1.5个会被替换为随机词，这个噪声水平足够低，不会对模型的语言能力造成实质性的损害。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: Masked Language Model — Input Construction (Devlin et al., 2019)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入: token序列 x = (x_1, x_2, ..., x_n), 遮蔽比例 p = 0.15
输出: 修改后的输入 x̃, 目标标签 y

1. 随机选择 ⌊n × p⌋ 个位置，构成遮蔽集合 M
2. FOR each position i ∈ M:
3.     采样随机数 r ~ Uniform(0, 1)
4.     IF r &lt; 0.8:
5.         x̃_i ← [MASK]          # 80%: 替换为 [MASK] 标记
6.     ELSE IF r &lt; 0.9:
7.         x̃_i ← random(V)       # 10%: 替换为词汇表中的随机token
8.     ELSE:
9.         x̃_i ← x_i             # 10%: 保持原始token不变
10.    y_i ← x_i                  # 目标始终是原始token
11. FOR each position i ∉ M:
12.    x̃_i ← x_i                 # 未选中的位置保持不变
13.    y_i ← IGNORE               # 不参与损失计算
14. RETURN x̃, y</code></pre>
<p><em>改编自: Devlin et al.&nbsp;(2019) “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, Section 3.1 及 Appendix C.2. <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></em></p>
</div>
</div>
<p>MLM的损失函数可以形式化为：</p>
<p><span class="math display">\[
L_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log P(x_i \mid \tilde{\mathbf{x}}; \theta)
\]</span></p>
<p>其中<span class="math inline">\(\mathcal{M}\)</span>是被遮蔽的token位置集合，<span class="math inline">\(\tilde{\mathbf{x}}\)</span>是经过80-10-10处理后的输入序列，<span class="math inline">\(x_i\)</span>是位置<span class="math inline">\(i\)</span>的原始token。只有被选中的15%的位置参与损失计算。</p>
</section>
<section id="预训练目标二下一句预测nsp" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="预训练目标二下一句预测nsp"><span class="header-section-number">4.4</span> 预训练目标二：下一句预测（NSP）</h3>
<p>许多重要的NLP任务（如问答、自然语言推理）需要理解两个句子之间的关系。标准的语言建模——无论是GPT的因果LM还是BERT的MLM——都主要关注token级别的语言理解，缺乏显式的句子间关系建模。NSP正是为了填补这个空白。</p>
<p>NSP的设计非常简单：在构造预训练数据时，对于每个训练样本，50%的概率选择真实的连续句对（标签为<code>IsNext</code>），50%的概率将第二个句子替换为语料库中随机抽取的句子（标签为<code>NotNext</code>）。然后让模型基于<code>[CLS]</code>的最终表示，做二分类预测。</p>
<p><span class="math display">\[
P(\text{IsNext} \mid h_{[\text{CLS}]}) = \text{softmax}(h_{[\text{CLS}]} \cdot W_{\text{NSP}})
\]</span></p>
<p>NSP的总损失为：</p>
<p><span class="math display">\[
L_{\text{NSP}} = -\left[y \cdot \log P(\text{IsNext}) + (1 - y) \cdot \log P(\text{NotNext})\right]
\]</span></p>
<p>BERT的总预训练损失是两个目标的简单求和：</p>
<p><span class="math display">\[
L = L_{\text{MLM}} + L_{\text{NSP}}
\]</span></p>
<p>然而，NSP后来成为BERT设计中争议最大的部分。RoBERTa（Liu et al., 2019）通过系统的消融实验发现，<strong>去掉NSP反而能提高模型的性能</strong>。这个反直觉的结果在学术界引起了广泛讨论，我们将在”深入理解”一节中详细分析。</p>
<p>下面的算法将MLM和NSP两个目标的数据构造过程整合在一起，展示了一个完整的BERT预训练样本是如何从原始语料库中生成的。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 2: BERT Pre-training Sample Construction (Devlin et al., 2019)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入: 语料库 D（按文档组织），最大序列长度 T = 512
输出: 预训练样本 (x̃, segment_ids, is_next, mlm_targets)

# —— Step 1: 构造句对（用于 NSP）——
1. 从语料库中随机选择文档 d，从中选择连续句子 A
2. 采样随机数 r ~ Uniform(0, 1)
3. IF r &lt; 0.5:
4.     B ← A 在文档 d 中的真实下一句           # IsNext
5.     is_next ← True
6. ELSE:
7.     B ← 从语料库中随机抽取的句子             # NotNext
8.     is_next ← False
9. 截断使 len(A) + len(B) ≤ T - 3             # 为 [CLS], [SEP], [SEP] 预留

# —— Step 2: 构造输入序列 ——
10. tokens    ← [CLS] + A + [SEP] + B + [SEP]
11. segments  ← [A...A] (len(A)+2个) + [B...B] (len(B)+1个)
12. positions ← [0, 1, 2, ..., len(tokens)-1]

# —— Step 3: 应用 MLM 遮蔽（Algorithm 1）——
13. x̃, mlm_targets ← MLM_MASK(tokens, p=0.15)  # 调用 Algorithm 1

14. RETURN (x̃, segments, positions, is_next, mlm_targets)</code></pre>
<p><em>改编自: Devlin et al.&nbsp;(2019) “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, Section 3.1, 3.2 及 Appendix A.2. <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></em></p>
</div>
</div>
</section>
<section id="预训练数据" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="预训练数据"><span class="header-section-number">4.5</span> 预训练数据</h3>
<p>BERT在两个大规模语料库上预训练：</p>
<p><strong>BooksCorpus</strong>（800M词）——与GPT-1使用的相同数据源，包含超过7000本未出版书籍的连续文本。</p>
<p><strong>英文Wikipedia</strong>（2,500M词）——只提取正文文本，去掉了列表、表格和标题。</p>
<p>两个数据源合计约33亿词（3.3B words），比GPT-1的8亿词大了约4倍。这个数据量的差异是BERT性能优势的一个贡献因素，尽管不是主要因素——后来的消融实验表明，双向性本身带来的提升远大于数据量增加带来的提升。</p>
<p>训练使用256个序列的batch size，每个序列最长512 token，总共训练1,000,000步（约40个epoch）。这在当时需要4个Cloud TPU（BERT-Base）或16个Cloud TPU（BERT-Large）训练4天。</p>
</section>
<section id="关键设计决策" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="关键设计决策"><span class="header-section-number">4.6</span> 关键设计决策</h3>
<p>让我们系统地审视BERT的几个关键设计决策，理解每个选择背后的权衡。</p>
<section id="决策1encoder-only-而非-decoder-only-或-encoder-decoder" class="level4" data-number="4.6.1">
<h4 data-number="4.6.1" class="anchored" data-anchor-id="决策1encoder-only-而非-decoder-only-或-encoder-decoder"><span class="header-section-number">4.6.1</span> 决策1：Encoder-only 而非 Decoder-only 或 Encoder-Decoder</h4>
<p><strong>决策</strong>：BERT只使用Transformer Encoder，没有Decoder。</p>
<p><strong>原因</strong>：BERT的目标是学习通用的语言理解表示，而不是进行序列生成。Encoder的双向注意力天然适合理解任务——每个位置可以综合全局信息形成表示。Decoder的因果注意力限制了信息流动，主要适用于生成任务。</p>
<p><strong>替代方案</strong>：使用Encoder-Decoder结构（如后来的T5）可以同时处理理解和生成任务，但需要设计更复杂的预训练目标。使用Decoder-only（如GPT）则无法获得双向表示。</p>
<p><strong>影响</strong>：Encoder-only的选择让BERT在理解任务上获得了巨大优势，但同时也意味着BERT<strong>不适合直接用于文本生成</strong>。生成任务需要自回归地逐token产出，而Encoder的双向注意力在推理时无法处理”还没有生成的未来token”。这个限制后来促使研究者探索Encoder-Decoder（T5）和Decoder-only（GPT系列）架构。</p>
</section>
<section id="决策2wordpiece-而非-bpe" class="level4" data-number="4.6.2">
<h4 data-number="4.6.2" class="anchored" data-anchor-id="决策2wordpiece-而非-bpe"><span class="header-section-number">4.6.2</span> 决策2：WordPiece 而非 BPE</h4>
<p><strong>决策</strong>：BERT使用WordPiece分词，词汇表大小30,522。</p>
<p><strong>原因</strong>：WordPiece是Google内部广泛使用的子词分词方法。与BPE的贪心合并策略不同，WordPiece使用最大化训练数据似然的标准来选择合并。在实践中，两者的差异不大，但WordPiece在Google的工程基础设施中有更好的支持。</p>
<p><strong>影响</strong>：GPT使用BPE（<sub>40,000词），BERT使用WordPiece（</sub>30,000词）。词汇表大小的差异意味着同样的文本在BERT中可能被切分成更多的子词，序列长度更长。但30,000的词汇表在覆盖率和效率之间取得了较好的平衡。</p>
</section>
<section id="决策3为什么cls放在句首" class="level4" data-number="4.6.3">
<h4 data-number="4.6.3" class="anchored" data-anchor-id="决策3为什么cls放在句首"><span class="header-section-number">4.6.3</span> 决策3：为什么<code>[CLS]</code>放在句首？</h4>
<p><strong>决策</strong>：<code>[CLS]</code>始终作为序列的第一个token。</p>
<p><strong>原因</strong>：Devlin等人的设计意图是让<code>[CLS]</code>作为整个序列的”汇聚器”——经过多层双向注意力后，它融合了序列中所有位置的信息。你可能会问：为什么不放在句末（像GPT那样取最后一个token的表示）？在双向Encoder中，放在哪里其实差别不大——因为每个位置都能看到所有其他位置，理论上任何位置经过充分的注意力交互后都可以汇聚全局信息。选择句首可能只是一个工程上的便利，使得<code>[CLS]</code>的位置固定且容易索引。</p>
</section>
</section>
<section id="完整数值示例mlm的工作过程" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="完整数值示例mlm的工作过程"><span class="header-section-number">4.7</span> 完整数值示例：MLM的工作过程</h3>
<p>让我们通过一个具体的小规模例子来追踪BERT的MLM预训练如何工作。</p>
<p><strong>设定</strong>：句子”The cat sat on the mat”，<span class="math inline">\(H = 4\)</span>（隐藏维度），词汇表大小<span class="math inline">\(V = 10\)</span>（简化），单层单头注意力。</p>
<p><strong>Step 1: 选择遮蔽位置</strong></p>
<p>序列长度为6，遮蔽比例15%。<span class="math inline">\(6 \times 15\% \approx 1\)</span>，选择1个token进行遮蔽。假设随机选中了位置3的”sat”。</p>
<p><strong>Step 2: 应用80-10-10策略</strong></p>
<p>对于被选中的”sat”，按概率执行：</p>
<ul>
<li>80%概率：替换为<code>[MASK]</code> → 输入变为”The cat <code>[MASK]</code> on the mat”</li>
<li>10%概率：替换为随机词（比如”blue”）→ 输入变为”The cat blue on the mat”</li>
<li>10%概率：保持不变 → 输入保持”The cat sat on the mat”</li>
</ul>
<p>假设这次落入80%的分支，输入变为 <span class="math inline">\([\text{CLS}], \text{The}, \text{cat}, [\text{MASK}], \text{on}, \text{the}, \text{mat}, [\text{SEP}]\)</span>。</p>
<p><strong>Step 3: 嵌入</strong></p>
<p>假设Token + Position嵌入后得到（<span class="math inline">\(H = 4\)</span>，只展示关键位置）：</p>
<p><span class="math display">\[
h_0 = \begin{bmatrix}
\text{[CLS]:} &amp; 0.1 &amp; 0.2 &amp; 0.1 &amp; 0.3 \\
\text{The:} &amp; 1.0 &amp; 0.5 &amp; 0.2 &amp; 0.1 \\
\text{cat:} &amp; 0.3 &amp; 1.2 &amp; 0.8 &amp; 0.4 \\
\text{[MASK]:} &amp; 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\
\text{on:} &amp; 0.5 &amp; 0.3 &amp; 1.0 &amp; 0.2 \\
\text{the:} &amp; 0.9 &amp; 0.5 &amp; 0.2 &amp; 0.1 \\
\text{mat:} &amp; 0.4 &amp; 0.8 &amp; 0.6 &amp; 1.1 \\
\text{[SEP]:} &amp; 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.2
\end{bmatrix}
\]</span></p>
<p>注意<code>[MASK]</code>的初始嵌入接近零——它是一个特殊标记，没有携带原词”sat”的任何信息。</p>
<p><strong>Step 4: 双向注意力（关键差异！）</strong></p>
<p>与GPT不同，BERT<strong>没有因果掩码</strong>。注意力分数矩阵没有<span class="math inline">\(-\infty\)</span>的上三角遮蔽——每个位置可以自由地关注所有其他位置。</p>
<p><span class="math display">\[
\text{Attention Mask} = \begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\
\vdots &amp; &amp; &amp; &amp; &amp; &amp; &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>全1矩阵——完全双向。对比上一章GPT的下三角矩阵，差异一目了然。</p>
<p>这意味着在处理<code>[MASK]</code>位置（原来的”sat”）时，注意力可以<strong>同时</strong>关注左侧的”The cat”和右侧的”on the mat”。假设经过注意力计算后，<code>[MASK]</code>位置的注意力权重为：</p>
<p><span class="math display">\[
\alpha_{\text{[MASK]}} = [0.02, 0.08, 0.25, 0.05, 0.18, 0.07, 0.30, 0.05]
\]</span></p>
<p><strong>解读</strong>：模型在预测<code>[MASK]</code>（原词”sat”）时，最关注的是”mat”（0.30）和”cat”（0.25），其次是”on”（0.18）。这非常合理——“cat”提供了主语信息（谁做了这个动作），“on the mat”提供了动作的目标（坐在什么上面），它们共同强烈暗示被遮蔽的词是某个表示位置关系的动词，如”sat”。</p>
<p><strong>Step 5: 预测被遮蔽的词</strong></p>
<p>经过12层（实际中）Transformer Encoder后，<code>[MASK]</code>位置的最终隐藏状态<span class="math inline">\(h_L^{\text{[MASK]}}\)</span>融合了来自所有方向的丰富信息。将其通过一个线性层映射到词汇表维度，然后softmax：</p>
<p><span class="math display">\[
\text{logits} = h_L^{\text{[MASK]}} \cdot W_{\text{vocab}}^T + b \quad \in \mathbb{R}^{V}
\]</span></p>
<p><span class="math display">\[
P(\cdot \mid \tilde{\mathbf{x}}) = \text{softmax}(\text{logits})
\]</span></p>
<p>假设得到的概率分布为：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>词</th>
<th>sat</th>
<th>ran</th>
<th>slept</th>
<th>stood</th>
<th>on</th>
<th>cat</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>概率</td>
<td><strong>0.42</strong></td>
<td>0.15</td>
<td>0.12</td>
<td>0.10</td>
<td>0.03</td>
<td>0.02</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>模型给”sat”分配了最高的概率0.42，说明它成功地从双向上下文中推断出了被遮蔽的词。损失为 <span class="math inline">\(-\log(0.42) \approx 0.87\)</span>。</p>
<p><strong>对比GPT</strong>：如果用GPT的因果注意力来预测位置3的词，模型只能看到”The cat”——仅凭这两个词，“sat”、“ran”、“slept”、“ate”都是合理的续写。模型无法利用右侧的”on the mat”来缩小候选范围。BERT的双向注意力让”on the mat”直接参与了预测，显著提高了预测精度。</p>
</section>
<section id="微调一个模型多种任务" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="微调一个模型多种任务"><span class="header-section-number">4.8</span> 微调：一个模型，多种任务</h3>
<p>BERT的微调方式延续了GPT的思路——在预训练模型顶部添加一个简单的任务特定层，然后微调整个模型的参数。但得益于双向Encoder的强大表示能力和灵活的输入格式（<code>[CLS]</code> + 句对 + <code>[SEP]</code>），BERT的微调框架更加统一。</p>
<p><strong>句子级分类（如情感分析、语义相似度）</strong>。取<code>[CLS]</code>的最终表示<span class="math inline">\(h_{[\text{CLS}]} \in \mathbb{R}^H\)</span>，通过一个新增的线性层<span class="math inline">\(W \in \mathbb{R}^{K \times H}\)</span>映射到<span class="math inline">\(K\)</span>个类别：</p>
<p><span class="math display">\[
P(c \mid \mathbf{x}) = \text{softmax}(h_{[\text{CLS}]} \cdot W^T)
\]</span></p>
<p><strong>句对分类（如NLI）</strong>。输入格式为<code>[CLS] Sentence A [SEP] Sentence B [SEP]</code>，同样取<code>[CLS]</code>的表示做分类。Segment Embedding帮助模型区分两个句子。</p>
<p><strong>Token级分类（如命名实体识别）</strong>。每个token的最终表示<span class="math inline">\(h_i\)</span>分别通过一个共享的线性层做分类：</p>
<p><span class="math display">\[
P(t_i \mid \mathbf{x}) = \text{softmax}(h_i \cdot W_{\text{tag}}^T) \quad \forall i
\]</span></p>
<p><strong>抽取式问答（如SQuAD）</strong>。输入格式为<code>[CLS] Question [SEP] Passage [SEP]</code>。模型需要从Passage中找到答案的起始和结束位置。具体做法是学习两个向量<span class="math inline">\(S, E \in \mathbb{R}^H\)</span>，对Passage中每个位置<span class="math inline">\(i\)</span>，计算它作为答案起始/结束位置的概率：</p>
<p><span class="math display">\[
P_{\text{start}}(i) = \frac{e^{S \cdot h_i}}{\sum_j e^{S \cdot h_j}}, \quad P_{\text{end}}(i) = \frac{e^{E \cdot h_i}}{\sum_j e^{E \cdot h_j}}
\]</span></p>
<p>最终答案是使得<span class="math inline">\(P_{\text{start}}(i) \times P_{\text{end}}(j)\)</span>最大的合法跨度<span class="math inline">\((i, j)\)</span>（要求<span class="math inline">\(j \geq i\)</span>）。</p>
<p>微调时新增的参数量非常少——分类任务只需要一个<span class="math inline">\(K \times H\)</span>的线性层（对于BERT-Base和二分类任务，只有<span class="math inline">\(2 \times 768 = 1536\)</span>个新参数），问答任务只需要两个<span class="math inline">\(H\)</span>维的向量（<span class="math inline">\(2 \times 768 = 1536\)</span>个新参数）。与预训练模型的1.1亿参数相比，微调新增的参数几乎可以忽略不计。</p>
<div id="fig-bert-finetune" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-finetune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-13/original/fig4-finetuning-tasks.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-finetune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: BERT在四类下游任务上的微调示意。(a) 句对分类（MNLI、QQP、QNLI等）：取<code>[CLS]</code>表示做分类；(b) 单句分类（SST-2、CoLA）：同样取<code>[CLS]</code>；(c) 抽取式问答（SQuAD）：预测答案的起始和结束位置；(d) 序列标注（CoNLL NER）：每个token独立分类。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Devlin et al.&nbsp;(2019) “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, Figure 4. <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></em></p>
</div>
</section>
<section id="复杂度分析" class="level3" data-number="4.9">
<h3 data-number="4.9" class="anchored" data-anchor-id="复杂度分析"><span class="header-section-number">4.9</span> 复杂度分析</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 36%">
<col style="width: 23%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>BERT-Base</th>
<th>GPT-1</th>
<th>ELMo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>预训练参数</td>
<td>110M</td>
<td>117M</td>
<td>93.6M</td>
</tr>
<tr class="even">
<td>预训练数据</td>
<td>3.3B词</td>
<td>0.8B词</td>
<td>1B词</td>
</tr>
<tr class="odd">
<td>预训练方向</td>
<td>双向</td>
<td>单向（左→右）</td>
<td>分离式双向</td>
</tr>
<tr class="even">
<td>预训练信号效率</td>
<td>15%（仅<code>[MASK]</code>位置）</td>
<td>100%（每个位置）</td>
<td>100%（每个位置）</td>
</tr>
<tr class="odd">
<td>微调新增参数</td>
<td><span class="math inline">\(K \times H\)</span>（极少）</td>
<td><span class="math inline">\(K \times H\)</span></td>
<td><span class="math inline">\(L + 2\)</span>个标量</td>
</tr>
<tr class="even">
<td>推理复杂度</td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span></td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span></td>
<td><span class="math inline">\(O(n \cdot d^2)\)</span></td>
</tr>
</tbody>
</table>
<p>一个值得注意的权衡是预训练信号效率。GPT在每个位置都做预测（预测下一个词），每个token都贡献损失；BERT只在被遮蔽的15%的位置做预测。这意味着BERT的每个训练步提供的信号量只有GPT的约<span class="math inline">\(1/7\)</span>。BERT通过更大的数据集（3.3B vs 0.8B词）和更长的训练时间（1M步 vs ~100个epoch）来弥补这个效率差距。</p>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="使用hugging-face微调bert" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="使用hugging-face微调bert"><span class="header-section-number">5.1</span> 使用Hugging Face微调BERT</h3>
<p>下面展示在情感分析任务上微调BERT的标准工作流：</p>
<div id="a9a30213" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer, TrainingArguments</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的 BERT-Base 模型和分词器</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bert-base-uncased"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    num_labels<span class="op">=</span><span class="dv">2</span>  <span class="co"># 二分类：正面/负面</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备数据</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This movie is absolutely fantastic!"</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Terrible waste of time, awful acting."</span>,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A masterpiece of modern cinema."</span>,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I fell asleep halfway through."</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 正面=1, 负面=0</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>encodings <span class="op">=</span> tokenizer(</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    texts,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 Dataset</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentDataset(torch.utils.data.Dataset):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> {k: v[idx] <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        item[<span class="st">"labels"</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> SentimentDataset(encodings, labels)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="co"># 微调配置</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./bert-sentiment"</span>,</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,     <span class="co"># BERT 微调推荐: 2e-5 到 5e-5</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset,</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="从零实现bert的mlm预训练头" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="从零实现bert的mlm预训练头"><span class="header-section-number">5.2</span> 从零实现BERT的MLM预训练头</h3>
<p>为了深入理解MLM的工作机制，下面是MLM预训练头的简化实现：</p>
<div id="c24e0349" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BertMLMHead(nn.Module):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""BERT 的掩码语言模型预训练头"""</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size, vocab_size):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 与论文一致：先过一个全连接层 + GELU + LayerNorm</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gelu <span class="op">=</span> nn.GELU()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(hidden_size)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 映射到词汇表大小</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Linear(hidden_size, vocab_size)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states, masked_positions):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">            hidden_states: Encoder输出 [batch, seq_len, hidden]</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">            masked_positions: 被遮蔽的位置索引 [batch, num_masked]</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 只取被遮蔽位置的隐藏状态</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> hidden_states.size(<span class="dv">0</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        num_masked <span class="op">=</span> masked_positions.size(<span class="dv">1</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 提取被遮蔽位置的表示</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        masked_hidden <span class="op">=</span> torch.gather(</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            hidden_states, <span class="dv">1</span>,</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            masked_positions.unsqueeze(<span class="op">-</span><span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, hidden_states.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        )  <span class="co"># [batch, num_masked, hidden]</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 预测原始 token</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.dense(masked_hidden)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.gelu(h)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.layer_norm(h)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.decoder(h)  <span class="co"># [batch, num_masked, vocab_size]</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_mlm_batch(tokens, vocab_size, mask_token_id, mask_prob<span class="op">=</span><span class="fl">0.15</span>):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co">    创建 MLM 训练样本：实现 80-10-10 策略</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co">        tokens: 原始 token ids [batch, seq_len]</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_size: 词汇表大小</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="co">        mask_token_id: [MASK] 的 token id</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co">        mask_prob: 遮蔽概率（默认 15%）</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> tokens.clone()</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> tokens.clone()</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 随机选择 15% 的位置</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    probability_matrix <span class="op">=</span> torch.full(tokens.shape, mask_prob)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 特殊 token（[CLS], [SEP], [PAD]）不遮蔽</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    special_tokens_mask <span class="op">=</span> (tokens <span class="op">==</span> <span class="dv">0</span>) <span class="op">|</span> (tokens <span class="op">==</span> <span class="dv">101</span>) <span class="op">|</span> (tokens <span class="op">==</span> <span class="dv">102</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    probability_matrix.masked_fill_(special_tokens_mask, value<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    masked_indices <span class="op">=</span> torch.bernoulli(probability_matrix).<span class="bu">bool</span>()</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 未被遮蔽的位置不参与损失计算</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    labels[<span class="op">~</span>masked_indices] <span class="op">=</span> <span class="op">-</span><span class="dv">100</span>  <span class="co"># CrossEntropy 会忽略 -100</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 80-10-10 策略</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 80% 替换为 [MASK]</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    indices_replaced <span class="op">=</span> (</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        torch.bernoulli(torch.full(tokens.shape, <span class="fl">0.8</span>)).<span class="bu">bool</span>()</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;</span> masked_indices</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    input_ids[indices_replaced] <span class="op">=</span> mask_token_id</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 10% 替换为随机 token</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    indices_random <span class="op">=</span> (</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        torch.bernoulli(torch.full(tokens.shape, <span class="fl">0.5</span>)).<span class="bu">bool</span>()</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;</span> masked_indices</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;</span> <span class="op">~</span>indices_replaced</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>    random_words <span class="op">=</span> torch.randint(vocab_size, tokens.shape, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>    input_ids[indices_random] <span class="op">=</span> random_words[indices_random]</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 剩余 10% 保持不变（input_ids 已经是原始值）</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> input_ids, labels</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="复现论文的关键细节" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="复现论文的关键细节"><span class="header-section-number">5.3</span> 复现论文的关键细节</h3>
<p>如果你要复现BERT的原始实验，以下几个容易被忽略的细节值得注意。</p>
<p><strong>两阶段训练策略</strong>。BERT的预训练分为两个阶段：第一阶段使用序列长度128训练90%的步数（约900K步），第二阶段使用序列长度512训练剩余10%的步数（约100K步）。这个设计的原因是注意力计算的复杂度为<span class="math inline">\(O(n^2)\)</span>——用长度128训练大部分步数可以显著加速训练，然后用长度512训练少量步数让模型适应长序列。</p>
<p><strong>Whole Word Masking</strong>。原始BERT在子词（WordPiece token）级别做随机遮蔽，这意味着一个完整的词可能只有部分子词被遮蔽。例如”playing”可能被分为”play”和”##ing”两个子词，如果只遮蔽了”##ing”，模型很容易从”play”推断出答案。后来的改进版BERT采用了<strong>整词遮蔽（Whole Word Masking, WWM）</strong>——如果一个词的任何子词被选中，则该词的所有子词都被遮蔽。这迫使模型进行更深层的语义推理，而不是利用子词之间的拼写线索。</p>
<p><strong>微调超参数的敏感性</strong>。Devlin等人推荐的微调超参数范围是：学习率<span class="math inline">\(\{2 \times 10^{-5}, 3 \times 10^{-5}, 5 \times 10^{-5}\}\)</span>，batch size<span class="math inline">\(\{16, 32\}\)</span>，训练<span class="math inline">\(\{2, 3, 4\}\)</span>个epoch。他们报告说，在小数据集（如MRPC，3,668条训练样本）上，不同的超参数设置可能导致结果差异很大，因此建议进行网格搜索并使用开发集选择最佳配置。</p>
</section>
<section id="实验结果" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="实验结果"><span class="header-section-number">5.4</span> 实验结果</h3>
<p>BERT在发布时在11个NLP基准任务上取得了最佳成绩，全面超越了GPT和ELMo：</p>
<p><strong>GLUE基准</strong>（General Language Understanding Evaluation）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>任务</th>
<th>类型</th>
<th>GPT</th>
<th>BERT-Base</th>
<th>BERT-Large</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNLI</td>
<td>文本蕴含</td>
<td>82.1</td>
<td>84.6</td>
<td><strong>86.7</strong></td>
</tr>
<tr class="even">
<td>QQP</td>
<td>释义检测</td>
<td>70.3</td>
<td>71.2</td>
<td><strong>72.1</strong></td>
</tr>
<tr class="odd">
<td>QNLI</td>
<td>问答蕴含</td>
<td>88.1</td>
<td>90.5</td>
<td><strong>92.7</strong></td>
</tr>
<tr class="even">
<td>SST-2</td>
<td>情感分析</td>
<td>91.3</td>
<td>93.5</td>
<td><strong>94.9</strong></td>
</tr>
<tr class="odd">
<td>CoLA</td>
<td>语言可接受性</td>
<td>45.4</td>
<td>52.1</td>
<td><strong>60.5</strong></td>
</tr>
<tr class="even">
<td>STS-B</td>
<td>语义相似度</td>
<td>—</td>
<td>85.8</td>
<td><strong>86.5</strong></td>
</tr>
<tr class="odd">
<td>MRPC</td>
<td>释义检测</td>
<td>82.3</td>
<td>88.9</td>
<td><strong>89.3</strong></td>
</tr>
<tr class="even">
<td>RTE</td>
<td>文本蕴含</td>
<td>56.0</td>
<td>66.4</td>
<td><strong>70.1</strong></td>
</tr>
<tr class="odd">
<td><strong>平均</strong></td>
<td></td>
<td>72.8</td>
<td>79.6</td>
<td><strong>82.1</strong></td>
</tr>
</tbody>
</table>
<p><strong>SQuAD问答</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>任务</th>
<th>之前SOTA</th>
<th>BERT-Large</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SQuAD v1.1 (F1)</td>
<td>91.6</td>
<td><strong>93.2</strong></td>
<td>+1.6</td>
</tr>
<tr class="even">
<td>SQuAD v2.0 (F1)</td>
<td>66.3</td>
<td><strong>83.1</strong></td>
<td>+16.8</td>
</tr>
</tbody>
</table>
<p><strong>SWAG常识推理</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>指标</th>
<th>之前SOTA</th>
<th>BERT-Large</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>65.2</td>
<td><strong>86.3</strong></td>
<td>+21.1</td>
</tr>
</tbody>
</table>
<p>几个结果值得特别关注。首先，BERT-Base（110M参数）在几乎所有任务上都优于GPT-1（117M参数），而两者的参数量几乎相同。这清楚地表明，<strong>双向预训练（Encoder + MLM）相比单向预训练（Decoder + CLM）在理解任务上有本质优势</strong>。其次，BERT-Large的GLUE平均分比BERT-Base高出2.5个百分点，显示了增大模型规模带来的收益。最引人注目的是SQuAD v2.0上的巨大提升（+16.8%）——这个任务要求模型不仅能找到答案，还要判断问题是否无法回答，需要深层的阅读理解能力。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨BERT为什么有效、NSP的争议、消融实验的洞察、以及BERT在预训练技术谱系中的位置</p>
</blockquote>
<section id="为什么双向比单向更好消融实验的证据" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么双向比单向更好消融实验的证据"><span class="header-section-number">6.1</span> 为什么双向比单向更好？——消融实验的证据</h3>
<p>Devlin等人在论文中进行了一个关键的消融实验，直接对比了三种预训练策略对下游任务的影响：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>预训练方向</th>
<th>MNLI</th>
<th>MRPC</th>
<th>QNLI</th>
<th>SST-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-Base</td>
<td>双向（MLM）</td>
<td><strong>84.4</strong></td>
<td><strong>88.0</strong></td>
<td><strong>90.7</strong></td>
<td><strong>92.7</strong></td>
</tr>
<tr class="even">
<td>Left-to-Right (LTR)</td>
<td>单向（左→右）</td>
<td>82.3</td>
<td>84.7</td>
<td>87.7</td>
<td>91.6</td>
</tr>
<tr class="odd">
<td>LTR + BiLSTM</td>
<td>单向 + 双向LSTM头</td>
<td>82.1</td>
<td>85.8</td>
<td>86.9</td>
<td>91.6</td>
</tr>
</tbody>
</table>
<p>这个实验的设计非常精妙。“Left-to-Right”模型使用与BERT完全相同的架构（12层Transformer）和数据，但将MLM替换为标准的从左到右语言建模（与GPT类似）。“LTR + BiLSTM”在LTR模型的顶部加了一个双向LSTM层，试图在微调阶段引入一些双向性。</p>
<p>结果清楚地表明，双向预训练（MLM）在所有任务上都优于单向预训练（LTR），差距在MNLI上约2个百分点，在MRPC上约3个百分点，在QNLI上约3个百分点。更有意思的是，在LTR模型顶部加BiLSTM层<strong>并没有帮助</strong>——在QNLI上甚至更差了。这说明<strong>双向性需要在预训练阶段就建立起来，仅在微调阶段引入双向性是不够的</strong>。预训练时的单向表示已经”定型”了，微调阶段的浅层双向处理无法弥补这个根本缺陷。</p>
</section>
<section id="nsp的争议它真的有用吗" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="nsp的争议它真的有用吗"><span class="header-section-number">6.2</span> NSP的争议：它真的有用吗？</h3>
<p>BERT的NSP目标在发表时被认为对句子对任务（如NLI和QA）至关重要。Devlin等人的消融实验也支持了这一点——去掉NSP后，QNLI下降了3.5个百分点，MNLI下降了0.3个百分点。</p>
<p>然而，2019年的后续研究对NSP提出了越来越多的质疑。RoBERTa（Liu et al., 2019）进行了迄今为止最系统的消融分析，对比了四种设定：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>设定</th>
<th>描述</th>
<th>SQuAD</th>
<th>MNLI</th>
<th>SST-2</th>
<th>RACE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Segment-Pair + NSP</td>
<td>BERT原始设定</td>
<td>90.4</td>
<td>84.0</td>
<td>92.9</td>
<td>64.2</td>
</tr>
<tr class="even">
<td>Sentence-Pair + NSP</td>
<td>用真实句子（非段落）</td>
<td>84.7</td>
<td>82.9</td>
<td>92.6</td>
<td>63.0</td>
</tr>
<tr class="odd">
<td>Full-Sentences (无NSP)</td>
<td>连续文本，无NSP</td>
<td><strong>90.4</strong></td>
<td><strong>84.7</strong></td>
<td><strong>93.3</strong></td>
<td><strong>64.8</strong></td>
</tr>
<tr class="even">
<td>Doc-Sentences (无NSP)</td>
<td>单文档连续文本</td>
<td>90.6</td>
<td>84.7</td>
<td>92.7</td>
<td>65.6</td>
</tr>
</tbody>
</table>
<p>RoBERTa发现，<strong>去掉NSP并使用全长度的连续文本反而能提高性能</strong>。这个反直觉的结果可以从几个角度理解。</p>
<p>第一，NSP任务可能太”简单”了。在BERT的数据构造中，负样本（NotNext）是从不同文档中随机抽取的句子。来自不同文档的句子往往在主题和风格上有明显差异，模型可能只是学会了判断”这两个句子是否来自同一篇文章”（主题匹配），而不是真正理解句子间的逻辑关系。这种”捷径学习”提供的语义信号有限。</p>
<p>第二，NSP损坏了MLM的数据效率。为了构造句对，BERT必须将512 token的序列切分为两个较短的句子（通常远短于256 token）。这意味着每个训练样本中实际的文本长度减少了，MLM的学习效率降低。RoBERTa的”Full-Sentences”设定使用跨文档的连续文本填满512 token，最大化了每个样本中的MLM训练信号。</p>
<p>第三，句子级关系的学习可能不需要一个显式的预训练目标。双向Encoder在处理<code>[CLS] Sentence A [SEP] Sentence B [SEP]</code>格式的微调输入时，通过注意力机制自然地建立了句子间的交互——模型可以在微调阶段直接学习句子间关系，不需要预训练时的预热。</p>
<p>这个发现深刻地影响了后续的预训练模型设计。ALBERT用句子顺序预测（Sentence Order Prediction, SOP）替代了NSP，RoBERTa直接去掉了NSP，后来的大多数预训练模型都没有采用NSP。</p>
</section>
<section id="模型规模的影响" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="模型规模的影响"><span class="header-section-number">6.3</span> 模型规模的影响</h3>
<p>Devlin等人的消融实验也揭示了模型规模对性能的影响：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>层数</th>
<th>隐藏维度</th>
<th>参数量</th>
<th>MNLI</th>
<th>MRPC</th>
<th>SST-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-Base</td>
<td>12</td>
<td>768</td>
<td>110M</td>
<td>84.4</td>
<td>88.0</td>
<td>92.7</td>
</tr>
<tr class="even">
<td>BERT-Large</td>
<td>24</td>
<td>1024</td>
<td>340M</td>
<td><strong>86.6</strong></td>
<td><strong>89.2</strong></td>
<td><strong>93.2</strong></td>
</tr>
</tbody>
</table>
<p>从BERT-Base到BERT-Large，参数量增加了约3倍，MNLI提升了2.2个百分点，MRPC提升了1.2个百分点。这个结果表明，增大模型规模在”预训练 + 微调”范式下仍然有效，为后来的规模化探索（GPT-2、GPT-3、第17章的Scaling Laws）提供了早期证据。</p>
<p>一个有趣的细节是，BERT-Large在小数据集上的表现不稳定。Devlin等人报告，BERT-Large在某些小数据集上的微调需要更多的随机重启（random restarts）才能获得稳定的结果。这暗示了一个后来被广泛研究的问题：<strong>大模型在小数据集上更容易过拟合</strong>，微调的稳定性需要额外的技巧（如更小的学习率、更多的warmup步数、或使用参数高效微调方法如LoRA）。</p>
</section>
<section id="微调-vs-特征提取" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="微调-vs-特征提取"><span class="header-section-number">6.4</span> 微调 vs 特征提取</h3>
<p>Devlin等人还对比了BERT作为特征提取器（冻结参数）和微调模型的性能差异，以NER（命名实体识别）任务为例：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>微调（所有参数）</td>
<td><strong>96.4</strong></td>
</tr>
<tr class="even">
<td>特征提取：拼接最后4层</td>
<td>96.1</td>
</tr>
<tr class="odd">
<td>特征提取：加权求和最后4层</td>
<td>95.9</td>
</tr>
<tr class="even">
<td>特征提取：仅最后一层</td>
<td>94.9</td>
</tr>
<tr class="odd">
<td>特征提取：倒数第二层</td>
<td>95.6</td>
</tr>
</tbody>
</table>
<p>结果显示，微调优于特征提取，但差距不大（0.3个百分点）。特征提取方式中，拼接最后4层效果最好，接近微调的性能。这说明BERT的预训练表示已经非常丰富——即使不做微调，直接提取特征也能获得有竞争力的结果。</p>
<p>这个发现对实际应用有重要的工程意义。在某些场景下（比如需要同时运行多个下游任务，或者计算资源有限），使用BERT作为固定的特征提取器可以大幅降低计算成本（不需要为每个任务微调一份完整模型），代价只是一点点性能损失。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.5</span> 方法的边界条件</h3>
<p><strong>假设一：MLM的遮蔽比例是最优的</strong>。15%的遮蔽率是一个经验选择，没有严格的理论依据。后来的研究（如SpanBERT）发现，遮蔽连续的span可能比随机遮蔽单个token更有效，因为它迫使模型进行更高层次的推理。</p>
<p><strong>假设二：WordPiece分词足够好</strong>。BERT的语言能力受限于其分词器。对于数学表达式（如”12345”可能被切为”123”和”##45”）、代码、非英文文本等，WordPiece的切分方式可能不是最优的。后来的模型如GPT-2/3转向了Byte-level BPE，XLM-RoBERTa使用了SentencePiece，都在不同程度上解决了这些问题。</p>
<p><strong>假设三：512 token足够长</strong>。BERT的最大序列长度是512 token。对于长文档（如法律文件、学术论文），512 token只能覆盖很小的一部分。后来的Longformer、BigBird等模型通过稀疏注意力将序列长度扩展到4096甚至更长。</p>
<p><strong>失效条件</strong>。BERT在以下场景中表现不佳：文本生成任务（Encoder没有因果掩码，不适合自回归生成）、需要极长上下文的任务（超过512 token）、需要对两段文本进行非对称处理的任务（如信息检索中的query和document，BERT将它们放在同一个Encoder中，缺乏query-document的层次结构）。</p>
</section>
<section id="开放研究问题2018-2019年视角" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="开放研究问题2018-2019年视角"><span class="header-section-number">6.6</span> 开放研究问题（2018-2019年视角）</h3>
<p><strong>MLM的最优设计是什么？</strong> BERT的MLM随机遮蔽单个token，但是否有更好的遮蔽策略？遮蔽连续的span？遮蔽整个词？遮蔽实体？遮蔽比例应该是多少？这些问题催生了SpanBERT、ERNIE等后续工作。</p>
<p><strong>NSP的替代方案</strong>。既然NSP被证明效果有限，有没有更好的句子级预训练目标？ALBERT后来提出了句子顺序预测（SOP），证明判断两个句子的顺序比判断是否连续更有意义。</p>
<p><strong>Encoder-only vs Decoder-only vs Encoder-Decoder</strong>。BERT选择了Encoder-only，GPT选择了Decoder-only，哪种架构在根本上更好？T5（2019）尝试了Encoder-Decoder架构，将所有任务统一为Text-to-Text格式。这三种架构的根本权衡是什么？这个问题在第16章中进一步讨论。</p>
<p><strong>预训练目标与下游任务的关系</strong>。MLM让模型学习token级别的预测能力，但下游任务往往需要句子级别或段落级别的理解。预训练目标的粒度与下游任务的粒度之间存在”鸿沟”。如何设计更好的预训练目标来缩小这个鸿沟？这催生了ELECTRA（替换词检测）、T5（Span Corruption）等一系列创新。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="mlm的内在效率问题" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="mlm的内在效率问题"><span class="header-section-number">7.1</span> MLM的内在效率问题</h3>
<p>BERT的MLM目标存在一个根本性的效率限制：每个训练样本中，只有被遮蔽的15%的token参与损失计算，其余85%的token虽然参与了前向计算（提供上下文），但不提供直接的训练信号。</p>
<p>对比GPT的因果语言建模——每个位置都预测下一个词，100%的token都贡献训练信号——BERT的信号效率大约只有GPT的<span class="math inline">\(1/7\)</span>。这意味着BERT需要更多的训练步数和更多的数据才能达到同等水平的语言理解能力。</p>
<p>ELECTRA（Clark et al., 2020）后来以一种巧妙的方式解决了这个问题：用一个小的生成器生成”伪造”的token替换真实token，然后让BERT判断每个位置的token是”原始的”还是”被替换的”。这样，所有100%的位置都参与了训练，信号效率大幅提升。这是第14章的内容之一。</p>
</section>
<section id="预训练-微调不一致" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="预训练-微调不一致"><span class="header-section-number">7.2</span> 预训练-微调不一致</h3>
<p>尽管80-10-10策略试图缓解预训练与微调之间的分布差异，但不一致问题并未被完全解决。在预训练时，模型看到的输入中包含<code>[MASK]</code>标记和偶尔的随机替换词；在微调时，输入是干净的自然文本。模型在两种不同的输入分布上工作，这可能导致预训练学到的某些模式在微调时无法完全发挥作用。</p>
<p>XLNet（Yang et al., 2019）后来提出了排列语言建模（Permutation Language Modeling），在保持双向性的同时完全避免了<code>[MASK]</code>标记的使用——这是一个理论上更优雅的解决方案，但实现复杂度更高。</p>
</section>
<section id="不适合文本生成" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="不适合文本生成"><span class="header-section-number">7.3</span> 不适合文本生成</h3>
<p>BERT的Encoder-only架构天然不适合文本生成。在训练时，Encoder的每个位置都能看到所有其他位置（包括”未来”的token），但在生成时，“未来”的token还不存在。这种训练与推理的不一致使得BERT无法直接用于自回归生成。</p>
<p>虽然存在一些变通方案（如将BERT用于条件生成或迭代式生成），但这些方案要么效果不如专门的自回归模型（GPT系列），要么推理效率很低。后来的T5和BART采用Encoder-Decoder架构，同时保留了编码器的双向性和解码器的生成能力，在一定程度上解决了这个问题。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.4</span> 这些局限导向了什么？</h3>
<p>BERT的局限精确地催生了2019-2020年预训练技术的多个发展方向。</p>
<p>MLM的效率问题和遮蔽策略的不完美催生了<strong>新的预训练目标探索</strong>——XLNet的排列语言建模、ELECTRA的替换词检测、T5的Span Corruption——这些工作从不同角度改进了BERT的预训练机制。这是下一章（第14章：预训练目标的演进）的核心主题。</p>
<p>模型规模和训练策略的限制催生了<strong>训练优化的系统研究</strong>——RoBERTa证明了”训练得更好”比”设计更巧妙的目标”更重要，通过更多数据、更大batch、去掉NSP、动态遮蔽等简单改进，就能大幅超越原始BERT。这是第15章的内容。</p>
<p>Encoder-only的局限催生了<strong>架构选择的深入讨论</strong>——Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么优势？为什么Decoder-only最终在规模化的道路上走得最远？这是第16章的话题。</p>
<blockquote class="blockquote">
<p>下一章预告：第14章将探讨BERT之后预训练目标的多元演进——XLNet的排列语言建模如何在保持双向性的同时避免<code>[MASK]</code>标记？ELECTRA的替换词检测如何将预训练信号效率从15%提升到100%？T5的Text-to-Text统一框架如何消除任务格式的差异？每一个改进都是对BERT某个具体局限的回应。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>这一章我们详细介绍了BERT——第一个真正实现深层双向预训练的Transformer模型。</p>
<p>核心问题是如何让预训练模型在每个位置同时利用左右两侧的上下文。GPT的因果注意力限制了模型只能看到左侧上下文，ELMo的分离式双向让两个方向的信息无法深度融合。</p>
<p>BERT的核心洞察是将预训练任务从”预测下一个词”变为”预测被遮蔽的词”（完形填空）。这个简单的变换解除了双向性与语言建模之间的矛盾——被遮蔽的词不在输入中，因此不存在信息泄漏，模型可以自由地使用双向上下文。</p>
<p>BERT的技术方案包含两个预训练目标：MLM（掩码语言模型，使用80-10-10策略处理被遮蔽的token）和NSP（下一句预测，虽然后来被证明并非必要）。输入表示由Token Embedding + Segment Embedding + Position Embedding三者相加，<code>[CLS]</code>作为序列的汇聚表示用于分类任务。</p>
<p>实验结果全面验证了双向预训练的优势。在参数量几乎相同的条件下（BERT-Base 110M vs GPT-1 117M），BERT在GLUE平均分上超出GPT近7个百分点（79.6 vs 72.8）。消融实验直接证明，双向性（MLM）而非其他因素是性能提升的主要来源。</p>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\text{Input} = E_{\text{token}} + E_{\text{segment}} + E_{\text{position}}\)</span></td>
<td>BERT输入表示</td>
</tr>
<tr class="even">
<td><span class="math inline">\(L_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log P(x_i \mid \tilde{\mathbf{x}})\)</span></td>
<td>MLM损失（仅在被遮蔽位置计算）</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L = L_{\text{MLM}} + L_{\text{NSP}}\)</span></td>
<td>BERT总预训练损失</td>
</tr>
<tr class="even">
<td><span class="math inline">\(P(c) = \text{softmax}(h_{[\text{CLS}]} \cdot W^T)\)</span></td>
<td>微调时的分类预测</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P_{\text{start}}(i) = \text{softmax}(S \cdot h_i)\)</span></td>
<td>问答任务的答案起始位置预测</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> BERT的<code>[MASK]</code>标记在预训练时频繁出现，但在微调时从不出现。80-10-10策略如何缓解这个预训练-微调不一致问题？如果将策略改为100-0-0（全部替换为<code>[MASK]</code>），你预期会在哪些任务上看到性能下降？</p></li>
<li><p><strong>[数学推导]</strong> 计算BERT-Base的总参数量。12层Transformer Encoder，<span class="math inline">\(H = 768\)</span>，<span class="math inline">\(A = 12\)</span>，FFN内部维度3072，WordPiece词汇表大小30,522，最大位置512。需要分别计算：Token Embedding、Segment Embedding、Position Embedding、每层的多头注意力、每层的FFN、每层的LayerNorm（两个）、以及MLM头和NSP头的参数。</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 在SQuAD v1.1数据集上微调BERT-Base，对比以下设置的性能：(a) 标准微调（学习率2e-5）；(b) 使用特征提取（冻结BERT参数，只训练答案预测头）；(c) 仅微调最后4层（冻结前8层）。分析不同程度的微调对性能和训练速度的影响。</p></li>
<li><p><strong>[研究思考]</strong> BERT的MLM目标每次只预测15%的token，而GPT的CLM目标预测100%的token。假设两个模型在相同大小的数据集上训练相同的步数，BERT”看到”的有效训练信号大约是GPT的<span class="math inline">\(1/7\)</span>。你认为BERT如何弥补这个效率差距？更大的数据、更长的训练、还是MLM本身提供了比CLM更”高质量”的信号？</p></li>
<li><p><strong>[对比分析]</strong> ELMo、GPT和BERT分别代表了预训练技术的三种路线：LSTM+特征提取、单向Transformer+微调、双向Transformer+微调。从表示能力、训练效率、生成能力、扩展性四个维度对比这三种方法，并分析为什么BERT在理解任务上胜出，但GPT路线最终在规模化上走得更远。</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”</strong>。BERT的原始论文。重点阅读：Section 3（BERT模型设计，包括MLM和NSP的详细描述）、Section 5.1（消融实验——NSP的作用、双向性的验证）。可快速浏览：Section 4中各任务的微调细节。<a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></p>
</section>
<section id="前驱工作" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="前驱工作"><span class="header-section-number">9.2</span> 前驱工作</h3>
<p><strong>Radford et al.&nbsp;(2018). “Improving Language Understanding by Generative Pre-Training” (GPT)</strong>。上一章的主题。与BERT对比阅读，可以清晰地看到”单向 vs 双向”和”Decoder vs Encoder”两个维度的差异。</p>
<p><strong>Taylor, W.L. (1953). “Cloze Procedure: A New Tool for Measuring Readability”</strong>。MLM的思想来源。完形填空测试（cloze test）最早由Taylor在1953年提出，用于衡量文本可读性。60多年后，这个语言学测试方法被BERT重新发现并应用于预训练。</p>
</section>
<section id="后续改进" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="后续改进"><span class="header-section-number">9.3</span> 后续改进</h3>
<p><strong>Liu, Y. et al.&nbsp;(2019). “RoBERTa: A Robustly Optimized BERT Pretraining Approach”</strong>。通过系统的消融实验发现BERT”训练不足”，去掉NSP、使用动态遮蔽、更多数据、更大batch即可显著提升性能。这是对BERT最重要的工程优化工作。<a href="https://arxiv.org/abs/1907.11692">arXiv:1907.11692</a></p>
<p><strong>Lan, Z. et al.&nbsp;(2020). “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations”</strong>。通过跨层参数共享和Embedding分解大幅减少参数量，同时引入SOP替代NSP。<a href="https://arxiv.org/abs/1909.11942">arXiv:1909.11942</a></p>
<p><strong>Joshi, M. et al.&nbsp;(2020). “SpanBERT: Improving Pre-training by Representing and Predicting Spans”</strong>。用连续span遮蔽替代随机token遮蔽，并去掉NSP，在多项任务上超越BERT。<a href="https://arxiv.org/abs/1907.10529">arXiv:1907.10529</a></p>
</section>
<section id="挑战质疑" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="挑战质疑"><span class="header-section-number">9.4</span> 挑战/质疑</h3>
<p><strong>Yang, Z. et al.&nbsp;(2019). “XLNet: Generalized Autoregressive Pretraining for Language Understanding”</strong>。指出BERT的<code>[MASK]</code>标记带来了预训练-微调不一致问题，提出排列语言建模作为替代方案。<a href="https://arxiv.org/abs/1906.08237">arXiv:1906.08237</a></p>
<p><strong>Clark, K. et al.&nbsp;(2020). “ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators”</strong>。指出BERT的MLM信号效率低下（仅15%），提出替换词检测目标实现100%的信号利用。<a href="https://arxiv.org/abs/2003.10555">arXiv:2003.10555</a></p>
</section>
<section id="综述与教程" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">9.5</span> 综述与教程</h3>
<p><strong>Rogers, A., Kovaleva, O., &amp; Rumshisky, A. (2020). “A Primer in BERTology: What We Know About How BERT Works”</strong>。对BERT内部工作机制的全面综述——不同层学到了什么？不同Head在做什么？BERT的知识存储在哪里？<a href="https://arxiv.org/abs/2002.12327">arXiv:2002.12327</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.6">
<h3 data-number="9.6" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.6</span> 代码资源</h3>
<ul>
<li><strong>Hugging Face Transformers</strong>：<a href="https://huggingface.co/bert-base-uncased">huggingface.co/bert-base-uncased</a> — 预训练BERT模型，支持直接微调</li>
<li><strong>Google Research BERT</strong>：<a href="https://github.com/google-research/bert">github.com/google-research/bert</a> — 官方TensorFlow实现</li>
<li><strong>D2L BERT实现</strong>：<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html">d2l.ai Chapter 15.8-15.10</a> — 从零实现BERT的教学代码</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>BERT的名字是 <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers 的缩写，但它同时也是美国经典儿童节目《芝麻街》（Sesame Street）中的角色名。这不是巧合——ELMo（2018年2月）也是《芝麻街》中的角色。BERT的作者们在命名时显然是有意向ELMo致敬，同时也在暗示BERT是ELMo的”进化版”。这种学术命名的幽默感在NLP社区引起了会心一笑，也开启了一段时间内NLP论文用各种缩写致敬《芝麻街》角色的风潮——后来出现了ERNIE（Baidu和清华大学）、Grover（UW）、Big Bird（Google）等模型。</p>
<p>一个值得深思的时间线是2018年的”预训练三部曲”：ELMo（2月）→ GPT（6月）→ BERT（10月）。三个模型在8个月内相继发表，走出了三条不同的路线。BERT在发表后仅两个月内就被下载了数十万次，成为NLP历史上被采用最快的模型之一。2019年到2020年的两年间，几乎所有NLP竞赛的获胜方案都以BERT（或其变体）为基础——这段时期被社区称为”BERT时代”（BERTology）。</p>
<p>然而，历史总是充满反转。BERT在理解任务上的统治地位从2020年开始被GPT-3的In-Context Learning所动摇——一个足够大的单向模型，通过提示（prompt）就能完成各种理解任务，不需要任何微调。到2023年，GPT-4和Claude等大型语言模型已经在几乎所有理解基准上超越了BERT系列。BERT选择的Encoder-only架构在理解任务上曾经占据绝对优势，但最终被Decoder-only架构通过暴力规模化所超越。这个故事告诉我们，技术选择的”最优性”往往依赖于当时的规模和计算资源——在百万级参数时代双向Encoder更优，在千亿级参数时代单向Decoder反而更有扩展潜力。这个深层的权衡，是后续章节将持续探讨的主题。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第13章：BERT——双向预训练路线"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Pre-Training of Deep Bidirectional Transformers for Language Understanding"</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-26"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, Pre-training, BERT, Transformer]</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [BERT, MLM, NSP, 双向预训练, 微调, Transformer Encoder, Masked Language Model]</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "BERT：用掩码语言模型（MLM）实现真正的双向预训练。通过随机遮蔽输入token并让模型根据完整的左右上下文来预测被遮蔽的词，BERT在11个NLP基准任务上全面超越了GPT和ELMo，开启了预训练模型的'BERT时代'。"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-13/original/fig1-pretraining-finetuning.png"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何让预训练模型在每个位置同时利用左右两侧的上下文，获得真正的双向表示？</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2018年10月 </span><span class="pp">|</span><span class="at"> Devlin et al. "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding" </span><span class="pp">|</span><span class="at"> Google AI Language</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Devlin et al. (2019)** "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding" (arXiv:1810.04805) — 参考了 Section 3（BERT模型设计）、Section 4（实验）、Section 5（消融实验）；**从论文PDF提取了4张原图**：Figure 1（预训练与微调流程）、Figure 2（输入表示）、Figure 3（BERT/GPT/ELMo架构对比）、Figure 4（四类微调任务）；改编了2个算法伪代码框（MLM 80-10-10策略、预训练样本构造流程）</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radford et al. (2018)** "Improving Language Understanding by Generative Pre-Training" (GPT) — 参考了与BERT的架构对比</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liu et al. (2019)** "RoBERTa: A Robustly Optimized BERT Pretraining Approach" (arXiv:1907.11692) — 参考了对NSP的消融分析</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Section 15.8-15.10 — 参考了教学组织方式和代码实现思路</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SLP3** Chapter 11 — 参考了BERT在NLP任务中的定位和讲解框架</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 9 (2025) "Pretraining" — 参考了BERT预训练目标的讲解角度</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Princeton COS 484** (Danqi Chen) — 参考了BERT作者之一对预训练设计决策的解释</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>上一章我们详细介绍了GPT——第一个将Transformer与全模型微调相结合的预训练方案。GPT用Transformer Decoder做自回归语言建模，然后在下游任务上微调整个模型参数，确立了"预训练 + 微调"的现代NLP范式。在12个基准任务中的9个上，GPT达到了当时的最佳水平，特别是在常识推理（+8.9%）和语言可接受性（+10.4%）上的提升令人印象深刻。</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>然而，上一章结尾我们也揭示了GPT的一个根本限制：**单向注意力**。</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>因为GPT使用因果掩码（causal mask），每个位置只能看到它左侧的token。回忆我们在上一章中构造的数值例子——在处理"The cat sat"时，"The"的注意力权重是$<span class="co">[</span><span class="ot">1.00, 0.00, 0.00</span><span class="co">]</span>$，它只能关注自己；"cat"的权重是$<span class="co">[</span><span class="ot">0.35, 0.65, 0.00</span><span class="co">]</span>$，能看到"The"和自己，但完全看不到"sat"。这种单向限制对于语言生成来说是自然的——你不能在写下一个词的时候"偷看"未来——但对于语言理解来说却是一个严重的障碍。</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>考虑句子"I went to the bank to deposit my check"。当模型处理"bank"这个词时，如果只能看到左侧的"I went to the"，它无法确定"bank"指的是金融机构还是河岸。只有看到右侧的"to deposit my check"，正确的语义才能被锁定。GPT的单向注意力让模型在处理"bank"时对右侧的关键线索一无所知。</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>ELMo试图解决这个问题，但它的方案是"分离式双向"——两个独立的单向LSTM分别从左到右和从右到左处理序列，然后将表示拼接在一起。这种拼接并不等于真正的双向：前向LSTM在编码"bank"时只看到了左侧上下文，后向LSTM在编码"bank"时只看到了右侧上下文，两者从未在模型内部进行深度交互。</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>2018年10月，也就是GPT发表仅4个月之后，Google AI Language团队的Jacob Devlin等人提出了一个优雅的解决方案。</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：用**掩码语言模型（Masked Language Model, MLM）**替代因果语言建模，让Transformer Encoder在预训练时真正融合双向上下文——不是拼接两个单向模型的输出，而是在同一个深层网络的每一层都让信息双向流动。</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理解任务为什么需要双向上下文？</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>在深入BERT的技术细节之前，让我们先理解一个更基本的问题：为什么语言理解任务天然地需要双向信息？</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>自然语言的语义解析往往需要"全局视野"。一个词的含义不仅取决于它前面的内容，也取决于它后面的内容。语言学中有一个经典概念叫做**消歧（disambiguation）**——同一个词形在不同上下文中可能表达完全不同的语义。英语中约有7%的常用词是多义词，而在实际文本中，多义词出现的频率远高于7%，因为高频词往往有更多的义项。</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>更深层地说，语言理解不仅需要"逐词理解"，还需要"关系推理"。在自然语言推理（NLI）任务中，模型需要判断"A man is sleeping"是否可以从"A man is resting on a couch"推导出来。这需要"sleeping"和"resting"之间建立语义对应，"a man"在两个句子中建立共指关系。这些关系的建立需要模型在同一时刻看到两个句子的完整内容，并在它们之间建立交叉注意力。</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>那么，为什么不能简单地使用双向Transformer Encoder来做语言建模呢？这里有一个看似简单却非常深刻的技术障碍。</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a><span class="fu">### 双向性与语言建模的矛盾</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>标准的语言建模目标是给定前文预测下一个词：$P(w_t \mid w_1, \ldots, w_{t-1})$。GPT使用因果掩码来实现这一点——每个位置只能看到左侧的token，因此预测$w_t$时不会"偷看"到$w_t$本身或它之后的内容。</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>但如果我们使用双向Transformer Encoder（没有因果掩码），每个位置可以看到所有其他位置——包括要预测的位置本身。在这种情况下，让模型"预测下一个词"变得毫无意义：模型可以直接通过注意力机制"看到"目标词，然后原样复制输出。这不是学习语言，而是作弊。</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>ELMo用一种保守的方式回避了这个矛盾：训练两个独立的单向模型，确保每个方向都不会泄漏信息，最后拼接。但这种方案的代价是两个方向的信息永远无法在模型内部深度融合。</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>GPT选择了另一种保守的方式：接受单向的限制，换取架构的简洁性和生成的能力。</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>那么，有没有一种方法可以**既使用双向Transformer Encoder，又避免信息泄漏**？</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>BERT的回答是：**不要预测下一个词——预测被遮蔽的词**。</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从"预测下一个"到"填空"</span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>BERT的核心洞察可以用一个直觉来理解：把预训练任务从"续写作文"变成"完形填空"。</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>传统的语言建模就像写作文——给你一个开头"Once upon a time"，让你续写下一个词。续写天然地是从左到右的过程，因此模型只能看到左侧的上下文。</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>BERT的掩码语言模型则像完形填空——给你一个完整的句子"I went to the ___ to deposit my check"，让你猜被遮蔽的词。填空不要求特定的方向——你自然地会同时参考左侧的"I went to the"和右侧的"to deposit my check"来确定答案是"bank"。</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>这个简单的变换解决了双向性与语言建模之间的矛盾：被遮蔽的词不在输入中（或被替换为特殊标记），所以模型无法"作弊"直接复制；同时，所有未被遮蔽的位置都可以看到彼此，信息可以双向自由流动。</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键洞察：完形填空式的预训练</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>BERT的全名是 **Bidirectional Encoder Representations from Transformers**——双向Transformer编码器表示。名字本身就在强调"双向"（Bidirectional），因为这正是它相对于GPT的核心创新。</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>如果把GPT比作一个只能从左往右阅读的读者，那么BERT就像一个可以纵览全文的读者。GPT在处理每个词时就像戴着一副"遮光眼镜"——只能看到左边，右边一片漆黑。BERT则摘掉了这副眼镜，让每个词都能同时感知左右两侧的完整上下文。</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>但BERT面临的挑战是：如果读者能看到全文，那让他"猜下一个词"就没有意义了（答案就在眼前）。BERT的解决方案极其巧妙——在全文中随机"涂掉"一些词，然后让读者根据剩余的内容来猜测被涂掉的是什么。这就是掩码语言模型（MLM）的本质。</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### 两个预训练目标</span></span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>BERT使用两个预训练目标联合训练。</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>**目标一：掩码语言模型（Masked Language Model, MLM）**。随机选择输入中15%的token进行"遮蔽"，然后让模型预测被遮蔽的原始token。这迫使模型学习深层的双向语言理解能力——要准确填空，模型必须同时理解左右两侧的上下文，以及词汇之间的语法和语义关系。</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>**目标二：下一句预测（Next Sentence Prediction, NSP）**。给定两个句子A和B，模型判断B是否是A在原文中的真实下一句。这个目标旨在让模型学习句子级别的关系理解——许多重要的NLP任务（如问答、自然语言推理）需要理解两个句子之间的关系。</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a><span class="fu">### 架构选择：为什么是Transformer Encoder？</span></span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>GPT选择了Transformer Decoder（因果注意力），BERT选择了Transformer Encoder（双向注意力）。这个选择背后有清晰的逻辑。</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>GPT的目标是因果语言建模——预测下一个词，因此需要因果掩码来防止信息泄漏。但BERT的目标是MLM——预测被遮蔽的词，遮蔽本身已经防止了信息泄漏（被遮蔽的词不在输入中），因此不需要因果掩码。没有了因果掩码的约束，Transformer Encoder的每个位置都可以自由地关注所有其他位置，实现真正的双向表示学习。</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>换句话说，GPT和BERT在"如何防止作弊"这个问题上选择了不同的策略。GPT通过**限制视野**来防止作弊——你看不到答案所在的位置。BERT通过**隐藏答案**来防止作弊——答案所在的位置被替换成了一个无意义的<span class="in">`[MASK]`</span>标记，即使你能看到那个位置，也得不到任何有用的信息。</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>下图直观地展示了三种预训练架构的核心差异。BERT的双向Transformer让每个位置都能关注所有其他位置（箭头交叉连接）；OpenAI GPT的单向Transformer只允许从左到右的信息流动（箭头单向连接）；ELMo则使用两个独立的LSTM分别处理前向和后向，最后拼接表示。</span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a><span class="al">![三种预训练模型架构的对比。BERT使用双向Transformer，每个位置可以关注所有其他位置；OpenAI GPT使用单向Transformer（从左到右）；ELMo使用独立训练的前向和后向LSTM，最后拼接。三者中，只有BERT在模型内部实现了真正的深层双向交互。](figures/chapter-13/original/fig3-architecture-comparison.png)</span>{#fig-architecture-comparison width=90%}</span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>*Source: Devlin et al. (2019) "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", Figure 3. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)*</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a><span class="al">![BERT的预训练与微调流程。左侧展示了预训练阶段的两个目标（NSP和Mask LM），右侧展示了微调阶段如何将同一模型适配到不同的下游任务（MNLI、NER、SQuAD等）。注意预训练和微调使用完全相同的架构，只是输出层不同。](figures/chapter-13/original/fig1-pretraining-finetuning.png)</span>{#fig-bert-pretraining width=90%}</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>*Source: Devlin et al. (2019) "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", Figure 1. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)*</span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型架构</span></span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>BERT的架构是标准的Transformer Encoder——与第8章介绍的Transformer编码器完全一致。论文提供了两个规模的模型：</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 超参数 <span class="pp">|</span> BERT-Base <span class="pp">|</span> BERT-Large <span class="pp">|</span></span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----------|------------|</span></span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 层数（$L$） <span class="pp">|</span> 12 <span class="pp">|</span> 24 <span class="pp">|</span></span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 隐藏维度（$H$） <span class="pp">|</span> 768 <span class="pp">|</span> 1024 <span class="pp">|</span></span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 注意力头数（$A$） <span class="pp">|</span> 12 <span class="pp">|</span> 16 <span class="pp">|</span></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 每头维度 <span class="pp">|</span> 64 <span class="pp">|</span> 64 <span class="pp">|</span></span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FFN内部维度 <span class="pp">|</span> 3072 <span class="pp">|</span> 4096 <span class="pp">|</span></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 总参数量 <span class="pp">|</span> 110M <span class="pp">|</span> 340M <span class="pp">|</span></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>BERT-Base的参数量与GPT-1（117M）几乎相同，这是有意为之的——Devlin等人想要在公平的条件下对比双向Encoder与单向Decoder的优劣。BERT-Large则将参数量扩大到340M，探索更大规模带来的增益。</span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>值得注意的是BERT使用了**可学习的位置嵌入**，最大序列长度为512 token，这与GPT的选择一致。分词器使用的是**WordPiece**（第3章讨论的子词分词方法之一），词汇表大小为30,522个token。</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a><span class="fu">### 输入表示</span></span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>BERT的输入表示设计是它区别于GPT的一个重要方面，因为BERT需要处理**单句和句对**两种输入格式。</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a>每个输入序列的第一个token总是特殊标记<span class="in">`[CLS]`</span>（classification的缩写）。如果输入是一个句对，两个句子用特殊标记<span class="in">`[SEP]`</span>分隔。最终的输入嵌入是三种嵌入的逐元素相加：</span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a>\text{Input} = \text{Token Embedding} + \text{Segment Embedding} + \text{Position Embedding}</span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>**Token Embedding** 是标准的WordPiece嵌入，将每个token映射到一个$H$维的向量。</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a>**Segment Embedding** 用来区分两个句子。属于句子A的所有token共享一个可学习的嵌入$E_A$，属于句子B的所有token共享另一个可学习的嵌入$E_B$。如果输入只有一个句子，则所有token都使用$E_A$。这种设计让模型可以从嵌入层面区分"这个词属于哪个句子"。</span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>**Position Embedding** 是可学习的位置嵌入，与GPT相同，最大支持512个位置。</span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a>以一个具体的例子来说明。假设输入是句对"I love NLP"和"It is fun"，经过WordPiece分词后，完整的输入序列是：</span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span> \;\; \text{I} \;\; \text{love} \;\; \text{NLP} \;\; <span class="co">[</span><span class="ot">\text{SEP}</span><span class="co">]</span> \;\; \text{It} \;\; \text{is} \;\; \text{fun} \;\; <span class="co">[</span><span class="ot">\text{SEP}</span><span class="co">]</span></span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a>对应的Segment ID是：</span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a>A \;\; A \;\; A \;\; A \;\; A \;\; B \;\; B \;\; B \;\; B</span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a><span class="al">![BERT的输入表示：由Token Embedding、Segment Embedding和Position Embedding三者逐元素相加而成。示例句对"my dog is cute"（句子A）和"he likes play ##ing"（句子B）的完整输入构造过程。](figures/chapter-13/original/fig2-input-representation.png)</span>{#fig-bert-input width=90%}</span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a>*Source: Devlin et al. (2019) "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", Figure 2. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)*</span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a><span class="in">`[CLS]`</span>的设计值得单独讨论。它始终位于序列的第一个位置，经过12层（或24层）双向注意力之后，<span class="in">`[CLS]`</span>的最终表示$h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>}$已经融合了整个序列的信息——因为在每一层中，它都可以通过注意力机制"看到"所有其他token。因此，$h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>}$可以作为整个句子（或句对）的汇聚表示（pooled representation），直接用于句子级别的分类任务。这与GPT取序列最后一个token的表示来做分类是异曲同工的，但<span class="in">`[CLS]`</span>的优势在于它从第一层开始就在"综合"全局信息，而不是像GPT那样只能利用单向累积的信息。</span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练目标一：掩码语言模型（MLM）</span></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a>MLM的核心思想很简单：随机选择输入中的一些token，将它们"遮蔽"掉，然后让模型预测被遮蔽的原始token。但细节中隐藏着许多精妙的设计决策。</span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a>**为什么遮蔽15%？** Devlin等人选择遮蔽每个序列中15%的token。这个比例是一个权衡的结果。如果遮蔽太少（比如5%），每个序列能提供的训练信号就太少——模型需要处理很多次才能积累足够的监督信号，训练效率低下。如果遮蔽太多（比如50%），剩余的上下文就不够丰富，模型很难做出准确的预测——就像一道完形填空题如果挖掉一半的字，人类也很难填对。15%在"训练信号充足"和"上下文足够丰富"之间取得了一个经验上的平衡点。</span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a>这里有一个微妙但重要的效率问题。在标准的语言建模中（如GPT），模型对序列中的每一个位置都做预测（预测下一个词），因此每个token都贡献一个训练信号。但在MLM中，只有被遮蔽的15%的token贡献训练信号，其余85%的token不参与损失计算。这意味着**BERT需要比GPT多看大约6-7倍的数据才能获得同等数量的训练信号**。从训练效率的角度看，这是MLM相对于标准语言建模的一个内在代价。</span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a>**为什么不能全部替换为`[MASK]`？** 一个直觉的实现方式是把所有被选中的token都替换成`[MASK]`标记。但这会造成一个严重的**预训练-微调不一致**问题：在预训练时，输入中充满了<span class="in">`[MASK]`</span>标记；在微调时，输入是正常的自然文本，没有任何<span class="in">`[MASK]`</span>。模型在预训练时学会了"看到<span class="in">`[MASK]`</span>就预测原词"，但在微调时永远不会看到<span class="in">`[MASK]`</span>，这让预训练学到的部分模式变得无用。</span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a>**80-10-10策略**。为了缓解这个不一致，BERT对被选中的15%的token采用了三种不同的处理方式：</span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**80%的概率**：替换为<span class="in">`[MASK]`</span>标记（标准遮蔽）</span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**10%的概率**：替换为词汇表中的一个随机token</span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**10%的概率**：保持不变</span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a>这个设计的逻辑是什么？80%的<span class="in">`[MASK]`</span>确保模型的主要训练信号来自于真正的"填空"——模型看到一个<span class="in">`[MASK]`</span>，需要根据上下文推断原词。10%的随机替换迫使模型不能完全信任输入——即使看到一个正常的词，它也可能是错误的替换，模型需要有能力判断"这个词在上下文中是否合理"。10%的保持不变则向模型展示了正常输入的样子——这个词没有被动过，它的表示应该尽量接近真实含义。后两种策略共同缓解了预训练与微调之间的分布差异。</span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a>你可能会问：随机替换不会"污染"模型的表示学习吗？考虑到随机替换的概率只有$15\% \times 10\% = 1.5\%$，平均每100个token中只有1.5个会被替换为随机词，这个噪声水平足够低，不会对模型的语言能力造成实质性的损害。</span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: Masked Language Model — Input Construction (Devlin et al., 2019)</span></span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a><span class="in">输入: token序列 x = (x_1, x_2, ..., x_n), 遮蔽比例 p = 0.15</span></span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a><span class="in">输出: 修改后的输入 x̃, 目标标签 y</span></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a><span class="in">1. 随机选择 ⌊n × p⌋ 个位置，构成遮蔽集合 M</span></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a><span class="in">2. FOR each position i ∈ M:</span></span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a><span class="in">3.     采样随机数 r ~ Uniform(0, 1)</span></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a><span class="in">4.     IF r &lt; 0.8:</span></span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a><span class="in">5.         x̃_i ← [MASK]          # 80%: 替换为 [MASK] 标记</span></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a><span class="in">6.     ELSE IF r &lt; 0.9:</span></span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a><span class="in">7.         x̃_i ← random(V)       # 10%: 替换为词汇表中的随机token</span></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a><span class="in">8.     ELSE:</span></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a><span class="in">9.         x̃_i ← x_i             # 10%: 保持原始token不变</span></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a><span class="in">10.    y_i ← x_i                  # 目标始终是原始token</span></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a><span class="in">11. FOR each position i ∉ M:</span></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a><span class="in">12.    x̃_i ← x_i                 # 未选中的位置保持不变</span></span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a><span class="in">13.    y_i ← IGNORE               # 不参与损失计算</span></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a><span class="in">14. RETURN x̃, y</span></span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a>*改编自: Devlin et al. (2019) "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", Section 3.1 及 Appendix C.2. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)*</span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a>MLM的损失函数可以形式化为：</span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a>L_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log P(x_i \mid \tilde{\mathbf{x}}; \theta)</span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a>其中$\mathcal{M}$是被遮蔽的token位置集合，$\tilde{\mathbf{x}}$是经过80-10-10处理后的输入序列，$x_i$是位置$i$的原始token。只有被选中的15%的位置参与损失计算。</span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练目标二：下一句预测（NSP）</span></span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>许多重要的NLP任务（如问答、自然语言推理）需要理解两个句子之间的关系。标准的语言建模——无论是GPT的因果LM还是BERT的MLM——都主要关注token级别的语言理解，缺乏显式的句子间关系建模。NSP正是为了填补这个空白。</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a>NSP的设计非常简单：在构造预训练数据时，对于每个训练样本，50%的概率选择真实的连续句对（标签为<span class="in">`IsNext`</span>），50%的概率将第二个句子替换为语料库中随机抽取的句子（标签为<span class="in">`NotNext`</span>）。然后让模型基于<span class="in">`[CLS]`</span>的最终表示，做二分类预测。</span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a>P(\text{IsNext} \mid h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>}) = \text{softmax}(h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>} \cdot W_{\text{NSP}})</span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a>NSP的总损失为：</span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a>L_{\text{NSP}} = -\left<span class="co">[</span><span class="ot">y \cdot \log P(\text{IsNext}) + (1 - y) \cdot \log P(\text{NotNext})\right</span><span class="co">]</span></span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a>BERT的总预训练损失是两个目标的简单求和：</span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a>L = L_{\text{MLM}} + L_{\text{NSP}}</span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a>然而，NSP后来成为BERT设计中争议最大的部分。RoBERTa（Liu et al., 2019）通过系统的消融实验发现，**去掉NSP反而能提高模型的性能**。这个反直觉的结果在学术界引起了广泛讨论，我们将在"深入理解"一节中详细分析。</span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a>下面的算法将MLM和NSP两个目标的数据构造过程整合在一起，展示了一个完整的BERT预训练样本是如何从原始语料库中生成的。</span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 2: BERT Pre-training Sample Construction (Devlin et al., 2019)</span></span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a><span class="in">输入: 语料库 D（按文档组织），最大序列长度 T = 512</span></span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a><span class="in">输出: 预训练样本 (x̃, segment_ids, is_next, mlm_targets)</span></span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a><span class="in"># —— Step 1: 构造句对（用于 NSP）——</span></span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a><span class="in">1. 从语料库中随机选择文档 d，从中选择连续句子 A</span></span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a><span class="in">2. 采样随机数 r ~ Uniform(0, 1)</span></span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a><span class="in">3. IF r &lt; 0.5:</span></span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a><span class="in">4.     B ← A 在文档 d 中的真实下一句           # IsNext</span></span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a><span class="in">5.     is_next ← True</span></span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a><span class="in">6. ELSE:</span></span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a><span class="in">7.     B ← 从语料库中随机抽取的句子             # NotNext</span></span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a><span class="in">8.     is_next ← False</span></span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a><span class="in">9. 截断使 len(A) + len(B) ≤ T - 3             # 为 [CLS], [SEP], [SEP] 预留</span></span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a><span class="in"># —— Step 2: 构造输入序列 ——</span></span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a><span class="in">10. tokens    ← [CLS] + A + [SEP] + B + [SEP]</span></span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a><span class="in">11. segments  ← [A...A] (len(A)+2个) + [B...B] (len(B)+1个)</span></span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a><span class="in">12. positions ← [0, 1, 2, ..., len(tokens)-1]</span></span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a><span class="in"># —— Step 3: 应用 MLM 遮蔽（Algorithm 1）——</span></span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a><span class="in">13. x̃, mlm_targets ← MLM_MASK(tokens, p=0.15)  # 调用 Algorithm 1</span></span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a><span class="in">14. RETURN (x̃, segments, positions, is_next, mlm_targets)</span></span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a>*改编自: Devlin et al. (2019) "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", Section 3.1, 3.2 及 Appendix A.2. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)*</span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练数据</span></span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a>BERT在两个大规模语料库上预训练：</span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a>**BooksCorpus**（800M词）——与GPT-1使用的相同数据源，包含超过7000本未出版书籍的连续文本。</span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a>**英文Wikipedia**（2,500M词）——只提取正文文本，去掉了列表、表格和标题。</span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a>两个数据源合计约33亿词（3.3B words），比GPT-1的8亿词大了约4倍。这个数据量的差异是BERT性能优势的一个贡献因素，尽管不是主要因素——后来的消融实验表明，双向性本身带来的提升远大于数据量增加带来的提升。</span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a>训练使用256个序列的batch size，每个序列最长512 token，总共训练1,000,000步（约40个epoch）。这在当时需要4个Cloud TPU（BERT-Base）或16个Cloud TPU（BERT-Large）训练4天。</span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键设计决策</span></span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a>让我们系统地审视BERT的几个关键设计决策，理解每个选择背后的权衡。</span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 决策1：Encoder-only 而非 Decoder-only 或 Encoder-Decoder</span></span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a>**决策**：BERT只使用Transformer Encoder，没有Decoder。</span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a>**原因**：BERT的目标是学习通用的语言理解表示，而不是进行序列生成。Encoder的双向注意力天然适合理解任务——每个位置可以综合全局信息形成表示。Decoder的因果注意力限制了信息流动，主要适用于生成任务。</span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a>**替代方案**：使用Encoder-Decoder结构（如后来的T5）可以同时处理理解和生成任务，但需要设计更复杂的预训练目标。使用Decoder-only（如GPT）则无法获得双向表示。</span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a>**影响**：Encoder-only的选择让BERT在理解任务上获得了巨大优势，但同时也意味着BERT**不适合直接用于文本生成**。生成任务需要自回归地逐token产出，而Encoder的双向注意力在推理时无法处理"还没有生成的未来token"。这个限制后来促使研究者探索Encoder-Decoder（T5）和Decoder-only（GPT系列）架构。</span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 决策2：WordPiece 而非 BPE</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a>**决策**：BERT使用WordPiece分词，词汇表大小30,522。</span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a>**原因**：WordPiece是Google内部广泛使用的子词分词方法。与BPE的贪心合并策略不同，WordPiece使用最大化训练数据似然的标准来选择合并。在实践中，两者的差异不大，但WordPiece在Google的工程基础设施中有更好的支持。</span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a>**影响**：GPT使用BPE（~40,000词），BERT使用WordPiece（~30,000词）。词汇表大小的差异意味着同样的文本在BERT中可能被切分成更多的子词，序列长度更长。但30,000的词汇表在覆盖率和效率之间取得了较好的平衡。</span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 决策3：为什么`[CLS]`放在句首？</span></span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a>**决策**：<span class="in">`[CLS]`</span>始终作为序列的第一个token。</span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a>**原因**：Devlin等人的设计意图是让<span class="in">`[CLS]`</span>作为整个序列的"汇聚器"——经过多层双向注意力后，它融合了序列中所有位置的信息。你可能会问：为什么不放在句末（像GPT那样取最后一个token的表示）？在双向Encoder中，放在哪里其实差别不大——因为每个位置都能看到所有其他位置，理论上任何位置经过充分的注意力交互后都可以汇聚全局信息。选择句首可能只是一个工程上的便利，使得<span class="in">`[CLS]`</span>的位置固定且容易索引。</span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：MLM的工作过程</span></span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a>让我们通过一个具体的小规模例子来追踪BERT的MLM预训练如何工作。</span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a>**设定**：句子"The cat sat on the mat"，$H = 4$（隐藏维度），词汇表大小$V = 10$（简化），单层单头注意力。</span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a>**Step 1: 选择遮蔽位置**</span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a>序列长度为6，遮蔽比例15%。$6 \times 15\% \approx 1$，选择1个token进行遮蔽。假设随机选中了位置3的"sat"。</span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a>**Step 2: 应用80-10-10策略**</span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a>对于被选中的"sat"，按概率执行：</span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>80%概率：替换为<span class="in">`[MASK]`</span> → 输入变为"The cat <span class="in">`[MASK]`</span> on the mat"</span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>10%概率：替换为随机词（比如"blue"）→ 输入变为"The cat blue on the mat"</span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>10%概率：保持不变 → 输入保持"The cat sat on the mat"</span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a>假设这次落入80%的分支，输入变为 $<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>, \text{The}, \text{cat}, <span class="co">[</span><span class="ot">\text{MASK}</span><span class="co">]</span>, \text{on}, \text{the}, \text{mat}, <span class="co">[</span><span class="ot">\text{SEP}</span><span class="co">]</span>$。</span>
<span id="cb5-368"><a href="#cb5-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-369"><a href="#cb5-369" aria-hidden="true" tabindex="-1"></a>**Step 3: 嵌入**</span>
<span id="cb5-370"><a href="#cb5-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-371"><a href="#cb5-371" aria-hidden="true" tabindex="-1"></a>假设Token + Position嵌入后得到（$H = 4$，只展示关键位置）：</span>
<span id="cb5-372"><a href="#cb5-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-373"><a href="#cb5-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-374"><a href="#cb5-374" aria-hidden="true" tabindex="-1"></a>h_0 = \begin{bmatrix}</span>
<span id="cb5-375"><a href="#cb5-375" aria-hidden="true" tabindex="-1"></a>\text{<span class="co">[</span><span class="ot">CLS</span><span class="co">]</span>:} &amp; 0.1 &amp; 0.2 &amp; 0.1 &amp; 0.3 <span class="sc">\\</span></span>
<span id="cb5-376"><a href="#cb5-376" aria-hidden="true" tabindex="-1"></a>\text{The:} &amp; 1.0 &amp; 0.5 &amp; 0.2 &amp; 0.1 <span class="sc">\\</span></span>
<span id="cb5-377"><a href="#cb5-377" aria-hidden="true" tabindex="-1"></a>\text{cat:} &amp; 0.3 &amp; 1.2 &amp; 0.8 &amp; 0.4 <span class="sc">\\</span></span>
<span id="cb5-378"><a href="#cb5-378" aria-hidden="true" tabindex="-1"></a>\text{<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>:} &amp; 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 <span class="sc">\\</span></span>
<span id="cb5-379"><a href="#cb5-379" aria-hidden="true" tabindex="-1"></a>\text{on:} &amp; 0.5 &amp; 0.3 &amp; 1.0 &amp; 0.2 <span class="sc">\\</span></span>
<span id="cb5-380"><a href="#cb5-380" aria-hidden="true" tabindex="-1"></a>\text{the:} &amp; 0.9 &amp; 0.5 &amp; 0.2 &amp; 0.1 <span class="sc">\\</span></span>
<span id="cb5-381"><a href="#cb5-381" aria-hidden="true" tabindex="-1"></a>\text{mat:} &amp; 0.4 &amp; 0.8 &amp; 0.6 &amp; 1.1 <span class="sc">\\</span></span>
<span id="cb5-382"><a href="#cb5-382" aria-hidden="true" tabindex="-1"></a>\text{<span class="co">[</span><span class="ot">SEP</span><span class="co">]</span>:} &amp; 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.2</span>
<span id="cb5-383"><a href="#cb5-383" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb5-384"><a href="#cb5-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-385"><a href="#cb5-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-386"><a href="#cb5-386" aria-hidden="true" tabindex="-1"></a>注意<span class="in">`[MASK]`</span>的初始嵌入接近零——它是一个特殊标记，没有携带原词"sat"的任何信息。</span>
<span id="cb5-387"><a href="#cb5-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-388"><a href="#cb5-388" aria-hidden="true" tabindex="-1"></a>**Step 4: 双向注意力（关键差异！）**</span>
<span id="cb5-389"><a href="#cb5-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-390"><a href="#cb5-390" aria-hidden="true" tabindex="-1"></a>与GPT不同，BERT**没有因果掩码**。注意力分数矩阵没有$-\infty$的上三角遮蔽——每个位置可以自由地关注所有其他位置。</span>
<span id="cb5-391"><a href="#cb5-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-392"><a href="#cb5-392" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-393"><a href="#cb5-393" aria-hidden="true" tabindex="-1"></a>\text{Attention Mask} = \begin{bmatrix}</span>
<span id="cb5-394"><a href="#cb5-394" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb5-395"><a href="#cb5-395" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb5-396"><a href="#cb5-396" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb5-397"><a href="#cb5-397" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb5-398"><a href="#cb5-398" aria-hidden="true" tabindex="-1"></a>\vdots &amp; &amp; &amp; &amp; &amp; &amp; &amp; \vdots <span class="sc">\\</span></span>
<span id="cb5-399"><a href="#cb5-399" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1</span>
<span id="cb5-400"><a href="#cb5-400" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb5-401"><a href="#cb5-401" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-402"><a href="#cb5-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-403"><a href="#cb5-403" aria-hidden="true" tabindex="-1"></a>全1矩阵——完全双向。对比上一章GPT的下三角矩阵，差异一目了然。</span>
<span id="cb5-404"><a href="#cb5-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-405"><a href="#cb5-405" aria-hidden="true" tabindex="-1"></a>这意味着在处理<span class="in">`[MASK]`</span>位置（原来的"sat"）时，注意力可以**同时**关注左侧的"The cat"和右侧的"on the mat"。假设经过注意力计算后，<span class="in">`[MASK]`</span>位置的注意力权重为：</span>
<span id="cb5-406"><a href="#cb5-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-407"><a href="#cb5-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-408"><a href="#cb5-408" aria-hidden="true" tabindex="-1"></a>\alpha_{\text{<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>}} = <span class="co">[</span><span class="ot">0.02, 0.08, 0.25, 0.05, 0.18, 0.07, 0.30, 0.05</span><span class="co">]</span></span>
<span id="cb5-409"><a href="#cb5-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-410"><a href="#cb5-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-411"><a href="#cb5-411" aria-hidden="true" tabindex="-1"></a>**解读**：模型在预测<span class="in">`[MASK]`</span>（原词"sat"）时，最关注的是"mat"（0.30）和"cat"（0.25），其次是"on"（0.18）。这非常合理——"cat"提供了主语信息（谁做了这个动作），"on the mat"提供了动作的目标（坐在什么上面），它们共同强烈暗示被遮蔽的词是某个表示位置关系的动词，如"sat"。</span>
<span id="cb5-412"><a href="#cb5-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-413"><a href="#cb5-413" aria-hidden="true" tabindex="-1"></a>**Step 5: 预测被遮蔽的词**</span>
<span id="cb5-414"><a href="#cb5-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-415"><a href="#cb5-415" aria-hidden="true" tabindex="-1"></a>经过12层（实际中）Transformer Encoder后，<span class="in">`[MASK]`</span>位置的最终隐藏状态$h_L^{\text{<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>}}$融合了来自所有方向的丰富信息。将其通过一个线性层映射到词汇表维度，然后softmax：</span>
<span id="cb5-416"><a href="#cb5-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-417"><a href="#cb5-417" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-418"><a href="#cb5-418" aria-hidden="true" tabindex="-1"></a>\text{logits} = h_L^{\text{<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>}} \cdot W_{\text{vocab}}^T + b \quad \in \mathbb{R}^{V}</span>
<span id="cb5-419"><a href="#cb5-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-420"><a href="#cb5-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-421"><a href="#cb5-421" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-422"><a href="#cb5-422" aria-hidden="true" tabindex="-1"></a>P(\cdot \mid \tilde{\mathbf{x}}) = \text{softmax}(\text{logits})</span>
<span id="cb5-423"><a href="#cb5-423" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-424"><a href="#cb5-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-425"><a href="#cb5-425" aria-hidden="true" tabindex="-1"></a>假设得到的概率分布为：</span>
<span id="cb5-426"><a href="#cb5-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-427"><a href="#cb5-427" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词 <span class="pp">|</span> sat <span class="pp">|</span> ran <span class="pp">|</span> slept <span class="pp">|</span> stood <span class="pp">|</span> on <span class="pp">|</span> cat <span class="pp">|</span> ... <span class="pp">|</span></span>
<span id="cb5-428"><a href="#cb5-428" aria-hidden="true" tabindex="-1"></a><span class="pp">|----|-----|-----|-------|-------|----|-----|-----|</span></span>
<span id="cb5-429"><a href="#cb5-429" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 概率 <span class="pp">|</span> **0.42** <span class="pp">|</span> 0.15 <span class="pp">|</span> 0.12 <span class="pp">|</span> 0.10 <span class="pp">|</span> 0.03 <span class="pp">|</span> 0.02 <span class="pp">|</span> ... <span class="pp">|</span></span>
<span id="cb5-430"><a href="#cb5-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-431"><a href="#cb5-431" aria-hidden="true" tabindex="-1"></a>模型给"sat"分配了最高的概率0.42，说明它成功地从双向上下文中推断出了被遮蔽的词。损失为 $-\log(0.42) \approx 0.87$。</span>
<span id="cb5-432"><a href="#cb5-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-433"><a href="#cb5-433" aria-hidden="true" tabindex="-1"></a>**对比GPT**：如果用GPT的因果注意力来预测位置3的词，模型只能看到"The cat"——仅凭这两个词，"sat"、"ran"、"slept"、"ate"都是合理的续写。模型无法利用右侧的"on the mat"来缩小候选范围。BERT的双向注意力让"on the mat"直接参与了预测，显著提高了预测精度。</span>
<span id="cb5-434"><a href="#cb5-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-435"><a href="#cb5-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### 微调：一个模型，多种任务</span></span>
<span id="cb5-436"><a href="#cb5-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-437"><a href="#cb5-437" aria-hidden="true" tabindex="-1"></a>BERT的微调方式延续了GPT的思路——在预训练模型顶部添加一个简单的任务特定层，然后微调整个模型的参数。但得益于双向Encoder的强大表示能力和灵活的输入格式（<span class="in">`[CLS]`</span> + 句对 + <span class="in">`[SEP]`</span>），BERT的微调框架更加统一。</span>
<span id="cb5-438"><a href="#cb5-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-439"><a href="#cb5-439" aria-hidden="true" tabindex="-1"></a>**句子级分类（如情感分析、语义相似度）**。取<span class="in">`[CLS]`</span>的最终表示$h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>} \in \mathbb{R}^H$，通过一个新增的线性层$W \in \mathbb{R}^{K \times H}$映射到$K$个类别：</span>
<span id="cb5-440"><a href="#cb5-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-441"><a href="#cb5-441" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-442"><a href="#cb5-442" aria-hidden="true" tabindex="-1"></a>P(c \mid \mathbf{x}) = \text{softmax}(h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>} \cdot W^T)</span>
<span id="cb5-443"><a href="#cb5-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-444"><a href="#cb5-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-445"><a href="#cb5-445" aria-hidden="true" tabindex="-1"></a>**句对分类（如NLI）**。输入格式为<span class="in">`[CLS] Sentence A [SEP] Sentence B [SEP]`</span>，同样取<span class="in">`[CLS]`</span>的表示做分类。Segment Embedding帮助模型区分两个句子。</span>
<span id="cb5-446"><a href="#cb5-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-447"><a href="#cb5-447" aria-hidden="true" tabindex="-1"></a>**Token级分类（如命名实体识别）**。每个token的最终表示$h_i$分别通过一个共享的线性层做分类：</span>
<span id="cb5-448"><a href="#cb5-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-449"><a href="#cb5-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-450"><a href="#cb5-450" aria-hidden="true" tabindex="-1"></a>P(t_i \mid \mathbf{x}) = \text{softmax}(h_i \cdot W_{\text{tag}}^T) \quad \forall i</span>
<span id="cb5-451"><a href="#cb5-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-452"><a href="#cb5-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-453"><a href="#cb5-453" aria-hidden="true" tabindex="-1"></a>**抽取式问答（如SQuAD）**。输入格式为<span class="in">`[CLS] Question [SEP] Passage [SEP]`</span>。模型需要从Passage中找到答案的起始和结束位置。具体做法是学习两个向量$S, E \in \mathbb{R}^H$，对Passage中每个位置$i$，计算它作为答案起始/结束位置的概率：</span>
<span id="cb5-454"><a href="#cb5-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-455"><a href="#cb5-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-456"><a href="#cb5-456" aria-hidden="true" tabindex="-1"></a>P_{\text{start}}(i) = \frac{e^{S \cdot h_i}}{\sum_j e^{S \cdot h_j}}, \quad P_{\text{end}}(i) = \frac{e^{E \cdot h_i}}{\sum_j e^{E \cdot h_j}}</span>
<span id="cb5-457"><a href="#cb5-457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-458"><a href="#cb5-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-459"><a href="#cb5-459" aria-hidden="true" tabindex="-1"></a>最终答案是使得$P_{\text{start}}(i) \times P_{\text{end}}(j)$最大的合法跨度$(i, j)$（要求$j \geq i$）。</span>
<span id="cb5-460"><a href="#cb5-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-461"><a href="#cb5-461" aria-hidden="true" tabindex="-1"></a>微调时新增的参数量非常少——分类任务只需要一个$K \times H$的线性层（对于BERT-Base和二分类任务，只有$2 \times 768 = 1536$个新参数），问答任务只需要两个$H$维的向量（$2 \times 768 = 1536$个新参数）。与预训练模型的1.1亿参数相比，微调新增的参数几乎可以忽略不计。</span>
<span id="cb5-462"><a href="#cb5-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-463"><a href="#cb5-463" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">BERT在四类下游任务上的微调示意。(a) 句对分类（MNLI、QQP、QNLI等）：取`[CLS]`表示做分类；(b) 单句分类（SST-2、CoLA）：同样取`[CLS]`；(c) 抽取式问答（SQuAD）：预测答案的起始和结束位置；(d) 序列标注（CoNLL NER）：每个token独立分类。</span><span class="co">](figures/chapter-13/original/fig4-finetuning-tasks.png)</span>{#fig-bert-finetune width=85%}</span>
<span id="cb5-464"><a href="#cb5-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-465"><a href="#cb5-465" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb5-466"><a href="#cb5-466" aria-hidden="true" tabindex="-1"></a>*Source: Devlin et al. (2019) "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding", Figure 4. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)*</span>
<span id="cb5-467"><a href="#cb5-467" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-468"><a href="#cb5-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-469"><a href="#cb5-469" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度分析</span></span>
<span id="cb5-470"><a href="#cb5-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-471"><a href="#cb5-471" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> BERT-Base <span class="pp">|</span> GPT-1 <span class="pp">|</span> ELMo <span class="pp">|</span></span>
<span id="cb5-472"><a href="#cb5-472" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|-------|------|</span></span>
<span id="cb5-473"><a href="#cb5-473" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练参数 <span class="pp">|</span> 110M <span class="pp">|</span> 117M <span class="pp">|</span> 93.6M <span class="pp">|</span></span>
<span id="cb5-474"><a href="#cb5-474" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练数据 <span class="pp">|</span> 3.3B词 <span class="pp">|</span> 0.8B词 <span class="pp">|</span> 1B词 <span class="pp">|</span></span>
<span id="cb5-475"><a href="#cb5-475" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练方向 <span class="pp">|</span> 双向 <span class="pp">|</span> 单向（左→右） <span class="pp">|</span> 分离式双向 <span class="pp">|</span></span>
<span id="cb5-476"><a href="#cb5-476" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练信号效率 <span class="pp">|</span> 15%（仅<span class="in">`[MASK]`</span>位置） <span class="pp">|</span> 100%（每个位置） <span class="pp">|</span> 100%（每个位置） <span class="pp">|</span></span>
<span id="cb5-477"><a href="#cb5-477" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 微调新增参数 <span class="pp">|</span> $K \times H$（极少） <span class="pp">|</span> $K \times H$ <span class="pp">|</span> $L + 2$个标量 <span class="pp">|</span></span>
<span id="cb5-478"><a href="#cb5-478" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 推理复杂度 <span class="pp">|</span> $O(n^2 \cdot d)$ <span class="pp">|</span> $O(n^2 \cdot d)$ <span class="pp">|</span> $O(n \cdot d^2)$ <span class="pp">|</span></span>
<span id="cb5-479"><a href="#cb5-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-480"><a href="#cb5-480" aria-hidden="true" tabindex="-1"></a>一个值得注意的权衡是预训练信号效率。GPT在每个位置都做预测（预测下一个词），每个token都贡献损失；BERT只在被遮蔽的15%的位置做预测。这意味着BERT的每个训练步提供的信号量只有GPT的约$1/7$。BERT通过更大的数据集（3.3B vs 0.8B词）和更长的训练时间（1M步 vs ~100个epoch）来弥补这个效率差距。</span>
<span id="cb5-481"><a href="#cb5-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-482"><a href="#cb5-482" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-483"><a href="#cb5-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-484"><a href="#cb5-484" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb5-485"><a href="#cb5-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-486"><a href="#cb5-486" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Hugging Face微调BERT</span></span>
<span id="cb5-487"><a href="#cb5-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-488"><a href="#cb5-488" aria-hidden="true" tabindex="-1"></a>下面展示在情感分析任务上微调BERT的标准工作流：</span>
<span id="cb5-489"><a href="#cb5-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-492"><a href="#cb5-492" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-493"><a href="#cb5-493" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb5-494"><a href="#cb5-494" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb5-495"><a href="#cb5-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-496"><a href="#cb5-496" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification</span>
<span id="cb5-497"><a href="#cb5-497" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer, TrainingArguments</span>
<span id="cb5-498"><a href="#cb5-498" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-499"><a href="#cb5-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-500"><a href="#cb5-500" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的 BERT-Base 模型和分词器</span></span>
<span id="cb5-501"><a href="#cb5-501" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb5-502"><a href="#cb5-502" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained(</span>
<span id="cb5-503"><a href="#cb5-503" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bert-base-uncased"</span>,</span>
<span id="cb5-504"><a href="#cb5-504" aria-hidden="true" tabindex="-1"></a>    num_labels<span class="op">=</span><span class="dv">2</span>  <span class="co"># 二分类：正面/负面</span></span>
<span id="cb5-505"><a href="#cb5-505" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-506"><a href="#cb5-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-507"><a href="#cb5-507" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备数据</span></span>
<span id="cb5-508"><a href="#cb5-508" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [</span>
<span id="cb5-509"><a href="#cb5-509" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This movie is absolutely fantastic!"</span>,</span>
<span id="cb5-510"><a href="#cb5-510" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Terrible waste of time, awful acting."</span>,</span>
<span id="cb5-511"><a href="#cb5-511" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A masterpiece of modern cinema."</span>,</span>
<span id="cb5-512"><a href="#cb5-512" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I fell asleep halfway through."</span></span>
<span id="cb5-513"><a href="#cb5-513" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-514"><a href="#cb5-514" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 正面=1, 负面=0</span></span>
<span id="cb5-515"><a href="#cb5-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-516"><a href="#cb5-516" aria-hidden="true" tabindex="-1"></a>encodings <span class="op">=</span> tokenizer(</span>
<span id="cb5-517"><a href="#cb5-517" aria-hidden="true" tabindex="-1"></a>    texts,</span>
<span id="cb5-518"><a href="#cb5-518" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-519"><a href="#cb5-519" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-520"><a href="#cb5-520" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb5-521"><a href="#cb5-521" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb5-522"><a href="#cb5-522" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-523"><a href="#cb5-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-524"><a href="#cb5-524" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 Dataset</span></span>
<span id="cb5-525"><a href="#cb5-525" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentDataset(torch.utils.data.Dataset):</span>
<span id="cb5-526"><a href="#cb5-526" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encodings, labels):</span>
<span id="cb5-527"><a href="#cb5-527" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encodings <span class="op">=</span> encodings</span>
<span id="cb5-528"><a href="#cb5-528" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb5-529"><a href="#cb5-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-530"><a href="#cb5-530" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb5-531"><a href="#cb5-531" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> {k: v[idx] <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb5-532"><a href="#cb5-532" aria-hidden="true" tabindex="-1"></a>        item[<span class="st">"labels"</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.labels[idx])</span>
<span id="cb5-533"><a href="#cb5-533" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span>
<span id="cb5-534"><a href="#cb5-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-535"><a href="#cb5-535" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb5-536"><a href="#cb5-536" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.labels)</span>
<span id="cb5-537"><a href="#cb5-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-538"><a href="#cb5-538" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> SentimentDataset(encodings, labels)</span>
<span id="cb5-539"><a href="#cb5-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-540"><a href="#cb5-540" aria-hidden="true" tabindex="-1"></a><span class="co"># 微调配置</span></span>
<span id="cb5-541"><a href="#cb5-541" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb5-542"><a href="#cb5-542" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./bert-sentiment"</span>,</span>
<span id="cb5-543"><a href="#cb5-543" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-544"><a href="#cb5-544" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb5-545"><a href="#cb5-545" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,     <span class="co"># BERT 微调推荐: 2e-5 到 5e-5</span></span>
<span id="cb5-546"><a href="#cb5-546" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb5-547"><a href="#cb5-547" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb5-548"><a href="#cb5-548" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb5-549"><a href="#cb5-549" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-550"><a href="#cb5-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-551"><a href="#cb5-551" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb5-552"><a href="#cb5-552" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb5-553"><a href="#cb5-553" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb5-554"><a href="#cb5-554" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset,</span>
<span id="cb5-555"><a href="#cb5-555" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-556"><a href="#cb5-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-557"><a href="#cb5-557" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb5-558"><a href="#cb5-558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-559"><a href="#cb5-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-560"><a href="#cb5-560" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现BERT的MLM预训练头</span></span>
<span id="cb5-561"><a href="#cb5-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-562"><a href="#cb5-562" aria-hidden="true" tabindex="-1"></a>为了深入理解MLM的工作机制，下面是MLM预训练头的简化实现：</span>
<span id="cb5-563"><a href="#cb5-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-566"><a href="#cb5-566" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-567"><a href="#cb5-567" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb5-568"><a href="#cb5-568" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb5-569"><a href="#cb5-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-570"><a href="#cb5-570" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-571"><a href="#cb5-571" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-572"><a href="#cb5-572" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-573"><a href="#cb5-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-574"><a href="#cb5-574" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BertMLMHead(nn.Module):</span>
<span id="cb5-575"><a href="#cb5-575" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""BERT 的掩码语言模型预训练头"""</span></span>
<span id="cb5-576"><a href="#cb5-576" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size, vocab_size):</span>
<span id="cb5-577"><a href="#cb5-577" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-578"><a href="#cb5-578" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 与论文一致：先过一个全连接层 + GELU + LayerNorm</span></span>
<span id="cb5-579"><a href="#cb5-579" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb5-580"><a href="#cb5-580" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gelu <span class="op">=</span> nn.GELU()</span>
<span id="cb5-581"><a href="#cb5-581" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(hidden_size)</span>
<span id="cb5-582"><a href="#cb5-582" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 映射到词汇表大小</span></span>
<span id="cb5-583"><a href="#cb5-583" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Linear(hidden_size, vocab_size)</span>
<span id="cb5-584"><a href="#cb5-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-585"><a href="#cb5-585" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states, masked_positions):</span>
<span id="cb5-586"><a href="#cb5-586" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-587"><a href="#cb5-587" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-588"><a href="#cb5-588" aria-hidden="true" tabindex="-1"></a><span class="co">            hidden_states: Encoder输出 [batch, seq_len, hidden]</span></span>
<span id="cb5-589"><a href="#cb5-589" aria-hidden="true" tabindex="-1"></a><span class="co">            masked_positions: 被遮蔽的位置索引 [batch, num_masked]</span></span>
<span id="cb5-590"><a href="#cb5-590" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-591"><a href="#cb5-591" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 只取被遮蔽位置的隐藏状态</span></span>
<span id="cb5-592"><a href="#cb5-592" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> hidden_states.size(<span class="dv">0</span>)</span>
<span id="cb5-593"><a href="#cb5-593" aria-hidden="true" tabindex="-1"></a>        num_masked <span class="op">=</span> masked_positions.size(<span class="dv">1</span>)</span>
<span id="cb5-594"><a href="#cb5-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-595"><a href="#cb5-595" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 提取被遮蔽位置的表示</span></span>
<span id="cb5-596"><a href="#cb5-596" aria-hidden="true" tabindex="-1"></a>        masked_hidden <span class="op">=</span> torch.gather(</span>
<span id="cb5-597"><a href="#cb5-597" aria-hidden="true" tabindex="-1"></a>            hidden_states, <span class="dv">1</span>,</span>
<span id="cb5-598"><a href="#cb5-598" aria-hidden="true" tabindex="-1"></a>            masked_positions.unsqueeze(<span class="op">-</span><span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, hidden_states.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-599"><a href="#cb5-599" aria-hidden="true" tabindex="-1"></a>        )  <span class="co"># [batch, num_masked, hidden]</span></span>
<span id="cb5-600"><a href="#cb5-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-601"><a href="#cb5-601" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 预测原始 token</span></span>
<span id="cb5-602"><a href="#cb5-602" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.dense(masked_hidden)</span>
<span id="cb5-603"><a href="#cb5-603" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.gelu(h)</span>
<span id="cb5-604"><a href="#cb5-604" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.layer_norm(h)</span>
<span id="cb5-605"><a href="#cb5-605" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.decoder(h)  <span class="co"># [batch, num_masked, vocab_size]</span></span>
<span id="cb5-606"><a href="#cb5-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-607"><a href="#cb5-607" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb5-608"><a href="#cb5-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-609"><a href="#cb5-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-610"><a href="#cb5-610" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_mlm_batch(tokens, vocab_size, mask_token_id, mask_prob<span class="op">=</span><span class="fl">0.15</span>):</span>
<span id="cb5-611"><a href="#cb5-611" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-612"><a href="#cb5-612" aria-hidden="true" tabindex="-1"></a><span class="co">    创建 MLM 训练样本：实现 80-10-10 策略</span></span>
<span id="cb5-613"><a href="#cb5-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-614"><a href="#cb5-614" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-615"><a href="#cb5-615" aria-hidden="true" tabindex="-1"></a><span class="co">        tokens: 原始 token ids [batch, seq_len]</span></span>
<span id="cb5-616"><a href="#cb5-616" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_size: 词汇表大小</span></span>
<span id="cb5-617"><a href="#cb5-617" aria-hidden="true" tabindex="-1"></a><span class="co">        mask_token_id: [MASK] 的 token id</span></span>
<span id="cb5-618"><a href="#cb5-618" aria-hidden="true" tabindex="-1"></a><span class="co">        mask_prob: 遮蔽概率（默认 15%）</span></span>
<span id="cb5-619"><a href="#cb5-619" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-620"><a href="#cb5-620" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> tokens.clone()</span>
<span id="cb5-621"><a href="#cb5-621" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> tokens.clone()</span>
<span id="cb5-622"><a href="#cb5-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-623"><a href="#cb5-623" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 随机选择 15% 的位置</span></span>
<span id="cb5-624"><a href="#cb5-624" aria-hidden="true" tabindex="-1"></a>    probability_matrix <span class="op">=</span> torch.full(tokens.shape, mask_prob)</span>
<span id="cb5-625"><a href="#cb5-625" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 特殊 token（[CLS], [SEP], [PAD]）不遮蔽</span></span>
<span id="cb5-626"><a href="#cb5-626" aria-hidden="true" tabindex="-1"></a>    special_tokens_mask <span class="op">=</span> (tokens <span class="op">==</span> <span class="dv">0</span>) <span class="op">|</span> (tokens <span class="op">==</span> <span class="dv">101</span>) <span class="op">|</span> (tokens <span class="op">==</span> <span class="dv">102</span>)</span>
<span id="cb5-627"><a href="#cb5-627" aria-hidden="true" tabindex="-1"></a>    probability_matrix.masked_fill_(special_tokens_mask, value<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb5-628"><a href="#cb5-628" aria-hidden="true" tabindex="-1"></a>    masked_indices <span class="op">=</span> torch.bernoulli(probability_matrix).<span class="bu">bool</span>()</span>
<span id="cb5-629"><a href="#cb5-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-630"><a href="#cb5-630" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 未被遮蔽的位置不参与损失计算</span></span>
<span id="cb5-631"><a href="#cb5-631" aria-hidden="true" tabindex="-1"></a>    labels[<span class="op">~</span>masked_indices] <span class="op">=</span> <span class="op">-</span><span class="dv">100</span>  <span class="co"># CrossEntropy 会忽略 -100</span></span>
<span id="cb5-632"><a href="#cb5-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-633"><a href="#cb5-633" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 80-10-10 策略</span></span>
<span id="cb5-634"><a href="#cb5-634" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 80% 替换为 [MASK]</span></span>
<span id="cb5-635"><a href="#cb5-635" aria-hidden="true" tabindex="-1"></a>    indices_replaced <span class="op">=</span> (</span>
<span id="cb5-636"><a href="#cb5-636" aria-hidden="true" tabindex="-1"></a>        torch.bernoulli(torch.full(tokens.shape, <span class="fl">0.8</span>)).<span class="bu">bool</span>()</span>
<span id="cb5-637"><a href="#cb5-637" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;</span> masked_indices</span>
<span id="cb5-638"><a href="#cb5-638" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-639"><a href="#cb5-639" aria-hidden="true" tabindex="-1"></a>    input_ids[indices_replaced] <span class="op">=</span> mask_token_id</span>
<span id="cb5-640"><a href="#cb5-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-641"><a href="#cb5-641" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 10% 替换为随机 token</span></span>
<span id="cb5-642"><a href="#cb5-642" aria-hidden="true" tabindex="-1"></a>    indices_random <span class="op">=</span> (</span>
<span id="cb5-643"><a href="#cb5-643" aria-hidden="true" tabindex="-1"></a>        torch.bernoulli(torch.full(tokens.shape, <span class="fl">0.5</span>)).<span class="bu">bool</span>()</span>
<span id="cb5-644"><a href="#cb5-644" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;</span> masked_indices</span>
<span id="cb5-645"><a href="#cb5-645" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;</span> <span class="op">~</span>indices_replaced</span>
<span id="cb5-646"><a href="#cb5-646" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-647"><a href="#cb5-647" aria-hidden="true" tabindex="-1"></a>    random_words <span class="op">=</span> torch.randint(vocab_size, tokens.shape, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb5-648"><a href="#cb5-648" aria-hidden="true" tabindex="-1"></a>    input_ids[indices_random] <span class="op">=</span> random_words[indices_random]</span>
<span id="cb5-649"><a href="#cb5-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-650"><a href="#cb5-650" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 剩余 10% 保持不变（input_ids 已经是原始值）</span></span>
<span id="cb5-651"><a href="#cb5-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-652"><a href="#cb5-652" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> input_ids, labels</span>
<span id="cb5-653"><a href="#cb5-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-654"><a href="#cb5-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-655"><a href="#cb5-655" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复现论文的关键细节</span></span>
<span id="cb5-656"><a href="#cb5-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-657"><a href="#cb5-657" aria-hidden="true" tabindex="-1"></a>如果你要复现BERT的原始实验，以下几个容易被忽略的细节值得注意。</span>
<span id="cb5-658"><a href="#cb5-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-659"><a href="#cb5-659" aria-hidden="true" tabindex="-1"></a>**两阶段训练策略**。BERT的预训练分为两个阶段：第一阶段使用序列长度128训练90%的步数（约900K步），第二阶段使用序列长度512训练剩余10%的步数（约100K步）。这个设计的原因是注意力计算的复杂度为$O(n^2)$——用长度128训练大部分步数可以显著加速训练，然后用长度512训练少量步数让模型适应长序列。</span>
<span id="cb5-660"><a href="#cb5-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-661"><a href="#cb5-661" aria-hidden="true" tabindex="-1"></a>**Whole Word Masking**。原始BERT在子词（WordPiece token）级别做随机遮蔽，这意味着一个完整的词可能只有部分子词被遮蔽。例如"playing"可能被分为"play"和"##ing"两个子词，如果只遮蔽了"##ing"，模型很容易从"play"推断出答案。后来的改进版BERT采用了**整词遮蔽（Whole Word Masking, WWM）**——如果一个词的任何子词被选中，则该词的所有子词都被遮蔽。这迫使模型进行更深层的语义推理，而不是利用子词之间的拼写线索。</span>
<span id="cb5-662"><a href="#cb5-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-663"><a href="#cb5-663" aria-hidden="true" tabindex="-1"></a>**微调超参数的敏感性**。Devlin等人推荐的微调超参数范围是：学习率$<span class="sc">\{</span>2 \times 10^{-5}, 3 \times 10^{-5}, 5 \times 10^{-5}<span class="sc">\}</span>$，batch size$<span class="sc">\{</span>16, 32<span class="sc">\}</span>$，训练$<span class="sc">\{</span>2, 3, 4<span class="sc">\}</span>$个epoch。他们报告说，在小数据集（如MRPC，3,668条训练样本）上，不同的超参数设置可能导致结果差异很大，因此建议进行网格搜索并使用开发集选择最佳配置。</span>
<span id="cb5-664"><a href="#cb5-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-665"><a href="#cb5-665" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实验结果</span></span>
<span id="cb5-666"><a href="#cb5-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-667"><a href="#cb5-667" aria-hidden="true" tabindex="-1"></a>BERT在发布时在11个NLP基准任务上取得了最佳成绩，全面超越了GPT和ELMo：</span>
<span id="cb5-668"><a href="#cb5-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-669"><a href="#cb5-669" aria-hidden="true" tabindex="-1"></a>**GLUE基准**（General Language Understanding Evaluation）：</span>
<span id="cb5-670"><a href="#cb5-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-671"><a href="#cb5-671" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> 类型 <span class="pp">|</span> GPT <span class="pp">|</span> BERT-Base <span class="pp">|</span> BERT-Large <span class="pp">|</span></span>
<span id="cb5-672"><a href="#cb5-672" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|-----|-----------|------------|</span></span>
<span id="cb5-673"><a href="#cb5-673" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MNLI <span class="pp">|</span> 文本蕴含 <span class="pp">|</span> 82.1 <span class="pp">|</span> 84.6 <span class="pp">|</span> **86.7** <span class="pp">|</span></span>
<span id="cb5-674"><a href="#cb5-674" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> QQP <span class="pp">|</span> 释义检测 <span class="pp">|</span> 70.3 <span class="pp">|</span> 71.2 <span class="pp">|</span> **72.1** <span class="pp">|</span></span>
<span id="cb5-675"><a href="#cb5-675" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> QNLI <span class="pp">|</span> 问答蕴含 <span class="pp">|</span> 88.1 <span class="pp">|</span> 90.5 <span class="pp">|</span> **92.7** <span class="pp">|</span></span>
<span id="cb5-676"><a href="#cb5-676" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SST-2 <span class="pp">|</span> 情感分析 <span class="pp">|</span> 91.3 <span class="pp">|</span> 93.5 <span class="pp">|</span> **94.9** <span class="pp">|</span></span>
<span id="cb5-677"><a href="#cb5-677" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CoLA <span class="pp">|</span> 语言可接受性 <span class="pp">|</span> 45.4 <span class="pp">|</span> 52.1 <span class="pp">|</span> **60.5** <span class="pp">|</span></span>
<span id="cb5-678"><a href="#cb5-678" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> STS-B <span class="pp">|</span> 语义相似度 <span class="pp">|</span> — <span class="pp">|</span> 85.8 <span class="pp">|</span> **86.5** <span class="pp">|</span></span>
<span id="cb5-679"><a href="#cb5-679" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MRPC <span class="pp">|</span> 释义检测 <span class="pp">|</span> 82.3 <span class="pp">|</span> 88.9 <span class="pp">|</span> **89.3** <span class="pp">|</span></span>
<span id="cb5-680"><a href="#cb5-680" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RTE <span class="pp">|</span> 文本蕴含 <span class="pp">|</span> 56.0 <span class="pp">|</span> 66.4 <span class="pp">|</span> **70.1** <span class="pp">|</span></span>
<span id="cb5-681"><a href="#cb5-681" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **平均** | | 72.8 | 79.6 | **82.1** <span class="pp">|</span></span>
<span id="cb5-682"><a href="#cb5-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-683"><a href="#cb5-683" aria-hidden="true" tabindex="-1"></a>**SQuAD问答**：</span>
<span id="cb5-684"><a href="#cb5-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-685"><a href="#cb5-685" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> 之前SOTA <span class="pp">|</span> BERT-Large <span class="pp">|</span> 提升 <span class="pp">|</span></span>
<span id="cb5-686"><a href="#cb5-686" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|------------|------|</span></span>
<span id="cb5-687"><a href="#cb5-687" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SQuAD v1.1 (F1) <span class="pp">|</span> 91.6 <span class="pp">|</span> **93.2** <span class="pp">|</span> +1.6 <span class="pp">|</span></span>
<span id="cb5-688"><a href="#cb5-688" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SQuAD v2.0 (F1) <span class="pp">|</span> 66.3 <span class="pp">|</span> **83.1** <span class="pp">|</span> +16.8 <span class="pp">|</span></span>
<span id="cb5-689"><a href="#cb5-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-690"><a href="#cb5-690" aria-hidden="true" tabindex="-1"></a>**SWAG常识推理**：</span>
<span id="cb5-691"><a href="#cb5-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-692"><a href="#cb5-692" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 指标 <span class="pp">|</span> 之前SOTA <span class="pp">|</span> BERT-Large <span class="pp">|</span> 提升 <span class="pp">|</span></span>
<span id="cb5-693"><a href="#cb5-693" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|------------|------|</span></span>
<span id="cb5-694"><a href="#cb5-694" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Accuracy <span class="pp">|</span> 65.2 <span class="pp">|</span> **86.3** <span class="pp">|</span> +21.1 <span class="pp">|</span></span>
<span id="cb5-695"><a href="#cb5-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-696"><a href="#cb5-696" aria-hidden="true" tabindex="-1"></a>几个结果值得特别关注。首先，BERT-Base（110M参数）在几乎所有任务上都优于GPT-1（117M参数），而两者的参数量几乎相同。这清楚地表明，**双向预训练（Encoder + MLM）相比单向预训练（Decoder + CLM）在理解任务上有本质优势**。其次，BERT-Large的GLUE平均分比BERT-Base高出2.5个百分点，显示了增大模型规模带来的收益。最引人注目的是SQuAD v2.0上的巨大提升（+16.8%）——这个任务要求模型不仅能找到答案，还要判断问题是否无法回答，需要深层的阅读理解能力。</span>
<span id="cb5-697"><a href="#cb5-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-698"><a href="#cb5-698" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-699"><a href="#cb5-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-700"><a href="#cb5-700" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb5-701"><a href="#cb5-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-702"><a href="#cb5-702" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨BERT为什么有效、NSP的争议、消融实验的洞察、以及BERT在预训练技术谱系中的位置</span></span>
<span id="cb5-703"><a href="#cb5-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-704"><a href="#cb5-704" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么双向比单向更好？——消融实验的证据</span></span>
<span id="cb5-705"><a href="#cb5-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-706"><a href="#cb5-706" aria-hidden="true" tabindex="-1"></a>Devlin等人在论文中进行了一个关键的消融实验，直接对比了三种预训练策略对下游任务的影响：</span>
<span id="cb5-707"><a href="#cb5-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-708"><a href="#cb5-708" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 预训练方向 <span class="pp">|</span> MNLI <span class="pp">|</span> MRPC <span class="pp">|</span> QNLI <span class="pp">|</span> SST-2 <span class="pp">|</span></span>
<span id="cb5-709"><a href="#cb5-709" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|------|------|------|-------|</span></span>
<span id="cb5-710"><a href="#cb5-710" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT-Base <span class="pp">|</span> 双向（MLM） <span class="pp">|</span> **84.4** | **88.0** | **90.7** | **92.7** <span class="pp">|</span></span>
<span id="cb5-711"><a href="#cb5-711" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Left-to-Right (LTR) <span class="pp">|</span> 单向（左→右） <span class="pp">|</span> 82.3 <span class="pp">|</span> 84.7 <span class="pp">|</span> 87.7 <span class="pp">|</span> 91.6 <span class="pp">|</span></span>
<span id="cb5-712"><a href="#cb5-712" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LTR + BiLSTM <span class="pp">|</span> 单向 + 双向LSTM头 <span class="pp">|</span> 82.1 <span class="pp">|</span> 85.8 <span class="pp">|</span> 86.9 <span class="pp">|</span> 91.6 <span class="pp">|</span></span>
<span id="cb5-713"><a href="#cb5-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-714"><a href="#cb5-714" aria-hidden="true" tabindex="-1"></a>这个实验的设计非常精妙。"Left-to-Right"模型使用与BERT完全相同的架构（12层Transformer）和数据，但将MLM替换为标准的从左到右语言建模（与GPT类似）。"LTR + BiLSTM"在LTR模型的顶部加了一个双向LSTM层，试图在微调阶段引入一些双向性。</span>
<span id="cb5-715"><a href="#cb5-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-716"><a href="#cb5-716" aria-hidden="true" tabindex="-1"></a>结果清楚地表明，双向预训练（MLM）在所有任务上都优于单向预训练（LTR），差距在MNLI上约2个百分点，在MRPC上约3个百分点，在QNLI上约3个百分点。更有意思的是，在LTR模型顶部加BiLSTM层**并没有帮助**——在QNLI上甚至更差了。这说明**双向性需要在预训练阶段就建立起来，仅在微调阶段引入双向性是不够的**。预训练时的单向表示已经"定型"了，微调阶段的浅层双向处理无法弥补这个根本缺陷。</span>
<span id="cb5-717"><a href="#cb5-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-718"><a href="#cb5-718" aria-hidden="true" tabindex="-1"></a><span class="fu">### NSP的争议：它真的有用吗？</span></span>
<span id="cb5-719"><a href="#cb5-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-720"><a href="#cb5-720" aria-hidden="true" tabindex="-1"></a>BERT的NSP目标在发表时被认为对句子对任务（如NLI和QA）至关重要。Devlin等人的消融实验也支持了这一点——去掉NSP后，QNLI下降了3.5个百分点，MNLI下降了0.3个百分点。</span>
<span id="cb5-721"><a href="#cb5-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-722"><a href="#cb5-722" aria-hidden="true" tabindex="-1"></a>然而，2019年的后续研究对NSP提出了越来越多的质疑。RoBERTa（Liu et al., 2019）进行了迄今为止最系统的消融分析，对比了四种设定：</span>
<span id="cb5-723"><a href="#cb5-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-724"><a href="#cb5-724" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 设定 <span class="pp">|</span> 描述 <span class="pp">|</span> SQuAD <span class="pp">|</span> MNLI <span class="pp">|</span> SST-2 <span class="pp">|</span> RACE <span class="pp">|</span></span>
<span id="cb5-725"><a href="#cb5-725" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|-------|------|-------|------|</span></span>
<span id="cb5-726"><a href="#cb5-726" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Segment-Pair + NSP <span class="pp">|</span> BERT原始设定 <span class="pp">|</span> 90.4 <span class="pp">|</span> 84.0 <span class="pp">|</span> 92.9 <span class="pp">|</span> 64.2 <span class="pp">|</span></span>
<span id="cb5-727"><a href="#cb5-727" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Sentence-Pair + NSP <span class="pp">|</span> 用真实句子（非段落） <span class="pp">|</span> 84.7 <span class="pp">|</span> 82.9 <span class="pp">|</span> 92.6 <span class="pp">|</span> 63.0 <span class="pp">|</span></span>
<span id="cb5-728"><a href="#cb5-728" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Full-Sentences (无NSP) <span class="pp">|</span> 连续文本，无NSP <span class="pp">|</span> **90.4** | **84.7** | **93.3** | **64.8** <span class="pp">|</span></span>
<span id="cb5-729"><a href="#cb5-729" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Doc-Sentences (无NSP) <span class="pp">|</span> 单文档连续文本 <span class="pp">|</span> 90.6 <span class="pp">|</span> 84.7 <span class="pp">|</span> 92.7 <span class="pp">|</span> 65.6 <span class="pp">|</span></span>
<span id="cb5-730"><a href="#cb5-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-731"><a href="#cb5-731" aria-hidden="true" tabindex="-1"></a>RoBERTa发现，**去掉NSP并使用全长度的连续文本反而能提高性能**。这个反直觉的结果可以从几个角度理解。</span>
<span id="cb5-732"><a href="#cb5-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-733"><a href="#cb5-733" aria-hidden="true" tabindex="-1"></a>第一，NSP任务可能太"简单"了。在BERT的数据构造中，负样本（NotNext）是从不同文档中随机抽取的句子。来自不同文档的句子往往在主题和风格上有明显差异，模型可能只是学会了判断"这两个句子是否来自同一篇文章"（主题匹配），而不是真正理解句子间的逻辑关系。这种"捷径学习"提供的语义信号有限。</span>
<span id="cb5-734"><a href="#cb5-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-735"><a href="#cb5-735" aria-hidden="true" tabindex="-1"></a>第二，NSP损坏了MLM的数据效率。为了构造句对，BERT必须将512 token的序列切分为两个较短的句子（通常远短于256 token）。这意味着每个训练样本中实际的文本长度减少了，MLM的学习效率降低。RoBERTa的"Full-Sentences"设定使用跨文档的连续文本填满512 token，最大化了每个样本中的MLM训练信号。</span>
<span id="cb5-736"><a href="#cb5-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-737"><a href="#cb5-737" aria-hidden="true" tabindex="-1"></a>第三，句子级关系的学习可能不需要一个显式的预训练目标。双向Encoder在处理<span class="in">`[CLS] Sentence A [SEP] Sentence B [SEP]`</span>格式的微调输入时，通过注意力机制自然地建立了句子间的交互——模型可以在微调阶段直接学习句子间关系，不需要预训练时的预热。</span>
<span id="cb5-738"><a href="#cb5-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-739"><a href="#cb5-739" aria-hidden="true" tabindex="-1"></a>这个发现深刻地影响了后续的预训练模型设计。ALBERT用句子顺序预测（Sentence Order Prediction, SOP）替代了NSP，RoBERTa直接去掉了NSP，后来的大多数预训练模型都没有采用NSP。</span>
<span id="cb5-740"><a href="#cb5-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-741"><a href="#cb5-741" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型规模的影响</span></span>
<span id="cb5-742"><a href="#cb5-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-743"><a href="#cb5-743" aria-hidden="true" tabindex="-1"></a>Devlin等人的消融实验也揭示了模型规模对性能的影响：</span>
<span id="cb5-744"><a href="#cb5-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-745"><a href="#cb5-745" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 层数 <span class="pp">|</span> 隐藏维度 <span class="pp">|</span> 参数量 <span class="pp">|</span> MNLI <span class="pp">|</span> MRPC <span class="pp">|</span> SST-2 <span class="pp">|</span></span>
<span id="cb5-746"><a href="#cb5-746" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|---------|--------|------|------|-------|</span></span>
<span id="cb5-747"><a href="#cb5-747" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT-Base <span class="pp">|</span> 12 <span class="pp">|</span> 768 <span class="pp">|</span> 110M <span class="pp">|</span> 84.4 <span class="pp">|</span> 88.0 <span class="pp">|</span> 92.7 <span class="pp">|</span></span>
<span id="cb5-748"><a href="#cb5-748" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT-Large <span class="pp">|</span> 24 <span class="pp">|</span> 1024 <span class="pp">|</span> 340M <span class="pp">|</span> **86.6** | **89.2** | **93.2** <span class="pp">|</span></span>
<span id="cb5-749"><a href="#cb5-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-750"><a href="#cb5-750" aria-hidden="true" tabindex="-1"></a>从BERT-Base到BERT-Large，参数量增加了约3倍，MNLI提升了2.2个百分点，MRPC提升了1.2个百分点。这个结果表明，增大模型规模在"预训练 + 微调"范式下仍然有效，为后来的规模化探索（GPT-2、GPT-3、第17章的Scaling Laws）提供了早期证据。</span>
<span id="cb5-751"><a href="#cb5-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-752"><a href="#cb5-752" aria-hidden="true" tabindex="-1"></a>一个有趣的细节是，BERT-Large在小数据集上的表现不稳定。Devlin等人报告，BERT-Large在某些小数据集上的微调需要更多的随机重启（random restarts）才能获得稳定的结果。这暗示了一个后来被广泛研究的问题：**大模型在小数据集上更容易过拟合**，微调的稳定性需要额外的技巧（如更小的学习率、更多的warmup步数、或使用参数高效微调方法如LoRA）。</span>
<span id="cb5-753"><a href="#cb5-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-754"><a href="#cb5-754" aria-hidden="true" tabindex="-1"></a><span class="fu">### 微调 vs 特征提取</span></span>
<span id="cb5-755"><a href="#cb5-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-756"><a href="#cb5-756" aria-hidden="true" tabindex="-1"></a>Devlin等人还对比了BERT作为特征提取器（冻结参数）和微调模型的性能差异，以NER（命名实体识别）任务为例：</span>
<span id="cb5-757"><a href="#cb5-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-758"><a href="#cb5-758" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> F1 <span class="pp">|</span></span>
<span id="cb5-759"><a href="#cb5-759" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----|</span></span>
<span id="cb5-760"><a href="#cb5-760" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 微调（所有参数） <span class="pp">|</span> **96.4** <span class="pp">|</span></span>
<span id="cb5-761"><a href="#cb5-761" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特征提取：拼接最后4层 <span class="pp">|</span> 96.1 <span class="pp">|</span></span>
<span id="cb5-762"><a href="#cb5-762" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特征提取：加权求和最后4层 <span class="pp">|</span> 95.9 <span class="pp">|</span></span>
<span id="cb5-763"><a href="#cb5-763" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特征提取：仅最后一层 <span class="pp">|</span> 94.9 <span class="pp">|</span></span>
<span id="cb5-764"><a href="#cb5-764" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特征提取：倒数第二层 <span class="pp">|</span> 95.6 <span class="pp">|</span></span>
<span id="cb5-765"><a href="#cb5-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-766"><a href="#cb5-766" aria-hidden="true" tabindex="-1"></a>结果显示，微调优于特征提取，但差距不大（0.3个百分点）。特征提取方式中，拼接最后4层效果最好，接近微调的性能。这说明BERT的预训练表示已经非常丰富——即使不做微调，直接提取特征也能获得有竞争力的结果。</span>
<span id="cb5-767"><a href="#cb5-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-768"><a href="#cb5-768" aria-hidden="true" tabindex="-1"></a>这个发现对实际应用有重要的工程意义。在某些场景下（比如需要同时运行多个下游任务，或者计算资源有限），使用BERT作为固定的特征提取器可以大幅降低计算成本（不需要为每个任务微调一份完整模型），代价只是一点点性能损失。</span>
<span id="cb5-769"><a href="#cb5-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-770"><a href="#cb5-770" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb5-771"><a href="#cb5-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-772"><a href="#cb5-772" aria-hidden="true" tabindex="-1"></a>**假设一：MLM的遮蔽比例是最优的**。15%的遮蔽率是一个经验选择，没有严格的理论依据。后来的研究（如SpanBERT）发现，遮蔽连续的span可能比随机遮蔽单个token更有效，因为它迫使模型进行更高层次的推理。</span>
<span id="cb5-773"><a href="#cb5-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-774"><a href="#cb5-774" aria-hidden="true" tabindex="-1"></a>**假设二：WordPiece分词足够好**。BERT的语言能力受限于其分词器。对于数学表达式（如"12345"可能被切为"123"和"##45"）、代码、非英文文本等，WordPiece的切分方式可能不是最优的。后来的模型如GPT-2/3转向了Byte-level BPE，XLM-RoBERTa使用了SentencePiece，都在不同程度上解决了这些问题。</span>
<span id="cb5-775"><a href="#cb5-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-776"><a href="#cb5-776" aria-hidden="true" tabindex="-1"></a>**假设三：512 token足够长**。BERT的最大序列长度是512 token。对于长文档（如法律文件、学术论文），512 token只能覆盖很小的一部分。后来的Longformer、BigBird等模型通过稀疏注意力将序列长度扩展到4096甚至更长。</span>
<span id="cb5-777"><a href="#cb5-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-778"><a href="#cb5-778" aria-hidden="true" tabindex="-1"></a>**失效条件**。BERT在以下场景中表现不佳：文本生成任务（Encoder没有因果掩码，不适合自回归生成）、需要极长上下文的任务（超过512 token）、需要对两段文本进行非对称处理的任务（如信息检索中的query和document，BERT将它们放在同一个Encoder中，缺乏query-document的层次结构）。</span>
<span id="cb5-779"><a href="#cb5-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-780"><a href="#cb5-780" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题（2018-2019年视角）</span></span>
<span id="cb5-781"><a href="#cb5-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-782"><a href="#cb5-782" aria-hidden="true" tabindex="-1"></a>**MLM的最优设计是什么？** BERT的MLM随机遮蔽单个token，但是否有更好的遮蔽策略？遮蔽连续的span？遮蔽整个词？遮蔽实体？遮蔽比例应该是多少？这些问题催生了SpanBERT、ERNIE等后续工作。</span>
<span id="cb5-783"><a href="#cb5-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-784"><a href="#cb5-784" aria-hidden="true" tabindex="-1"></a>**NSP的替代方案**。既然NSP被证明效果有限，有没有更好的句子级预训练目标？ALBERT后来提出了句子顺序预测（SOP），证明判断两个句子的顺序比判断是否连续更有意义。</span>
<span id="cb5-785"><a href="#cb5-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-786"><a href="#cb5-786" aria-hidden="true" tabindex="-1"></a>**Encoder-only vs Decoder-only vs Encoder-Decoder**。BERT选择了Encoder-only，GPT选择了Decoder-only，哪种架构在根本上更好？T5（2019）尝试了Encoder-Decoder架构，将所有任务统一为Text-to-Text格式。这三种架构的根本权衡是什么？这个问题在第16章中进一步讨论。</span>
<span id="cb5-787"><a href="#cb5-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-788"><a href="#cb5-788" aria-hidden="true" tabindex="-1"></a>**预训练目标与下游任务的关系**。MLM让模型学习token级别的预测能力，但下游任务往往需要句子级别或段落级别的理解。预训练目标的粒度与下游任务的粒度之间存在"鸿沟"。如何设计更好的预训练目标来缩小这个鸿沟？这催生了ELECTRA（替换词检测）、T5（Span Corruption）等一系列创新。</span>
<span id="cb5-789"><a href="#cb5-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-790"><a href="#cb5-790" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-791"><a href="#cb5-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-792"><a href="#cb5-792" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb5-793"><a href="#cb5-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-794"><a href="#cb5-794" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLM的内在效率问题</span></span>
<span id="cb5-795"><a href="#cb5-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-796"><a href="#cb5-796" aria-hidden="true" tabindex="-1"></a>BERT的MLM目标存在一个根本性的效率限制：每个训练样本中，只有被遮蔽的15%的token参与损失计算，其余85%的token虽然参与了前向计算（提供上下文），但不提供直接的训练信号。</span>
<span id="cb5-797"><a href="#cb5-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-798"><a href="#cb5-798" aria-hidden="true" tabindex="-1"></a>对比GPT的因果语言建模——每个位置都预测下一个词，100%的token都贡献训练信号——BERT的信号效率大约只有GPT的$1/7$。这意味着BERT需要更多的训练步数和更多的数据才能达到同等水平的语言理解能力。</span>
<span id="cb5-799"><a href="#cb5-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-800"><a href="#cb5-800" aria-hidden="true" tabindex="-1"></a>ELECTRA（Clark et al., 2020）后来以一种巧妙的方式解决了这个问题：用一个小的生成器生成"伪造"的token替换真实token，然后让BERT判断每个位置的token是"原始的"还是"被替换的"。这样，所有100%的位置都参与了训练，信号效率大幅提升。这是第14章的内容之一。</span>
<span id="cb5-801"><a href="#cb5-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-802"><a href="#cb5-802" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练-微调不一致</span></span>
<span id="cb5-803"><a href="#cb5-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-804"><a href="#cb5-804" aria-hidden="true" tabindex="-1"></a>尽管80-10-10策略试图缓解预训练与微调之间的分布差异，但不一致问题并未被完全解决。在预训练时，模型看到的输入中包含<span class="in">`[MASK]`</span>标记和偶尔的随机替换词；在微调时，输入是干净的自然文本。模型在两种不同的输入分布上工作，这可能导致预训练学到的某些模式在微调时无法完全发挥作用。</span>
<span id="cb5-805"><a href="#cb5-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-806"><a href="#cb5-806" aria-hidden="true" tabindex="-1"></a>XLNet（Yang et al., 2019）后来提出了排列语言建模（Permutation Language Modeling），在保持双向性的同时完全避免了<span class="in">`[MASK]`</span>标记的使用——这是一个理论上更优雅的解决方案，但实现复杂度更高。</span>
<span id="cb5-807"><a href="#cb5-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-808"><a href="#cb5-808" aria-hidden="true" tabindex="-1"></a><span class="fu">### 不适合文本生成</span></span>
<span id="cb5-809"><a href="#cb5-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-810"><a href="#cb5-810" aria-hidden="true" tabindex="-1"></a>BERT的Encoder-only架构天然不适合文本生成。在训练时，Encoder的每个位置都能看到所有其他位置（包括"未来"的token），但在生成时，"未来"的token还不存在。这种训练与推理的不一致使得BERT无法直接用于自回归生成。</span>
<span id="cb5-811"><a href="#cb5-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-812"><a href="#cb5-812" aria-hidden="true" tabindex="-1"></a>虽然存在一些变通方案（如将BERT用于条件生成或迭代式生成），但这些方案要么效果不如专门的自回归模型（GPT系列），要么推理效率很低。后来的T5和BART采用Encoder-Decoder架构，同时保留了编码器的双向性和解码器的生成能力，在一定程度上解决了这个问题。</span>
<span id="cb5-813"><a href="#cb5-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-814"><a href="#cb5-814" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb5-815"><a href="#cb5-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-816"><a href="#cb5-816" aria-hidden="true" tabindex="-1"></a>BERT的局限精确地催生了2019-2020年预训练技术的多个发展方向。</span>
<span id="cb5-817"><a href="#cb5-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-818"><a href="#cb5-818" aria-hidden="true" tabindex="-1"></a>MLM的效率问题和遮蔽策略的不完美催生了**新的预训练目标探索**——XLNet的排列语言建模、ELECTRA的替换词检测、T5的Span Corruption——这些工作从不同角度改进了BERT的预训练机制。这是下一章（第14章：预训练目标的演进）的核心主题。</span>
<span id="cb5-819"><a href="#cb5-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-820"><a href="#cb5-820" aria-hidden="true" tabindex="-1"></a>模型规模和训练策略的限制催生了**训练优化的系统研究**——RoBERTa证明了"训练得更好"比"设计更巧妙的目标"更重要，通过更多数据、更大batch、去掉NSP、动态遮蔽等简单改进，就能大幅超越原始BERT。这是第15章的内容。</span>
<span id="cb5-821"><a href="#cb5-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-822"><a href="#cb5-822" aria-hidden="true" tabindex="-1"></a>Encoder-only的局限催生了**架构选择的深入讨论**——Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么优势？为什么Decoder-only最终在规模化的道路上走得最远？这是第16章的话题。</span>
<span id="cb5-823"><a href="#cb5-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-824"><a href="#cb5-824" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第14章将探讨BERT之后预训练目标的多元演进——XLNet的排列语言建模如何在保持双向性的同时避免</span><span class="in">`[MASK]`</span><span class="at">标记？ELECTRA的替换词检测如何将预训练信号效率从15%提升到100%？T5的Text-to-Text统一框架如何消除任务格式的差异？每一个改进都是对BERT某个具体局限的回应。</span></span>
<span id="cb5-825"><a href="#cb5-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-826"><a href="#cb5-826" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-827"><a href="#cb5-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-828"><a href="#cb5-828" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb5-829"><a href="#cb5-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-830"><a href="#cb5-830" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb5-831"><a href="#cb5-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-832"><a href="#cb5-832" aria-hidden="true" tabindex="-1"></a>这一章我们详细介绍了BERT——第一个真正实现深层双向预训练的Transformer模型。</span>
<span id="cb5-833"><a href="#cb5-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-834"><a href="#cb5-834" aria-hidden="true" tabindex="-1"></a>核心问题是如何让预训练模型在每个位置同时利用左右两侧的上下文。GPT的因果注意力限制了模型只能看到左侧上下文，ELMo的分离式双向让两个方向的信息无法深度融合。</span>
<span id="cb5-835"><a href="#cb5-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-836"><a href="#cb5-836" aria-hidden="true" tabindex="-1"></a>BERT的核心洞察是将预训练任务从"预测下一个词"变为"预测被遮蔽的词"（完形填空）。这个简单的变换解除了双向性与语言建模之间的矛盾——被遮蔽的词不在输入中，因此不存在信息泄漏，模型可以自由地使用双向上下文。</span>
<span id="cb5-837"><a href="#cb5-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-838"><a href="#cb5-838" aria-hidden="true" tabindex="-1"></a>BERT的技术方案包含两个预训练目标：MLM（掩码语言模型，使用80-10-10策略处理被遮蔽的token）和NSP（下一句预测，虽然后来被证明并非必要）。输入表示由Token Embedding + Segment Embedding + Position Embedding三者相加，<span class="in">`[CLS]`</span>作为序列的汇聚表示用于分类任务。</span>
<span id="cb5-839"><a href="#cb5-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-840"><a href="#cb5-840" aria-hidden="true" tabindex="-1"></a>实验结果全面验证了双向预训练的优势。在参数量几乎相同的条件下（BERT-Base 110M vs GPT-1 117M），BERT在GLUE平均分上超出GPT近7个百分点（79.6 vs 72.8）。消融实验直接证明，双向性（MLM）而非其他因素是性能提升的主要来源。</span>
<span id="cb5-841"><a href="#cb5-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-842"><a href="#cb5-842" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb5-843"><a href="#cb5-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-844"><a href="#cb5-844" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 含义 <span class="pp">|</span></span>
<span id="cb5-845"><a href="#cb5-845" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb5-846"><a href="#cb5-846" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\text{Input} = E_{\text{token}} + E_{\text{segment}} + E_{\text{position}}$ <span class="pp">|</span> BERT输入表示 <span class="pp">|</span></span>
<span id="cb5-847"><a href="#cb5-847" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $L_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log P(x_i \mid \tilde{\mathbf{x}})$ <span class="pp">|</span> MLM损失（仅在被遮蔽位置计算） <span class="pp">|</span></span>
<span id="cb5-848"><a href="#cb5-848" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $L = L_{\text{MLM}} + L_{\text{NSP}}$ <span class="pp">|</span> BERT总预训练损失 <span class="pp">|</span></span>
<span id="cb5-849"><a href="#cb5-849" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P(c) = \text{softmax}(h_{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>} \cdot W^T)$ <span class="pp">|</span> 微调时的分类预测 <span class="pp">|</span></span>
<span id="cb5-850"><a href="#cb5-850" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P_{\text{start}}(i) = \text{softmax}(S \cdot h_i)$ <span class="pp">|</span> 问答任务的答案起始位置预测 <span class="pp">|</span></span>
<span id="cb5-851"><a href="#cb5-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-852"><a href="#cb5-852" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb5-853"><a href="#cb5-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-854"><a href="#cb5-854" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** BERT的<span class="in">`[MASK]`</span>标记在预训练时频繁出现，但在微调时从不出现。80-10-10策略如何缓解这个预训练-微调不一致问题？如果将策略改为100-0-0（全部替换为<span class="in">`[MASK]`</span>），你预期会在哪些任务上看到性能下降？</span>
<span id="cb5-855"><a href="#cb5-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-856"><a href="#cb5-856" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 计算BERT-Base的总参数量。12层Transformer Encoder，$H = 768$，$A = 12$，FFN内部维度3072，WordPiece词汇表大小30,522，最大位置512。需要分别计算：Token Embedding、Segment Embedding、Position Embedding、每层的多头注意力、每层的FFN、每层的LayerNorm（两个）、以及MLM头和NSP头的参数。</span>
<span id="cb5-857"><a href="#cb5-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-858"><a href="#cb5-858" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 在SQuAD v1.1数据集上微调BERT-Base，对比以下设置的性能：(a) 标准微调（学习率2e-5）；(b) 使用特征提取（冻结BERT参数，只训练答案预测头）；(c) 仅微调最后4层（冻结前8层）。分析不同程度的微调对性能和训练速度的影响。</span>
<span id="cb5-859"><a href="#cb5-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-860"><a href="#cb5-860" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** BERT的MLM目标每次只预测15%的token，而GPT的CLM目标预测100%的token。假设两个模型在相同大小的数据集上训练相同的步数，BERT"看到"的有效训练信号大约是GPT的$1/7$。你认为BERT如何弥补这个效率差距？更大的数据、更长的训练、还是MLM本身提供了比CLM更"高质量"的信号？</span>
<span id="cb5-861"><a href="#cb5-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-862"><a href="#cb5-862" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[对比分析]** ELMo、GPT和BERT分别代表了预训练技术的三种路线：LSTM+特征提取、单向Transformer+微调、双向Transformer+微调。从表示能力、训练效率、生成能力、扩展性四个维度对比这三种方法，并分析为什么BERT在理解任务上胜出，但GPT路线最终在规模化上走得更远。</span>
<span id="cb5-863"><a href="#cb5-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-864"><a href="#cb5-864" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-865"><a href="#cb5-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-866"><a href="#cb5-866" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb5-867"><a href="#cb5-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-868"><a href="#cb5-868" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb5-869"><a href="#cb5-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-870"><a href="#cb5-870" aria-hidden="true" tabindex="-1"></a>**Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"**。BERT的原始论文。重点阅读：Section 3（BERT模型设计，包括MLM和NSP的详细描述）、Section 5.1（消融实验——NSP的作用、双向性的验证）。可快速浏览：Section 4中各任务的微调细节。<span class="co">[</span><span class="ot">arXiv:1810.04805</span><span class="co">](https://arxiv.org/abs/1810.04805)</span></span>
<span id="cb5-871"><a href="#cb5-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-872"><a href="#cb5-872" aria-hidden="true" tabindex="-1"></a><span class="fu">### 前驱工作</span></span>
<span id="cb5-873"><a href="#cb5-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-874"><a href="#cb5-874" aria-hidden="true" tabindex="-1"></a>**Radford et al. (2018). "Improving Language Understanding by Generative Pre-Training" (GPT)**。上一章的主题。与BERT对比阅读，可以清晰地看到"单向 vs 双向"和"Decoder vs Encoder"两个维度的差异。</span>
<span id="cb5-875"><a href="#cb5-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-876"><a href="#cb5-876" aria-hidden="true" tabindex="-1"></a>**Taylor, W.L. (1953). "Cloze Procedure: A New Tool for Measuring Readability"**。MLM的思想来源。完形填空测试（cloze test）最早由Taylor在1953年提出，用于衡量文本可读性。60多年后，这个语言学测试方法被BERT重新发现并应用于预训练。</span>
<span id="cb5-877"><a href="#cb5-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-878"><a href="#cb5-878" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续改进</span></span>
<span id="cb5-879"><a href="#cb5-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-880"><a href="#cb5-880" aria-hidden="true" tabindex="-1"></a>**Liu, Y. et al. (2019). "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**。通过系统的消融实验发现BERT"训练不足"，去掉NSP、使用动态遮蔽、更多数据、更大batch即可显著提升性能。这是对BERT最重要的工程优化工作。<span class="co">[</span><span class="ot">arXiv:1907.11692</span><span class="co">](https://arxiv.org/abs/1907.11692)</span></span>
<span id="cb5-881"><a href="#cb5-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-882"><a href="#cb5-882" aria-hidden="true" tabindex="-1"></a>**Lan, Z. et al. (2020). "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"**。通过跨层参数共享和Embedding分解大幅减少参数量，同时引入SOP替代NSP。<span class="co">[</span><span class="ot">arXiv:1909.11942</span><span class="co">](https://arxiv.org/abs/1909.11942)</span></span>
<span id="cb5-883"><a href="#cb5-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-884"><a href="#cb5-884" aria-hidden="true" tabindex="-1"></a>**Joshi, M. et al. (2020). "SpanBERT: Improving Pre-training by Representing and Predicting Spans"**。用连续span遮蔽替代随机token遮蔽，并去掉NSP，在多项任务上超越BERT。<span class="co">[</span><span class="ot">arXiv:1907.10529</span><span class="co">](https://arxiv.org/abs/1907.10529)</span></span>
<span id="cb5-885"><a href="#cb5-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-886"><a href="#cb5-886" aria-hidden="true" tabindex="-1"></a><span class="fu">### 挑战/质疑</span></span>
<span id="cb5-887"><a href="#cb5-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-888"><a href="#cb5-888" aria-hidden="true" tabindex="-1"></a>**Yang, Z. et al. (2019). "XLNet: Generalized Autoregressive Pretraining for Language Understanding"**。指出BERT的<span class="in">`[MASK]`</span>标记带来了预训练-微调不一致问题，提出排列语言建模作为替代方案。<span class="co">[</span><span class="ot">arXiv:1906.08237</span><span class="co">](https://arxiv.org/abs/1906.08237)</span></span>
<span id="cb5-889"><a href="#cb5-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-890"><a href="#cb5-890" aria-hidden="true" tabindex="-1"></a>**Clark, K. et al. (2020). "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"**。指出BERT的MLM信号效率低下（仅15%），提出替换词检测目标实现100%的信号利用。<span class="co">[</span><span class="ot">arXiv:2003.10555</span><span class="co">](https://arxiv.org/abs/2003.10555)</span></span>
<span id="cb5-891"><a href="#cb5-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-892"><a href="#cb5-892" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb5-893"><a href="#cb5-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-894"><a href="#cb5-894" aria-hidden="true" tabindex="-1"></a>**Rogers, A., Kovaleva, O., &amp; Rumshisky, A. (2020). "A Primer in BERTology: What We Know About How BERT Works"**。对BERT内部工作机制的全面综述——不同层学到了什么？不同Head在做什么？BERT的知识存储在哪里？<span class="co">[</span><span class="ot">arXiv:2002.12327</span><span class="co">](https://arxiv.org/abs/2002.12327)</span></span>
<span id="cb5-895"><a href="#cb5-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-896"><a href="#cb5-896" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb5-897"><a href="#cb5-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-898"><a href="#cb5-898" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Transformers**：<span class="co">[</span><span class="ot">huggingface.co/bert-base-uncased</span><span class="co">](https://huggingface.co/bert-base-uncased)</span> — 预训练BERT模型，支持直接微调</span>
<span id="cb5-899"><a href="#cb5-899" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Google Research BERT**：<span class="co">[</span><span class="ot">github.com/google-research/bert</span><span class="co">](https://github.com/google-research/bert)</span> — 官方TensorFlow实现</span>
<span id="cb5-900"><a href="#cb5-900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L BERT实现**：<span class="co">[</span><span class="ot">d2l.ai Chapter 15.8-15.10</span><span class="co">](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html)</span> — 从零实现BERT的教学代码</span>
<span id="cb5-901"><a href="#cb5-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-902"><a href="#cb5-902" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-903"><a href="#cb5-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-904"><a href="#cb5-904" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb5-905"><a href="#cb5-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-906"><a href="#cb5-906" aria-hidden="true" tabindex="-1"></a>BERT的名字是 **B**idirectional **E**ncoder **R**epresentations from **T**ransformers 的缩写，但它同时也是美国经典儿童节目《芝麻街》（Sesame Street）中的角色名。这不是巧合——ELMo（2018年2月）也是《芝麻街》中的角色。BERT的作者们在命名时显然是有意向ELMo致敬，同时也在暗示BERT是ELMo的"进化版"。这种学术命名的幽默感在NLP社区引起了会心一笑，也开启了一段时间内NLP论文用各种缩写致敬《芝麻街》角色的风潮——后来出现了ERNIE（Baidu和清华大学）、Grover（UW）、Big Bird（Google）等模型。</span>
<span id="cb5-907"><a href="#cb5-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-908"><a href="#cb5-908" aria-hidden="true" tabindex="-1"></a>一个值得深思的时间线是2018年的"预训练三部曲"：ELMo（2月）→ GPT（6月）→ BERT（10月）。三个模型在8个月内相继发表，走出了三条不同的路线。BERT在发表后仅两个月内就被下载了数十万次，成为NLP历史上被采用最快的模型之一。2019年到2020年的两年间，几乎所有NLP竞赛的获胜方案都以BERT（或其变体）为基础——这段时期被社区称为"BERT时代"（BERTology）。</span>
<span id="cb5-909"><a href="#cb5-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-910"><a href="#cb5-910" aria-hidden="true" tabindex="-1"></a>然而，历史总是充满反转。BERT在理解任务上的统治地位从2020年开始被GPT-3的In-Context Learning所动摇——一个足够大的单向模型，通过提示（prompt）就能完成各种理解任务，不需要任何微调。到2023年，GPT-4和Claude等大型语言模型已经在几乎所有理解基准上超越了BERT系列。BERT选择的Encoder-only架构在理解任务上曾经占据绝对优势，但最终被Decoder-only架构通过暴力规模化所超越。这个故事告诉我们，技术选择的"最优性"往往依赖于当时的规模和计算资源——在百万级参数时代双向Encoder更优，在千亿级参数时代单向Decoder反而更有扩展潜力。这个深层的权衡，是后续章节将持续探讨的主题。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>