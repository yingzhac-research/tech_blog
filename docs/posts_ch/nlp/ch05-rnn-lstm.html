<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-25">
<meta name="description" content="序列建模的第一个黄金时代：RNN如何学会记忆，LSTM/GRU如何解决梯度消失，以及Seq2Seq架构的信息瓶颈问题。">

<title>第5章：循环神经网络时代 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#序列建模的核心挑战" id="toc-序列建模的核心挑战" class="nav-link" data-scroll-target="#序列建模的核心挑战"><span class="header-section-number">2.1</span> 序列建模的核心挑战</a></li>
  <li><a href="#为什么不能简单地拼接词向量" id="toc-为什么不能简单地拼接词向量" class="nav-link" data-scroll-target="#为什么不能简单地拼接词向量"><span class="header-section-number">2.2</span> 为什么不能简单地拼接词向量？</a></li>
  </ul></li>
  <li><a href="#rnn时间维度上的权重共享" id="toc-rnn时间维度上的权重共享" class="nav-link" data-scroll-target="#rnn时间维度上的权重共享"><span class="header-section-number">3</span> RNN：时间维度上的权重共享</a>
  <ul class="collapse">
  <li><a href="#核心思想" id="toc-核心思想" class="nav-link" data-scroll-target="#核心思想"><span class="header-section-number">3.1</span> 核心思想</a></li>
  <li><a href="#数学形式化" id="toc-数学形式化" class="nav-link" data-scroll-target="#数学形式化"><span class="header-section-number">3.2</span> 数学形式化</a></li>
  <li><a href="#完整数值示例rnn前向传播" id="toc-完整数值示例rnn前向传播" class="nav-link" data-scroll-target="#完整数值示例rnn前向传播"><span class="header-section-number">3.3</span> 完整数值示例：RNN前向传播</a></li>
  <li><a href="#rnn的计算图与参数共享" id="toc-rnn的计算图与参数共享" class="nav-link" data-scroll-target="#rnn的计算图与参数共享"><span class="header-section-number">3.4</span> RNN的计算图与参数共享</a></li>
  </ul></li>
  <li><a href="#梯度消失与梯度爆炸" id="toc-梯度消失与梯度爆炸" class="nav-link" data-scroll-target="#梯度消失与梯度爆炸"><span class="header-section-number">4</span> 梯度消失与梯度爆炸</a>
  <ul class="collapse">
  <li><a href="#问题的本质" id="toc-问题的本质" class="nav-link" data-scroll-target="#问题的本质"><span class="header-section-number">4.1</span> 问题的本质</a></li>
  <li><a href="#数学分析为什么会消失或爆炸" id="toc-数学分析为什么会消失或爆炸" class="nav-link" data-scroll-target="#数学分析为什么会消失或爆炸"><span class="header-section-number">4.2</span> 数学分析：为什么会消失或爆炸？</a></li>
  <li><a href="#直觉理解信息的衰减" id="toc-直觉理解信息的衰减" class="nav-link" data-scroll-target="#直觉理解信息的衰减"><span class="header-section-number">4.3</span> 直觉理解：信息的”衰减”</a></li>
  <li><a href="#梯度爆炸的简单修复梯度裁剪" id="toc-梯度爆炸的简单修复梯度裁剪" class="nav-link" data-scroll-target="#梯度爆炸的简单修复梯度裁剪"><span class="header-section-number">4.4</span> 梯度爆炸的简单修复：梯度裁剪</a></li>
  </ul></li>
  <li><a href="#lstm门控机制的智慧" id="toc-lstm门控机制的智慧" class="nav-link" data-scroll-target="#lstm门控机制的智慧"><span class="header-section-number">5</span> LSTM：门控机制的智慧</a>
  <ul class="collapse">
  <li><a href="#核心洞察" id="toc-核心洞察" class="nav-link" data-scroll-target="#核心洞察"><span class="header-section-number">5.1</span> 核心洞察</a></li>
  <li><a href="#直觉细胞状态作为传送带" id="toc-直觉细胞状态作为传送带" class="nav-link" data-scroll-target="#直觉细胞状态作为传送带"><span class="header-section-number">5.2</span> 直觉：细胞状态作为”传送带”</a></li>
  <li><a href="#数学形式化-1" id="toc-数学形式化-1" class="nav-link" data-scroll-target="#数学形式化-1"><span class="header-section-number">5.3</span> 数学形式化</a></li>
  <li><a href="#为什么lstm解决了梯度消失" id="toc-为什么lstm解决了梯度消失" class="nav-link" data-scroll-target="#为什么lstm解决了梯度消失"><span class="header-section-number">5.4</span> 为什么LSTM解决了梯度消失？</a></li>
  <li><a href="#完整数值示例lstm前向传播" id="toc-完整数值示例lstm前向传播" class="nav-link" data-scroll-target="#完整数值示例lstm前向传播"><span class="header-section-number">5.5</span> 完整数值示例：LSTM前向传播</a></li>
  </ul></li>
  <li><a href="#gru门控机制的简化" id="toc-gru门控机制的简化" class="nav-link" data-scroll-target="#gru门控机制的简化"><span class="header-section-number">6</span> GRU：门控机制的简化</a>
  <ul class="collapse">
  <li><a href="#动机" id="toc-动机" class="nav-link" data-scroll-target="#动机"><span class="header-section-number">6.1</span> 动机</a></li>
  <li><a href="#gru-单元结构" id="toc-gru-单元结构" class="nav-link" data-scroll-target="#gru-单元结构"><span class="header-section-number">6.2</span> GRU 单元结构</a></li>
  <li><a href="#数学形式化-2" id="toc-数学形式化-2" class="nav-link" data-scroll-target="#数学形式化-2"><span class="header-section-number">6.3</span> 数学形式化</a></li>
  <li><a href="#lstm-vs-gru" id="toc-lstm-vs-gru" class="nav-link" data-scroll-target="#lstm-vs-gru"><span class="header-section-number">6.4</span> LSTM vs GRU</a></li>
  </ul></li>
  <li><a href="#seq2seqencoder-decoder架构" id="toc-seq2seqencoder-decoder架构" class="nav-link" data-scroll-target="#seq2seqencoder-decoder架构"><span class="header-section-number">7</span> Seq2Seq：Encoder-Decoder架构</a>
  <ul class="collapse">
  <li><a href="#核心问题变长到变长的映射" id="toc-核心问题变长到变长的映射" class="nav-link" data-scroll-target="#核心问题变长到变长的映射"><span class="header-section-number">7.1</span> 核心问题：变长到变长的映射</a></li>
  <li><a href="#架构设计" id="toc-架构设计" class="nav-link" data-scroll-target="#架构设计"><span class="header-section-number">7.2</span> 架构设计</a></li>
  <li><a href="#训练teacher-forcing" id="toc-训练teacher-forcing" class="nav-link" data-scroll-target="#训练teacher-forcing"><span class="header-section-number">7.3</span> 训练：Teacher Forcing</a></li>
  <li><a href="#推理贪心搜索与束搜索" id="toc-推理贪心搜索与束搜索" class="nav-link" data-scroll-target="#推理贪心搜索与束搜索"><span class="header-section-number">7.4</span> 推理：贪心搜索与束搜索</a></li>
  </ul></li>
  <li><a href="#信息瓶颈seq2seq的终极局限" id="toc-信息瓶颈seq2seq的终极局限" class="nav-link" data-scroll-target="#信息瓶颈seq2seq的终极局限"><span class="header-section-number">8</span> 信息瓶颈：Seq2Seq的终极局限</a>
  <ul class="collapse">
  <li><a href="#问题描述" id="toc-问题描述" class="nav-link" data-scroll-target="#问题描述"><span class="header-section-number">8.1</span> 问题描述</a></li>
  <li><a href="#信息论视角" id="toc-信息论视角" class="nav-link" data-scroll-target="#信息论视角"><span class="header-section-number">8.2</span> 信息论视角</a></li>
  <li><a href="#一个思考实验" id="toc-一个思考实验" class="nav-link" data-scroll-target="#一个思考实验"><span class="header-section-number">8.3</span> 一个思考实验</a></li>
  </ul></li>
  <li><a href="#工程实践lstm文本分类" id="toc-工程实践lstm文本分类" class="nav-link" data-scroll-target="#工程实践lstm文本分类"><span class="header-section-number">9</span> 工程实践：LSTM文本分类</a>
  <ul class="collapse">
  <li><a href="#模型定义" id="toc-模型定义" class="nav-link" data-scroll-target="#模型定义"><span class="header-section-number">9.1</span> 模型定义</a></li>
  <li><a href="#关键实现细节" id="toc-关键实现细节" class="nav-link" data-scroll-target="#关键实现细节"><span class="header-section-number">9.2</span> 关键实现细节</a></li>
  <li><a href="#训练技巧" id="toc-训练技巧" class="nav-link" data-scroll-target="#训练技巧"><span class="header-section-number">9.3</span> 训练技巧</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">10</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么lstm有效更深入的理论视角" id="toc-为什么lstm有效更深入的理论视角" class="nav-link" data-scroll-target="#为什么lstm有效更深入的理论视角"><span class="header-section-number">10.1</span> 为什么LSTM有效？——更深入的理论视角</a></li>
  <li><a href="#边界条件与失效模式" id="toc-边界条件与失效模式" class="nav-link" data-scroll-target="#边界条件与失效模式"><span class="header-section-number">10.2</span> 边界条件与失效模式</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">10.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与展望" id="toc-局限性与展望" class="nav-link" data-scroll-target="#局限性与展望"><span class="header-section-number">11</span> 局限性与展望</a>
  <ul class="collapse">
  <li><a href="#本章方法的核心局限" id="toc-本章方法的核心局限" class="nav-link" data-scroll-target="#本章方法的核心局限"><span class="header-section-number">11.1</span> 本章方法的核心局限</a></li>
  <li><a href="#这些局限指向什么" id="toc-这些局限指向什么" class="nav-link" data-scroll-target="#这些局限指向什么"><span class="header-section-number">11.2</span> 这些局限指向什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">12</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">12.1</span> 关键公式速查</a></li>
  </ul></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">13</span> 思考题</a></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">14</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">14.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">14.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">14.3</span> 后续发展</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第5章：循环神经网络时代</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">当神经网络学会记忆：从RNN到Seq2Seq的序列建模之路</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">RNN</div>
    <div class="quarto-category">LSTM</div>
    <div class="quarto-category">GRU</div>
    <div class="quarto-category">Seq2Seq</div>
    <div class="quarto-category">序列建模</div>
  </div>
  </div>

<div>
  <div class="description">
    序列建模的第一个黄金时代：RNN如何学会记忆，LSTM/GRU如何解决梯度消失，以及Seq2Seq架构的信息瓶颈问题。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何让神经网络处理变长序列，并捕获序列中的依赖关系？</p>
<p><strong>历史坐标</strong>：1997 (LSTM) → 2014 (GRU, Seq2Seq) | Hochreiter &amp; Schmidhuber, Cho et al., Sutskever et al.</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们解决了一个基础但关键的问题：如何将文本切分为模型可处理的单元。无论是BPE、WordPiece还是SentencePiece，Tokenizer的输出都是一个<strong>token序列</strong>。</p>
<p>现在我们面临下一个问题：拿到这个序列后，该怎么办？</p>
<p>回顾第3章的词向量。Word2Vec和GloVe为我们提供了将token映射为向量的方法。于是，一个句子”I love NLP”变成了三个向量的序列：<span class="math inline">\([\mathbf{v}_\text{I}, \mathbf{v}_\text{love}, \mathbf{v}_\text{NLP}]\)</span>。</p>
<p>但这里有一个根本性的问题：<strong>这些向量是独立的</strong>。<span class="math inline">\(\mathbf{v}_\text{love}\)</span> 不知道它前面是”I”还是”They”，也不知道后面是”NLP”还是”cats”。每个词向量都是在真空中计算的。</p>
<p>这在很多任务中是致命的。考虑这两个句子：</p>
<ul>
<li>“The <strong>bank</strong> of the river was muddy.”</li>
<li>“I went to the <strong>bank</strong> to deposit money.”</li>
</ul>
<p>静态词向量给”bank”的表示是相同的，但它们的含义完全不同。模型需要看到上下文才能区分。</p>
<p>更广泛地说，语言是<strong>序列性</strong>的。词的顺序至关重要——“dog bites man”和”man bites dog”含义截然不同。我们需要一种能够<strong>处理序列、捕获上下文</strong>的模型架构。</p>
<p>这就引出了本章的主角：<strong>循环神经网络（RNN）</strong>及其变体。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：RNN通过”隐藏状态”在时间维度上传递信息，让模型拥有了”记忆”。但这种记忆是有限的——长距离依赖会衰减，梯度会消失。LSTM和GRU通过门控机制缓解了这个问题，但并没有根本解决。Seq2Seq架构将RNN推向应用巅峰，也暴露了其终极瓶颈：所有信息必须压缩到一个固定长度的向量中。这个瓶颈将直接催生下一章的Attention机制。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="序列建模的核心挑战" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="序列建模的核心挑战"><span class="header-section-number">2.1</span> 序列建模的核心挑战</h3>
<p>让我们精确地定义我们要解决的问题。</p>
<p>给定一个输入序列 <span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_T)\)</span>，我们希望模型能够：</p>
<ol type="1">
<li><strong>理解上下文</strong>：<span class="math inline">\(x_t\)</span> 的表示应该依赖于 <span class="math inline">\(x_1, \ldots, x_{t-1}\)</span>（以及可能的 <span class="math inline">\(x_{t+1}, \ldots, x_T\)</span>）</li>
<li><strong>处理变长输入</strong>：不同的序列可能有不同的长度 <span class="math inline">\(T\)</span></li>
<li><strong>捕获长距离依赖</strong>：<span class="math inline">\(x_1\)</span> 可能影响 <span class="math inline">\(x_{100}\)</span> 的理解</li>
<li><strong>输出灵活</strong>：可能输出单个标签（分类）、同长序列（标注）、或不同长度序列（翻译）</li>
</ol>
<p>传统的前馈神经网络（MLP）无法满足这些要求。MLP接受固定大小的输入，输出固定大小的结果，没有任何机制来处理序列结构。</p>
</section>
<section id="为什么不能简单地拼接词向量" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="为什么不能简单地拼接词向量"><span class="header-section-number">2.2</span> 为什么不能简单地拼接词向量？</h3>
<p>一个直觉的想法是：把所有词向量拼接起来，送入一个大的MLP。</p>
<p><span class="math display">\[
\mathbf{h} = \text{MLP}([\mathbf{v}_{x_1}; \mathbf{v}_{x_2}; \ldots; \mathbf{v}_{x_T}])
\]</span></p>
<p>这个方案有三个致命问题：</p>
<p><strong>第一，输入维度固定</strong>。如果我们设计网络处理最长100个词，那么99个词的句子需要填充，101个词的句子无法处理。实际文本的长度变化极大——一条推文可能5个词，一篇论文可能5000个词。</p>
<p><strong>第二，参数爆炸</strong>。假设词向量维度是300，最大序列长度是1000，那么第一层的输入维度就是300,000。这意味着海量的参数和计算。</p>
<p><strong>第三，没有位置共享</strong>。模型需要单独学习”第1个位置的’love’“和”第50个位置的’love’“的含义，无法泛化位置信息。一个词在不同位置的处理方式应该是相似的。</p>
<p>我们需要一种更优雅的方案：让模型<strong>逐步处理序列</strong>，在每一步累积信息，用有限的参数处理任意长度的输入。</p>
<hr>
</section>
</section>
<section id="rnn时间维度上的权重共享" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="rnn时间维度上的权重共享"><span class="header-section-number">3</span> RNN：时间维度上的权重共享</h2>
<section id="核心思想" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="核心思想"><span class="header-section-number">3.1</span> 核心思想</h3>
<p>RNN的核心洞察是：<strong>用同一组参数处理序列中的每一个位置，通过隐藏状态在时间步之间传递信息</strong>。</p>
<p>想象你在读一本小说。你不会把整本书一次性塞进大脑，而是一个词一个词地读。关键是，当你读到第100页时，你的大脑保留着前99页的”记忆”——虽然不是逐字记忆，而是一种压缩的、与当前阅读相关的状态。这个状态会影响你对第100页的理解。</p>
<p>RNN就是这个过程的数学模型：</p>
<p><span class="math display">\[
\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{h}_t\)</span> 是第 <span class="math inline">\(t\)</span> 步的<strong>隐藏状态</strong>（hidden state），它编码了到目前为止序列的所有信息。<span class="math inline">\(f\)</span> 是一个函数，它接收上一步的隐藏状态和当前的输入，产生新的隐藏状态。</p>
<p>关键点在于：<strong><span class="math inline">\(f\)</span> 在每一个时间步都是相同的</strong>。这就是”时间维度上的权重共享”——不管序列有多长，我们只需要一组参数。</p>
</section>
<section id="数学形式化" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="数学形式化"><span class="header-section-number">3.2</span> 数学形式化</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Vanilla RNN Forward Pass (Elman, 1990)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_forward(x_sequence, h_0, W_hh, W_xh, b_h):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""RNN 前向传播（处理整个序列）"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> h_0  <span class="co"># 初始隐藏状态，通常为零向量</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> []</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x_t <span class="kw">in</span> x_sequence:</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 核心公式：线性变换 + 非线性激活</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        h_t <span class="op">=</span> tanh(W_hh <span class="op">@</span> h_t <span class="op">+</span> W_xh <span class="op">@</span> x_t <span class="op">+</span> b_h)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        hidden_states.append(h_t)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hidden_states  <span class="co"># 返回所有时间步的隐藏状态</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Reference: Elman (1990) “Finding Structure in Time”, Cognitive Science 14(2):179-211</em></p>
</div>
</div>
<p>最简单的RNN（vanilla RNN）使用线性变换加非线性激活：</p>
<p><span class="math display">\[
\mathbf{h}_t = \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(\mathbf{W}_{hh} \in \mathbb{R}^{d_h \times d_h}\)</span>：隐藏状态到隐藏状态的权重矩阵</li>
<li><span class="math inline">\(\mathbf{W}_{xh} \in \mathbb{R}^{d_h \times d_x}\)</span>：输入到隐藏状态的权重矩阵</li>
<li><span class="math inline">\(\mathbf{b}_h \in \mathbb{R}^{d_h}\)</span>：偏置向量</li>
<li><span class="math inline">\(\tanh\)</span>：激活函数，将输出压缩到 <span class="math inline">\((-1, 1)\)</span></li>
</ul>
<p>初始隐藏状态 <span class="math inline">\(\mathbf{h}_0\)</span> 通常初始化为零向量。</p>
<p>如果需要输出（比如每一步预测下一个词），可以加一个输出层：</p>
<p><span class="math display">\[
\mathbf{y}_t = \mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_y
\]</span></p>
</section>
<section id="完整数值示例rnn前向传播" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="完整数值示例rnn前向传播"><span class="header-section-number">3.3</span> 完整数值示例：RNN前向传播</h3>
<p>让我们用一个极简的例子走一遍RNN的计算过程。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>输入序列：两个token，维度 <span class="math inline">\(d_x = 2\)</span></li>
<li>隐藏状态维度：<span class="math inline">\(d_h = 3\)</span></li>
<li>输入：<span class="math inline">\(\mathbf{x}_1 = [1, 0]^T\)</span>，<span class="math inline">\(\mathbf{x}_2 = [0, 1]^T\)</span></li>
</ul>
<p><strong>参数</strong>（简化的小数值）：</p>
<p><span class="math display">\[
\mathbf{W}_{xh} = \begin{bmatrix} 0.5 &amp; 0.3 \\ 0.2 &amp; 0.4 \\ 0.1 &amp; 0.6 \end{bmatrix}, \quad
\mathbf{W}_{hh} = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.1 \\ 0.3 &amp; 0.1 &amp; 0.2 \\ 0.2 &amp; 0.3 &amp; 0.1 \end{bmatrix}, \quad
\mathbf{b}_h = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\]</span></p>
<p><strong>Step 1：初始化</strong></p>
<p><span class="math display">\[
\mathbf{h}_0 = [0, 0, 0]^T
\]</span></p>
<p><strong>Step 2：处理第一个token</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_1 &amp;= \mathbf{W}_{hh}\mathbf{h}_0 + \mathbf{W}_{xh}\mathbf{x}_1 + \mathbf{b}_h \\
&amp;= \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} + \begin{bmatrix} 0.5 \cdot 1 + 0.3 \cdot 0 \\ 0.2 \cdot 1 + 0.4 \cdot 0 \\ 0.1 \cdot 1 + 0.6 \cdot 0 \end{bmatrix} \\
&amp;= \begin{bmatrix} 0.5 \\ 0.2 \\ 0.1 \end{bmatrix}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\mathbf{h}_1 = \tanh(\mathbf{z}_1) = \begin{bmatrix} \tanh(0.5) \\ \tanh(0.2) \\ \tanh(0.1) \end{bmatrix} \approx \begin{bmatrix} 0.462 \\ 0.197 \\ 0.100 \end{bmatrix}
\]</span></p>
<p><strong>Step 3：处理第二个token</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_2 &amp;= \mathbf{W}_{hh}\mathbf{h}_1 + \mathbf{W}_{xh}\mathbf{x}_2 + \mathbf{b}_h \\
&amp;= \begin{bmatrix} 0.1 \cdot 0.462 + 0.2 \cdot 0.197 + 0.1 \cdot 0.100 \\ 0.3 \cdot 0.462 + 0.1 \cdot 0.197 + 0.2 \cdot 0.100 \\ 0.2 \cdot 0.462 + 0.3 \cdot 0.197 + 0.1 \cdot 0.100 \end{bmatrix} + \begin{bmatrix} 0.3 \\ 0.4 \\ 0.6 \end{bmatrix} \\
&amp;= \begin{bmatrix} 0.046 + 0.039 + 0.010 + 0.3 \\ 0.139 + 0.020 + 0.020 + 0.4 \\ 0.092 + 0.059 + 0.010 + 0.6 \end{bmatrix} \\
&amp;= \begin{bmatrix} 0.395 \\ 0.579 \\ 0.761 \end{bmatrix}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\mathbf{h}_2 = \tanh(\mathbf{z}_2) \approx \begin{bmatrix} 0.375 \\ 0.521 \\ 0.642 \end{bmatrix}
\]</span></p>
<p><strong>解读</strong>：<span class="math inline">\(\mathbf{h}_2\)</span> 编码了整个序列 <span class="math inline">\([\mathbf{x}_1, \mathbf{x}_2]\)</span> 的信息。注意它不仅受 <span class="math inline">\(\mathbf{x}_2\)</span> 影响，也包含了 <span class="math inline">\(\mathbf{x}_1\)</span> 通过 <span class="math inline">\(\mathbf{h}_1\)</span> 传递过来的信息。</p>
</section>
<section id="rnn的计算图与参数共享" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="rnn的计算图与参数共享"><span class="header-section-number">3.4</span> RNN的计算图与参数共享</h3>
<p>下图展示了RNN在时间维度上的”展开”（unrolling）：</p>
<div id="fig-rnn-unrolled" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-unrolled-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig-rnn-unrolled-colah.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-unrolled-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RNN展开图：左侧是循环表示，右侧是展开后的时间步。每个时间步接收输入 <span class="math inline">\(x_t\)</span>，产生隐藏状态 <span class="math inline">\(h_t\)</span>。所有时间步共享同一组参数 A。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Christopher Olah (2015) “<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>”</em></p>
</div>
<p>关键观察：<strong>所有”RNN Cell”共享相同的参数</strong>（<span class="math inline">\(\mathbf{W}_{hh}, \mathbf{W}_{xh}, \mathbf{b}_h\)</span>）。这就是权重共享——无论序列有10个token还是1000个token，参数量都是固定的。</p>
<p>另一种理解方式是把RNN”展开”（unroll）成时间步：展开后看起来像一个很深的前馈网络，但每一层用的是同一组参数。</p>
<hr>
</section>
</section>
<section id="梯度消失与梯度爆炸" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="梯度消失与梯度爆炸"><span class="header-section-number">4</span> 梯度消失与梯度爆炸</h2>
<section id="问题的本质" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="问题的本质"><span class="header-section-number">4.1</span> 问题的本质</h3>
<p>RNN理论上可以捕获任意长距离的依赖——<span class="math inline">\(\mathbf{h}_{100}\)</span> 包含了 <span class="math inline">\(\mathbf{x}_1\)</span> 的信息（通过连续的传递）。但在实践中，这个”理论上”往往不成立。</p>
<p>问题出在<strong>梯度的传播</strong>上。</p>
<p>考虑损失函数 <span class="math inline">\(\mathcal{L}\)</span> 对早期隐藏状态 <span class="math inline">\(\mathbf{h}_1\)</span> 的梯度。根据链式法则：</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{h}_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_T} \cdot \frac{\partial \mathbf{h}_T}{\partial \mathbf{h}_{T-1}} \cdot \frac{\partial \mathbf{h}_{T-1}}{\partial \mathbf{h}_{T-2}} \cdots \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1}
\]</span></p>
<p>关键在于这些连乘的雅可比矩阵 <span class="math inline">\(\frac{\partial \mathbf{h}_{t}}{\partial \mathbf{h}_{t-1}}\)</span>。</p>
<p>对于 vanilla RNN，我们有：</p>
<p><span class="math display">\[
\mathbf{h}_t = \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span></p>
<p>因此：</p>
<p><span class="math display">\[
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = \text{diag}(1 - \mathbf{h}_t^2) \cdot \mathbf{W}_{hh}
\]</span></p>
<p>其中 <span class="math inline">\(\text{diag}(1 - \mathbf{h}_t^2)\)</span> 是 <span class="math inline">\(\tanh\)</span> 的导数（因为 <span class="math inline">\(\tanh'(x) = 1 - \tanh^2(x)\)</span>）。</p>
</section>
<section id="数学分析为什么会消失或爆炸" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="数学分析为什么会消失或爆炸"><span class="header-section-number">4.2</span> 数学分析：为什么会消失或爆炸？</h3>
<p>当我们连乘 <span class="math inline">\(T-1\)</span> 个这样的雅可比矩阵时：</p>
<p><span class="math display">\[
\prod_{t=2}^{T} \frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = \prod_{t=2}^{T} \text{diag}(1 - \mathbf{h}_t^2) \cdot \mathbf{W}_{hh}
\]</span></p>
<p>简化分析：假设所有 <span class="math inline">\(\mathbf{h}_t\)</span> 接近零（激活值小），那么 <span class="math inline">\(1 - \mathbf{h}_t^2 \approx 1\)</span>，连乘大约是：</p>
<p><span class="math display">\[
\mathbf{W}_{hh}^{T-1}
\]</span></p>
<p>现在，根据 <span class="math inline">\(\mathbf{W}_{hh}\)</span> 的<strong>特征值</strong>分布：</p>
<ul>
<li>如果最大特征值 <span class="math inline">\(|\lambda_{\max}| &lt; 1\)</span>：<span class="math inline">\(\mathbf{W}_{hh}^{T-1} \to 0\)</span>（指数衰减）→ <strong>梯度消失</strong></li>
<li>如果最大特征值 <span class="math inline">\(|\lambda_{\max}| &gt; 1\)</span>：<span class="math inline">\(\mathbf{W}_{hh}^{T-1} \to \infty\)</span>（指数增长）→ <strong>梯度爆炸</strong></li>
</ul>
<p>实际情况更复杂，因为 <span class="math inline">\(\tanh\)</span> 的导数在激活值大时趋近于0，这会<strong>加剧梯度消失</strong>。</p>
</section>
<section id="直觉理解信息的衰减" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="直觉理解信息的衰减"><span class="header-section-number">4.3</span> 直觉理解：信息的”衰减”</h3>
<p>用一个类比来理解。想象你在玩”传话游戏”：第一个人说一句话，传给第二个人，第二个人传给第三个人……传100个人之后，原始信息还剩多少？</p>
<p>在vanilla RNN中，信息每经过一个时间步，都要经过一次”压缩和混合”（矩阵乘法+非线性）。如果这个过程是”有损”的（信息被衰减），那么经过100步后，第1步的信息几乎完全消失。</p>
<p>这就是为什么vanilla RNN无法捕获长距离依赖：不是模型没有”记住”早期信息的能力，而是梯度无法有效地流回早期时间步，导致模型学不到长距离的模式。</p>
</section>
<section id="梯度爆炸的简单修复梯度裁剪" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="梯度爆炸的简单修复梯度裁剪"><span class="header-section-number">4.4</span> 梯度爆炸的简单修复：梯度裁剪</h3>
<p>梯度爆炸相对容易处理：当梯度的范数超过某个阈值时，按比例缩小。</p>
<p><span class="math display">\[
\mathbf{g} \leftarrow \begin{cases}
\mathbf{g} &amp; \text{if } \|\mathbf{g}\| \leq \theta \\
\frac{\theta}{\|\mathbf{g}\|} \mathbf{g} &amp; \text{if } \|\mathbf{g}\| &gt; \theta
\end{cases}
\]</span></p>
<p>这被称为<strong>梯度裁剪</strong>（gradient clipping），几乎是所有RNN训练的标配。</p>
<p>但梯度消失没有这么简单的修复方法。我们需要更根本的架构改变。</p>
<hr>
</section>
</section>
<section id="lstm门控机制的智慧" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="lstm门控机制的智慧"><span class="header-section-number">5</span> LSTM：门控机制的智慧</h2>
<section id="核心洞察" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="核心洞察"><span class="header-section-number">5.1</span> 核心洞察</h3>
<p>1997年，Hochreiter和Schmidhuber提出了<strong>长短期记忆网络</strong>（Long Short-Term Memory, LSTM）。他们的核心洞察是：</p>
<blockquote class="blockquote">
<p>问题不在于RNN没有记忆能力，而在于信息在时间步之间传递时被”过度处理”了。如果我们能让某些信息<strong>不经修改地直接传递</strong>，就能保留长距离依赖。</p>
</blockquote>
<p>这个想法的具体实现是<strong>门控机制</strong>：用可学习的”门”来控制信息的流动——哪些信息要保留，哪些要遗忘，哪些要更新。</p>
</section>
<section id="直觉细胞状态作为传送带" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="直觉细胞状态作为传送带"><span class="header-section-number">5.2</span> 直觉：细胞状态作为”传送带”</h3>
<p>LSTM引入了一个新的概念：<strong>细胞状态</strong>（cell state）<span class="math inline">\(\mathbf{c}_t\)</span>，它像一条传送带一样在时间步之间传递。</p>
<p>想象一个工厂的传送带：物品（信息）在传送带上移动。沿途有几个工作站：</p>
<ol type="1">
<li><strong>遗忘门</strong>：决定丢弃传送带上的哪些物品</li>
<li><strong>输入门</strong>：决定往传送带上放哪些新物品</li>
<li><strong>输出门</strong>：决定从传送带上取出哪些物品作为当前的输出</li>
</ol>
<p>关键在于：传送带本身的传递是<strong>近乎恒等的</strong>——没有被遗忘的信息可以不经修改地传到下游。这就避免了梯度消失问题。</p>
<p>下图展示了LSTM的内部结构。与简单RNN只有一个tanh层不同，LSTM有<strong>四个相互作用的层</strong>（三个sigmoid门 + 一个tanh层）：</p>
<div id="fig-lstm-chain" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lstm-chain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig-lstm-chain-colah.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lstm-chain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM单元结构：黄色方框是神经网络层（σ = sigmoid，tanh = tanh），粉色圆圈是逐元素操作（× = 乘法，+ = 加法）。顶部的横线是细胞状态 <span class="math inline">\(c_t\)</span>（“传送带”），底部的横线是隐藏状态 <span class="math inline">\(h_t\)</span>。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Christopher Olah (2015) “<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>”</em></p>
</div>
</section>
<section id="数学形式化-1" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="数学形式化-1"><span class="header-section-number">5.3</span> 数学形式化</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: LSTM Forward Pass (Hochreiter &amp; Schmidhuber, 1997)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lstm_forward(x_t, h_prev, c_prev, W_f, W_i, W_c, W_o):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""LSTM 单步前向传播"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    concat <span class="op">=</span> [h_prev, x_t]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 遗忘门：决定丢弃多少旧的细胞状态</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    f_t <span class="op">=</span> sigmoid(W_f <span class="op">@</span> concat)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输入门：决定写入多少新信息</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    i_t <span class="op">=</span> sigmoid(W_i <span class="op">@</span> concat)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    c_tilde <span class="op">=</span> tanh(W_c <span class="op">@</span> concat)  <span class="co"># 候选细胞状态</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 更新细胞状态：遗忘旧信息 + 写入新信息</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    c_t <span class="op">=</span> f_t <span class="op">*</span> c_prev <span class="op">+</span> i_t <span class="op">*</span> c_tilde</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输出门：决定输出多少细胞状态</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    o_t <span class="op">=</span> sigmoid(W_o <span class="op">@</span> concat)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> o_t <span class="op">*</span> tanh(c_t)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t, c_t</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Hochreiter &amp; Schmidhuber (1997) “Long Short-Term Memory”, Neural Computation 9(8):1735-1780</em></p>
</div>
</div>
<p>LSTM在每个时间步计算以下量：</p>
<p><strong>遗忘门</strong>：决定遗忘多少旧的细胞状态</p>
<p><span class="math display">\[
\mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)
\]</span></p>
<p><strong>输入门</strong>：决定写入多少新信息</p>
<p><span class="math display">\[
\mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)
\]</span></p>
<p><strong>候选细胞状态</strong>：新信息的候选值</p>
<p><span class="math display">\[
\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c)
\]</span></p>
<p><strong>细胞状态更新</strong>：</p>
<p><span class="math display">\[
\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t
\]</span></p>
<p><strong>输出门</strong>：决定输出多少细胞状态</p>
<p><span class="math display">\[
\mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)
\]</span></p>
<p><strong>隐藏状态</strong>：</p>
<p><span class="math display">\[
\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
\]</span></p>
<p>其中 <span class="math inline">\(\sigma\)</span> 是sigmoid函数（输出在 <span class="math inline">\((0, 1)\)</span>），<span class="math inline">\(\odot\)</span> 是逐元素乘法。</p>
</section>
<section id="为什么lstm解决了梯度消失" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="为什么lstm解决了梯度消失"><span class="header-section-number">5.4</span> 为什么LSTM解决了梯度消失？</h3>
<p>关键在于细胞状态的更新公式：</p>
<p><span class="math display">\[
\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t
\]</span></p>
<p>考虑梯度 <span class="math inline">\(\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}}\)</span>：</p>
<p><span class="math display">\[
\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \text{diag}(\mathbf{f}_t) + \text{其他项}
\]</span></p>
<p>如果遗忘门 <span class="math inline">\(\mathbf{f}_t\)</span> 接近1，那么 <span class="math inline">\(\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} \approx \mathbf{I}\)</span>（单位矩阵）。</p>
<p>这意味着：<strong>梯度可以几乎不衰减地流过多个时间步</strong>。只要遗忘门学习到保持信息（<span class="math inline">\(\mathbf{f}_t \approx 1\)</span>），早期时间步的信息就能对后期产生影响，模型也能学到长距离依赖。</p>
<p>这和ResNet中残差连接的原理非常类似：提供一条”高速公路”让梯度直接流动。</p>
</section>
<section id="完整数值示例lstm前向传播" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="完整数值示例lstm前向传播"><span class="header-section-number">5.5</span> 完整数值示例：LSTM前向传播</h3>
<p><strong>设定</strong>：</p>
<ul>
<li>输入维度 <span class="math inline">\(d_x = 2\)</span>，隐藏维度 <span class="math inline">\(d_h = 2\)</span></li>
<li>输入序列：<span class="math inline">\(\mathbf{x}_1 = [1, 0]^T\)</span>，<span class="math inline">\(\mathbf{x}_2 = [0, 1]^T\)</span></li>
<li>初始状态：<span class="math inline">\(\mathbf{h}_0 = [0, 0]^T\)</span>，<span class="math inline">\(\mathbf{c}_0 = [0, 0]^T\)</span></li>
</ul>
<p><strong>简化参数</strong>（为便于计算，使用小值）：</p>
<p><span class="math display">\[
\mathbf{W}_f = \mathbf{W}_i = \mathbf{W}_c = \mathbf{W}_o = \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.3 &amp; 0.3 \\ 0.3 &amp; 0.3 &amp; 0.5 &amp; 0.5 \end{bmatrix}
\]</span></p>
<p>所有偏置为零。</p>
<p><strong>Step 1：处理 <span class="math inline">\(\mathbf{x}_1\)</span></strong></p>
<p>拼接输入：<span class="math inline">\([\mathbf{h}_0, \mathbf{x}_1] = [0, 0, 1, 0]^T\)</span></p>
<p>计算各个门（省略详细矩阵乘法）：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{f}_1 &amp;= \sigma([0.3, 0.5]^T) = [0.574, 0.622]^T \\
\mathbf{i}_1 &amp;= \sigma([0.3, 0.5]^T) = [0.574, 0.622]^T \\
\tilde{\mathbf{c}}_1 &amp;= \tanh([0.3, 0.5]^T) = [0.291, 0.462]^T \\
\mathbf{o}_1 &amp;= \sigma([0.3, 0.5]^T) = [0.574, 0.622]^T
\end{aligned}
\]</span></p>
<p>更新细胞状态：</p>
<p><span class="math display">\[
\mathbf{c}_1 = \mathbf{f}_1 \odot \mathbf{c}_0 + \mathbf{i}_1 \odot \tilde{\mathbf{c}}_1 = [0, 0] + [0.167, 0.287]^T = [0.167, 0.287]^T
\]</span></p>
<p>计算隐藏状态：</p>
<p><span class="math display">\[
\mathbf{h}_1 = \mathbf{o}_1 \odot \tanh(\mathbf{c}_1) = [0.574, 0.622] \odot [0.165, 0.279] = [0.095, 0.174]^T
\]</span></p>
<p><strong>Step 2：处理 <span class="math inline">\(\mathbf{x}_2\)</span></strong></p>
<p>拼接输入：<span class="math inline">\([\mathbf{h}_1, \mathbf{x}_2] = [0.095, 0.174, 0, 1]^T\)</span></p>
<p>（类似计算，省略细节）</p>
<p>最终得到 <span class="math inline">\(\mathbf{h}_2\)</span> 和 <span class="math inline">\(\mathbf{c}_2\)</span>，编码了整个序列的信息。</p>
<p><strong>关键观察</strong>：<span class="math inline">\(\mathbf{c}_1\)</span> 中的信息通过遗忘门控制，部分保留到 <span class="math inline">\(\mathbf{c}_2\)</span>。如果 <span class="math inline">\(\mathbf{f}_2 \approx 1\)</span>，则 <span class="math inline">\(\mathbf{x}_1\)</span> 的信息几乎完整传递。</p>
<hr>
</section>
</section>
<section id="gru门控机制的简化" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="gru门控机制的简化"><span class="header-section-number">6</span> GRU：门控机制的简化</h2>
<section id="动机" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="动机"><span class="header-section-number">6.1</span> 动机</h3>
<p>LSTM有4个门、两个状态（<span class="math inline">\(\mathbf{h}\)</span> 和 <span class="math inline">\(\mathbf{c}\)</span>），参数量较大。2014年，Cho等人提出了<strong>门控循环单元</strong>（Gated Recurrent Unit, GRU），用更少的参数达到相似的效果。</p>
<p>GRU的核心简化：</p>
<ol type="1">
<li><strong>合并细胞状态和隐藏状态</strong>：只保留一个状态 <span class="math inline">\(\mathbf{h}\)</span></li>
<li><strong>合并遗忘门和输入门</strong>：用一个”更新门”同时控制遗忘和写入</li>
</ol>
</section>
<section id="gru-单元结构" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="gru-单元结构"><span class="header-section-number">6.2</span> GRU 单元结构</h3>
<div id="fig-gru-cell" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gru-cell-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig-gru-cell.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gru-cell-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: GRU hidden unit 结构图：update gate (z) 控制保留多少旧状态，reset gate (r) 控制如何将旧状态与新输入结合。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Cho et al.&nbsp;(2014) “Learning Phrase Representations using RNN Encoder-Decoder”, Figure 2. <a href="https://arxiv.org/abs/1406.1078">arXiv:1406.1078</a></em></p>
</div>
</section>
<section id="数学形式化-2" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="数学形式化-2"><span class="header-section-number">6.3</span> 数学形式化</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: GRU Forward Pass (Cho et al., 2014)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gru_forward(x_t, h_prev, W_r, W_z, W_h):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""GRU 单步前向传播"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 重置门：决定忽略多少旧状态</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    r_t <span class="op">=</span> sigmoid(W_r <span class="op">@</span> concat(h_prev, x_t))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 更新门：决定保留多少旧状态</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    z_t <span class="op">=</span> sigmoid(W_z <span class="op">@</span> concat(h_prev, x_t))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 候选状态：用重置门过滤旧状态后，与新输入结合</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    h_tilde <span class="op">=</span> tanh(W_h <span class="op">@</span> concat(r_t <span class="op">*</span> h_prev, x_t))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 最终状态：旧状态与候选状态的插值</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> z_t <span class="op">*</span> h_prev <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> z_t) <span class="op">*</span> h_tilde</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Adapted from: Cho et al.&nbsp;(2014) Equations 5-8. <a href="https://arxiv.org/abs/1406.1078">arXiv:1406.1078</a></em></p>
</div>
</div>
<p><strong>重置门</strong>：决定如何将过去的信息与新输入结合</p>
<p><span class="math display">\[
\mathbf{r}_t = \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_r)
\]</span></p>
<p><strong>更新门</strong>：决定保留多少旧状态</p>
<p><span class="math display">\[
\mathbf{z}_t = \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_z)
\]</span></p>
<p><strong>候选隐藏状态</strong>：</p>
<p><span class="math display">\[
\tilde{\mathbf{h}}_t = \tanh(\mathbf{W}_h [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_h)
\]</span></p>
<p><strong>隐藏状态更新</strong>：</p>
<p><span class="math display">\[
\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
\]</span></p>
</section>
<section id="lstm-vs-gru" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="lstm-vs-gru"><span class="header-section-number">6.4</span> LSTM vs GRU</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方面</th>
<th>LSTM</th>
<th>GRU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>状态数量</strong></td>
<td>2（<span class="math inline">\(\mathbf{h}\)</span>, <span class="math inline">\(\mathbf{c}\)</span>）</td>
<td>1（<span class="math inline">\(\mathbf{h}\)</span>）</td>
</tr>
<tr class="even">
<td><strong>门数量</strong></td>
<td>3（遗忘、输入、输出）</td>
<td>2（重置、更新）</td>
</tr>
<tr class="odd">
<td><strong>参数量</strong></td>
<td>较多</td>
<td>较少（约75%）</td>
</tr>
<tr class="even">
<td><strong>性能</strong></td>
<td>通常略好</td>
<td>相当，有时更好</td>
</tr>
<tr class="odd">
<td><strong>训练速度</strong></td>
<td>较慢</td>
<td>较快</td>
</tr>
</tbody>
</table>
<p>实践中的选择：</p>
<ul>
<li><strong>数据量大、任务复杂</strong>：LSTM可能略有优势</li>
<li><strong>计算资源有限、需要快速迭代</strong>：GRU是更好的选择</li>
<li><strong>很多情况下差异不大</strong>：先用GRU快速验证，必要时换LSTM</li>
</ul>
<hr>
</section>
</section>
<section id="seq2seqencoder-decoder架构" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="seq2seqencoder-decoder架构"><span class="header-section-number">7</span> Seq2Seq：Encoder-Decoder架构</h2>
<section id="核心问题变长到变长的映射" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="核心问题变长到变长的映射"><span class="header-section-number">7.1</span> 核心问题：变长到变长的映射</h3>
<p>之前我们讨论的是”理解”任务——输入一个序列，输出一个向量（分类）或同长度序列（标注）。但很多重要任务需要<strong>输入和输出都是序列，且长度不同</strong>：</p>
<ul>
<li><strong>机器翻译</strong>：“I love NLP” → “我喜欢自然语言处理”（3词 → 6词）</li>
<li><strong>文本摘要</strong>：长文章 → 短摘要</li>
<li><strong>对话生成</strong>：问题 → 回答</li>
</ul>
<p>这就需要一种新的架构：<strong>Seq2Seq</strong>（Sequence-to-Sequence），也称为Encoder-Decoder架构。</p>
</section>
<section id="架构设计" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="架构设计"><span class="header-section-number">7.2</span> 架构设计</h3>
<p>Seq2Seq由两个RNN组成：</p>
<ol type="1">
<li><strong>编码器（Encoder）</strong>：读取输入序列，将其压缩为一个固定长度的向量</li>
<li><strong>解码器（Decoder）</strong>：接收这个向量，生成输出序列</li>
</ol>
<div id="fig-encoder-decoder" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encoder-decoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig-encoder-decoder.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encoder-decoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: RNN Encoder-Decoder 架构：Encoder 将输入序列 <span class="math inline">\((x_1, ..., x_T)\)</span> 编码为上下文向量 <span class="math inline">\(c\)</span>，Decoder 从 <span class="math inline">\(c\)</span> 生成输出序列 <span class="math inline">\((y_1, ..., y_{T'})\)</span>。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Cho et al.&nbsp;(2014) “Learning Phrase Representations using RNN Encoder-Decoder”, Figure 1. <a href="https://arxiv.org/abs/1406.1078">arXiv:1406.1078</a></em></p>
</div>
<p>下图展示了 Sutskever 等人的经典 Seq2Seq 架构，注意<strong>输入序列是反转的</strong>（ABC 读取为 CBA），这是他们发现的一个有效技巧：</p>
<div id="fig-seq2seq" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-seq2seq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-5/original/fig-seq2seq.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-seq2seq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Seq2Seq 架构（Sutskever et al., 2014）：输入 “ABC” 被反转后读取，输出 “WXYZ”。模型在输出 <code>&lt;EOS&gt;</code> 后停止生成。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Sutskever et al.&nbsp;(2014) “Sequence to Sequence Learning with Neural Networks”, Figure 1. <a href="https://arxiv.org/abs/1409.3215">arXiv:1409.3215</a></em></p>
</div>
<p><strong>编码过程</strong>：</p>
<p><span class="math display">\[
\mathbf{h}_t^{enc} = \text{LSTM}_{enc}(\mathbf{h}_{t-1}^{enc}, \mathbf{x}_t)
\]</span></p>
<p><span class="math display">\[
\mathbf{c} = \mathbf{h}_T^{enc} \quad \text{（最后一步的隐藏状态作为上下文向量）}
\]</span></p>
<p><strong>解码过程</strong>：</p>
<p><span class="math display">\[
\mathbf{h}_t^{dec} = \text{LSTM}_{dec}(\mathbf{h}_{t-1}^{dec}, \mathbf{y}_{t-1})
\]</span></p>
<p><span class="math display">\[
P(\mathbf{y}_t | \mathbf{y}_{&lt;t}, \mathbf{x}) = \text{softmax}(\mathbf{W}_o \mathbf{h}_t^{dec})
\]</span></p>
<p>解码器的初始隐藏状态设为上下文向量：<span class="math inline">\(\mathbf{h}_0^{dec} = \mathbf{c}\)</span>。</p>
</section>
<section id="训练teacher-forcing" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="训练teacher-forcing"><span class="header-section-number">7.3</span> 训练：Teacher Forcing</h3>
<p>在训练时，我们使用<strong>teacher forcing</strong>：解码器的每一步输入是<strong>真实的前一个词</strong>（ground truth），而不是模型预测的词。</p>
<p><span class="math display">\[
\mathbf{h}_t^{dec} = \text{LSTM}_{dec}(\mathbf{h}_{t-1}^{dec}, \mathbf{y}_{t-1}^{*})
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{y}_{t-1}^{*}\)</span> 是真实的第 <span class="math inline">\(t-1\)</span> 个词。</p>
<p>这样做的原因是：如果用模型预测的词，早期的错误会累积，导致训练不稳定。</p>
<p>但这也带来一个问题：训练和推理时的分布不一致（<strong>exposure bias</strong>）。训练时解码器总是看到正确的前缀，推理时却要依赖自己的预测。</p>
</section>
<section id="推理贪心搜索与束搜索" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="推理贪心搜索与束搜索"><span class="header-section-number">7.4</span> 推理：贪心搜索与束搜索</h3>
<p>在推理时，我们需要生成输出序列。两种常见策略：</p>
<p><strong>贪心搜索</strong>：每一步选择概率最高的词</p>
<p><span class="math display">\[
\hat{y}_t = \arg\max_y P(y | \hat{y}_{&lt;t}, \mathbf{x})
\]</span></p>
<p>简单快速，但不保证全局最优。</p>
<p><strong>束搜索（Beam Search）</strong>：维护 <span class="math inline">\(k\)</span> 个候选序列，每一步扩展所有候选，保留得分最高的 <span class="math inline">\(k\)</span> 个。</p>
<p>这是一种在搜索空间和计算成本之间的权衡。实践中 <span class="math inline">\(k=4\)</span> 或 <span class="math inline">\(k=5\)</span> 通常足够。</p>
<hr>
</section>
</section>
<section id="信息瓶颈seq2seq的终极局限" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="信息瓶颈seq2seq的终极局限"><span class="header-section-number">8</span> 信息瓶颈：Seq2Seq的终极局限</h2>
<section id="问题描述" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="问题描述"><span class="header-section-number">8.1</span> 问题描述</h3>
<p>回顾Seq2Seq的核心假设：整个输入序列的信息被压缩到<strong>一个固定长度的向量</strong> <span class="math inline">\(\mathbf{c}\)</span> 中。</p>
<p>这意味着，无论输入是5个词还是500个词，都要塞进同一个维度的向量。</p>
<p>想象一下：你要把一本500页的书的所有信息压缩成一个1024维的向量，然后仅凭这个向量翻译成另一种语言。这能行吗？</p>
<p>实验证据也验证了这个担忧。Sutskever等人(2014)在机器翻译任务上发现：当输入句子超过20个词时，翻译质量急剧下降。</p>
</section>
<section id="信息论视角" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="信息论视角"><span class="header-section-number">8.2</span> 信息论视角</h3>
<p>从信息论角度，这个问题可以精确表述。</p>
<p>假设输入序列 <span class="math inline">\(\mathbf{x}\)</span> 的信息熵是 <span class="math inline">\(H(\mathbf{x})\)</span>。上下文向量 <span class="math inline">\(\mathbf{c}\)</span> 是 <span class="math inline">\(\mathbf{x}\)</span> 的一个有损压缩。根据数据处理不等式：</p>
<p><span class="math display">\[
I(\mathbf{y}; \mathbf{c}) \leq I(\mathbf{y}; \mathbf{x})
\]</span></p>
<p>其中 <span class="math inline">\(I\)</span> 是互信息。也就是说，<span class="math inline">\(\mathbf{c}\)</span> 中关于 <span class="math inline">\(\mathbf{y}\)</span> 的信息不可能超过 <span class="math inline">\(\mathbf{x}\)</span> 中的。</p>
<p>如果 <span class="math inline">\(\mathbf{c}\)</span> 的维度是 <span class="math inline">\(d\)</span>，它最多携带 <span class="math inline">\(O(d \log |\mathcal{V}|)\)</span> 比特的信息（<span class="math inline">\(|\mathcal{V}|\)</span> 是取值范围）。当输入序列很长时，必然有信息丢失。</p>
</section>
<section id="一个思考实验" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="一个思考实验"><span class="header-section-number">8.3</span> 一个思考实验</h3>
<p>考虑机器翻译任务。输入是英语句子，输出是法语句子。</p>
<p>假设我们要翻译一个很长的句子，其中第100个词是一个人名”Claude”。在输出的法语句子中，这个名字应该保持不变。</p>
<p>但根据Seq2Seq的架构，“Claude”这个词首先要编码进隐藏状态 <span class="math inline">\(\mathbf{h}_{100}^{enc}\)</span>，然后经过剩下的编码步骤，最终压缩到上下文向量 <span class="math inline">\(\mathbf{c}\)</span> 中，再由解码器提取出来。</p>
<p>在这个过程中，“Claude”的信息可能与其他词的信息混合、被覆盖、或因梯度消失而无法有效学习。</p>
<p><strong>我们需要的是什么？</strong></p>
<p>理想情况下，当解码器生成这个名字时，它应该能够<strong>直接”看”到编码器中对应的位置</strong>，而不是只依赖一个压缩后的向量。</p>
<p>这正是Attention机制的核心思想——<strong>让解码器在每一步都能访问编码器的所有隐藏状态</strong>，而不是只有最后一个。</p>
<blockquote class="blockquote">
<p>这个洞察将在下一章详细展开。Seq2Seq + Attention的组合，将RNN推向了最后的辉煌，也为后来完全抛弃RNN的Transformer铺平了道路。</p>
</blockquote>
<hr>
</section>
</section>
<section id="工程实践lstm文本分类" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="工程实践lstm文本分类"><span class="header-section-number">9</span> 工程实践：LSTM文本分类</h2>
<p>让我们用PyTorch实现一个简单的LSTM文本分类器，体会RNN的实际应用。</p>
<section id="模型定义" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="模型定义"><span class="header-section-number">9.1</span> 模型定义</h3>
<div id="75cabcf9" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMClassifier(nn.Module):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, hidden_dim, num_classes, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            embed_dim,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            hidden_dim,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,  <span class="co"># 双向LSTM</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 双向LSTM输出维度是 hidden_dim * 2</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim <span class="op">*</span> <span class="dv">2</span>, num_classes)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, lengths<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: [batch_size, seq_len]</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(x))  <span class="co"># [batch_size, seq_len, embed_dim]</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LSTM前向传播</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> lengths <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 使用pack_padded_sequence处理变长序列</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            packed <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                embedded, lengths.cpu(), batch_first<span class="op">=</span><span class="va">True</span>, enforce_sorted<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            packed_output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.lstm(packed)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>            output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.lstm(embedded)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [num_layers * 2, batch_size, hidden_dim] (双向)</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 取最后一层的前向和后向隐藏状态</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        hidden_forward <span class="op">=</span> hidden[<span class="op">-</span><span class="dv">2</span>, :, :]  <span class="co"># [batch_size, hidden_dim]</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        hidden_backward <span class="op">=</span> hidden[<span class="op">-</span><span class="dv">1</span>, :, :]  <span class="co"># [batch_size, hidden_dim]</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        hidden_concat <span class="op">=</span> torch.cat([hidden_forward, hidden_backward], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch_size, hidden_dim * 2]</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.fc(<span class="va">self</span>.dropout(hidden_concat))  <span class="co"># [batch_size, num_classes]</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例：创建模型</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LSTMClassifier(</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>,</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    embed_dim<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    hidden_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"模型参数量: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>模型参数量: 2,071,554</code></pre>
</div>
</div>
</section>
<section id="关键实现细节" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="关键实现细节"><span class="header-section-number">9.2</span> 关键实现细节</h3>
<p><strong>1. 双向LSTM</strong></p>
<p>使用 <code>bidirectional=True</code>，模型同时从左到右和从右到左处理序列，输出维度翻倍。这对于分类任务很有用——一个词的语义既依赖前文也依赖后文。</p>
<p><strong>2. 变长序列处理</strong></p>
<p>实际文本长度不一，需要填充（padding）到相同长度。<code>pack_padded_sequence</code> 和 <code>pad_packed_sequence</code> 让LSTM忽略填充位置，避免污染隐藏状态。</p>
<p><strong>3. 分类策略</strong></p>
<p>对于分类任务，我们用最后一个时间步的隐藏状态（或所有时间步的平均）作为序列的表示，送入分类层。</p>
</section>
<section id="训练技巧" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="训练技巧"><span class="header-section-number">9.3</span> 训练技巧</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 梯度裁剪（防止梯度爆炸）</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 学习率调度</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    optimizer, mode<span class="op">=</span><span class="st">'min'</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">2</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">10</span> 深入理解</h2>
<section id="为什么lstm有效更深入的理论视角" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="为什么lstm有效更深入的理论视角"><span class="header-section-number">10.1</span> 为什么LSTM有效？——更深入的理论视角</h3>
<p>LSTM的成功不仅仅是”门控机制”这个技巧。从更深的角度看：</p>
<p><strong>1. 常微分方程视角</strong></p>
<p>可以把RNN看作离散化的常微分方程：</p>
<p><span class="math display">\[
\frac{d\mathbf{h}}{dt} = f(\mathbf{h}, \mathbf{x})
\]</span></p>
<p>Vanilla RNN对应简单的欧拉方法，而LSTM的细胞状态更新：</p>
<p><span class="math display">\[
\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t
\]</span></p>
<p>可以看作一种<strong>自适应步长</strong>的数值积分——遗忘门控制”衰减率”，输入门控制”增量”。</p>
<p><strong>2. 记忆与计算的分离</strong></p>
<p>LSTM将”记忆”（<span class="math inline">\(\mathbf{c}\)</span>）和”计算”（<span class="math inline">\(\mathbf{h}\)</span>）分离。细胞状态是长期记忆的载体，隐藏状态是当前的工作记忆。这种分离让模型能够同时保持长期信息和进行复杂的当前计算。</p>
<p><strong>3. 可学习的遗忘</strong></p>
<p>遗忘门的引入是关键创新。直觉上，保持信息似乎总是好的。但实际上，选择性遗忘同样重要——不相关的信息会干扰有用的信息。遗忘门让模型学习什么时候清除旧信息。</p>
</section>
<section id="边界条件与失效模式" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="边界条件与失效模式"><span class="header-section-number">10.2</span> 边界条件与失效模式</h3>
<p>LSTM并不完美。以下是它的已知局限：</p>
<p><strong>1. 顺序计算</strong></p>
<p>LSTM必须按顺序处理序列，无法并行。这在GPU时代是严重的效率瓶颈。一个1000步的序列需要1000次串行计算。</p>
<p><strong>2. 长距离仍有衰减</strong></p>
<p>虽然比vanilla RNN好很多，但LSTM在极长序列（&gt;1000步）上仍然会丢失信息。遗忘门不可能永远是1——那样模型就没有”忘记”的能力。</p>
<p><strong>3. 缺乏显式的位置信息</strong></p>
<p>LSTM通过顺序处理隐式编码位置，但无法像后来的位置编码那样精确地表示绝对或相对位置。</p>
</section>
<section id="开放研究问题" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">10.3</span> 开放研究问题</h3>
<ol type="1">
<li><strong>最优门控结构</strong>：LSTM和GRU的门控设计是启发式的，是否存在理论上最优的结构？</li>
<li><strong>长度泛化</strong>：在短序列上训练的模型能否泛化到更长的序列？</li>
<li><strong>RNN与Transformer的融合</strong>：是否可以结合RNN的归纳偏置和Transformer的并行性？（如线性RNN、Mamba等近期工作）</li>
</ol>
<hr>
</section>
</section>
<section id="局限性与展望" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="局限性与展望"><span class="header-section-number">11</span> 局限性与展望</h2>
<section id="本章方法的核心局限" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="本章方法的核心局限"><span class="header-section-number">11.1</span> 本章方法的核心局限</h3>
<p><strong>1. 信息瓶颈（最严重）</strong></p>
<p>Seq2Seq将整个输入压缩到一个固定向量，导致： - 长序列信息丢失 - 无法回溯查看输入的特定位置</p>
<p><strong>2. 顺序计算瓶颈</strong></p>
<p>RNN必须按时间步串行计算，无法利用现代GPU的并行能力。</p>
<p><strong>3. 梯度消失/爆炸（部分缓解）</strong></p>
<p>LSTM/GRU缓解但未根本解决。极长序列仍有问题。</p>
</section>
<section id="这些局限指向什么" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="这些局限指向什么"><span class="header-section-number">11.2</span> 这些局限指向什么？</h3>
<p>信息瓶颈问题促使研究者思考：<strong>解码器是否必须只看一个向量？能否让它在每一步都访问编码器的所有状态？</strong></p>
<p>这个问题的答案是<strong>Attention机制</strong>——下一章的主题。</p>
<p>Attention最初被设计为Seq2Seq的补充，让解码器能够”注意”编码器的不同位置。它将RNN推向了最后的辉煌。</p>
<p>但更具革命性的发现是：<strong>如果Attention足够强大，我们是否还需要RNN？</strong></p>
<p>这个问题的答案是Transformer——完全抛弃循环结构，用纯Attention建模序列。这将在第8章详细展开。</p>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">12</span> 本章小结</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>核心要点
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>RNN的核心思想</strong>：用共享参数的循环结构处理序列，通过隐藏状态在时间步之间传递信息</li>
<li><strong>梯度消失问题</strong>：vanilla RNN的梯度在时间维度上指数衰减/爆炸，无法学习长距离依赖</li>
<li><strong>LSTM的解决方案</strong>：通过门控机制（遗忘门、输入门、输出门）和细胞状态，让信息可以不衰减地传递</li>
<li><strong>GRU的简化</strong>：用更少的参数（重置门、更新门）达到类似效果</li>
<li><strong>Seq2Seq架构</strong>：用Encoder-Decoder结构处理变长到变长的映射</li>
<li><strong>信息瓶颈</strong>：Seq2Seq将整个输入压缩到一个向量，导致长序列信息丢失——这个问题催生了Attention机制</li>
</ul>
</div>
</div>
<section id="关键公式速查" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">12.1</span> 关键公式速查</h3>
<p><strong>Vanilla RNN</strong>：</p>
<p><span class="math display">\[
\mathbf{h}_t = \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span></p>
<p><strong>LSTM</strong>：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{f}_t &amp;= \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) \\
\mathbf{i}_t &amp;= \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) \\
\tilde{\mathbf{c}}_t &amp;= \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) \\
\mathbf{c}_t &amp;= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \\
\mathbf{o}_t &amp;= \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) \\
\mathbf{h}_t &amp;= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
\end{aligned}
\]</span></p>
<p><strong>GRU</strong>：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{r}_t &amp;= \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\mathbf{z}_t &amp;= \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\tilde{\mathbf{h}}_t &amp;= \tanh(\mathbf{W}_h [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\mathbf{h}_t &amp;= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
\end{aligned}
\]</span></p>
<hr>
</section>
</section>
<section id="思考题" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="思考题"><span class="header-section-number">13</span> 思考题</h2>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么说RNN实现了”时间维度上的权重共享”？这与CNN中的空间权重共享有什么异同？</p></li>
<li><p><strong>[数学推导]</strong> 证明：如果 <span class="math inline">\(\mathbf{W}_{hh}\)</span> 的所有特征值的绝对值都小于1，那么 <span class="math inline">\(\mathbf{W}_{hh}^T \to \mathbf{0}\)</span> 当 <span class="math inline">\(T \to \infty\)</span>。这如何解释vanilla RNN的梯度消失？</p></li>
<li><p><strong>[工程实践]</strong> 在PyTorch中，<code>nn.LSTM</code> 的 <code>hidden_size</code> 和 <code>num_layers</code> 参数如何影响模型的容量和计算成本？如果要增加模型容量，增加 <code>hidden_size</code> 和增加 <code>num_layers</code> 哪个更有效？</p></li>
<li><p><strong>[批判思考]</strong> GRU的更新公式 <span class="math inline">\(\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t\)</span> 可以看作是旧状态和新候选状态的插值。这个设计隐含了什么假设？有什么局限性？</p></li>
<li><p><strong>[开放问题]</strong> Seq2Seq的信息瓶颈问题有没有其他解决方案，不使用Attention？（提示：考虑使用多个向量表示输入，或使用记忆网络）</p></li>
</ol>
<hr>
</section>
<section id="延伸阅读" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">14</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="14.1">
<h3 data-number="14.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">14.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>[Hochreiter &amp; Schmidhuber, 1997] Long Short-Term Memory</strong>
<ul>
<li>LSTM的原始论文，提出门控机制</li>
<li>重点读：Section 3（LSTM架构）、Section 4（为什么能解决梯度消失）</li>
</ul></li>
<li><strong>[Cho et al., 2014] Learning Phrase Representations using RNN Encoder-Decoder</strong>
<ul>
<li>提出GRU和Encoder-Decoder架构</li>
<li>重点读：Section 3（GRU公式）、Section 2.2（Encoder-Decoder）</li>
</ul></li>
<li><strong>[Sutskever et al., 2014] Sequence to Sequence Learning with Neural Networks</strong>
<ul>
<li>Seq2Seq在机器翻译上的突破性工作</li>
<li>重点读：Section 3.4（反转输入技巧）、实验结果</li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="14.2">
<h3 data-number="14.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">14.2</span> 理论基础</h3>
<ul>
<li><strong>[Bengio et al., 1994] Learning Long-Term Dependencies with Gradient Descent is Difficult</strong>
<ul>
<li>梯度消失问题的理论分析</li>
<li>重点读：理论证明部分</li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="14.3">
<h3 data-number="14.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">14.3</span> 后续发展</h3>
<ul>
<li><strong>[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate</strong>
<ul>
<li>提出Attention机制，解决信息瓶颈问题</li>
<li>这是下一章的核心论文</li>
</ul></li>
<li><strong>[Gu et al., 2023] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</strong>
<ul>
<li>结合RNN和Transformer优点的最新尝试</li>
<li>重点读：与Transformer的对比分析</li>
</ul></li>
</ul>


<!-- -->

</section>
</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第5章：循环神经网络时代"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "当神经网络学会记忆：从RNN到Seq2Seq的序列建模之路"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-25"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, RNN, LSTM, GRU, Seq2Seq, 序列建模]</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [循环神经网络, 门控机制, 梯度消失, Encoder-Decoder, 信息瓶颈]</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "序列建模的第一个黄金时代：RNN如何学会记忆，LSTM/GRU如何解决梯度消失，以及Seq2Seq架构的信息瓶颈问题。"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何让神经网络处理变长序列，并捕获序列中的依赖关系？</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：1997 (LSTM) → 2014 (GRU, Seq2Seq) </span><span class="pp">|</span><span class="at"> Hochreiter &amp; Schmidhuber, Cho et al., Sutskever et al.</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>上一章我们解决了一个基础但关键的问题：如何将文本切分为模型可处理的单元。无论是BPE、WordPiece还是SentencePiece，Tokenizer的输出都是一个**token序列**。</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>现在我们面临下一个问题：拿到这个序列后，该怎么办？</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>回顾第3章的词向量。Word2Vec和GloVe为我们提供了将token映射为向量的方法。于是，一个句子"I love NLP"变成了三个向量的序列：$<span class="co">[</span><span class="ot">\mathbf{v}_\text{I}, \mathbf{v}_\text{love}, \mathbf{v}_\text{NLP}</span><span class="co">]</span>$。</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>但这里有一个根本性的问题：**这些向量是独立的**。$\mathbf{v}_\text{love}$ 不知道它前面是"I"还是"They"，也不知道后面是"NLP"还是"cats"。每个词向量都是在真空中计算的。</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>这在很多任务中是致命的。考虑这两个句子：</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **bank** of the river was muddy."</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I went to the **bank** to deposit money."</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>静态词向量给"bank"的表示是相同的，但它们的含义完全不同。模型需要看到上下文才能区分。</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>更广泛地说，语言是**序列性**的。词的顺序至关重要——"dog bites man"和"man bites dog"含义截然不同。我们需要一种能够**处理序列、捕获上下文**的模型架构。</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>这就引出了本章的主角：**循环神经网络（RNN）**及其变体。</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：RNN通过"隐藏状态"在时间维度上传递信息，让模型拥有了"记忆"。但这种记忆是有限的——长距离依赖会衰减，梯度会消失。LSTM和GRU通过门控机制缓解了这个问题，但并没有根本解决。Seq2Seq架构将RNN推向应用巅峰，也暴露了其终极瓶颈：所有信息必须压缩到一个固定长度的向量中。这个瓶颈将直接催生下一章的Attention机制。</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### 序列建模的核心挑战</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>让我们精确地定义我们要解决的问题。</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>给定一个输入序列 $\mathbf{x} = (x_1, x_2, \ldots, x_T)$，我们希望模型能够：</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**理解上下文**：$x_t$ 的表示应该依赖于 $x_1, \ldots, x_{t-1}$（以及可能的 $x_{t+1}, \ldots, x_T$）</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**处理变长输入**：不同的序列可能有不同的长度 $T$</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**捕获长距离依赖**：$x_1$ 可能影响 $x_{100}$ 的理解</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**输出灵活**：可能输出单个标签（分类）、同长序列（标注）、或不同长度序列（翻译）</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>传统的前馈神经网络（MLP）无法满足这些要求。MLP接受固定大小的输入，输出固定大小的结果，没有任何机制来处理序列结构。</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么不能简单地拼接词向量？</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>一个直觉的想法是：把所有词向量拼接起来，送入一个大的MLP。</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>\mathbf{h} = \text{MLP}(<span class="co">[</span><span class="ot">\mathbf{v}_{x_1}; \mathbf{v}_{x_2}; \ldots; \mathbf{v}_{x_T}</span><span class="co">]</span>)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>这个方案有三个致命问题：</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>**第一，输入维度固定**。如果我们设计网络处理最长100个词，那么99个词的句子需要填充，101个词的句子无法处理。实际文本的长度变化极大——一条推文可能5个词，一篇论文可能5000个词。</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>**第二，参数爆炸**。假设词向量维度是300，最大序列长度是1000，那么第一层的输入维度就是300,000。这意味着海量的参数和计算。</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>**第三，没有位置共享**。模型需要单独学习"第1个位置的'love'"和"第50个位置的'love'"的含义，无法泛化位置信息。一个词在不同位置的处理方式应该是相似的。</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>我们需要一种更优雅的方案：让模型**逐步处理序列**，在每一步累积信息，用有限的参数处理任意长度的输入。</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="fu">## RNN：时间维度上的权重共享</span></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心思想</span></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>RNN的核心洞察是：**用同一组参数处理序列中的每一个位置，通过隐藏状态在时间步之间传递信息**。</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>想象你在读一本小说。你不会把整本书一次性塞进大脑，而是一个词一个词地读。关键是，当你读到第100页时，你的大脑保留着前99页的"记忆"——虽然不是逐字记忆，而是一种压缩的、与当前阅读相关的状态。这个状态会影响你对第100页的理解。</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>RNN就是这个过程的数学模型：</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{h}_t$ 是第 $t$ 步的**隐藏状态**（hidden state），它编码了到目前为止序列的所有信息。$f$ 是一个函数，它接收上一步的隐藏状态和当前的输入，产生新的隐藏状态。</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>关键点在于：**$f$ 在每一个时间步都是相同的**。这就是"时间维度上的权重共享"——不管序列有多长，我们只需要一组参数。</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数学形式化</span></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Vanilla RNN Forward Pass (Elman, 1990)</span></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_forward(x_sequence, h_0, W_hh, W_xh, b_h):</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""RNN 前向传播（处理整个序列）"""</span></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> h_0  <span class="co"># 初始隐藏状态，通常为零向量</span></span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> []</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x_t <span class="kw">in</span> x_sequence:</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 核心公式：线性变换 + 非线性激活</span></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>        h_t <span class="op">=</span> tanh(W_hh <span class="op">@</span> h_t <span class="op">+</span> W_xh <span class="op">@</span> x_t <span class="op">+</span> b_h)</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>        hidden_states.append(h_t)</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hidden_states  <span class="co"># 返回所有时间步的隐藏状态</span></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>*Reference: Elman (1990) "Finding Structure in Time", Cognitive Science 14(2):179-211*</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>最简单的RNN（vanilla RNN）使用线性变换加非线性激活：</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>其中：</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{W}_{hh} \in \mathbb{R}^{d_h \times d_h}$：隐藏状态到隐藏状态的权重矩阵</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{W}_{xh} \in \mathbb{R}^{d_h \times d_x}$：输入到隐藏状态的权重矩阵</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{b}_h \in \mathbb{R}^{d_h}$：偏置向量</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\tanh$：激活函数，将输出压缩到 $(-1, 1)$</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>初始隐藏状态 $\mathbf{h}_0$ 通常初始化为零向量。</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>如果需要输出（比如每一步预测下一个词），可以加一个输出层：</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>\mathbf{y}_t = \mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_y</span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：RNN前向传播</span></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>让我们用一个极简的例子走一遍RNN的计算过程。</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入序列：两个token，维度 $d_x = 2$</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>隐藏状态维度：$d_h = 3$</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入：$\mathbf{x}_1 = <span class="co">[</span><span class="ot">1, 0</span><span class="co">]</span>^T$，$\mathbf{x}_2 = <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^T$</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>**参数**（简化的小数值）：</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_{xh} = \begin{bmatrix} 0.5 &amp; 0.3 <span class="sc">\\</span> 0.2 &amp; 0.4 <span class="sc">\\</span> 0.1 &amp; 0.6 \end{bmatrix}, \quad</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_{hh} = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.1 <span class="sc">\\</span> 0.3 &amp; 0.1 &amp; 0.2 <span class="sc">\\</span> 0.2 &amp; 0.3 &amp; 0.1 \end{bmatrix}, \quad</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a>\mathbf{b}_h = \begin{bmatrix} 0 <span class="sc">\\</span> 0 <span class="sc">\\</span> 0 \end{bmatrix}</span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>**Step 1：初始化**</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_0 = <span class="co">[</span><span class="ot">0, 0, 0</span><span class="co">]</span>^T</span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a>**Step 2：处理第一个token**</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_1 &amp;= \mathbf{W}_{hh}\mathbf{h}_0 + \mathbf{W}_{xh}\mathbf{x}_1 + \mathbf{b}_h <span class="sc">\\</span></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix} 0 <span class="sc">\\</span> 0 <span class="sc">\\</span> 0 \end{bmatrix} + \begin{bmatrix} 0.5 \cdot 1 + 0.3 \cdot 0 <span class="sc">\\</span> 0.2 \cdot 1 + 0.4 \cdot 0 <span class="sc">\\</span> 0.1 \cdot 1 + 0.6 \cdot 0 \end{bmatrix} <span class="sc">\\</span></span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix} 0.5 <span class="sc">\\</span> 0.2 <span class="sc">\\</span> 0.1 \end{bmatrix}</span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_1 = \tanh(\mathbf{z}_1) = \begin{bmatrix} \tanh(0.5) <span class="sc">\\</span> \tanh(0.2) <span class="sc">\\</span> \tanh(0.1) \end{bmatrix} \approx \begin{bmatrix} 0.462 <span class="sc">\\</span> 0.197 <span class="sc">\\</span> 0.100 \end{bmatrix}</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>**Step 3：处理第二个token**</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_2 &amp;= \mathbf{W}_{hh}\mathbf{h}_1 + \mathbf{W}_{xh}\mathbf{x}_2 + \mathbf{b}_h <span class="sc">\\</span></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix} 0.1 \cdot 0.462 + 0.2 \cdot 0.197 + 0.1 \cdot 0.100 <span class="sc">\\</span> 0.3 \cdot 0.462 + 0.1 \cdot 0.197 + 0.2 \cdot 0.100 <span class="sc">\\</span> 0.2 \cdot 0.462 + 0.3 \cdot 0.197 + 0.1 \cdot 0.100 \end{bmatrix} + \begin{bmatrix} 0.3 <span class="sc">\\</span> 0.4 <span class="sc">\\</span> 0.6 \end{bmatrix} <span class="sc">\\</span></span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix} 0.046 + 0.039 + 0.010 + 0.3 <span class="sc">\\</span> 0.139 + 0.020 + 0.020 + 0.4 <span class="sc">\\</span> 0.092 + 0.059 + 0.010 + 0.6 \end{bmatrix} <span class="sc">\\</span></span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix} 0.395 <span class="sc">\\</span> 0.579 <span class="sc">\\</span> 0.761 \end{bmatrix}</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_2 = \tanh(\mathbf{z}_2) \approx \begin{bmatrix} 0.375 <span class="sc">\\</span> 0.521 <span class="sc">\\</span> 0.642 \end{bmatrix}</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>**解读**：$\mathbf{h}_2$ 编码了整个序列 $<span class="co">[</span><span class="ot">\mathbf{x}_1, \mathbf{x}_2</span><span class="co">]</span>$ 的信息。注意它不仅受 $\mathbf{x}_2$ 影响，也包含了 $\mathbf{x}_1$ 通过 $\mathbf{h}_1$ 传递过来的信息。</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a><span class="fu">### RNN的计算图与参数共享</span></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>下图展示了RNN在时间维度上的"展开"（unrolling）：</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a><span class="al">![RNN展开图：左侧是循环表示，右侧是展开后的时间步。每个时间步接收输入 $x_t$，产生隐藏状态 $h_t$。所有时间步共享同一组参数 A。](figures/chapter-5/original/fig-rnn-unrolled-colah.png)</span>{#fig-rnn-unrolled width=85%}</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a>*Source: Christopher Olah (2015) "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"*</span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>关键观察：**所有"RNN Cell"共享相同的参数**（$\mathbf{W}_{hh}, \mathbf{W}_{xh}, \mathbf{b}_h$）。这就是权重共享——无论序列有10个token还是1000个token，参数量都是固定的。</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>另一种理解方式是把RNN"展开"（unroll）成时间步：展开后看起来像一个很深的前馈网络，但每一层用的是同一组参数。</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a><span class="fu">## 梯度消失与梯度爆炸</span></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的本质</span></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a>RNN理论上可以捕获任意长距离的依赖——$\mathbf{h}_{100}$ 包含了 $\mathbf{x}_1$ 的信息（通过连续的传递）。但在实践中，这个"理论上"往往不成立。</span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>问题出在**梯度的传播**上。</span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a>考虑损失函数 $\mathcal{L}$ 对早期隐藏状态 $\mathbf{h}_1$ 的梯度。根据链式法则：</span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathcal{L}}{\partial \mathbf{h}_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_T} \cdot \frac{\partial \mathbf{h}_T}{\partial \mathbf{h}_{T-1}} \cdot \frac{\partial \mathbf{h}_{T-1}}{\partial \mathbf{h}_{T-2}} \cdots \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1}</span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>关键在于这些连乘的雅可比矩阵 $\frac{\partial \mathbf{h}_{t}}{\partial \mathbf{h}_{t-1}}$。</span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a>对于 vanilla RNN，我们有：</span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)</span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>因此：</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = \text{diag}(1 - \mathbf{h}_t^2) \cdot \mathbf{W}_{hh}</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a>其中 $\text{diag}(1 - \mathbf{h}_t^2)$ 是 $\tanh$ 的导数（因为 $\tanh'(x) = 1 - \tanh^2(x)$）。</span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数学分析：为什么会消失或爆炸？</span></span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a>当我们连乘 $T-1$ 个这样的雅可比矩阵时：</span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a>\prod_{t=2}^{T} \frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}} = \prod_{t=2}^{T} \text{diag}(1 - \mathbf{h}_t^2) \cdot \mathbf{W}_{hh}</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a>简化分析：假设所有 $\mathbf{h}_t$ 接近零（激活值小），那么 $1 - \mathbf{h}_t^2 \approx 1$，连乘大约是：</span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_{hh}^{T-1}</span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a>现在，根据 $\mathbf{W}_{hh}$ 的**特征值**分布：</span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>如果最大特征值 $|\lambda_{\max}| &lt; 1$：$\mathbf{W}_{hh}^{T-1} \to 0$（指数衰减）→ **梯度消失**</span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>如果最大特征值 $|\lambda_{\max}| &gt; 1$：$\mathbf{W}_{hh}^{T-1} \to \infty$（指数增长）→ **梯度爆炸**</span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a>实际情况更复杂，因为 $\tanh$ 的导数在激活值大时趋近于0，这会**加剧梯度消失**。</span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### 直觉理解：信息的"衰减"</span></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a>用一个类比来理解。想象你在玩"传话游戏"：第一个人说一句话，传给第二个人，第二个人传给第三个人……传100个人之后，原始信息还剩多少？</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a>在vanilla RNN中，信息每经过一个时间步，都要经过一次"压缩和混合"（矩阵乘法+非线性）。如果这个过程是"有损"的（信息被衰减），那么经过100步后，第1步的信息几乎完全消失。</span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a>这就是为什么vanilla RNN无法捕获长距离依赖：不是模型没有"记住"早期信息的能力，而是梯度无法有效地流回早期时间步，导致模型学不到长距离的模式。</span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a><span class="fu">### 梯度爆炸的简单修复：梯度裁剪</span></span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a>梯度爆炸相对容易处理：当梯度的范数超过某个阈值时，按比例缩小。</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a>\mathbf{g} \leftarrow \begin{cases}</span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>\mathbf{g} &amp; \text{if } <span class="sc">\|</span>\mathbf{g}<span class="sc">\|</span> \leq \theta <span class="sc">\\</span></span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a>\frac{\theta}{<span class="sc">\|</span>\mathbf{g}<span class="sc">\|</span>} \mathbf{g} &amp; \text{if } <span class="sc">\|</span>\mathbf{g}<span class="sc">\|</span> &gt; \theta</span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a>这被称为**梯度裁剪**（gradient clipping），几乎是所有RNN训练的标配。</span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a>但梯度消失没有这么简单的修复方法。我们需要更根本的架构改变。</span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a><span class="fu">## LSTM：门控机制的智慧</span></span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心洞察</span></span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>1997年，Hochreiter和Schmidhuber提出了**长短期记忆网络**（Long Short-Term Memory, LSTM）。他们的核心洞察是：</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 问题不在于RNN没有记忆能力，而在于信息在时间步之间传递时被"过度处理"了。如果我们能让某些信息**不经修改地直接传递**，就能保留长距离依赖。</span></span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>这个想法的具体实现是**门控机制**：用可学习的"门"来控制信息的流动——哪些信息要保留，哪些要遗忘，哪些要更新。</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a><span class="fu">### 直觉：细胞状态作为"传送带"</span></span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>LSTM引入了一个新的概念：**细胞状态**（cell state）$\mathbf{c}_t$，它像一条传送带一样在时间步之间传递。</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>想象一个工厂的传送带：物品（信息）在传送带上移动。沿途有几个工作站：</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**遗忘门**：决定丢弃传送带上的哪些物品</span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**输入门**：决定往传送带上放哪些新物品</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**输出门**：决定从传送带上取出哪些物品作为当前的输出</span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a>关键在于：传送带本身的传递是**近乎恒等的**——没有被遗忘的信息可以不经修改地传到下游。这就避免了梯度消失问题。</span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a>下图展示了LSTM的内部结构。与简单RNN只有一个tanh层不同，LSTM有**四个相互作用的层**（三个sigmoid门 + 一个tanh层）：</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a><span class="al">![LSTM单元结构：黄色方框是神经网络层（σ = sigmoid，tanh = tanh），粉色圆圈是逐元素操作（× = 乘法，+ = 加法）。顶部的横线是细胞状态 $c_t$（"传送带"），底部的横线是隐藏状态 $h_t$。](figures/chapter-5/original/fig-lstm-chain-colah.png)</span>{#fig-lstm-chain width=90%}</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>*Source: Christopher Olah (2015) "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"*</span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数学形式化</span></span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: LSTM Forward Pass (Hochreiter &amp; Schmidhuber, 1997)</span></span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lstm_forward(x_t, h_prev, c_prev, W_f, W_i, W_c, W_o):</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""LSTM 单步前向传播"""</span></span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a>    concat <span class="op">=</span> [h_prev, x_t]</span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 遗忘门：决定丢弃多少旧的细胞状态</span></span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a>    f_t <span class="op">=</span> sigmoid(W_f <span class="op">@</span> concat)</span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输入门：决定写入多少新信息</span></span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a>    i_t <span class="op">=</span> sigmoid(W_i <span class="op">@</span> concat)</span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a>    c_tilde <span class="op">=</span> tanh(W_c <span class="op">@</span> concat)  <span class="co"># 候选细胞状态</span></span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 更新细胞状态：遗忘旧信息 + 写入新信息</span></span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>    c_t <span class="op">=</span> f_t <span class="op">*</span> c_prev <span class="op">+</span> i_t <span class="op">*</span> c_tilde</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输出门：决定输出多少细胞状态</span></span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a>    o_t <span class="op">=</span> sigmoid(W_o <span class="op">@</span> concat)</span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> o_t <span class="op">*</span> tanh(c_t)</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t, c_t</span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a>*Source: Hochreiter &amp; Schmidhuber (1997) "Long Short-Term Memory", Neural Computation 9(8):1735-1780*</span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a>LSTM在每个时间步计算以下量：</span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a>**遗忘门**：决定遗忘多少旧的细胞状态</span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>\mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)</span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a>**输入门**：决定写入多少新信息</span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a>\mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)</span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a>**候选细胞状态**：新信息的候选值</span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c)</span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a>**细胞状态更新**：</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t</span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a>**输出门**：决定输出多少细胞状态</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a>\mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)</span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a>**隐藏状态**：</span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)</span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a>其中 $\sigma$ 是sigmoid函数（输出在 $(0, 1)$），$\odot$ 是逐元素乘法。</span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么LSTM解决了梯度消失？</span></span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a>关键在于细胞状态的更新公式：</span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t</span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a>考虑梯度 $\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}}$：</span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \text{diag}(\mathbf{f}_t) + \text{其他项}</span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a>如果遗忘门 $\mathbf{f}_t$ 接近1，那么 $\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} \approx \mathbf{I}$（单位矩阵）。</span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a>这意味着：**梯度可以几乎不衰减地流过多个时间步**。只要遗忘门学习到保持信息（$\mathbf{f}_t \approx 1$），早期时间步的信息就能对后期产生影响，模型也能学到长距离依赖。</span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a>这和ResNet中残差连接的原理非常类似：提供一条"高速公路"让梯度直接流动。</span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：LSTM前向传播</span></span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入维度 $d_x = 2$，隐藏维度 $d_h = 2$</span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入序列：$\mathbf{x}_1 = <span class="co">[</span><span class="ot">1, 0</span><span class="co">]</span>^T$，$\mathbf{x}_2 = <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^T$</span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>初始状态：$\mathbf{h}_0 = <span class="co">[</span><span class="ot">0, 0</span><span class="co">]</span>^T$，$\mathbf{c}_0 = <span class="co">[</span><span class="ot">0, 0</span><span class="co">]</span>^T$</span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a>**简化参数**（为便于计算，使用小值）：</span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_f = \mathbf{W}_i = \mathbf{W}_c = \mathbf{W}_o = \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.3 &amp; 0.3 <span class="sc">\\</span> 0.3 &amp; 0.3 &amp; 0.5 &amp; 0.5 \end{bmatrix}</span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a>所有偏置为零。</span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a>**Step 1：处理 $\mathbf{x}_1$**</span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a>拼接输入：$<span class="co">[</span><span class="ot">\mathbf{h}_0, \mathbf{x}_1</span><span class="co">]</span> = <span class="co">[</span><span class="ot">0, 0, 1, 0</span><span class="co">]</span>^T$</span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a>计算各个门（省略详细矩阵乘法）：</span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a>\mathbf{f}_1 &amp;= \sigma(<span class="co">[</span><span class="ot">0.3, 0.5</span><span class="co">]</span>^T) = <span class="co">[</span><span class="ot">0.574, 0.622</span><span class="co">]</span>^T <span class="sc">\\</span></span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a>\mathbf{i}_1 &amp;= \sigma(<span class="co">[</span><span class="ot">0.3, 0.5</span><span class="co">]</span>^T) = <span class="co">[</span><span class="ot">0.574, 0.622</span><span class="co">]</span>^T <span class="sc">\\</span></span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbf{c}}_1 &amp;= \tanh(<span class="co">[</span><span class="ot">0.3, 0.5</span><span class="co">]</span>^T) = <span class="co">[</span><span class="ot">0.291, 0.462</span><span class="co">]</span>^T <span class="sc">\\</span></span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a>\mathbf{o}_1 &amp;= \sigma(<span class="co">[</span><span class="ot">0.3, 0.5</span><span class="co">]</span>^T) = <span class="co">[</span><span class="ot">0.574, 0.622</span><span class="co">]</span>^T</span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a>更新细胞状态：</span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_1 = \mathbf{f}_1 \odot \mathbf{c}_0 + \mathbf{i}_1 \odot \tilde{\mathbf{c}}_1 = <span class="co">[</span><span class="ot">0, 0</span><span class="co">]</span> + <span class="co">[</span><span class="ot">0.167, 0.287</span><span class="co">]</span>^T = <span class="co">[</span><span class="ot">0.167, 0.287</span><span class="co">]</span>^T</span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a>计算隐藏状态：</span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_1 = \mathbf{o}_1 \odot \tanh(\mathbf{c}_1) = <span class="co">[</span><span class="ot">0.574, 0.622</span><span class="co">]</span> \odot <span class="co">[</span><span class="ot">0.165, 0.279</span><span class="co">]</span> = <span class="co">[</span><span class="ot">0.095, 0.174</span><span class="co">]</span>^T</span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a>**Step 2：处理 $\mathbf{x}_2$**</span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a>拼接输入：$<span class="co">[</span><span class="ot">\mathbf{h}_1, \mathbf{x}_2</span><span class="co">]</span> = <span class="co">[</span><span class="ot">0.095, 0.174, 0, 1</span><span class="co">]</span>^T$</span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a>（类似计算，省略细节）</span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a>最终得到 $\mathbf{h}_2$ 和 $\mathbf{c}_2$，编码了整个序列的信息。</span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a>**关键观察**：$\mathbf{c}_1$ 中的信息通过遗忘门控制，部分保留到 $\mathbf{c}_2$。如果 $\mathbf{f}_2 \approx 1$，则 $\mathbf{x}_1$ 的信息几乎完整传递。</span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a><span class="fu">## GRU：门控机制的简化</span></span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a><span class="fu">### 动机</span></span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a>LSTM有4个门、两个状态（$\mathbf{h}$ 和 $\mathbf{c}$），参数量较大。2014年，Cho等人提出了**门控循环单元**（Gated Recurrent Unit, GRU），用更少的参数达到相似的效果。</span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a>GRU的核心简化：</span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**合并细胞状态和隐藏状态**：只保留一个状态 $\mathbf{h}$</span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**合并遗忘门和输入门**：用一个"更新门"同时控制遗忘和写入</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a><span class="fu">### GRU 单元结构</span></span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a><span class="al">![GRU hidden unit 结构图：update gate (z) 控制保留多少旧状态，reset gate (r) 控制如何将旧状态与新输入结合。](figures/chapter-5/original/fig-gru-cell.png)</span>{#fig-gru-cell width=60%}</span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>*Source: Cho et al. (2014) "Learning Phrase Representations using RNN Encoder-Decoder", Figure 2. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)*</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数学形式化</span></span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: GRU Forward Pass (Cho et al., 2014)</span></span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gru_forward(x_t, h_prev, W_r, W_z, W_h):</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""GRU 单步前向传播"""</span></span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 重置门：决定忽略多少旧状态</span></span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a>    r_t <span class="op">=</span> sigmoid(W_r <span class="op">@</span> concat(h_prev, x_t))</span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 更新门：决定保留多少旧状态</span></span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a>    z_t <span class="op">=</span> sigmoid(W_z <span class="op">@</span> concat(h_prev, x_t))</span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 候选状态：用重置门过滤旧状态后，与新输入结合</span></span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a>    h_tilde <span class="op">=</span> tanh(W_h <span class="op">@</span> concat(r_t <span class="op">*</span> h_prev, x_t))</span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 最终状态：旧状态与候选状态的插值</span></span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> z_t <span class="op">*</span> h_prev <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> z_t) <span class="op">*</span> h_tilde</span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a>*Adapted from: Cho et al. (2014) Equations 5-8. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)*</span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a>**重置门**：决定如何将过去的信息与新输入结合</span>
<span id="cb7-515"><a href="#cb7-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a>\mathbf{r}_t = \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_r)</span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a>**更新门**：决定保留多少旧状态</span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_t = \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_z)</span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a>**候选隐藏状态**：</span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbf{h}}_t = \tanh(\mathbf{W}_h [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_h)</span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a>**隐藏状态更新**：</span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t</span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a><span class="fu">### LSTM vs GRU</span></span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方面 <span class="pp">|</span> LSTM <span class="pp">|</span> GRU <span class="pp">|</span></span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|-----|</span></span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **状态数量** <span class="pp">|</span> 2（$\mathbf{h}$, $\mathbf{c}$） <span class="pp">|</span> 1（$\mathbf{h}$） <span class="pp">|</span></span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **门数量** <span class="pp">|</span> 3（遗忘、输入、输出） <span class="pp">|</span> 2（重置、更新） <span class="pp">|</span></span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **参数量** <span class="pp">|</span> 较多 <span class="pp">|</span> 较少（约75%） <span class="pp">|</span></span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **性能** <span class="pp">|</span> 通常略好 <span class="pp">|</span> 相当，有时更好 <span class="pp">|</span></span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **训练速度** <span class="pp">|</span> 较慢 <span class="pp">|</span> 较快 <span class="pp">|</span></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a>实践中的选择：</span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**数据量大、任务复杂**：LSTM可能略有优势</span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**计算资源有限、需要快速迭代**：GRU是更好的选择</span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**很多情况下差异不大**：先用GRU快速验证，必要时换LSTM</span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a><span class="fu">## Seq2Seq：Encoder-Decoder架构</span></span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心问题：变长到变长的映射</span></span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a>之前我们讨论的是"理解"任务——输入一个序列，输出一个向量（分类）或同长度序列（标注）。但很多重要任务需要**输入和输出都是序列，且长度不同**：</span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**机器翻译**："I love NLP" → "我喜欢自然语言处理"（3词 → 6词）</span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**文本摘要**：长文章 → 短摘要</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**对话生成**：问题 → 回答</span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a>这就需要一种新的架构：**Seq2Seq**（Sequence-to-Sequence），也称为Encoder-Decoder架构。</span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a><span class="fu">### 架构设计</span></span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a>Seq2Seq由两个RNN组成：</span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**编码器（Encoder）**：读取输入序列，将其压缩为一个固定长度的向量</span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**解码器（Decoder）**：接收这个向量，生成输出序列</span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a><span class="al">![RNN Encoder-Decoder 架构：Encoder 将输入序列 $(x_1, ..., x_T)$ 编码为上下文向量 $c$，Decoder 从 $c$ 生成输出序列 $(y_1, ..., y_{T'})$。](figures/chapter-5/original/fig-encoder-decoder.png)</span>{#fig-encoder-decoder width=50%}</span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a>*Source: Cho et al. (2014) "Learning Phrase Representations using RNN Encoder-Decoder", Figure 1. [arXiv:1406.1078](https://arxiv.org/abs/1406.1078)*</span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a>下图展示了 Sutskever 等人的经典 Seq2Seq 架构，注意**输入序列是反转的**（ABC 读取为 CBA），这是他们发现的一个有效技巧：</span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a><span class="al">![Seq2Seq 架构（Sutskever et al., 2014）：输入 "ABC" 被反转后读取，输出 "WXYZ"。模型在输出 `&lt;EOS&gt;` 后停止生成。](figures/chapter-5/original/fig-seq2seq.png)</span>{#fig-seq2seq width=80%}</span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a>*Source: Sutskever et al. (2014) "Sequence to Sequence Learning with Neural Networks", Figure 1. [arXiv:1409.3215](https://arxiv.org/abs/1409.3215)*</span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a>**编码过程**：</span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t^{enc} = \text{LSTM}_{enc}(\mathbf{h}_{t-1}^{enc}, \mathbf{x}_t)</span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a>\mathbf{c} = \mathbf{h}_T^{enc} \quad \text{（最后一步的隐藏状态作为上下文向量）}</span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a>**解码过程**：</span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t^{dec} = \text{LSTM}_{dec}(\mathbf{h}_{t-1}^{dec}, \mathbf{y}_{t-1})</span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a>P(\mathbf{y}_t | \mathbf{y}_{&lt;t}, \mathbf{x}) = \text{softmax}(\mathbf{W}_o \mathbf{h}_t^{dec})</span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a>解码器的初始隐藏状态设为上下文向量：$\mathbf{h}_0^{dec} = \mathbf{c}$。</span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练：Teacher Forcing</span></span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-613"><a href="#cb7-613" aria-hidden="true" tabindex="-1"></a>在训练时，我们使用**teacher forcing**：解码器的每一步输入是**真实的前一个词**（ground truth），而不是模型预测的词。</span>
<span id="cb7-614"><a href="#cb7-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t^{dec} = \text{LSTM}_{dec}(\mathbf{h}_{t-1}^{dec}, \mathbf{y}_{t-1}^{*})</span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{y}_{t-1}^{*}$ 是真实的第 $t-1$ 个词。</span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a>这样做的原因是：如果用模型预测的词，早期的错误会累积，导致训练不稳定。</span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a>但这也带来一个问题：训练和推理时的分布不一致（**exposure bias**）。训练时解码器总是看到正确的前缀，推理时却要依赖自己的预测。</span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a><span class="fu">### 推理：贪心搜索与束搜索</span></span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a>在推理时，我们需要生成输出序列。两种常见策略：</span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a>**贪心搜索**：每一步选择概率最高的词</span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a>\hat{y}_t = \arg\max_y P(y | \hat{y}_{&lt;t}, \mathbf{x})</span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a>简单快速，但不保证全局最优。</span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a>**束搜索（Beam Search）**：维护 $k$ 个候选序列，每一步扩展所有候选，保留得分最高的 $k$ 个。</span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a>这是一种在搜索空间和计算成本之间的权衡。实践中 $k=4$ 或 $k=5$ 通常足够。</span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a><span class="fu">## 信息瓶颈：Seq2Seq的终极局限</span></span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题描述</span></span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a>回顾Seq2Seq的核心假设：整个输入序列的信息被压缩到**一个固定长度的向量** $\mathbf{c}$ 中。</span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a>这意味着，无论输入是5个词还是500个词，都要塞进同一个维度的向量。</span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a>想象一下：你要把一本500页的书的所有信息压缩成一个1024维的向量，然后仅凭这个向量翻译成另一种语言。这能行吗？</span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a>实验证据也验证了这个担忧。Sutskever等人(2014)在机器翻译任务上发现：当输入句子超过20个词时，翻译质量急剧下降。</span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a><span class="fu">### 信息论视角</span></span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a>从信息论角度，这个问题可以精确表述。</span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a>假设输入序列 $\mathbf{x}$ 的信息熵是 $H(\mathbf{x})$。上下文向量 $\mathbf{c}$ 是 $\mathbf{x}$ 的一个有损压缩。根据数据处理不等式：</span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a>I(\mathbf{y}; \mathbf{c}) \leq I(\mathbf{y}; \mathbf{x})</span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a>其中 $I$ 是互信息。也就是说，$\mathbf{c}$ 中关于 $\mathbf{y}$ 的信息不可能超过 $\mathbf{x}$ 中的。</span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a>如果 $\mathbf{c}$ 的维度是 $d$，它最多携带 $O(d \log |\mathcal{V}|)$ 比特的信息（$|\mathcal{V}|$ 是取值范围）。当输入序列很长时，必然有信息丢失。</span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个思考实验</span></span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a>考虑机器翻译任务。输入是英语句子，输出是法语句子。</span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a>假设我们要翻译一个很长的句子，其中第100个词是一个人名"Claude"。在输出的法语句子中，这个名字应该保持不变。</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a>但根据Seq2Seq的架构，"Claude"这个词首先要编码进隐藏状态 $\mathbf{h}_{100}^{enc}$，然后经过剩下的编码步骤，最终压缩到上下文向量 $\mathbf{c}$ 中，再由解码器提取出来。</span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a>在这个过程中，"Claude"的信息可能与其他词的信息混合、被覆盖、或因梯度消失而无法有效学习。</span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a>**我们需要的是什么？**</span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a>理想情况下，当解码器生成这个名字时，它应该能够**直接"看"到编码器中对应的位置**，而不是只依赖一个压缩后的向量。</span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a>这正是Attention机制的核心思想——**让解码器在每一步都能访问编码器的所有隐藏状态**，而不是只有最后一个。</span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 这个洞察将在下一章详细展开。Seq2Seq + Attention的组合，将RNN推向了最后的辉煌，也为后来完全抛弃RNN的Transformer铺平了道路。</span></span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：LSTM文本分类</span></span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a>让我们用PyTorch实现一个简单的LSTM文本分类器，体会RNN的实际应用。</span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型定义</span></span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMClassifier(nn.Module):</span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, hidden_dim, num_classes, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(</span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a>            embed_dim,</span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a>            hidden_dim,</span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,  <span class="co"># 双向LSTM</span></span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 双向LSTM输出维度是 hidden_dim * 2</span></span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim <span class="op">*</span> <span class="dv">2</span>, num_classes)</span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, lengths<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: [batch_size, seq_len]</span></span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(x))  <span class="co"># [batch_size, seq_len, embed_dim]</span></span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LSTM前向传播</span></span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> lengths <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 使用pack_padded_sequence处理变长序列</span></span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a>            packed <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a>                embedded, lengths.cpu(), batch_first<span class="op">=</span><span class="va">True</span>, enforce_sorted<span class="op">=</span><span class="va">False</span></span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a>            packed_output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.lstm(packed)</span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>            output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.lstm(embedded)</span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [num_layers * 2, batch_size, hidden_dim] (双向)</span></span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 取最后一层的前向和后向隐藏状态</span></span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a>        hidden_forward <span class="op">=</span> hidden[<span class="op">-</span><span class="dv">2</span>, :, :]  <span class="co"># [batch_size, hidden_dim]</span></span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a>        hidden_backward <span class="op">=</span> hidden[<span class="op">-</span><span class="dv">1</span>, :, :]  <span class="co"># [batch_size, hidden_dim]</span></span>
<span id="cb7-736"><a href="#cb7-736" aria-hidden="true" tabindex="-1"></a>        hidden_concat <span class="op">=</span> torch.cat([hidden_forward, hidden_backward], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch_size, hidden_dim * 2]</span></span>
<span id="cb7-737"><a href="#cb7-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-738"><a href="#cb7-738" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.fc(<span class="va">self</span>.dropout(hidden_concat))  <span class="co"># [batch_size, num_classes]</span></span>
<span id="cb7-739"><a href="#cb7-739" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb7-740"><a href="#cb7-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-741"><a href="#cb7-741" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例：创建模型</span></span>
<span id="cb7-742"><a href="#cb7-742" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LSTMClassifier(</span>
<span id="cb7-743"><a href="#cb7-743" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>,</span>
<span id="cb7-744"><a href="#cb7-744" aria-hidden="true" tabindex="-1"></a>    embed_dim<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb7-745"><a href="#cb7-745" aria-hidden="true" tabindex="-1"></a>    hidden_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb7-746"><a href="#cb7-746" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb7-747"><a href="#cb7-747" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-748"><a href="#cb7-748" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"模型参数量: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb7-749"><a href="#cb7-749" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-750"><a href="#cb7-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-751"><a href="#cb7-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键实现细节</span></span>
<span id="cb7-752"><a href="#cb7-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-753"><a href="#cb7-753" aria-hidden="true" tabindex="-1"></a>**1. 双向LSTM**</span>
<span id="cb7-754"><a href="#cb7-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-755"><a href="#cb7-755" aria-hidden="true" tabindex="-1"></a>使用 <span class="in">`bidirectional=True`</span>，模型同时从左到右和从右到左处理序列，输出维度翻倍。这对于分类任务很有用——一个词的语义既依赖前文也依赖后文。</span>
<span id="cb7-756"><a href="#cb7-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-757"><a href="#cb7-757" aria-hidden="true" tabindex="-1"></a>**2. 变长序列处理**</span>
<span id="cb7-758"><a href="#cb7-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-759"><a href="#cb7-759" aria-hidden="true" tabindex="-1"></a>实际文本长度不一，需要填充（padding）到相同长度。<span class="in">`pack_padded_sequence`</span> 和 <span class="in">`pad_packed_sequence`</span> 让LSTM忽略填充位置，避免污染隐藏状态。</span>
<span id="cb7-760"><a href="#cb7-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-761"><a href="#cb7-761" aria-hidden="true" tabindex="-1"></a>**3. 分类策略**</span>
<span id="cb7-762"><a href="#cb7-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-763"><a href="#cb7-763" aria-hidden="true" tabindex="-1"></a>对于分类任务，我们用最后一个时间步的隐藏状态（或所有时间步的平均）作为序列的表示，送入分类层。</span>
<span id="cb7-764"><a href="#cb7-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-765"><a href="#cb7-765" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练技巧</span></span>
<span id="cb7-766"><a href="#cb7-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-767"><a href="#cb7-767" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-768"><a href="#cb7-768" aria-hidden="true" tabindex="-1"></a><span class="co"># 梯度裁剪（防止梯度爆炸）</span></span>
<span id="cb7-769"><a href="#cb7-769" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb7-770"><a href="#cb7-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-771"><a href="#cb7-771" aria-hidden="true" tabindex="-1"></a><span class="co"># 学习率调度</span></span>
<span id="cb7-772"><a href="#cb7-772" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb7-773"><a href="#cb7-773" aria-hidden="true" tabindex="-1"></a>    optimizer, mode<span class="op">=</span><span class="st">'min'</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">2</span></span>
<span id="cb7-774"><a href="#cb7-774" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-775"><a href="#cb7-775" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-776"><a href="#cb7-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-777"><a href="#cb7-777" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-778"><a href="#cb7-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-779"><a href="#cb7-779" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb7-780"><a href="#cb7-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-781"><a href="#cb7-781" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么LSTM有效？——更深入的理论视角</span></span>
<span id="cb7-782"><a href="#cb7-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-783"><a href="#cb7-783" aria-hidden="true" tabindex="-1"></a>LSTM的成功不仅仅是"门控机制"这个技巧。从更深的角度看：</span>
<span id="cb7-784"><a href="#cb7-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-785"><a href="#cb7-785" aria-hidden="true" tabindex="-1"></a>**1. 常微分方程视角**</span>
<span id="cb7-786"><a href="#cb7-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-787"><a href="#cb7-787" aria-hidden="true" tabindex="-1"></a>可以把RNN看作离散化的常微分方程：</span>
<span id="cb7-788"><a href="#cb7-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-789"><a href="#cb7-789" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-790"><a href="#cb7-790" aria-hidden="true" tabindex="-1"></a>\frac{d\mathbf{h}}{dt} = f(\mathbf{h}, \mathbf{x})</span>
<span id="cb7-791"><a href="#cb7-791" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-792"><a href="#cb7-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-793"><a href="#cb7-793" aria-hidden="true" tabindex="-1"></a>Vanilla RNN对应简单的欧拉方法，而LSTM的细胞状态更新：</span>
<span id="cb7-794"><a href="#cb7-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-795"><a href="#cb7-795" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-796"><a href="#cb7-796" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t</span>
<span id="cb7-797"><a href="#cb7-797" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-798"><a href="#cb7-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-799"><a href="#cb7-799" aria-hidden="true" tabindex="-1"></a>可以看作一种**自适应步长**的数值积分——遗忘门控制"衰减率"，输入门控制"增量"。</span>
<span id="cb7-800"><a href="#cb7-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-801"><a href="#cb7-801" aria-hidden="true" tabindex="-1"></a>**2. 记忆与计算的分离**</span>
<span id="cb7-802"><a href="#cb7-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-803"><a href="#cb7-803" aria-hidden="true" tabindex="-1"></a>LSTM将"记忆"（$\mathbf{c}$）和"计算"（$\mathbf{h}$）分离。细胞状态是长期记忆的载体，隐藏状态是当前的工作记忆。这种分离让模型能够同时保持长期信息和进行复杂的当前计算。</span>
<span id="cb7-804"><a href="#cb7-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-805"><a href="#cb7-805" aria-hidden="true" tabindex="-1"></a>**3. 可学习的遗忘**</span>
<span id="cb7-806"><a href="#cb7-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-807"><a href="#cb7-807" aria-hidden="true" tabindex="-1"></a>遗忘门的引入是关键创新。直觉上，保持信息似乎总是好的。但实际上，选择性遗忘同样重要——不相关的信息会干扰有用的信息。遗忘门让模型学习什么时候清除旧信息。</span>
<span id="cb7-808"><a href="#cb7-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-809"><a href="#cb7-809" aria-hidden="true" tabindex="-1"></a><span class="fu">### 边界条件与失效模式</span></span>
<span id="cb7-810"><a href="#cb7-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-811"><a href="#cb7-811" aria-hidden="true" tabindex="-1"></a>LSTM并不完美。以下是它的已知局限：</span>
<span id="cb7-812"><a href="#cb7-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-813"><a href="#cb7-813" aria-hidden="true" tabindex="-1"></a>**1. 顺序计算**</span>
<span id="cb7-814"><a href="#cb7-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-815"><a href="#cb7-815" aria-hidden="true" tabindex="-1"></a>LSTM必须按顺序处理序列，无法并行。这在GPU时代是严重的效率瓶颈。一个1000步的序列需要1000次串行计算。</span>
<span id="cb7-816"><a href="#cb7-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-817"><a href="#cb7-817" aria-hidden="true" tabindex="-1"></a>**2. 长距离仍有衰减**</span>
<span id="cb7-818"><a href="#cb7-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-819"><a href="#cb7-819" aria-hidden="true" tabindex="-1"></a>虽然比vanilla RNN好很多，但LSTM在极长序列（&gt;1000步）上仍然会丢失信息。遗忘门不可能永远是1——那样模型就没有"忘记"的能力。</span>
<span id="cb7-820"><a href="#cb7-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-821"><a href="#cb7-821" aria-hidden="true" tabindex="-1"></a>**3. 缺乏显式的位置信息**</span>
<span id="cb7-822"><a href="#cb7-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-823"><a href="#cb7-823" aria-hidden="true" tabindex="-1"></a>LSTM通过顺序处理隐式编码位置，但无法像后来的位置编码那样精确地表示绝对或相对位置。</span>
<span id="cb7-824"><a href="#cb7-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-825"><a href="#cb7-825" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb7-826"><a href="#cb7-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-827"><a href="#cb7-827" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**最优门控结构**：LSTM和GRU的门控设计是启发式的，是否存在理论上最优的结构？</span>
<span id="cb7-828"><a href="#cb7-828" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**长度泛化**：在短序列上训练的模型能否泛化到更长的序列？</span>
<span id="cb7-829"><a href="#cb7-829" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**RNN与Transformer的融合**：是否可以结合RNN的归纳偏置和Transformer的并行性？（如线性RNN、Mamba等近期工作）</span>
<span id="cb7-830"><a href="#cb7-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-831"><a href="#cb7-831" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-832"><a href="#cb7-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-833"><a href="#cb7-833" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与展望</span></span>
<span id="cb7-834"><a href="#cb7-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-835"><a href="#cb7-835" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本章方法的核心局限</span></span>
<span id="cb7-836"><a href="#cb7-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-837"><a href="#cb7-837" aria-hidden="true" tabindex="-1"></a>**1. 信息瓶颈（最严重）**</span>
<span id="cb7-838"><a href="#cb7-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-839"><a href="#cb7-839" aria-hidden="true" tabindex="-1"></a>Seq2Seq将整个输入压缩到一个固定向量，导致：</span>
<span id="cb7-840"><a href="#cb7-840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>长序列信息丢失</span>
<span id="cb7-841"><a href="#cb7-841" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>无法回溯查看输入的特定位置</span>
<span id="cb7-842"><a href="#cb7-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-843"><a href="#cb7-843" aria-hidden="true" tabindex="-1"></a>**2. 顺序计算瓶颈**</span>
<span id="cb7-844"><a href="#cb7-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-845"><a href="#cb7-845" aria-hidden="true" tabindex="-1"></a>RNN必须按时间步串行计算，无法利用现代GPU的并行能力。</span>
<span id="cb7-846"><a href="#cb7-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-847"><a href="#cb7-847" aria-hidden="true" tabindex="-1"></a>**3. 梯度消失/爆炸（部分缓解）**</span>
<span id="cb7-848"><a href="#cb7-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-849"><a href="#cb7-849" aria-hidden="true" tabindex="-1"></a>LSTM/GRU缓解但未根本解决。极长序列仍有问题。</span>
<span id="cb7-850"><a href="#cb7-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-851"><a href="#cb7-851" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限指向什么？</span></span>
<span id="cb7-852"><a href="#cb7-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-853"><a href="#cb7-853" aria-hidden="true" tabindex="-1"></a>信息瓶颈问题促使研究者思考：**解码器是否必须只看一个向量？能否让它在每一步都访问编码器的所有状态？**</span>
<span id="cb7-854"><a href="#cb7-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-855"><a href="#cb7-855" aria-hidden="true" tabindex="-1"></a>这个问题的答案是**Attention机制**——下一章的主题。</span>
<span id="cb7-856"><a href="#cb7-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-857"><a href="#cb7-857" aria-hidden="true" tabindex="-1"></a>Attention最初被设计为Seq2Seq的补充，让解码器能够"注意"编码器的不同位置。它将RNN推向了最后的辉煌。</span>
<span id="cb7-858"><a href="#cb7-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-859"><a href="#cb7-859" aria-hidden="true" tabindex="-1"></a>但更具革命性的发现是：**如果Attention足够强大，我们是否还需要RNN？**</span>
<span id="cb7-860"><a href="#cb7-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-861"><a href="#cb7-861" aria-hidden="true" tabindex="-1"></a>这个问题的答案是Transformer——完全抛弃循环结构，用纯Attention建模序列。这将在第8章详细展开。</span>
<span id="cb7-862"><a href="#cb7-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-863"><a href="#cb7-863" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-864"><a href="#cb7-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-865"><a href="#cb7-865" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb7-866"><a href="#cb7-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-867"><a href="#cb7-867" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb7-868"><a href="#cb7-868" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心要点</span></span>
<span id="cb7-869"><a href="#cb7-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-870"><a href="#cb7-870" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RNN的核心思想**：用共享参数的循环结构处理序列，通过隐藏状态在时间步之间传递信息</span>
<span id="cb7-871"><a href="#cb7-871" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**梯度消失问题**：vanilla RNN的梯度在时间维度上指数衰减/爆炸，无法学习长距离依赖</span>
<span id="cb7-872"><a href="#cb7-872" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LSTM的解决方案**：通过门控机制（遗忘门、输入门、输出门）和细胞状态，让信息可以不衰减地传递</span>
<span id="cb7-873"><a href="#cb7-873" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GRU的简化**：用更少的参数（重置门、更新门）达到类似效果</span>
<span id="cb7-874"><a href="#cb7-874" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Seq2Seq架构**：用Encoder-Decoder结构处理变长到变长的映射</span>
<span id="cb7-875"><a href="#cb7-875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**信息瓶颈**：Seq2Seq将整个输入压缩到一个向量，导致长序列信息丢失——这个问题催生了Attention机制</span>
<span id="cb7-876"><a href="#cb7-876" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-877"><a href="#cb7-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-878"><a href="#cb7-878" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb7-879"><a href="#cb7-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-880"><a href="#cb7-880" aria-hidden="true" tabindex="-1"></a>**Vanilla RNN**：</span>
<span id="cb7-881"><a href="#cb7-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-882"><a href="#cb7-882" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-883"><a href="#cb7-883" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)</span>
<span id="cb7-884"><a href="#cb7-884" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-885"><a href="#cb7-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-886"><a href="#cb7-886" aria-hidden="true" tabindex="-1"></a>**LSTM**：</span>
<span id="cb7-887"><a href="#cb7-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-888"><a href="#cb7-888" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-889"><a href="#cb7-889" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb7-890"><a href="#cb7-890" aria-hidden="true" tabindex="-1"></a>\mathbf{f}_t &amp;= \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) <span class="sc">\\</span></span>
<span id="cb7-891"><a href="#cb7-891" aria-hidden="true" tabindex="-1"></a>\mathbf{i}_t &amp;= \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) <span class="sc">\\</span></span>
<span id="cb7-892"><a href="#cb7-892" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbf{c}}_t &amp;= \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) <span class="sc">\\</span></span>
<span id="cb7-893"><a href="#cb7-893" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_t &amp;= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t <span class="sc">\\</span></span>
<span id="cb7-894"><a href="#cb7-894" aria-hidden="true" tabindex="-1"></a>\mathbf{o}_t &amp;= \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) <span class="sc">\\</span></span>
<span id="cb7-895"><a href="#cb7-895" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t &amp;= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)</span>
<span id="cb7-896"><a href="#cb7-896" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb7-897"><a href="#cb7-897" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-898"><a href="#cb7-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-899"><a href="#cb7-899" aria-hidden="true" tabindex="-1"></a>**GRU**：</span>
<span id="cb7-900"><a href="#cb7-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-901"><a href="#cb7-901" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-902"><a href="#cb7-902" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb7-903"><a href="#cb7-903" aria-hidden="true" tabindex="-1"></a>\mathbf{r}_t &amp;= \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t]) <span class="sc">\\</span></span>
<span id="cb7-904"><a href="#cb7-904" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_t &amp;= \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t]) <span class="sc">\\</span></span>
<span id="cb7-905"><a href="#cb7-905" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbf{h}}_t &amp;= \tanh(\mathbf{W}_h [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t]) <span class="sc">\\</span></span>
<span id="cb7-906"><a href="#cb7-906" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t &amp;= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t</span>
<span id="cb7-907"><a href="#cb7-907" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb7-908"><a href="#cb7-908" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-909"><a href="#cb7-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-910"><a href="#cb7-910" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-911"><a href="#cb7-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-912"><a href="#cb7-912" aria-hidden="true" tabindex="-1"></a><span class="fu">## 思考题</span></span>
<span id="cb7-913"><a href="#cb7-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-914"><a href="#cb7-914" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么说RNN实现了"时间维度上的权重共享"？这与CNN中的空间权重共享有什么异同？</span>
<span id="cb7-915"><a href="#cb7-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-916"><a href="#cb7-916" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明：如果 $\mathbf{W}_{hh}$ 的所有特征值的绝对值都小于1，那么 $\mathbf{W}_{hh}^T \to \mathbf{0}$ 当 $T \to \infty$。这如何解释vanilla RNN的梯度消失？</span>
<span id="cb7-917"><a href="#cb7-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-918"><a href="#cb7-918" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 在PyTorch中，<span class="in">`nn.LSTM`</span> 的 <span class="in">`hidden_size`</span> 和 <span class="in">`num_layers`</span> 参数如何影响模型的容量和计算成本？如果要增加模型容量，增加 <span class="in">`hidden_size`</span> 和增加 <span class="in">`num_layers`</span> 哪个更有效？</span>
<span id="cb7-919"><a href="#cb7-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-920"><a href="#cb7-920" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[批判思考]** GRU的更新公式 $\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t$ 可以看作是旧状态和新候选状态的插值。这个设计隐含了什么假设？有什么局限性？</span>
<span id="cb7-921"><a href="#cb7-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-922"><a href="#cb7-922" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放问题]** Seq2Seq的信息瓶颈问题有没有其他解决方案，不使用Attention？（提示：考虑使用多个向量表示输入，或使用记忆网络）</span>
<span id="cb7-923"><a href="#cb7-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-924"><a href="#cb7-924" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-925"><a href="#cb7-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-926"><a href="#cb7-926" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb7-927"><a href="#cb7-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-928"><a href="#cb7-928" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb7-929"><a href="#cb7-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-930"><a href="#cb7-930" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Hochreiter &amp; Schmidhuber, 1997] Long Short-Term Memory**</span>
<span id="cb7-931"><a href="#cb7-931" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>LSTM的原始论文，提出门控机制</span>
<span id="cb7-932"><a href="#cb7-932" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（LSTM架构）、Section 4（为什么能解决梯度消失）</span>
<span id="cb7-933"><a href="#cb7-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-934"><a href="#cb7-934" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Cho et al., 2014] Learning Phrase Representations using RNN Encoder-Decoder**</span>
<span id="cb7-935"><a href="#cb7-935" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出GRU和Encoder-Decoder架构</span>
<span id="cb7-936"><a href="#cb7-936" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（GRU公式）、Section 2.2（Encoder-Decoder）</span>
<span id="cb7-937"><a href="#cb7-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-938"><a href="#cb7-938" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Sutskever et al., 2014] Sequence to Sequence Learning with Neural Networks**</span>
<span id="cb7-939"><a href="#cb7-939" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Seq2Seq在机器翻译上的突破性工作</span>
<span id="cb7-940"><a href="#cb7-940" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3.4（反转输入技巧）、实验结果</span>
<span id="cb7-941"><a href="#cb7-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-942"><a href="#cb7-942" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb7-943"><a href="#cb7-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-944"><a href="#cb7-944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Bengio et al., 1994] Learning Long-Term Dependencies with Gradient Descent is Difficult**</span>
<span id="cb7-945"><a href="#cb7-945" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>梯度消失问题的理论分析</span>
<span id="cb7-946"><a href="#cb7-946" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：理论证明部分</span>
<span id="cb7-947"><a href="#cb7-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-948"><a href="#cb7-948" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb7-949"><a href="#cb7-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-950"><a href="#cb7-950" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**</span>
<span id="cb7-951"><a href="#cb7-951" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出Attention机制，解决信息瓶颈问题</span>
<span id="cb7-952"><a href="#cb7-952" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>这是下一章的核心论文</span>
<span id="cb7-953"><a href="#cb7-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-954"><a href="#cb7-954" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Gu et al., 2023] Mamba: Linear-Time Sequence Modeling with Selective State Spaces**</span>
<span id="cb7-955"><a href="#cb7-955" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>结合RNN和Transformer优点的最新尝试</span>
<span id="cb7-956"><a href="#cb7-956" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：与Transformer的对比分析</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>