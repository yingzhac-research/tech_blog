<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-27">
<meta name="description" content="上一章Scaling Laws将大模型训练从炼金术变成了可预测的科学，但隐含了一个关键假设——训练能够正常完成。现实中，PaLM 540B训练出现约20次loss spike需要人工重启，OPT-175B的100页训练日志记录了35次重启和无数次手动干预。本章系统讲述大模型训练稳定性的工程艺术：从Adam到AdamW的权重衰减修正、从FP32到BF16的数值精度演进、从手动warmup到RAdam的自动化——每一步都是对具体痛点的回应。核心论文包括Adam (Kingma &amp; Ba, 2014)、AdamW (Loshchilov &amp; Hutter, 2017)、Mixed Precision Training (Micikevicius et al., 2017)、RAdam (Liu et al., 2019)、Adafactor (Shazeer &amp; Stern, 2018)、Lion (Chen et al., 2023)，以及PaLM和OPT的训练稳定性实战报告。">

<title>第18章：训练稳定性与数值工程 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#为什么大模型比小模型更难训练" id="toc-为什么大模型比小模型更难训练" class="nav-link" data-scroll-target="#为什么大模型比小模型更难训练"><span class="header-section-number">2.1</span> 为什么大模型比小模型更难训练？</a></li>
  <li><a href="#稳定性与速度的根本矛盾" id="toc-稳定性与速度的根本矛盾" class="nav-link" data-scroll-target="#稳定性与速度的根本矛盾"><span class="header-section-number">2.2</span> 稳定性与速度的根本矛盾</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#优化器从全局统一到各参数自治" id="toc-优化器从全局统一到各参数自治" class="nav-link" data-scroll-target="#优化器从全局统一到各参数自治"><span class="header-section-number">3.1</span> 优化器：从”全局统一”到”各参数自治”</a></li>
  <li><a href="#学习率训练的油门控制" id="toc-学习率训练的油门控制" class="nav-link" data-scroll-target="#学习率训练的油门控制"><span class="header-section-number">3.2</span> 学习率：训练的”油门控制”</a></li>
  <li><a href="#数值精度用更粗糙的数字换取速度" id="toc-数值精度用更粗糙的数字换取速度" class="nav-link" data-scroll-target="#数值精度用更粗糙的数字换取速度"><span class="header-section-number">3.3</span> 数值精度：用更”粗糙”的数字换取速度</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#优化器的演进" id="toc-优化器的演进" class="nav-link" data-scroll-target="#优化器的演进"><span class="header-section-number">4.1</span> 优化器的演进</a></li>
  <li><a href="#学习率策略" id="toc-学习率策略" class="nav-link" data-scroll-target="#学习率策略"><span class="header-section-number">4.2</span> 学习率策略</a></li>
  <li><a href="#混合精度训练" id="toc-混合精度训练" class="nav-link" data-scroll-target="#混合精度训练"><span class="header-section-number">4.3</span> 混合精度训练</a></li>
  <li><a href="#训练稳定性诊断" id="toc-训练稳定性诊断" class="nav-link" data-scroll-target="#训练稳定性诊断"><span class="header-section-number">4.4</span> 训练稳定性诊断</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#现代-llm-训练配方" id="toc-现代-llm-训练配方" class="nav-link" data-scroll-target="#现代-llm-训练配方"><span class="header-section-number">5.1</span> 现代 LLM 训练配方</a></li>
  <li><a href="#关键超参数速查" id="toc-关键超参数速查" class="nav-link" data-scroll-target="#关键超参数速查"><span class="header-section-number">5.2</span> 关键超参数速查</a></li>
  <li><a href="#从零实现-adam-优化器" id="toc-从零实现-adam-优化器" class="nav-link" data-scroll-target="#从零实现-adam-优化器"><span class="header-section-number">5.3</span> 从零实现 Adam 优化器</a></li>
  <li><a href="#复现论文的关键细节" id="toc-复现论文的关键细节" class="nav-link" data-scroll-target="#复现论文的关键细节"><span class="header-section-number">5.4</span> 复现论文的关键细节</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么-adam-特别适合-transformer" id="toc-为什么-adam-特别适合-transformer" class="nav-link" data-scroll-target="#为什么-adam-特别适合-transformer"><span class="header-section-number">6.1</span> 为什么 Adam 特别适合 Transformer？</a></li>
  <li><a href="#µp超参数如何跨规模迁移" id="toc-µp超参数如何跨规模迁移" class="nav-link" data-scroll-target="#µp超参数如何跨规模迁移"><span class="header-section-number">6.2</span> µP：超参数如何跨规模迁移</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#被动而非预测" id="toc-被动而非预测" class="nav-link" data-scroll-target="#被动而非预测"><span class="header-section-number">7.1</span> 被动而非预测</a></li>
  <li><a href="#超参数仍然依赖经验" id="toc-超参数仍然依赖经验" class="nav-link" data-scroll-target="#超参数仍然依赖经验"><span class="header-section-number">7.2</span> 超参数仍然依赖经验</a></li>
  <li><a href="#从单卡到多卡新的挑战" id="toc-从单卡到多卡新的挑战" class="nav-link" data-scroll-target="#从单卡到多卡新的挑战"><span class="header-section-number">7.3</span> 从单卡到多卡：新的挑战</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">9.2</span> 理论基础</a></li>
  <li><a href="#训练实战报告" id="toc-训练实战报告" class="nav-link" data-scroll-target="#训练实战报告"><span class="header-section-number">9.3</span> 训练实战报告</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.4</span> 后续发展</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">9.5</span> 综述与教程</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第18章：训练稳定性与数值工程</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">Taming the Chaos: How to Train Billion-Parameter Models Without Blowing Up</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">训练稳定性</div>
    <div class="quarto-category">优化器</div>
    <div class="quarto-category">混合精度</div>
  </div>
  </div>

<div>
  <div class="description">
    上一章Scaling Laws将大模型训练从炼金术变成了可预测的科学，但隐含了一个关键假设——训练能够正常完成。现实中，PaLM 540B训练出现约20次loss spike需要人工重启，OPT-175B的100页训练日志记录了35次重启和无数次手动干预。本章系统讲述大模型训练稳定性的工程艺术：从Adam到AdamW的权重衰减修正、从FP32到BF16的数值精度演进、从手动warmup到RAdam的自动化——每一步都是对具体痛点的回应。核心论文包括Adam (Kingma &amp; Ba, 2014)、AdamW (Loshchilov &amp; Hutter, 2017)、Mixed Precision Training (Micikevicius et al., 2017)、RAdam (Liu et al., 2019)、Adafactor (Shazeer &amp; Stern, 2018)、Lion (Chen et al., 2023)，以及PaLM和OPT的训练稳定性实战报告。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 27, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：为什么百亿参数的模型训练容易不稳定？如何通过优化器设计、学习率策略、混合精度训练等技术，让大规模训练既快速又稳定？</p>
<p><strong>历史坐标</strong>：2014–2023 | Adam (Kingma &amp; Ba, 2014) → AdamW (Loshchilov &amp; Hutter, 2017) → Mixed Precision (Micikevicius et al., 2017) → RAdam (Liu et al., 2019) → Lion (Chen et al., 2023) | 从手工调参到系统化训练工程</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Kingma &amp; Ba (2014)</strong> “Adam: A Method for Stochastic Optimization” (arXiv:1412.6980) — 参考了 Algorithm 1 (Adam 完整伪代码)、偏差修正的数学推导 (Section 3)；本章优化器演进的起点</li>
<li><strong>Loshchilov &amp; Hutter (2017/2019)</strong> “Decoupled Weight Decay Regularization” (arXiv:1711.05101, ICLR 2019) — 参考了 Algorithm 2 (Adam vs AdamW 对比)、Figure 2 (超参数可分离性热力图)；揭示了 L2 正则化 ≠ 权重衰减的关键洞察</li>
<li><strong>Micikevicius et al.&nbsp;(2017)</strong> “Mixed Precision Training” (arXiv:1710.03740, ICLR 2018) — 参考了 Figure 1 (混合精度训练循环图)、Figure 3 (67% 梯度归零的直方图)；混合精度训练的奠基论文</li>
<li><strong>Liu et al.&nbsp;(2019)</strong> “On the Variance of the Adaptive Learning Rate and Beyond” (arXiv:1908.03265, ICLR 2020) — 参考了 Algorithm 2 (RAdam 伪代码)、Figure 2 (早期梯度分布失真)；对 warmup 必要性的理论解释</li>
<li><strong>Shazeer &amp; Stern (2018)</strong> “Adafactor: Adaptive Learning Rates with Sublinear Memory Cost” (arXiv:1804.04235, ICML 2018) — 参考了 Algorithm 4 (Adafactor 完整伪代码)、分解二阶矩的数学推导</li>
<li><strong>Chen et al.&nbsp;(2023)</strong> “Symbolic Discovery of Optimization Algorithms” (arXiv:2302.06675, NeurIPS 2023) — 参考了 Program 1 (Lion 伪代码)、Figure 1 (精度 vs 计算量对比)；进化搜索发现的优化器</li>
<li><strong>Dettmers et al.&nbsp;(2021)</strong> “8-bit Optimizers via Block-wise Quantization” (arXiv:2110.02861, ICLR 2022) — 参考了 Figure 1 (分块量化示意图)、Figure 2 (动态树量化布局)</li>
<li><strong>Chowdhery et al.&nbsp;(2022)</strong> “PaLM: Scaling Language Modeling with Pathways” (arXiv:2204.02311) — 参考了 Section 5.1 (训练不稳定性)、Figure 8 (loss spike 和重启策略)</li>
<li><strong>Zhang et al.&nbsp;(2022)</strong> “OPT: Open Pre-trained Transformer Language Models” (arXiv:2205.01068) — 参考了 Figure 2 (训练曲线与干预)、OPT-175B 训练日志</li>
<li><strong>Loshchilov &amp; Hutter (2016)</strong> “SGDR: Stochastic Gradient Descent with Warm Restarts” (arXiv:1608.03983, ICLR 2017) — 参考了余弦退火公式和温重启思想</li>
<li><strong>Yang et al.&nbsp;(2022)</strong> “Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer” (arXiv:2203.03466) — 参考了 Figure 1 (µP 下最优学习率跨宽度稳定)、µTransfer 范式</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Chapter 12 (Optimization Algorithms) — 参考了 Section 12.6 (Momentum)、Section 12.10 (Adam)、Section 12.11 (Learning Rate Scheduling) 的教学组织方式和 SVG 图解</li>
<li><strong>D2L</strong> Section 5.4 (Numerical Stability and Initialization) — 参考了梯度消失/爆炸的可视化</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 5-6 (Winter 2025) — 参考了优化和训练的教学框架</li>
<li><strong>CMU 11-711 ANLP</strong> (Fall 2024) — 参考了大规模训练工程的讲解角度</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了 Scaling Laws 的优雅力量：Kaplan 等人发现语言模型的损失与参数量、数据量、计算量之间遵循简洁的幂律关系，Chinchilla 进一步修正了最优资源分配策略。这些发现将大模型训练从”炼金术”变成了”可预测的工程科学”——给定计算预算 <span class="math inline">\(C\)</span>，你可以精确计算出最优的模型大小 <span class="math inline">\(N^*\)</span> 和训练数据量 <span class="math inline">\(D^*\)</span>。</p>
<p>但 Scaling Laws 的所有推导都建立在一个看似理所当然、实则极其脆弱的假设之上：<strong>训练能够正常完成</strong>。</p>
<p>这个假设有多脆弱？Google 在训练 540B 参数的 PaLM 时，尽管已经启用了梯度裁剪等标准防护措施，训练过程中仍然出现了<strong>大约 20 次 loss spike</strong>——损失函数突然飙升，有时飙升到训练前期的水平。每次 spike 后，团队不得不回滚到 spike 前约 100 步的 checkpoint，跳过 200–500 个 batch 的数据，然后重新开始训练。更令人困惑的是，他们尝试用引发 spike 的同一批数据从更早的 checkpoint 训练，spike 却不会再次出现——这意味着 loss spike 不是”坏数据”造成的，而是<strong>特定数据与特定参数状态的混沌交互</strong>。</p>
<p>Meta 训练 OPT-175B 的经历更加戏剧性。他们在 992 块 A100 GPU 上训练了约 33 天，期间经历了 <strong>35 次训练重启</strong>和超过 <strong>100 台机器更换</strong>。团队留下了一份长达 100 页的训练日志，详细记录了每一次手动干预：多次降低学习率、在 Adam 和 SGD 之间反复切换、处理 dynamic loss scalar 崩溃到 0 的紧急情况。这份日志后来成为大模型训练领域最透明的公开记录，也让整个社区第一次真切地意识到：<strong>大模型训练不是”设好参数，等着收结果”，而是一场需要 24 小时值班的马拉松</strong>。</p>
<div id="fig-training-instability" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-training-instability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-18/fig-training-instability-glm130b.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-training-instability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 大模型训练中的 Loss Spike 现象。图中展示了 GLM-130B 不同规模变体的训练损失曲线：多条曲线均在训练过程中出现突发的 loss 飙升（彩色圆点标记了人工干预/重启点），且较大模型（如 176B，绿色线）在训练后期出现更剧烈的不稳定。这直观地说明了为什么”训练能正常完成”远非理所当然。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Zeng et al.&nbsp;(2022). “GLM-130B: An Open Bilingual Pre-trained Model”. <a href="https://arxiv.org/abs/2210.02414">arXiv:2210.02414</a></em></p>
</div>
<p>Scaling Laws 告诉你应该训练一个多大的模型，但它没有告诉你<strong>怎样才能让训练不崩溃</strong>。这就像一位建筑师根据力学公式设计了一座 100 层的摩天大楼，图纸完美无缺，但施工队在第 70 层发现混凝土开裂、钢筋变形——如果施工工艺跟不上设计，再优美的图纸都没有意义。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：大模型训练的稳定性不是某个单一技巧能解决的，而是<strong>优化器设计、学习率策略、数值精度、梯度裁剪</strong>四个维度的精密协调。从 Adam 到 AdamW 的权重衰减修正、从 FP32 到 BF16 的精度演进、从手动 warmup 到 RAdam 的自动化——每一步改进都是对一个具体工程痛点的回应。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="为什么大模型比小模型更难训练" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="为什么大模型比小模型更难训练"><span class="header-section-number">2.1</span> 为什么大模型比小模型更难训练？</h3>
<p>表面上看，大模型和小模型用的是同一套优化算法，同样是随机梯度下降的变体。一个 BERT-base（1.1 亿参数）可以在几块 GPU 上稳定训练几天，为什么换成 PaLM-540B（5400 亿参数）就会出现各种不稳定？这个问题的答案涉及多个相互交织的因素。</p>
<p>第一个因素是<strong>损失函数的地形</strong>。随着参数量的增加，损失函数的景观（loss landscape）变得更加崎岖。高维空间中的鞍点远比局部极小值更常见，而不同方向上的曲率差异（条件数）可以相差几个数量级。这意味着某些参数方向上需要大步前进，而另一些方向上哪怕走一小步就会”翻山越岭”。任何单一的学习率都无法同时满足所有方向的需求。</p>
<p>第二个因素是<strong>数值精度的放大效应</strong>。在 FP32 精度下，每个参数的更新都有大约 <span class="math inline">\(10^{-7}\)</span> 的相对误差。对于一个 1 亿参数的模型，这些微小误差可能互相抵消；但对于一个 5000 亿参数的模型，误差的累积效应完全不同。更关键的是，为了在有限的 GPU 显存中装下如此多的参数，大模型几乎必须使用半精度（FP16 或 BF16）训练，而半精度的数值范围和精度都远不如 FP32——这就好像用一把刻度更粗糙的尺子去测量一个需要更高精度的对象。</p>
<p>第三个因素是<strong>训练时间的累积风险</strong>。大模型的训练通常需要数周甚至数月。假设每一步训练有 99.99% 的概率是”正常的”——这听起来相当安全。但如果你需要训练 100 万步，那么至少出现一次异常的概率是 <span class="math inline">\(1 - 0.9999^{1000000} \approx 1 - e^{-100} \approx 1\)</span>，几乎是确定性事件。训练时间越长，罕见的不稳定事件就越有可能发生。</p>
</section>
<section id="稳定性与速度的根本矛盾" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="稳定性与速度的根本矛盾"><span class="header-section-number">2.2</span> 稳定性与速度的根本矛盾</h3>
<p>训练大模型面临一个根本性的权衡：<strong>稳定性和速度往往是矛盾的</strong>。</p>
<p>保守的训练设置——小学习率、FP32 精度、频繁的梯度裁剪——可以让训练非常稳定，但代价是速度极慢。一个原本需要 30 天的训练可能变成 90 天，这不仅意味着 3 倍的 GPU 费用，还意味着 3 倍的碳排放和 3 倍的研发周期。在竞争激烈的 AI 领域，训练速度本身就是竞争力的一部分。</p>
<p>激进的训练设置——大学习率、FP16 精度、最少的防护措施——可以让训练飞速推进，但随时可能崩溃。一次 loss spike 可能浪费几天的计算；如果 spike 发生在训练后期且没有可用的 checkpoint，可能意味着整个训练从头再来——这是数百万美元的代价。</p>
<p>我们需要的不是”安全第一”或”速度第一”，而是一套<strong>在保持高速的同时最大化稳定性</strong>的技术栈。这个技术栈需要在四个维度上同时发力：</p>
<ul>
<li><strong>优化器</strong>：如何计算参数更新的方向和步长？</li>
<li><strong>学习率策略</strong>：如何在训练过程中动态调整更新的激进程度？</li>
<li><strong>数值精度</strong>：如何在降低精度（省内存、提速度）的同时保持数值稳定？</li>
<li><strong>异常处理</strong>：如何检测和恢复训练中的异常事件？</li>
</ul>
<p>接下来的几节，我们将逐一深入这四个维度。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="优化器从全局统一到各参数自治" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="优化器从全局统一到各参数自治"><span class="header-section-number">3.1</span> 优化器：从”全局统一”到”各参数自治”</h3>
<p>训练神经网络的本质是求解一个高维优化问题：找到参数 <span class="math inline">\(\boldsymbol{\theta}\)</span> 使得损失函数 <span class="math inline">\(\mathcal{L}(\boldsymbol{\theta})\)</span> 最小。最朴素的方法是梯度下降——沿着损失函数下降最快的方向更新参数。但原始梯度下降有一个根本性的缺陷：它用<strong>同一个学习率</strong>更新所有参数。</p>
<p>这为什么是个问题？想象你在一个椭圆形的山谷中——沿着山谷的长轴方向，坡度很缓，需要大步前进；沿着短轴方向，坡度很陡，稍微走远就会”翻到对面去”。如果学习率适合长轴方向，短轴方向就会来回震荡；如果学习率适合短轴方向，长轴方向就会前进得极其缓慢。这就是所谓的<strong>病态条件</strong>（ill-conditioning）问题。</p>
<p>优化器的演进历史，本质上就是不断寻找更好的方式来为每个参数”量身定制”合适的更新步长：</p>
<p><strong>动量</strong>（Momentum）是第一个重要改进。它的思想很直觉：不要只看当前这一步的梯度方向，而是<strong>记住过去几步的方向</strong>，像一个重球滚下山坡一样积累惯性。这样可以在一致的方向上加速，在震荡的方向上互相抵消。</p>
<p><strong>Adam</strong> 则更进一步——它不仅记住了梯度的<strong>方向</strong>（一阶矩），还记住了梯度的<strong>幅度</strong>（二阶矩）。对于那些梯度一直很大的参数，Adam 会自动减小它们的步长；对于那些梯度一直很小的参数，Adam 会自动增大它们的步长。这就实现了<strong>每个参数有自己的自适应学习率</strong>。在 Transformer 中，不同层、不同类型的参数（attention 权重 vs FFN 权重 vs 嵌入向量）的梯度尺度可以相差好几个数量级，这使得 Adam 的自适应特性尤为重要。</p>
<p><strong>AdamW</strong> 修正了一个微妙但重要的 bug——Adam 中的 L2 正则化与权重衰减并不等价（后文会详细解释）。</p>
<p><strong>Lion</strong> 则走了一条截然不同的路：它用进化搜索从数学程序空间中自动发现了一个新优化器，只使用梯度和动量的<strong>符号</strong>（sign）作为更新方向，产生均匀幅度的更新——更简单、更省内存，而且在大 batch 训练时表现更好。</p>
</section>
<section id="学习率训练的油门控制" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="学习率训练的油门控制"><span class="header-section-number">3.2</span> 学习率：训练的”油门控制”</h3>
<p>如果说优化器决定了”往哪个方向走、走多大步”，那么学习率调度（learning rate schedule）就是训练过程中的”油门控制”——它决定了整体更新的激进程度如何随时间变化。</p>
<p>现代大模型训练几乎无一例外地采用同一种学习率策略：<strong>先 warmup，再衰减</strong>。这背后的直觉是什么？</p>
<p>训练刚开始时，模型的参数是随机初始化的，损失函数的景观对于优化器来说是完全陌生的。Adam 的自适应学习率依赖于梯度的二阶矩估计，而在最初的几十步里，这些估计是基于极少量样本的——方差极大，完全不可靠。如果此时就用大学习率更新，就像蒙着眼睛在悬崖边全速奔跑。Warmup 的作用就是<strong>让优化器有时间”熟悉地形”</strong>：在最初的几百到几千步里，学习率从接近 0 缓慢上升到峰值，给二阶矩估计留出足够的样本来稳定下来。</p>
<p>到了训练中期，优化器已经对损失函数的地形有了可靠的估计，此时应该用最大的学习率进行”高速探索”——快速穿越损失函数的平坦区域，寻找更好的局部最优。</p>
<p>到了训练后期，模型已经接近最优解的邻域，继续用大学习率会导致在最优点附近来回震荡。此时应该逐渐降低学习率，让模型”精确着陆”。余弦退火（cosine annealing）提供了一种从探索到收敛的平滑过渡——它在中期保持相对较高的学习率，在末期快速衰减，形态恰好符合这种需求。</p>
</section>
<section id="数值精度用更粗糙的数字换取速度" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="数值精度用更粗糙的数字换取速度"><span class="header-section-number">3.3</span> 数值精度：用更”粗糙”的数字换取速度</h3>
<p>训练的第三个维度是数值精度。标准的 32 位浮点数（FP32）有 23 位尾数和 8 位指数，提供约 7 位有效数字和 <span class="math inline">\(\pm 3.4 \times 10^{38}\)</span> 的动态范围。但 FP32 的缺点是显而易见的——每个参数占 4 字节，一个 70B 参数的模型仅参数就需要 280GB 显存，再加上优化器状态、激活值和梯度，单张 GPU 根本装不下。</p>
<p>混合精度训练的核心思想是：<strong>前向传播和反向传播用半精度（16 位），只在参数更新时回到 FP32</strong>。这样不仅内存减半，还能利用 GPU 上专门为半精度设计的 Tensor Core 来加速计算——在 A100 GPU 上，FP16 的吞吐量是 FP32 的 2–8 倍。</p>
<p>但半精度有一个致命的陷阱：<strong>动态范围太窄</strong>。FP16 的最小正数约为 <span class="math inline">\(6 \times 10^{-8}\)</span>，最大值约为 <span class="math inline">\(6.5 \times 10^{4}\)</span>。而梯度的值经常小于 <span class="math inline">\(10^{-8}\)</span>——这些值在 FP16 中会被直接截断为 0，造成”梯度下溢”（gradient underflow）。Micikevicius 等人的实验发现，在不做任何处理的情况下，FP16 训练中<strong>高达 67% 的梯度会变成零</strong>。</p>
<p>Google 提出的 BF16（Brain Floating-Point 16）用一种巧妙的方式解决了这个问题：它牺牲了精度（只有 7 位尾数，比 FP16 的 10 位少），换取了与 FP32 完全相同的动态范围（8 位指数）。这意味着 BF16 几乎不会发生梯度下溢，而且不需要 loss scaling 这种额外的工程技巧。精度的损失在实践中影响很小——训练最终收敛到的损失值几乎与 FP32 一致。这就是为什么 BF16 成为了 LLM 训练的事实标准。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="优化器的演进" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="优化器的演进"><span class="header-section-number">4.1</span> 优化器的演进</h3>
<section id="adam自适应学习率的胜利" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="adam自适应学习率的胜利"><span class="header-section-number">4.1.1</span> Adam：自适应学习率的胜利</h4>
<p>Adam 是目前 Transformer 训练中使用最广泛的优化器。它的名字来自”Adaptive Moment Estimation”，因为它同时估计梯度的一阶矩（均值，即动量）和二阶矩（未中心化方差）。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: Adam (Kingma &amp; Ba, 2014)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入: 学习率 α (default: 0.001)
      衰减率 β₁ = 0.9, β₂ = 0.999
      数值稳定常数 ε = 1e-8
      损失函数 f(θ)
初始化: θ₀, m₀ = 0, v₀ = 0, t = 0

while θ 未收敛:
    t ← t + 1
    g_t ← ∇_θ f_t(θ_{t-1})          # 计算梯度
    m_t ← β₁ · m_{t-1} + (1-β₁) · g_t    # 更新一阶矩（动量）
    v_t ← β₂ · v_{t-1} + (1-β₂) · g_t²   # 更新二阶矩（自适应项）
    m̂_t ← m_t / (1 - β₁ᵗ)           # 偏差修正一阶矩
    v̂_t ← v_t / (1 - β₂ᵗ)           # 偏差修正二阶矩
    θ_t ← θ_{t-1} - α · m̂_t / (√v̂_t + ε)   # 参数更新

return θ_t</code></pre>
<p><em>Source: Kingma, D. P. &amp; Ba, J. (2014). “Adam: A Method for Stochastic Optimization”. <a href="https://arxiv.org/abs/1412.6980">arXiv:1412.6980</a>, Algorithm 1</em></p>
</div>
</div>
<p>Adam 中有一个容易被忽略但至关重要的设计：<strong>偏差修正</strong>（bias correction）。因为 <span class="math inline">\(m_0 = 0\)</span> 和 <span class="math inline">\(v_0 = 0\)</span>，在训练初期，指数加权平均严重偏向 0。以一阶矩为例，如果梯度的真实期望是 <span class="math inline">\(\mathbb{E}[g]\)</span>，那么：</p>
<p><span class="math display">\[
\mathbb{E}[m_t] = (1 - \beta_1^t) \cdot \mathbb{E}[g]
\]</span></p>
<p>当 <span class="math inline">\(t\)</span> 很小时，<span class="math inline">\(\beta_1^t\)</span> 接近 1，<span class="math inline">\(m_t\)</span> 严重低估了真实的梯度期望。除以 <span class="math inline">\((1 - \beta_1^t)\)</span> 就精确地修正了这个偏差。二阶矩的偏差修正尤其关键——<span class="math inline">\(\beta_2 = 0.999\)</span> 意味着偏差需要上千步才能自然消退，而如果不修正，早期的参数更新会因为 <span class="math inline">\(v_t\)</span> 被低估而变得异常大。</p>
</section>
<section id="完整数值示例adam-的前-3-步更新" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="完整数值示例adam-的前-3-步更新"><span class="header-section-number">4.1.2</span> 完整数值示例：Adam 的前 3 步更新</h4>
<p>为了建立直觉，让我们用一个具体的数值例子走完 Adam 的前 3 步。假设我们只有一个标量参数 <span class="math inline">\(\theta\)</span>，使用默认超参数 <span class="math inline">\(\alpha = 0.001\)</span>，<span class="math inline">\(\beta_1 = 0.9\)</span>，<span class="math inline">\(\beta_2 = 0.999\)</span>，<span class="math inline">\(\varepsilon = 10^{-8}\)</span>。</p>
<p><strong>初始状态</strong>：<span class="math inline">\(\theta_0 = 0.5\)</span>，<span class="math inline">\(m_0 = 0\)</span>，<span class="math inline">\(v_0 = 0\)</span></p>
<p>假设前 3 步的梯度分别为 <span class="math inline">\(g_1 = 0.1\)</span>，<span class="math inline">\(g_2 = 0.2\)</span>，<span class="math inline">\(g_3 = 0.15\)</span>。</p>
<p><strong>Step 1</strong> (<span class="math inline">\(t=1\)</span>, <span class="math inline">\(g_1 = 0.1\)</span>)：</p>
<p><span class="math display">\[
\begin{aligned}
m_1 &amp;= 0.9 \times 0 + 0.1 \times 0.1 = 0.01 \\
v_1 &amp;= 0.999 \times 0 + 0.001 \times 0.01 = 0.00001 \\
\hat{m}_1 &amp;= \frac{0.01}{1 - 0.9^1} = \frac{0.01}{0.1} = 0.1 \\
\hat{v}_1 &amp;= \frac{0.00001}{1 - 0.999^1} = \frac{0.00001}{0.001} = 0.01 \\
\theta_1 &amp;= 0.5 - 0.001 \times \frac{0.1}{\sqrt{0.01} + 10^{-8}} = 0.5 - 0.001 \times \frac{0.1}{0.1} = 0.499
\end{aligned}
\]</span></p>
<p>注意偏差修正的效果：未修正的 <span class="math inline">\(m_1 = 0.01\)</span>，修正后 <span class="math inline">\(\hat{m}_1 = 0.1\)</span>——放大了 10 倍！未修正的 <span class="math inline">\(v_1 = 0.00001\)</span>，修正后 <span class="math inline">\(\hat{v}_1 = 0.01\)</span>——放大了 1000 倍！而最终更新量 <span class="math inline">\(\hat{m}_1 / \sqrt{\hat{v}_1} = 0.1 / 0.1 = 1.0\)</span>，实际步长为 <span class="math inline">\(\alpha \times 1.0 = 0.001\)</span>，<strong>恰好等于学习率本身</strong>。这是 Adam 的一个有趣性质：在训练初期，每个参数的实际更新幅度近似等于学习率 <span class="math inline">\(\alpha\)</span>，与梯度的绝对值几乎无关。</p>
<p><strong>Step 2</strong> (<span class="math inline">\(t=2\)</span>, <span class="math inline">\(g_2 = 0.2\)</span>)：</p>
<p><span class="math display">\[
\begin{aligned}
m_2 &amp;= 0.9 \times 0.01 + 0.1 \times 0.2 = 0.029 \\
v_2 &amp;= 0.999 \times 0.00001 + 0.001 \times 0.04 = 0.0000499 \\
\hat{m}_2 &amp;= \frac{0.029}{1 - 0.81} = \frac{0.029}{0.19} \approx 0.1526 \\
\hat{v}_2 &amp;= \frac{0.0000499}{1 - 0.998} = \frac{0.0000499}{0.002} \approx 0.02495 \\
\theta_2 &amp;\approx 0.499 - 0.001 \times \frac{0.1526}{\sqrt{0.02495}} \approx 0.499 - 0.001 \times 0.966 \approx 0.49803
\end{aligned}
\]</span></p>
<p><strong>Step 3</strong> (<span class="math inline">\(t=3\)</span>, <span class="math inline">\(g_3 = 0.15\)</span>)：</p>
<p><span class="math display">\[
\begin{aligned}
m_3 &amp;= 0.9 \times 0.029 + 0.1 \times 0.15 = 0.0411 \\
v_3 &amp;= 0.999 \times 0.0000499 + 0.001 \times 0.0225 \approx 0.0000722 \\
\hat{m}_3 &amp;= \frac{0.0411}{1 - 0.729} = \frac{0.0411}{0.271} \approx 0.1517 \\
\hat{v}_3 &amp;= \frac{0.0000722}{1 - 0.997} = \frac{0.0000722}{0.003} \approx 0.02407 \\
\theta_3 &amp;\approx 0.49803 - 0.001 \times \frac{0.1517}{\sqrt{0.02407}} \approx 0.49803 - 0.001 \times 0.978 \approx 0.49705
\end{aligned}
\]</span></p>
<p>从这个例子可以观察到几个关键现象。首先，实际更新幅度（约 0.001）始终接近学习率 <span class="math inline">\(\alpha\)</span>，这就是 Adam “自适应”的含义：无论梯度是 0.1 还是 0.2，Adam 都会自动调整为大约相同量级的更新步长。其次，偏差修正在前几步的影响非常显著——如果去掉偏差修正，<span class="math inline">\(v_1\)</span> 会被严重低估，导致更新幅度暴增。</p>
</section>
<section id="adamw修正权重衰减的-bug" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="adamw修正权重衰减的-bug"><span class="header-section-number">4.1.3</span> AdamW：修正权重衰减的 bug</h4>
<p>Adam 的普及也暴露了一个微妙的问题。在传统的 SGD 中，L2 正则化和权重衰减是<strong>数学等价的</strong>：</p>
<p><span class="math display">\[
\text{L2 正则化: } \theta_{t+1} = \theta_t - \eta \left(\nabla f(\theta_t) + \lambda \theta_t\right) = (1 - \eta\lambda)\theta_t - \eta \nabla f(\theta_t)
\]</span></p>
<p><span class="math display">\[
\text{权重衰减: } \theta_{t+1} = (1 - \lambda')\theta_t - \eta \nabla f(\theta_t)
\]</span></p>
<p>令 <span class="math inline">\(\lambda' = \eta\lambda\)</span>，两者完全一样。但是在 Adam 中，这种等价性<strong>被打破了</strong>。</p>
<p>在 Adam + L2 正则化中，正则化项 <span class="math inline">\(\lambda\theta_t\)</span> 被加到梯度里，<strong>一起经过了自适应缩放</strong>：</p>
<p><span class="math display">\[
\text{Adam + L2: } \quad g_t = \nabla f(\theta_t) + \lambda\theta_t, \quad \theta_{t+1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}
\]</span></p>
<p>问题在于：正则化梯度 <span class="math inline">\(\lambda\theta_t\)</span> 被 <span class="math inline">\(1/\sqrt{\hat{v}_t}\)</span> 缩放了。对于那些历史梯度很大的参数（<span class="math inline">\(\hat{v}_t\)</span> 大），正则化的效果被<strong>削弱</strong>了；对于那些历史梯度很小的参数，正则化反而被<strong>放大</strong>了。这完全不是我们想要的——权重衰减的初衷是对所有参数施加统一的缩小压力，与它们的梯度历史无关。</p>
<p>AdamW 的修正很简单但意义重大：将权重衰减从梯度中<strong>解耦</strong>出来，直接作用于参数本身：</p>
<p><span class="math display">\[
\text{AdamW: } \quad \theta_{t+1} = (1 - \lambda)\theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 2: AdamW — 关键差异 (Loshchilov &amp; Hutter, 2017)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code># Adam + L2（错误的方式）:
g_t ← ∇f(θ) + λ·θ          # 正则化进入梯度
m_t, v_t ← 更新矩估计(g_t)    # 正则化被自适应缩放扭曲
θ ← θ - α · m̂_t / (√v̂_t + ε) # 正则化效果依赖于梯度历史

# AdamW（正确的方式）:
g_t ← ∇f(θ)                  # 梯度不含正则化项
m_t, v_t ← 更新矩估计(g_t)    # 自适应缩放只作用于真实梯度
θ ← (1 - λ)·θ - α · m̂_t / (√v̂_t + ε)  # 权重衰减独立于梯度</code></pre>
<p><em>Source: Loshchilov, I. &amp; Hutter, F. (2017). “Decoupled Weight Decay Regularization”. <a href="https://arxiv.org/abs/1711.05101">arXiv:1711.05101</a>, Algorithm 2</em></p>
</div>
</div>
<p>Loshchilov 和 Hutter 的实验还揭示了一个重要的工程洞察：在 AdamW 中，最优学习率和最优权重衰减是<strong>可分离的</strong>——你可以独立调节它们，最优区域在超参数空间中形成一个矩形。而在 Adam + L2 中，两者是<strong>耦合的</strong>，最优区域是对角线形状，调一个就必须同时调另一个。这使得 AdamW 的超参数搜索效率大大提高。</p>
<div id="fig-adamw-heatmap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adamw-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-18/fig-adamw-heatmap.png" class="img-fluid figure-img" style="width:55.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adamw-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: AdamW 中学习率与权重衰减的最优区域呈<strong>矩形</strong>（左图），两者可以独立调节。在 Adam + L2 中，最优区域是对角线形状，调一个必须同时调另一个。热力图中暖色（红色）表示更好的验证性能，冷色（蓝色）表示较差性能。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Loshchilov, I. &amp; Hutter, F. (2017). “Decoupled Weight Decay Regularization”. <a href="https://arxiv.org/abs/1711.05101">arXiv:1711.05101</a>, Figure 2</em></p>
</div>
</section>
<section id="adafactor当显存是瓶颈" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="adafactor当显存是瓶颈"><span class="header-section-number">4.1.4</span> Adafactor：当显存是瓶颈</h4>
<p>Adam 的优势伴随着一个沉重的代价：它需要为每个参数维护两个状态变量（<span class="math inline">\(m\)</span> 和 <span class="math inline">\(v\)</span>），这意味着<strong>优化器状态的显存是模型参数的 2 倍</strong>。对于一个 70B 参数的模型（FP32 存储需 280GB），Adam 的优化器状态额外需要 560GB——这在任何单张 GPU 上都是不可能的。</p>
<p>Shazeer 和 Stern 提出的 Adafactor 用一个巧妙的线性代数技巧大幅削减了这个开销。对于一个 <span class="math inline">\(n \times m\)</span> 的权重矩阵，Adam 需要存储完整的 <span class="math inline">\(n \times m\)</span> 二阶矩矩阵 <span class="math inline">\(V\)</span>。Adafactor 观察到：与其存储完整的 <span class="math inline">\(V\)</span>，不如只存储它的<strong>行和</strong>（<span class="math inline">\(n\)</span> 个值）和<strong>列和</strong>（<span class="math inline">\(m\)</span> 个值），然后通过外积重构完整矩阵：</p>
<p><span class="math display">\[
\hat{V}_t = \frac{R_t \cdot C_t^{\top}}{\mathbf{1}_n^{\top} R_t}
\]</span></p>
<p>其中 <span class="math inline">\(R_t \in \mathbb{R}^n\)</span> 是行和向量，<span class="math inline">\(C_t \in \mathbb{R}^m\)</span> 是列和向量。这将二阶矩的存储从 <span class="math inline">\(O(nm)\)</span> 降低到 <span class="math inline">\(O(n + m)\)</span>。对于一个 <span class="math inline">\(4096 \times 4096\)</span> 的权重矩阵，存储量从 1600 万个值降低到 8192 个值——压缩了近 2000 倍。</p>
<p>Adafactor 还做出了几个额外的设计选择：去掉了一阶矩（momentum），改用更新裁剪（update clipping）来替代；使用相对步长（relative step size）而非绝对学习率；使用随时间衰减的 <span class="math inline">\(\beta_2\)</span>。这些选择的共同效果是：<strong>Adafactor 的优化器状态几乎不需要额外显存</strong>，同时在 Transformer 翻译任务上的表现与 Adam 相当。T5 的训练就使用了 Adafactor。</p>
</section>
<section id="lion进化搜索发现的优化器" class="level4" data-number="4.1.5">
<h4 data-number="4.1.5" class="anchored" data-anchor-id="lion进化搜索发现的优化器"><span class="header-section-number">4.1.5</span> Lion：进化搜索发现的优化器</h4>
<p>2023 年，Google Brain 的 Chen 等人用一种完全不同的方法发现了新的优化器：他们定义了一个”数学程序”的搜索空间，用进化算法在这个空间中搜索最优的优化器——就像 NAS（Neural Architecture Search）搜索最优架构一样，但搜索对象换成了优化器的更新规则。</p>
<p>搜索发现的优化器被命名为 Lion（EvoLved Sign Momentum），它的更新规则出人意料地简单：</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Program 1: Lion (Chen et al., 2023)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入: 学习率 η, 权重衰减 λ, 衰减率 β₁ = 0.9, β₂ = 0.99

while θ 未收敛:
    g_t ← ∇_θ f_t(θ_{t-1})

    # 计算更新方向（只取符号！）
    update ← sign(β₁ · m_{t-1} + (1-β₁) · g_t)

    # 应用权重衰减和更新
    θ_t ← (1 - η·λ) · θ_{t-1} - η · update

    # 更新动量（注意：用不同的 β₂）
    m_t ← β₂ · m_{t-1} + (1-β₂) · g_t

return θ_t</code></pre>
<p><em>Source: Chen, X. et al.&nbsp;(2023). “Symbolic Discovery of Optimization Algorithms”. <a href="https://arxiv.org/abs/2302.06675">arXiv:2302.06675</a>, Program 1</em></p>
</div>
</div>
<p>Lion 与 Adam 的核心区别在于 <code>sign()</code> 操作——它将更新方向量化为 <span class="math inline">\(\{-1, 0, +1\}\)</span>，产生<strong>均匀幅度的更新</strong>。这有几个深远的影响：</p>
<p>第一，Lion 只需要存储一个状态变量（动量 <span class="math inline">\(m\)</span>），比 Adam 省一半的优化器显存。第二，因为 <code>sign()</code> 的输出范数远大于 Adam 的自适应更新，Lion 需要<strong>更小的学习率</strong>（通常是 Adam 的 3-10 倍小）和<strong>更大的权重衰减</strong>来补偿。第三，<code>sign()</code> 操作对梯度的绝对值不敏感，这使得 Lion 在<strong>大 batch 训练</strong>时特别有优势——当 batch size 增大时，梯度的方差减小但方向变得更可靠，而 Lion 恰好只利用方向信息。</p>
<div id="fig-lion-compute" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lion-compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-18/fig-lion-accuracy-vs-compute.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lion-compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Lion vs AdamW 性能对比。左图：ImageNet 分类精度 vs 训练计算量，Lion 达到相同精度所需计算量约为 AdamW 的 1/3（~3x speedup）。右图：扩散模型的 FID 分数 vs 训练迭代数，Lion 同样展现约 2.3 倍加速。Lion 的优势在大规模训练中尤为明显。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Chen, X. et al.&nbsp;(2023). “Symbolic Discovery of Optimization Algorithms”. <a href="https://arxiv.org/abs/2302.06675">arXiv:2302.06675</a>, Figure 1</em></p>
</div>
</section>
<section id="bit-adam量化优化器状态" class="level4" data-number="4.1.6">
<h4 data-number="4.1.6" class="anchored" data-anchor-id="bit-adam量化优化器状态"><span class="header-section-number">4.1.6</span> 8-bit Adam：量化优化器状态</h4>
<p>Dettmers 等人提出了另一种减少优化器显存的思路：既然 Adam 的状态变量不需要完全精确，为什么不直接把它们从 32 位量化到 8 位呢？</p>
<p>核心技术是<strong>分块量化</strong>（block-wise quantization）：将每个状态张量切分成 2048 个元素一组的小块，每个块有自己独立的归一化常数（绝对最大值）。在每个块内部，值被归一化到 [-1, 1] 范围后再量化。这种分块设计有两个好处：异常大的值（outlier）只会影响它所在的块，不会”传染”到整个张量；而且每个块的量化和反量化可以在 GPU 上并行执行。</p>
<p>配合一种称为”动态树量化”的非线性量化方案（在不同幅度范围内自适应地分配量化精度），8-bit Adam 将优化器状态的显存压缩到约原来的四分之一，同时在训练效果上与 32-bit Adam <strong>几乎完全一致</strong>。这一方法已经通过 <code>bitsandbytes</code> 库被广泛使用。</p>
</section>
<section id="优化器对比总结" class="level4" data-number="4.1.7">
<h4 data-number="4.1.7" class="anchored" data-anchor-id="优化器对比总结"><span class="header-section-number">4.1.7</span> 优化器对比总结</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>优化器</th>
<th>状态变量</th>
<th>每参数显存</th>
<th>核心特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SGD + Momentum</td>
<td><span class="math inline">\(m\)</span></td>
<td>1×</td>
<td>最简单，通用基线</td>
<td>小模型、CV任务</td>
</tr>
<tr class="even">
<td>Adam</td>
<td><span class="math inline">\(m, v\)</span></td>
<td>2×</td>
<td>自适应学习率</td>
<td>通用，Transformer标配</td>
</tr>
<tr class="odd">
<td>AdamW</td>
<td><span class="math inline">\(m, v\)</span></td>
<td>2×</td>
<td>正确的权重衰减</td>
<td>Adam的严格上位替代</td>
</tr>
<tr class="even">
<td>Adafactor</td>
<td><span class="math inline">\(R, C\)</span>（分解）</td>
<td>≈ 0×</td>
<td>亚线性显存</td>
<td>超大模型（T5）</td>
</tr>
<tr class="odd">
<td>Lion</td>
<td><span class="math inline">\(m\)</span></td>
<td>1×</td>
<td>符号更新，进化搜索发现</td>
<td>大batch，视觉/扩散模型</td>
</tr>
<tr class="even">
<td>8-bit Adam</td>
<td><span class="math inline">\(m, v\)</span>（量化）</td>
<td>≈ 0.5×</td>
<td>量化优化器状态</td>
<td>显存受限场景</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="学习率策略" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="学习率策略"><span class="header-section-number">4.2</span> 学习率策略</h3>
<section id="warmup-的必要性理论解释" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="warmup-的必要性理论解释"><span class="header-section-number">4.2.1</span> Warmup 的必要性：理论解释</h4>
<p>学习率 warmup 是大模型训练中最广泛使用的技巧之一，但长期以来它被视为一种”经验性的 hack”——大家都知道没有 warmup 训练会崩溃，却说不清楚为什么。Liu 等人（2019）的 RAdam 论文首次为 warmup 提供了严谨的理论解释。</p>
<p>核心分析如下：Adam 的自适应学习率是 <span class="math inline">\(\alpha_t = \alpha / \sqrt{\hat{v}_t}\)</span>，其中 <span class="math inline">\(\hat{v}_t\)</span> 是二阶矩的偏差修正估计。问题在于，<span class="math inline">\(\hat{v}_t\)</span> 本身是一个随机变量——它是基于有限样本的估计。刘等人证明，<span class="math inline">\(\hat{v}_t\)</span> 的方差近似为：</p>
<p><span class="math display">\[
\text{Var}\left[\frac{1}{\hat{v}_t}\right] \approx \frac{1}{\rho_t} \cdot \text{常数}
\]</span></p>
<p>其中 <span class="math inline">\(\rho_t\)</span> 是”简单移动平均长度”（SMA length），表示二阶矩估计中有效参与的样本数量：</p>
<p><span class="math display">\[
\rho_t = \rho_\infty - \frac{2t \cdot \beta_2^t}{1 - \beta_2^t}, \quad \rho_\infty = \frac{2}{1 - \beta_2} - 1
\]</span></p>
<p>在训练初期（<span class="math inline">\(t\)</span> 很小），<span class="math inline">\(\rho_t\)</span> 很小，这意味着自适应学习率 <span class="math inline">\(1/\sqrt{\hat{v}_t}\)</span> 的方差极大——它可能在一步之间从 10 跳到 0.1，让参数更新变得完全不可预测。Warmup 的作用正是在这个高方差阶段使用小学习率来”兜底”。</p>
<p>RAdam 将这个分析自动化了——它计算一个方差修正系数 <span class="math inline">\(r_t\)</span>，在 <span class="math inline">\(\rho_t\)</span> 足够大时使用完整的 Adam 更新，在 <span class="math inline">\(\rho_t\)</span> 过小时退化为普通 SGD。但 RAdam 的隐式 warmup 只有约 40 步（对于 <span class="math inline">\(\beta_2 = 0.999\)</span>），对于需要数千步 warmup 的 LLM 训练来说远远不够——这说明 LLM 的 warmup 需求不仅来自 Adam 的方差问题，还有更深层的原因（如损失函数景观的条件数在早期极差）。</p>
</section>
<section id="余弦退火从探索到收敛的平滑过渡" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="余弦退火从探索到收敛的平滑过渡"><span class="header-section-number">4.2.2</span> 余弦退火：从探索到收敛的平滑过渡</h4>
<p>确定了 warmup 的必要性后，接下来的问题是：warmup 之后学习率如何衰减？</p>
<p>余弦退火（cosine annealing）的公式是：</p>
<p><span class="math display">\[
\eta(t) = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T} \cdot \pi\right)\right)
\]</span></p>
<p>其中 <span class="math inline">\(\eta_{\max}\)</span> 是峰值学习率，<span class="math inline">\(\eta_{\min}\)</span> 是最终学习率（通常为峰值的 10%），<span class="math inline">\(T\)</span> 是总训练步数。这个公式的形状恰好符合训练的需求：前半段学习率下降缓慢（保持高速探索），后半段学习率快速衰减（精确收敛）。</p>
<p>现代大模型训练的标准学习率配方因此是：</p>
<p><span class="math display">\[
\eta(t) = \begin{cases}
\eta_{\max} \cdot \frac{t}{T_{\text{warmup}}} &amp; \text{if } t \leq T_{\text{warmup}} \quad \text{(线性 warmup)} \\
\eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t - T_{\text{warmup}}}{T - T_{\text{warmup}}} \cdot \pi\right)\right) &amp; \text{if } t &gt; T_{\text{warmup}} \quad \text{(余弦衰减)}
\end{cases}
\]</span></p>
<p>GPT-2、GPT-3、LLaMA、PaLM、Chinchilla 几乎无一例外地使用这种”线性 warmup + 余弦衰减”的组合。Chinchilla 的实验还发现，调度的具体形状（余弦 vs 线性衰减）并不是特别关键——真正重要的是有 warmup 和最终衰减到接近 0 这两个要素。</p>
<p>一个值得注意的实际问题是：余弦退火要求<strong>预先知道总训练步数</strong><span class="math inline">\(T\)</span>。这在大多数训练中不是问题（你计划训练多久就设多少），但在需要灵活延长训练的场景下就很不方便。为此，最近出现了一种替代方案——<strong>Warmup-Stable-Decay (WSD)</strong> 策略：warmup 后学习率保持在峰值不变（stable 阶段），只在最后的一小段”冷却”（cooldown）阶段快速衰减。这允许在 stable 阶段的任何时刻做出”停止还是继续训练”的决定。</p>
<div id="fig-cosine-schedule" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cosine-schedule-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-18/fig-cosine-schedule-warmup.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cosine-schedule-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: 线性 Warmup + 余弦衰减的学习率调度。横轴为训练步数（共 30 个 epoch），纵轴为学习率。前段（约 5 个 epoch）学习率从 0 线性上升到峰值（warmup 阶段），之后按余弦函数平滑衰减。曲线在中期保持相对较高的学习率（高速探索），后期快速下降（精确收敛）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: D2L, Section 12.11 (Learning Rate Scheduling). <a href="https://d2l.ai/">d2l.ai</a></em></p>
</div>
</section>
</section>
<section id="混合精度训练" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="混合精度训练"><span class="header-section-number">4.3</span> 混合精度训练</h3>
<section id="fp16bf16-与-fp32-的位布局" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="fp16bf16-与-fp32-的位布局"><span class="header-section-number">4.3.1</span> FP16、BF16 与 FP32 的位布局</h4>
<p>理解混合精度训练，首先需要理解三种浮点格式的本质区别。一个浮点数由三部分组成：符号位（sign）、指数位（exponent）和尾数位（mantissa/fraction）。</p>
<p><span class="math display">\[
\text{value} = (-1)^{\text{sign}} \times 2^{\text{exponent} - \text{bias}} \times (1 + \text{fraction})
\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>格式</th>
<th>总位数</th>
<th>符号</th>
<th>指数</th>
<th>尾数</th>
<th>动态范围</th>
<th>精度（有效数字）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FP32</td>
<td>32</td>
<td>1</td>
<td>8</td>
<td>23</td>
<td><span class="math inline">\(\pm 3.4 \times 10^{38}\)</span></td>
<td>~7 位</td>
</tr>
<tr class="even">
<td>FP16</td>
<td>16</td>
<td>1</td>
<td>5</td>
<td>10</td>
<td><span class="math inline">\(\pm 6.5 \times 10^{4}\)</span></td>
<td>~3 位</td>
</tr>
<tr class="odd">
<td>BF16</td>
<td>16</td>
<td>1</td>
<td>8</td>
<td>7</td>
<td><span class="math inline">\(\pm 3.4 \times 10^{38}\)</span></td>
<td>~2 位</td>
</tr>
</tbody>
</table>
<p>BF16 的设计哲学一目了然：它与 FP32 共享相同的 8 位指数，因此动态范围完全一致；代价是尾数只有 7 位（FP16 有 10 位），精度较低。对于深度学习训练来说，动态范围远比精度更重要——梯度的幅度可以跨越几十个数量级，但每个梯度值本身不需要高精度。</p>
</section>
<section id="数值示例梯度下溢问题" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="数值示例梯度下溢问题"><span class="header-section-number">4.3.2</span> 数值示例：梯度下溢问题</h4>
<p>假设某个参数的梯度为 <span class="math inline">\(g = 1.2 \times 10^{-6}\)</span>。在三种格式下：</p>
<ul>
<li><strong>FP32</strong>：可以精确表示（最小正数 <span class="math inline">\(\approx 1.2 \times 10^{-38}\)</span>）</li>
<li><strong>BF16</strong>：可以表示（最小正数 <span class="math inline">\(\approx 1.2 \times 10^{-38}\)</span>，与 FP32 相同）</li>
<li><strong>FP16</strong>：<strong>可以表示</strong>，但精度只有 3 位有效数字</li>
</ul>
<p>现在假设梯度更小，<span class="math inline">\(g = 3.5 \times 10^{-8}\)</span>：</p>
<ul>
<li><strong>FP32</strong>：正常表示</li>
<li><strong>BF16</strong>：正常表示</li>
<li><strong>FP16</strong>：<span class="math inline">\(\approx 5.96 \times 10^{-8}\)</span> 是 FP16 的最小正次正规数，<span class="math inline">\(3.5 \times 10^{-8}\)</span> 勉强可表示但精度极差</li>
</ul>
<p>再小一点，<span class="math inline">\(g = 1.0 \times 10^{-9}\)</span>：</p>
<ul>
<li><strong>FP32 / BF16</strong>：正常表示</li>
<li><strong>FP16</strong>：<strong>下溢为 0</strong>！这个梯度被完全丢弃了。</li>
</ul>
<p>Micikevicius 等人的实验显示，在 SSD（物体检测模型）的训练中，不做任何处理的 FP16 训练会让 <strong>67% 的激活梯度变成零</strong>。这解释了为什么 FP16 混合精度训练需要 loss scaling。</p>
</section>
<section id="混合精度训练的三大技术" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="混合精度训练的三大技术"><span class="header-section-number">4.3.3</span> 混合精度训练的三大技术</h4>
<p>混合精度训练并不是简单地”把所有数字换成 16 位”，而是一套精心设计的三重技术组合：</p>
<p><strong>技术一：FP32 主权重（Master Weights）</strong>。参数的”权威版本”始终保存在 FP32 中。每次前向传播前将它们转换为 FP16/BF16 用于计算，反向传播结束后将 FP32 格式的梯度应用于 FP32 主权重。这保证了参数更新的累积精度不会受半精度的损害。</p>
<p><strong>技术二：损失缩放（Loss Scaling，仅 FP16 需要）</strong>。在反向传播之前，将损失乘以一个大常数 <span class="math inline">\(S\)</span>（如 1024 或 65536）。由于反向传播是线性的，所有梯度也会被放大 <span class="math inline">\(S\)</span> 倍，将那些原本会下溢为 0 的小梯度”提升”到 FP16 可表示的范围。优化器更新前再将梯度除以 <span class="math inline">\(S\)</span> 恢复原始尺度。<span class="math inline">\(S\)</span> 可以是固定值，也可以动态调整——从大值开始，遇到数值溢出就减半，一段时间没有溢出就翻倍。</p>
<p><strong>技术三：FP32 累加（Accumulation）</strong>。某些运算（如矩阵乘法中的累加、BatchNorm/LayerNorm 中的均值和方差计算）对精度特别敏感。这些运算的中间结果用 FP32 累加，最终结果再转回 FP16/BF16。现代 GPU 的 Tensor Core 原生支持”FP16 输入、FP32 累加”的模式。</p>
<p>BF16 的出现大大简化了这个流程——由于 BF16 的动态范围与 FP32 相同，<strong>loss scaling 通常不再需要</strong>。这是 BF16 成为 LLM 训练标准的最重要原因之一：更少的工程复杂性，更少的潜在 bug。</p>
<div id="fig-mixed-precision" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mixed-precision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-18/fig-mixed-precision-training-loop.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mixed-precision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 混合精度训练的数据流。前向传播（FWD）和反向传播（BWD-Actv, BWD-Weight）均在 FP16 下执行以提高速度，只有权重更新（Weight Update）使用 FP32 的 Master Weights。<code>float2half</code> 将 FP32 主权重转换为 FP16 用于下一次前向传播。这种设计在保持训练精度的同时，将计算量减半并充分利用 Tensor Core。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Micikevicius, P. et al.&nbsp;(2017). “Mixed Precision Training”. <a href="https://arxiv.org/abs/1710.03740">arXiv:1710.03740</a>, Figure 1</em></p>
</div>
</section>
</section>
<section id="训练稳定性诊断" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="训练稳定性诊断"><span class="header-section-number">4.4</span> 训练稳定性诊断</h3>
<section id="loss-spike原因与应对" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="loss-spike原因与应对"><span class="header-section-number">4.4.1</span> Loss Spike：原因与应对</h4>
<p>Loss spike 是大模型训练中最令人头疼的问题之一。它的表现是损失函数在正常下降过程中突然飙升——有时飙升到训练初期的水平，有时甚至变成 NaN。</p>
<p>PaLM 540B 的训练提供了关于 loss spike 最详细的公开分析。Google 的团队发现：</p>
<p>第一，spike 的出现具有<strong>不可预测性</strong>。约 20 次 spike 发生在高度不规则的时间间隔，有时出现在训练的极晚期。使用相同的超参数在小模型上训练不会出现 spike——这表明 spike 是大规模独有的现象。</p>
<p>第二，spike 的原因是<strong>数据与参数状态的混沌交互</strong>。将引发 spike 的同一批数据从更早的 checkpoint 重新训练，spike 不会再次出现。这排除了”坏数据”假说，说明 spike 来自特定数据与特定参数状态的不幸组合。</p>
<p>第三，<strong>有效的应对策略是重启 + 跳过</strong>：回滚到 spike 前约 100 步的 checkpoint，跳过 spike 前后 200–500 个 batch 的数据，然后继续训练。这个策略虽然粗暴，但有效——在另一个参数状态下，这些数据不再引发 spike。</p>
<div id="fig-lr-interventions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lr-interventions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-18/fig-training-collapse-glm130b.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lr-interventions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: GLM-130B 训练过程中的实际学习率曲线。红色箭头标记了<strong>人工干预降低学习率</strong>的时刻——每一次骤降都对应一次训练不稳定事件。原计划的 warmup + 平滑衰减被频繁的手动干预打断，最终学习率呈不规则阶梯状下降。这张图生动展示了大模型训练的真实面貌：计划赶不上变化，人工值守不可或缺。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Zeng et al.&nbsp;(2022). “GLM-130B: An Open Bilingual Pre-trained Model”. <a href="https://arxiv.org/abs/2210.02414">arXiv:2210.02414</a></em></p>
</div>
<p>OPT-175B 的训练日志补充了另一个视角：<strong>动态 loss scalar 是不稳定性的先行指标</strong>。当 loss scalar 反复崩溃到 0 时，往往预示着即将发生更严重的不稳定。团队学会了在选择重启 checkpoint 时，挑选 loss scalar 仍处于”健康”状态（≥ 1.0）的时间点。</p>
</section>
<section id="梯度范数监控" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="梯度范数监控"><span class="header-section-number">4.4.2</span> 梯度范数监控</h4>
<p>持续监控梯度的 L2 范数是诊断训练健康状况的最重要手段之一。正常训练中，梯度范数应该呈现一种特征模式：初期较大（模型离最优解远），中期逐渐减小并趋于稳定，后期随学习率衰减而进一步减小。</p>
<p>以下信号需要警觉：</p>
<ul>
<li><strong>梯度范数频繁触及裁剪阈值</strong>：说明学习率可能过大，或者模型在损失函数的”悬崖”附近。</li>
<li><strong>梯度范数突然飙升</strong>：可能是遇到了异常数据、数值溢出或模型参数进入了不稳定区域。</li>
<li><strong>梯度范数持续为 0 或极小</strong>：可能是梯度消失，或 loss scaling 的缩放因子过小导致所有梯度下溢。</li>
<li><strong>不同层的梯度范数差异扩大</strong>：可能预示着训练不稳定性的积累。</li>
</ul>
</section>
<section id="梯度裁剪" class="level4" data-number="4.4.3">
<h4 data-number="4.4.3" class="anchored" data-anchor-id="梯度裁剪"><span class="header-section-number">4.4.3</span> 梯度裁剪</h4>
<p>梯度裁剪（gradient clipping）是防止梯度爆炸的最基本防线。标准做法是全局范数裁剪（global norm clipping）：计算所有参数梯度的 L2 范数，如果超过阈值 <span class="math inline">\(\theta\)</span>（通常为 1.0），则将所有梯度等比例缩小：</p>
<p><span class="math display">\[
\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|_2}\right) \cdot \mathbf{g}
\]</span></p>
<p>这个操作保证了梯度范数不超过 <span class="math inline">\(\theta\)</span>，而不改变梯度的方向。但正如 PaLM 的经验所示，梯度裁剪是<strong>必要但不充分的</strong>——它可以防止最严重的梯度爆炸，但无法阻止更微妙的不稳定性。</p>
</section>
<section id="风险卡片训练稳定性问题" class="level4" data-number="4.4.4">
<h4 data-number="4.4.4" class="anchored" data-anchor-id="风险卡片训练稳定性问题"><span class="header-section-number">4.4.4</span> 风险卡片：训练稳定性问题</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>常见症状</strong></td>
<td>Loss spike、NaN、训练后期震荡、loss scalar 反复崩溃</td>
</tr>
<tr class="even">
<td><strong>典型原因</strong></td>
<td>学习率过大、数值溢出（FP16 动态范围不足）、数据异常 batch、梯度爆炸、参数状态与数据的混沌交互</td>
</tr>
<tr class="odd">
<td><strong>诊断方法</strong></td>
<td>监控梯度范数、激活值分布、loss 曲线、loss scalar 状态</td>
</tr>
<tr class="even">
<td><strong>防护措施</strong></td>
<td>梯度裁剪（norm 1.0）、warmup（≥2000 步）、BF16（避免 loss scaling）、频繁保存 checkpoint、数据清洗</td>
</tr>
<tr class="odd">
<td><strong>应急策略</strong></td>
<td>回滚到 spike 前 ~100 步 checkpoint + 跳过 200–500 batch 数据</td>
</tr>
<tr class="even">
<td><strong>开放问题</strong></td>
<td>为什么 spike 只在大规模出现？能否预测而非被动应对？自适应裁剪（AdaGC, AGGC）能否替代固定阈值？</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="现代-llm-训练配方" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="现代-llm-训练配方"><span class="header-section-number">5.1</span> 现代 LLM 训练配方</h3>
<p>下面展示一个将本章所有技术整合在一起的 PyTorch 训练配方：</p>
<div id="ec2bf64d" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler, autocast</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 1. 优化器配置 ==========</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ...  <span class="co"># 你的 Transformer 模型</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW：正确的权重衰减方式</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    model.parameters(),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">3e-4</span>,          <span class="co"># 峰值学习率</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.95</span>), <span class="co"># LLM 常用设置（β₂ 比默认的 0.999 小）</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># 解耦的权重衰减</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    eps<span class="op">=</span><span class="fl">1e-8</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 2. 学习率调度：线性 warmup + 余弦衰减 ==========</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>warmup_steps <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="dv">300000</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>min_lr <span class="op">=</span> <span class="fl">3e-5</span>  <span class="co"># 峰值的 10%</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lr(step):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""线性 warmup + 余弦衰减"""</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">&lt;</span> warmup_steps:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 线性 warmup</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">3e-4</span> <span class="op">*</span> step <span class="op">/</span> warmup_steps</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 余弦衰减</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        progress <span class="op">=</span> (step <span class="op">-</span> warmup_steps) <span class="op">/</span> (total_steps <span class="op">-</span> warmup_steps)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> min_lr <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="fl">3e-4</span> <span class="op">-</span> min_lr) <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> math.cos(math.pi <span class="op">*</span> progress))</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    optimizer, lr_lambda<span class="op">=</span><span class="kw">lambda</span> step: get_lr(step) <span class="op">/</span> <span class="fl">3e-4</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 3. 混合精度训练（BF16）==========</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># BF16 不需要 GradScaler（动态范围与 FP32 相同）</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>use_bf16 <span class="op">=</span> torch.cuda.is_bf16_supported()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 4. 训练循环 ==========</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>max_grad_norm <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># 梯度裁剪阈值</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 前向传播（BF16）</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> autocast(dtype<span class="op">=</span>torch.bfloat16 <span class="cf">if</span> use_bf16 <span class="cf">else</span> torch.float16):</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model(batch)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 反向传播</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 梯度裁剪</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    grad_norm <span class="op">=</span> torch.nn.utils.clip_grad_norm_(</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        model.parameters(), max_grad_norm</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 参数更新</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ========== 5. 训练监控 ==========</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss"> | "</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"LR: </span><span class="sc">{</span>get_lr(step)<span class="sc">:.2e}</span><span class="ss"> | "</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Grad Norm: </span><span class="sc">{</span>grad_norm<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 检测异常</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.isnan(loss) <span class="kw">or</span> torch.isinf(loss):</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"⚠️ NaN/Inf detected at step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">! "</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Loading last checkpoint..."</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加载最近的 checkpoint 重启</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="关键超参数速查" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="关键超参数速查"><span class="header-section-number">5.2</span> 关键超参数速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>超参数</th>
<th>典型值</th>
<th>来源/依据</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>峰值学习率</td>
<td><span class="math inline">\(1 \times 10^{-4}\)</span> 到 <span class="math inline">\(3 \times 10^{-4}\)</span></td>
<td>Chinchilla, LLaMA</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1, \beta_2\)</span></td>
<td>0.9, 0.95</td>
<td>LLaMA（<span class="math inline">\(\beta_2\)</span> 比默认 0.999 小，有助于稳定性）</td>
</tr>
<tr class="odd">
<td>权重衰减</td>
<td>0.1</td>
<td>GPT-3, LLaMA</td>
</tr>
<tr class="even">
<td>Warmup 步数</td>
<td>总步数的 0.1–1%</td>
<td>通常 2000 步</td>
</tr>
<tr class="odd">
<td>梯度裁剪阈值</td>
<td>1.0</td>
<td>几乎所有大模型</td>
</tr>
<tr class="even">
<td>最终学习率</td>
<td>峰值的 10%</td>
<td>Chinchilla</td>
</tr>
<tr class="odd">
<td>Batch size</td>
<td>逐步增大或固定</td>
<td>GPT-3: 从 32K token 增到 3.2M token</td>
</tr>
</tbody>
</table>
</section>
<section id="从零实现-adam-优化器" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="从零实现-adam-优化器"><span class="header-section-number">5.3</span> 从零实现 Adam 优化器</h3>
<p>为了深入理解 Adam 的工作原理，下面给出一个从零实现的版本：</p>
<div id="d57dc3ca" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Adam 优化器的完整实现（展开查看）</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdamFromScratch:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    从零实现的 Adam 优化器，用于理解内部机制。</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    生产环境请使用 torch.optim.AdamW。</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr<span class="op">=</span><span class="fl">1e-3</span>, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.999</span>),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                 eps<span class="op">=</span><span class="fl">1e-8</span>, weight_decay<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params <span class="op">=</span> <span class="bu">list</span>(params)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta1, <span class="va">self</span>.beta2 <span class="op">=</span> betas</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_decay <span class="op">=</span> weight_decay</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 初始化状态</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m <span class="op">=</span> [torch.zeros_like(p) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params]  <span class="co"># 一阶矩</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> [torch.zeros_like(p) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params]  <span class="co"># 二阶矩</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                g <span class="op">=</span> p.grad</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 更新一阶矩（动量）</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.m[i] <span class="op">=</span> <span class="va">self</span>.beta1 <span class="op">*</span> <span class="va">self</span>.m[i] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta1) <span class="op">*</span> g</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 更新二阶矩（自适应项）</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.v[i] <span class="op">=</span> <span class="va">self</span>.beta2 <span class="op">*</span> <span class="va">self</span>.v[i] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta2) <span class="op">*</span> g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 偏差修正</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                m_hat <span class="op">=</span> <span class="va">self</span>.m[i] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta1<span class="op">**</span><span class="va">self</span>.t)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                v_hat <span class="op">=</span> <span class="va">self</span>.v[i] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta2<span class="op">**</span><span class="va">self</span>.t)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                <span class="co"># AdamW 风格：解耦权重衰减</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.weight_decay <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>                    p.data <span class="op">*=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> <span class="va">self</span>.weight_decay)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 参数更新</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>                p.data <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> m_hat <span class="op">/</span> (torch.sqrt(v_hat) <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>                p.grad.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</section>
<section id="复现论文的关键细节" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="复现论文的关键细节"><span class="header-section-number">5.4</span> 复现论文的关键细节</h3>
<p>在复现大模型训练论文时，以下细节经常在论文中被省略或一笔带过，但对复现结果至关重要：</p>
<p><strong><span class="math inline">\(\beta_2\)</span> 的选择</strong>：默认的 <span class="math inline">\(\beta_2 = 0.999\)</span> 在大模型训练中可能过于保守，导致二阶矩估计对最近的梯度变化反应太慢。LLaMA 使用 <span class="math inline">\(\beta_2 = 0.95\)</span>，PaLM 使用 <span class="math inline">\(\beta_2 = 0.99\)</span>。更小的 <span class="math inline">\(\beta_2\)</span> 让优化器更”敏锐”，但也更容易受噪声影响。</p>
<p><strong>权重衰减不作用于某些参数</strong>：通常，bias 参数、LayerNorm 的 scale 和 shift 参数<strong>不应用权重衰减</strong>。这是因为这些参数的量级本身就不大，权重衰减会把它们压得过小，反而损害表达能力。</p>
<p><strong>梯度累积的数值问题</strong>：当使用梯度累积（gradient accumulation）来模拟大 batch 时，需要在累积完所有 micro-batch 后再做梯度裁剪和优化器更新——而不是在每个 micro-batch 后都做。这个顺序错误是常见的 bug，会导致有效学习率不一致。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<section id="为什么-adam-特别适合-transformer" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么-adam-特别适合-transformer"><span class="header-section-number">6.1</span> 为什么 Adam 特别适合 Transformer？</h3>
<p>Adam 之所以成为 Transformer 训练的事实标准，不仅仅是因为它”效果好”——背后有深层的原因。</p>
<p>Transformer 架构中的 attention 层产生的梯度具有<strong>高度稀疏性</strong>。在 self-attention 中，每个 token 只”关注”序列中少数几个位置（即使 softmax 使所有位置都有非零权重，大部分权重实际上非常接近 0）。这导致 attention 权重矩阵的梯度在不同位置的量级差异极大。SGD 使用统一的学习率，要么对大梯度位置步子太大，要么对小梯度位置步子太小。Adam 的按参数自适应恰好解决了这个问题。</p>
<p>此外，Transformer 中不同类型的参数——词嵌入、positional encoding、Q/K/V 投影、FFN 权重、LayerNorm 参数——它们的梯度尺度可以相差好几个数量级。Adam 的二阶矩自动为每种参数”校准”了合适的步长，而 SGD + 动量做不到这一点。</p>
</section>
<section id="µp超参数如何跨规模迁移" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="µp超参数如何跨规模迁移"><span class="header-section-number">6.2</span> µP：超参数如何跨规模迁移</h3>
<p>训练大模型最昂贵的环节之一是<strong>超参数调优</strong>（hyperparameter tuning）。对于一个 7B 参数的模型，每次完整训练可能花费数十万美元。如果需要搜索 10 组超参数，成本就是数百万美元。能不能在小模型上调好超参数，然后<strong>直接迁移到大模型</strong>？</p>
<p>在标准参数化（Standard Parameterization, SP）下，答案是否定的。Yang 等人（2022）证明，在 SP 下，最优学习率与模型宽度成反比——宽度翻倍，最优学习率就要减半。这意味着在 128 维宽度上找到的最优学习率，到 4096 维宽度时已经完全不适用了。</p>
<p><strong>最大更新参数化</strong>（Maximal Update Parameterization, µP）通过调整初始化尺度和学习率的缩放方式，使得每一层接收到的更新幅度<strong>与模型宽度无关</strong>。这样，最优学习率在不同宽度的模型之间保持<strong>恒定</strong>。</p>
<p>µTransfer 的实际流程是：</p>
<ol type="1">
<li>将目标模型用 µP 参数化</li>
<li>在一个小型代理模型（如宽度 256）上搜索最优超参数</li>
<li>将这些超参数直接迁移到全尺寸模型（如宽度 4096+），无需任何额外调优</li>
</ol>
<p>实证结果令人印象深刻：Yang 等人将超参数从 40M 参数的模型迁移到 GPT-3 6.7B（168 倍大），结果超过了 GPT-3 6.7B 的已发布性能，而超参数搜索的成本仅为完整预训练的 <strong>7%</strong>。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</h3>
<section id="loss-spike-的不可预测性" class="level4" data-number="6.3.1">
<h4 data-number="6.3.1" class="anchored" data-anchor-id="loss-spike-的不可预测性"><span class="header-section-number">6.3.1</span> Loss Spike 的不可预测性</h4>
<p>目前所有的训练稳定性技术本质上都是<strong>被动的</strong>——它们在问题发生后做出响应（重启、裁剪、缩放），而不是在问题发生前预防。PaLM 的分析表明，loss spike 来自特定数据与特定参数状态的混沌交互，这意味着在理论上可能无法提前预测。</p>
<p>Wortsman 等人（2023）的后续工作发现了一个有趣的现象：大规模模型的训练不稳定性可以在<strong>小模型上用高学习率复现</strong>。他们识别出两种关键的不稳定模式——attention logit 增长（导致 softmax 熵崩溃）和 output logit 发散——并证明 <strong>qk-layernorm</strong> 和 <strong>z-loss 正则化</strong> 等缓解措施在大小模型上都有效。这为”先在小规模上调试稳定性”提供了理论依据。</p>
</section>
<section id="pre-ln-vs-post-ln-与-warmup-的关系" class="level4" data-number="6.3.2">
<h4 data-number="6.3.2" class="anchored" data-anchor-id="pre-ln-vs-post-ln-与-warmup-的关系"><span class="header-section-number">6.3.2</span> Pre-LN vs Post-LN 与 Warmup 的关系</h4>
<p>原始 Transformer 采用 Post-LN（LayerNorm 在残差连接之后），后来的研究发现 Pre-LN（LayerNorm 在 attention/FFN 之前）更稳定。这背后的原因与 warmup 有关：Post-LN 结构中，梯度到达残差路径之前要经过 LayerNorm 的雅可比矩阵，这可能扭曲梯度方向。Pre-LN 保证了一条”干净”的残差路径，使梯度可以不经修改地直接回传。</p>
<p>实际影响是：Pre-LN 结构通常<strong>不需要 warmup</strong>（或只需要很少的 warmup），而 Post-LN 结构必须使用 warmup 否则训练会发散。这也是为什么现代 LLM 几乎全部采用 Pre-LN 的原因之一。</p>
</section>
<section id="隐含假设与失效条件" class="level4" data-number="6.3.3">
<h4 data-number="6.3.3" class="anchored" data-anchor-id="隐含假设与失效条件"><span class="header-section-number">6.3.3</span> 隐含假设与失效条件</h4>
<p>本章讨论的技术栈假设了几个条件：</p>
<ol type="1">
<li><strong>数据质量良好</strong>：如果训练数据包含大量重复、损坏或极端分布的样本，再好的优化器也无法稳定训练。数据清洗和去重是稳定训练的前提。</li>
<li><strong>模型架构合理</strong>：某些架构选择（如极深的网络没有残差连接、attention 没有缩放）会使训练本质上不稳定，优化器层面的修补只能治标。</li>
<li><strong>硬件正常运行</strong>：大规模训练涉及数百甚至数千张 GPU，硬件故障（GPU 报错、网络抖动、NVLink 断开）是家常便饭。OPT-175B 训练中平均每天更换几台机器——这种级别的硬件不稳定需要系统层面的容错机制，超出了优化算法的范畴。</li>
</ol>
<hr>
</section>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="被动而非预测" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="被动而非预测"><span class="header-section-number">7.1</span> 被动而非预测</h3>
<p>当前的训练稳定性方法本质上是一套”保险措施”——warmup 防止早期崩溃、梯度裁剪防止梯度爆炸、BF16 防止数值溢出、checkpoint 重启防止 loss spike 后全部前功尽弃。但这些措施都不能<strong>预测</strong>问题何时发生，也不能<strong>根除</strong>问题的根源。我们缺少一个关于”训练何时以及为什么会变得不稳定”的完整理论。</p>
</section>
<section id="超参数仍然依赖经验" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="超参数仍然依赖经验"><span class="header-section-number">7.2</span> 超参数仍然依赖经验</h3>
<p>尽管 µP 提供了一种跨规模迁移超参数的方法，但”第一组超参数”仍然需要在小模型上搜索。warmup 步数选 2000 而不是 1000 或 5000，梯度裁剪阈值选 1.0 而不是 0.5 或 2.0——这些选择在很大程度上仍然是经验性的，缺乏从第一性原理推导出的最优值。</p>
</section>
<section id="从单卡到多卡新的挑战" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="从单卡到多卡新的挑战"><span class="header-section-number">7.3</span> 从单卡到多卡：新的挑战</h3>
<p>即使在单张 GPU 上完美解决了训练稳定性，我们仍然面临一个残酷的现实：一个 70B 参数的模型（FP32 需要 280GB，即使 BF16 也需要 140GB），加上 Adam 的优化器状态（额外 560GB FP32），总共需要 840GB 显存——<strong>没有任何单张 GPU 能装得下</strong>。A100 有 80GB，H100 有 80GB，即使是最新的硬件也差了一个数量级。</p>
<p>这就引出了下一章的核心问题：<strong>如何将训练分布到多张 GPU、多台机器上？</strong> 数据并行、模型并行、流水线并行、ZeRO——这些技术如何协同工作来解决”装不下”的问题，同时保持训练的稳定性和效率？</p>
<blockquote class="blockquote">
<p>💡 本章解决了”如何在一张 GPU 上稳定训练”；下一章将解决”如何在一千张 GPU 上高效训练”。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<ol type="1">
<li><strong>问题</strong>：大模型训练面临损失函数景观崎岖、数值精度放大、训练时间累积风险三重挑战，PaLM 和 OPT 的训练经历生动说明了这些挑战的严重性。</li>
<li><strong>优化器演进</strong>：从 SGD 到 Adam（自适应学习率）→ AdamW（修正权重衰减）→ Adafactor（亚线性显存）→ Lion（符号更新，进化搜索发现），每一步都解决了前一步的具体痛点。</li>
<li><strong>学习率策略</strong>：线性 warmup（让 Adam 的二阶矩估计稳定）+ 余弦衰减（从探索到收敛的平滑过渡）是 LLM 训练的标准配方。</li>
<li><strong>混合精度</strong>：BF16 用与 FP32 相同的动态范围换取一半的显存和更高的吞吐量，避免了 FP16 的梯度下溢和 loss scaling 的复杂性。</li>
<li><strong>稳定性诊断</strong>：梯度范数监控、loss spike 检测与 checkpoint 重启是必备的工程实践。</li>
</ol>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<ul>
<li><strong>Adam 更新</strong>: <span class="math inline">\(\theta_t = \theta_{t-1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \varepsilon)\)</span></li>
<li><strong>AdamW 权重衰减</strong>: <span class="math inline">\(\theta_t = (1-\lambda)\theta_{t-1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \varepsilon)\)</span></li>
<li><strong>偏差修正</strong>: <span class="math inline">\(\hat{m}_t = m_t / (1 - \beta_1^t)\)</span>，<span class="math inline">\(\hat{v}_t = v_t / (1 - \beta_2^t)\)</span></li>
<li><strong>余弦退火</strong>: <span class="math inline">\(\eta(t) = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi t / T))\)</span></li>
<li><strong>梯度裁剪</strong>: <span class="math inline">\(\mathbf{g} \leftarrow \min(1, \theta / \|\mathbf{g}\|_2) \cdot \mathbf{g}\)</span></li>
<li><strong>计算预算约束</strong>: <span class="math inline">\(C \approx 6ND\)</span></li>
</ul>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么 AdamW 中权重衰减必须与自适应梯度缩放解耦？如果不解耦，对于梯度稳定很大的参数和梯度稳定很小的参数，正则化效果分别会怎样？</p></li>
<li><p><strong>[数学推导]</strong> 证明 Adam 在第 <span class="math inline">\(t\)</span> 步的一阶矩期望为 <span class="math inline">\(\mathbb{E}[m_t] = (1 - \beta_1^t) \cdot \mathbb{E}[g]\)</span>，其中 <span class="math inline">\(g\)</span> 是梯度的真实期望。（提示：将递推式展开。）</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 假设你正在训练一个 13B 参数的模型，训练到第 50,000 步时 loss 突然飙升。请设计一个诊断和恢复流程：你会检查哪些指标？如何选择重启点？如何防止 spike 再次发生？</p></li>
<li><p><strong>[开放思考]</strong> Lion 优化器通过进化搜索发现，其更新规则出人意料地简单（只用 sign）。这是否意味着手工设计的优化器已经”够好了”，还是说搜索空间可能包含尚未发现的更好方案？如果你要设计一个优化器搜索实验，你会如何定义搜索空间和评估指标？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>Kingma &amp; Ba (2014)</strong> “Adam: A Method for Stochastic Optimization” — Adam 优化器的原始论文
<ul>
<li>重点读：Algorithm 1（完整伪代码）、Section 3（偏差修正推导）</li>
<li>可跳过：Section 6（AdaMax 变体）</li>
</ul></li>
<li><strong>Loshchilov &amp; Hutter (2017)</strong> “Decoupled Weight Decay Regularization” — AdamW 论文
<ul>
<li>重点读：Section 2（L2 ≠ 权重衰减的分析）、Figure 2（超参数可分离性）</li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">9.2</span> 理论基础</h3>
<ul>
<li><strong>Liu et al.&nbsp;(2019)</strong> “On the Variance of the Adaptive Learning Rate and Beyond” — RAdam，对 warmup 必要性的理论解释</li>
<li><strong>Yang et al.&nbsp;(2022)</strong> “Tensor Programs V” — µP / µTransfer，超参数跨规模迁移</li>
</ul>
</section>
<section id="训练实战报告" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="训练实战报告"><span class="header-section-number">9.3</span> 训练实战报告</h3>
<ul>
<li><strong>Chowdhery et al.&nbsp;(2022)</strong> “PaLM” — Section 5.1 是关于训练不稳定性的最详细公开分析</li>
<li><strong>Zhang et al.&nbsp;(2022)</strong> “OPT” — OPT-175B 训练日志是大模型训练工程的必读材料</li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.4</span> 后续发展</h3>
<ul>
<li><strong>Chen et al.&nbsp;(2023)</strong> “Symbolic Discovery of Optimization Algorithms” — Lion 优化器，进化搜索发现</li>
<li><strong>Wortsman et al.&nbsp;(2023)</strong> “Small-scale proxies for large-scale Transformer training instabilities” — 在小模型上复现大模型的不稳定性（ICLR 2024 Oral）</li>
<li><strong>Dettmers et al.&nbsp;(2021)</strong> “8-bit Optimizers” — 量化优化器状态，<code>bitsandbytes</code> 库</li>
</ul>
</section>
<section id="综述与教程" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">9.5</span> 综述与教程</h3>
<ul>
<li><strong>D2L Chapter 12</strong> — 优化算法的系统教学，从 SGD 到 Adam</li>
<li><strong>NVIDIA Mixed Precision Training Guide</strong> — 混合精度训练的工程实践指南</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>Adam 优化器论文是深度学习领域引用量最高的论文之一（截至 2025 年超过 20 万次引用），但它最初提交到 ICLR 2015 时只获得了”接受”而非”最佳论文”。事后看来，这篇论文可能是整个会议历史上影响力最大的工作。</p>
<p>AdamW 的故事同样值得玩味。Loshchilov 和 Hutter 在 2017 年发现了 Adam 中 L2 正则化和权重衰减不等价的问题，但直到 2019 年这篇论文才在 ICLR 上正式发表。在此期间，整个社区都在使用”错误的” Adam + L2 组合训练模型，而且似乎也训练出了不错的结果（包括 BERT）。这提醒我们：深度学习中”能用”和”最优”之间可能存在巨大的差距，而我们常常意识不到自己正在使用次优的方法。</p>
<p>OPT-175B 的训练日志是另一个历史性时刻。在此之前，大模型训练的困难和混乱都隐藏在科技公司的内部文档中。Meta 选择公开这份长达 100 页的原始日志——包括所有的错误、困惑、临时修补和凌晨三点的紧急干预——让整个社区第一次看到了”训练大模型到底是什么样的”。这份日志被很多研究者称为”大模型训练的《Surely You’re Joking, Mr.&nbsp;Feynman》“——一部充满人性的工程冒险记录。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第18章：训练稳定性与数值工程"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Taming the Chaos: How to Train Billion-Parameter Models Without Blowing Up"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-27"</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, 训练稳定性, 优化器, 混合精度]</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [Adam, AdamW, Adafactor, Lion, 混合精度, BF16, FP16, warmup, cosine annealing, gradient clipping, loss spike, µP, 训练工程]</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "上一章Scaling Laws将大模型训练从炼金术变成了可预测的科学，但隐含了一个关键假设——训练能够正常完成。现实中，PaLM 540B训练出现约20次loss spike需要人工重启，OPT-175B的100页训练日志记录了35次重启和无数次手动干预。本章系统讲述大模型训练稳定性的工程艺术：从Adam到AdamW的权重衰减修正、从FP32到BF16的数值精度演进、从手动warmup到RAdam的自动化——每一步都是对具体痛点的回应。核心论文包括Adam (Kingma &amp; Ba, 2014)、AdamW (Loshchilov &amp; Hutter, 2017)、Mixed Precision Training (Micikevicius et al., 2017)、RAdam (Liu et al., 2019)、Adafactor (Shazeer &amp; Stern, 2018)、Lion (Chen et al., 2023)，以及PaLM和OPT的训练稳定性实战报告。"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-18/original/fig-mixed-precision-training-loop.png"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：为什么百亿参数的模型训练容易不稳定？如何通过优化器设计、学习率策略、混合精度训练等技术，让大规模训练既快速又稳定？</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2014–2023 </span><span class="pp">|</span><span class="at"> Adam (Kingma &amp; Ba, 2014) → AdamW (Loshchilov &amp; Hutter, 2017) → Mixed Precision (Micikevicius et al., 2017) → RAdam (Liu et al., 2019) → Lion (Chen et al., 2023) </span><span class="pp">|</span><span class="at"> 从手工调参到系统化训练工程</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kingma &amp; Ba (2014)** "Adam: A Method for Stochastic Optimization" (arXiv:1412.6980) — 参考了 Algorithm 1 (Adam 完整伪代码)、偏差修正的数学推导 (Section 3)；本章优化器演进的起点</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Loshchilov &amp; Hutter (2017/2019)** "Decoupled Weight Decay Regularization" (arXiv:1711.05101, ICLR 2019) — 参考了 Algorithm 2 (Adam vs AdamW 对比)、Figure 2 (超参数可分离性热力图)；揭示了 L2 正则化 ≠ 权重衰减的关键洞察</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Micikevicius et al. (2017)** "Mixed Precision Training" (arXiv:1710.03740, ICLR 2018) — 参考了 Figure 1 (混合精度训练循环图)、Figure 3 (67% 梯度归零的直方图)；混合精度训练的奠基论文</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liu et al. (2019)** "On the Variance of the Adaptive Learning Rate and Beyond" (arXiv:1908.03265, ICLR 2020) — 参考了 Algorithm 2 (RAdam 伪代码)、Figure 2 (早期梯度分布失真)；对 warmup 必要性的理论解释</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Shazeer &amp; Stern (2018)** "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost" (arXiv:1804.04235, ICML 2018) — 参考了 Algorithm 4 (Adafactor 完整伪代码)、分解二阶矩的数学推导</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chen et al. (2023)** "Symbolic Discovery of Optimization Algorithms" (arXiv:2302.06675, NeurIPS 2023) — 参考了 Program 1 (Lion 伪代码)、Figure 1 (精度 vs 计算量对比)；进化搜索发现的优化器</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dettmers et al. (2021)** "8-bit Optimizers via Block-wise Quantization" (arXiv:2110.02861, ICLR 2022) — 参考了 Figure 1 (分块量化示意图)、Figure 2 (动态树量化布局)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chowdhery et al. (2022)** "PaLM: Scaling Language Modeling with Pathways" (arXiv:2204.02311) — 参考了 Section 5.1 (训练不稳定性)、Figure 8 (loss spike 和重启策略)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zhang et al. (2022)** "OPT: Open Pre-trained Transformer Language Models" (arXiv:2205.01068) — 参考了 Figure 2 (训练曲线与干预)、OPT-175B 训练日志</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Loshchilov &amp; Hutter (2016)** "SGDR: Stochastic Gradient Descent with Warm Restarts" (arXiv:1608.03983, ICLR 2017) — 参考了余弦退火公式和温重启思想</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Yang et al. (2022)** "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer" (arXiv:2203.03466) — 参考了 Figure 1 (µP 下最优学习率跨宽度稳定)、µTransfer 范式</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Chapter 12 (Optimization Algorithms) — 参考了 Section 12.6 (Momentum)、Section 12.10 (Adam)、Section 12.11 (Learning Rate Scheduling) 的教学组织方式和 SVG 图解</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Section 5.4 (Numerical Stability and Initialization) — 参考了梯度消失/爆炸的可视化</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 5-6 (Winter 2025) — 参考了优化和训练的教学框架</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CMU 11-711 ANLP** (Fall 2024) — 参考了大规模训练工程的讲解角度</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>上一章我们见证了 Scaling Laws 的优雅力量：Kaplan 等人发现语言模型的损失与参数量、数据量、计算量之间遵循简洁的幂律关系，Chinchilla 进一步修正了最优资源分配策略。这些发现将大模型训练从"炼金术"变成了"可预测的工程科学"——给定计算预算 $C$，你可以精确计算出最优的模型大小 $N^*$ 和训练数据量 $D^*$。</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>但 Scaling Laws 的所有推导都建立在一个看似理所当然、实则极其脆弱的假设之上：**训练能够正常完成**。</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>这个假设有多脆弱？Google 在训练 540B 参数的 PaLM 时，尽管已经启用了梯度裁剪等标准防护措施，训练过程中仍然出现了**大约 20 次 loss spike**——损失函数突然飙升，有时飙升到训练前期的水平。每次 spike 后，团队不得不回滚到 spike 前约 100 步的 checkpoint，跳过 200–500 个 batch 的数据，然后重新开始训练。更令人困惑的是，他们尝试用引发 spike 的同一批数据从更早的 checkpoint 训练，spike 却不会再次出现——这意味着 loss spike 不是"坏数据"造成的，而是**特定数据与特定参数状态的混沌交互**。</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>Meta 训练 OPT-175B 的经历更加戏剧性。他们在 992 块 A100 GPU 上训练了约 33 天，期间经历了 **35 次训练重启**和超过 **100 台机器更换**。团队留下了一份长达 100 页的训练日志，详细记录了每一次手动干预：多次降低学习率、在 Adam 和 SGD 之间反复切换、处理 dynamic loss scalar 崩溃到 0 的紧急情况。这份日志后来成为大模型训练领域最透明的公开记录，也让整个社区第一次真切地意识到：**大模型训练不是"设好参数，等着收结果"，而是一场需要 24 小时值班的马拉松**。</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="al">![大模型训练中的 Loss Spike 现象。图中展示了 GLM-130B 不同规模变体的训练损失曲线：多条曲线均在训练过程中出现突发的 loss 飙升（彩色圆点标记了人工干预/重启点），且较大模型（如 176B，绿色线）在训练后期出现更剧烈的不稳定。这直观地说明了为什么"训练能正常完成"远非理所当然。](figures/chapter-18/fig-training-instability-glm130b.png)</span>{#fig-training-instability width=75%}</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>*Source: Zeng et al. (2022). "GLM-130B: An Open Bilingual Pre-trained Model". [arXiv:2210.02414](https://arxiv.org/abs/2210.02414)*</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>Scaling Laws 告诉你应该训练一个多大的模型，但它没有告诉你**怎样才能让训练不崩溃**。这就像一位建筑师根据力学公式设计了一座 100 层的摩天大楼，图纸完美无缺，但施工队在第 70 层发现混凝土开裂、钢筋变形——如果施工工艺跟不上设计，再优美的图纸都没有意义。</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：大模型训练的稳定性不是某个单一技巧能解决的，而是**优化器设计、学习率策略、数值精度、梯度裁剪**四个维度的精密协调。从 Adam 到 AdamW 的权重衰减修正、从 FP32 到 BF16 的精度演进、从手动 warmup 到 RAdam 的自动化——每一步改进都是对一个具体工程痛点的回应。</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么大模型比小模型更难训练？</span></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>表面上看，大模型和小模型用的是同一套优化算法，同样是随机梯度下降的变体。一个 BERT-base（1.1 亿参数）可以在几块 GPU 上稳定训练几天，为什么换成 PaLM-540B（5400 亿参数）就会出现各种不稳定？这个问题的答案涉及多个相互交织的因素。</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>第一个因素是**损失函数的地形**。随着参数量的增加，损失函数的景观（loss landscape）变得更加崎岖。高维空间中的鞍点远比局部极小值更常见，而不同方向上的曲率差异（条件数）可以相差几个数量级。这意味着某些参数方向上需要大步前进，而另一些方向上哪怕走一小步就会"翻山越岭"。任何单一的学习率都无法同时满足所有方向的需求。</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>第二个因素是**数值精度的放大效应**。在 FP32 精度下，每个参数的更新都有大约 $10^{-7}$ 的相对误差。对于一个 1 亿参数的模型，这些微小误差可能互相抵消；但对于一个 5000 亿参数的模型，误差的累积效应完全不同。更关键的是，为了在有限的 GPU 显存中装下如此多的参数，大模型几乎必须使用半精度（FP16 或 BF16）训练，而半精度的数值范围和精度都远不如 FP32——这就好像用一把刻度更粗糙的尺子去测量一个需要更高精度的对象。</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>第三个因素是**训练时间的累积风险**。大模型的训练通常需要数周甚至数月。假设每一步训练有 99.99% 的概率是"正常的"——这听起来相当安全。但如果你需要训练 100 万步，那么至少出现一次异常的概率是 $1 - 0.9999^{1000000} \approx 1 - e^{-100} \approx 1$，几乎是确定性事件。训练时间越长，罕见的不稳定事件就越有可能发生。</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a><span class="fu">### 稳定性与速度的根本矛盾</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>训练大模型面临一个根本性的权衡：**稳定性和速度往往是矛盾的**。</span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>保守的训练设置——小学习率、FP32 精度、频繁的梯度裁剪——可以让训练非常稳定，但代价是速度极慢。一个原本需要 30 天的训练可能变成 90 天，这不仅意味着 3 倍的 GPU 费用，还意味着 3 倍的碳排放和 3 倍的研发周期。在竞争激烈的 AI 领域，训练速度本身就是竞争力的一部分。</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>激进的训练设置——大学习率、FP16 精度、最少的防护措施——可以让训练飞速推进，但随时可能崩溃。一次 loss spike 可能浪费几天的计算；如果 spike 发生在训练后期且没有可用的 checkpoint，可能意味着整个训练从头再来——这是数百万美元的代价。</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>我们需要的不是"安全第一"或"速度第一"，而是一套**在保持高速的同时最大化稳定性**的技术栈。这个技术栈需要在四个维度上同时发力：</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**优化器**：如何计算参数更新的方向和步长？</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**学习率策略**：如何在训练过程中动态调整更新的激进程度？</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**数值精度**：如何在降低精度（省内存、提速度）的同时保持数值稳定？</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**异常处理**：如何检测和恢复训练中的异常事件？</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>接下来的几节，我们将逐一深入这四个维度。</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a><span class="fu">### 优化器：从"全局统一"到"各参数自治"</span></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>训练神经网络的本质是求解一个高维优化问题：找到参数 $\boldsymbol{\theta}$ 使得损失函数 $\mathcal{L}(\boldsymbol{\theta})$ 最小。最朴素的方法是梯度下降——沿着损失函数下降最快的方向更新参数。但原始梯度下降有一个根本性的缺陷：它用**同一个学习率**更新所有参数。</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>这为什么是个问题？想象你在一个椭圆形的山谷中——沿着山谷的长轴方向，坡度很缓，需要大步前进；沿着短轴方向，坡度很陡，稍微走远就会"翻到对面去"。如果学习率适合长轴方向，短轴方向就会来回震荡；如果学习率适合短轴方向，长轴方向就会前进得极其缓慢。这就是所谓的**病态条件**（ill-conditioning）问题。</span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>优化器的演进历史，本质上就是不断寻找更好的方式来为每个参数"量身定制"合适的更新步长：</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a>**动量**（Momentum）是第一个重要改进。它的思想很直觉：不要只看当前这一步的梯度方向，而是**记住过去几步的方向**，像一个重球滚下山坡一样积累惯性。这样可以在一致的方向上加速，在震荡的方向上互相抵消。</span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>**Adam** 则更进一步——它不仅记住了梯度的**方向**（一阶矩），还记住了梯度的**幅度**（二阶矩）。对于那些梯度一直很大的参数，Adam 会自动减小它们的步长；对于那些梯度一直很小的参数，Adam 会自动增大它们的步长。这就实现了**每个参数有自己的自适应学习率**。在 Transformer 中，不同层、不同类型的参数（attention 权重 vs FFN 权重 vs 嵌入向量）的梯度尺度可以相差好几个数量级，这使得 Adam 的自适应特性尤为重要。</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>**AdamW** 修正了一个微妙但重要的 bug——Adam 中的 L2 正则化与权重衰减并不等价（后文会详细解释）。</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>**Lion** 则走了一条截然不同的路：它用进化搜索从数学程序空间中自动发现了一个新优化器，只使用梯度和动量的**符号**（sign）作为更新方向，产生均匀幅度的更新——更简单、更省内存，而且在大 batch 训练时表现更好。</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### 学习率：训练的"油门控制"</span></span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>如果说优化器决定了"往哪个方向走、走多大步"，那么学习率调度（learning rate schedule）就是训练过程中的"油门控制"——它决定了整体更新的激进程度如何随时间变化。</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a>现代大模型训练几乎无一例外地采用同一种学习率策略：**先 warmup，再衰减**。这背后的直觉是什么？</span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>训练刚开始时，模型的参数是随机初始化的，损失函数的景观对于优化器来说是完全陌生的。Adam 的自适应学习率依赖于梯度的二阶矩估计，而在最初的几十步里，这些估计是基于极少量样本的——方差极大，完全不可靠。如果此时就用大学习率更新，就像蒙着眼睛在悬崖边全速奔跑。Warmup 的作用就是**让优化器有时间"熟悉地形"**：在最初的几百到几千步里，学习率从接近 0 缓慢上升到峰值，给二阶矩估计留出足够的样本来稳定下来。</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>到了训练中期，优化器已经对损失函数的地形有了可靠的估计，此时应该用最大的学习率进行"高速探索"——快速穿越损失函数的平坦区域，寻找更好的局部最优。</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>到了训练后期，模型已经接近最优解的邻域，继续用大学习率会导致在最优点附近来回震荡。此时应该逐渐降低学习率，让模型"精确着陆"。余弦退火（cosine annealing）提供了一种从探索到收敛的平滑过渡——它在中期保持相对较高的学习率，在末期快速衰减，形态恰好符合这种需求。</span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数值精度：用更"粗糙"的数字换取速度</span></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>训练的第三个维度是数值精度。标准的 32 位浮点数（FP32）有 23 位尾数和 8 位指数，提供约 7 位有效数字和 $\pm 3.4 \times 10^{38}$ 的动态范围。但 FP32 的缺点是显而易见的——每个参数占 4 字节，一个 70B 参数的模型仅参数就需要 280GB 显存，再加上优化器状态、激活值和梯度，单张 GPU 根本装不下。</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>混合精度训练的核心思想是：**前向传播和反向传播用半精度（16 位），只在参数更新时回到 FP32**。这样不仅内存减半，还能利用 GPU 上专门为半精度设计的 Tensor Core 来加速计算——在 A100 GPU 上，FP16 的吞吐量是 FP32 的 2–8 倍。</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>但半精度有一个致命的陷阱：**动态范围太窄**。FP16 的最小正数约为 $6 \times 10^{-8}$，最大值约为 $6.5 \times 10^{4}$。而梯度的值经常小于 $10^{-8}$——这些值在 FP16 中会被直接截断为 0，造成"梯度下溢"（gradient underflow）。Micikevicius 等人的实验发现，在不做任何处理的情况下，FP16 训练中**高达 67% 的梯度会变成零**。</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a>Google 提出的 BF16（Brain Floating-Point 16）用一种巧妙的方式解决了这个问题：它牺牲了精度（只有 7 位尾数，比 FP16 的 10 位少），换取了与 FP32 完全相同的动态范围（8 位指数）。这意味着 BF16 几乎不会发生梯度下溢，而且不需要 loss scaling 这种额外的工程技巧。精度的损失在实践中影响很小——训练最终收敛到的损失值几乎与 FP32 一致。这就是为什么 BF16 成为了 LLM 训练的事实标准。</span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a><span class="fu">### 优化器的演进</span></span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adam：自适应学习率的胜利</span></span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a>Adam 是目前 Transformer 训练中使用最广泛的优化器。它的名字来自"Adaptive Moment Estimation"，因为它同时估计梯度的一阶矩（均值，即动量）和二阶矩（未中心化方差）。</span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: Adam (Kingma &amp; Ba, 2014)</span></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a><span class="in">输入: 学习率 α (default: 0.001)</span></span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a><span class="in">      衰减率 β₁ = 0.9, β₂ = 0.999</span></span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a><span class="in">      数值稳定常数 ε = 1e-8</span></span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a><span class="in">      损失函数 f(θ)</span></span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a><span class="in">初始化: θ₀, m₀ = 0, v₀ = 0, t = 0</span></span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a><span class="in">while θ 未收敛:</span></span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a><span class="in">    t ← t + 1</span></span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a><span class="in">    g_t ← ∇_θ f_t(θ_{t-1})          # 计算梯度</span></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a><span class="in">    m_t ← β₁ · m_{t-1} + (1-β₁) · g_t    # 更新一阶矩（动量）</span></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a><span class="in">    v_t ← β₂ · v_{t-1} + (1-β₂) · g_t²   # 更新二阶矩（自适应项）</span></span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a><span class="in">    m̂_t ← m_t / (1 - β₁ᵗ)           # 偏差修正一阶矩</span></span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a><span class="in">    v̂_t ← v_t / (1 - β₂ᵗ)           # 偏差修正二阶矩</span></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a><span class="in">    θ_t ← θ_{t-1} - α · m̂_t / (√v̂_t + ε)   # 参数更新</span></span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a><span class="in">return θ_t</span></span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a>*Source: Kingma, D. P. &amp; Ba, J. (2014). "Adam: A Method for Stochastic Optimization". [arXiv:1412.6980](https://arxiv.org/abs/1412.6980), Algorithm 1*</span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a>Adam 中有一个容易被忽略但至关重要的设计：**偏差修正**（bias correction）。因为 $m_0 = 0$ 和 $v_0 = 0$，在训练初期，指数加权平均严重偏向 0。以一阶矩为例，如果梯度的真实期望是 $\mathbb{E}<span class="co">[</span><span class="ot">g</span><span class="co">]</span>$，那么：</span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">m_t</span><span class="co">]</span> = (1 - \beta_1^t) \cdot \mathbb{E}<span class="co">[</span><span class="ot">g</span><span class="co">]</span></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>当 $t$ 很小时，$\beta_1^t$ 接近 1，$m_t$ 严重低估了真实的梯度期望。除以 $(1 - \beta_1^t)$ 就精确地修正了这个偏差。二阶矩的偏差修正尤其关键——$\beta_2 = 0.999$ 意味着偏差需要上千步才能自然消退，而如果不修正，早期的参数更新会因为 $v_t$ 被低估而变得异常大。</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：Adam 的前 3 步更新</span></span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a>为了建立直觉，让我们用一个具体的数值例子走完 Adam 的前 3 步。假设我们只有一个标量参数 $\theta$，使用默认超参数 $\alpha = 0.001$，$\beta_1 = 0.9$，$\beta_2 = 0.999$，$\varepsilon = 10^{-8}$。</span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>**初始状态**：$\theta_0 = 0.5$，$m_0 = 0$，$v_0 = 0$</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a>假设前 3 步的梯度分别为 $g_1 = 0.1$，$g_2 = 0.2$，$g_3 = 0.15$。</span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a>**Step 1** ($t=1$, $g_1 = 0.1$)：</span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a>m_1 &amp;= 0.9 \times 0 + 0.1 \times 0.1 = 0.01 <span class="sc">\\</span></span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a>v_1 &amp;= 0.999 \times 0 + 0.001 \times 0.01 = 0.00001 <span class="sc">\\</span></span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a>\hat{m}_1 &amp;= \frac{0.01}{1 - 0.9^1} = \frac{0.01}{0.1} = 0.1 <span class="sc">\\</span></span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a>\hat{v}_1 &amp;= \frac{0.00001}{1 - 0.999^1} = \frac{0.00001}{0.001} = 0.01 <span class="sc">\\</span></span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a>\theta_1 &amp;= 0.5 - 0.001 \times \frac{0.1}{\sqrt{0.01} + 10^{-8}} = 0.5 - 0.001 \times \frac{0.1}{0.1} = 0.499</span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a>注意偏差修正的效果：未修正的 $m_1 = 0.01$，修正后 $\hat{m}_1 = 0.1$——放大了 10 倍！未修正的 $v_1 = 0.00001$，修正后 $\hat{v}_1 = 0.01$——放大了 1000 倍！而最终更新量 $\hat{m}_1 / \sqrt{\hat{v}_1} = 0.1 / 0.1 = 1.0$，实际步长为 $\alpha \times 1.0 = 0.001$，**恰好等于学习率本身**。这是 Adam 的一个有趣性质：在训练初期，每个参数的实际更新幅度近似等于学习率 $\alpha$，与梯度的绝对值几乎无关。</span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a>**Step 2** ($t=2$, $g_2 = 0.2$)：</span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a>m_2 &amp;= 0.9 \times 0.01 + 0.1 \times 0.2 = 0.029 <span class="sc">\\</span></span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a>v_2 &amp;= 0.999 \times 0.00001 + 0.001 \times 0.04 = 0.0000499 <span class="sc">\\</span></span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a>\hat{m}_2 &amp;= \frac{0.029}{1 - 0.81} = \frac{0.029}{0.19} \approx 0.1526 <span class="sc">\\</span></span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a>\hat{v}_2 &amp;= \frac{0.0000499}{1 - 0.998} = \frac{0.0000499}{0.002} \approx 0.02495 <span class="sc">\\</span></span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a>\theta_2 &amp;\approx 0.499 - 0.001 \times \frac{0.1526}{\sqrt{0.02495}} \approx 0.499 - 0.001 \times 0.966 \approx 0.49803</span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a>**Step 3** ($t=3$, $g_3 = 0.15$)：</span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a>m_3 &amp;= 0.9 \times 0.029 + 0.1 \times 0.15 = 0.0411 <span class="sc">\\</span></span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a>v_3 &amp;= 0.999 \times 0.0000499 + 0.001 \times 0.0225 \approx 0.0000722 <span class="sc">\\</span></span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a>\hat{m}_3 &amp;= \frac{0.0411}{1 - 0.729} = \frac{0.0411}{0.271} \approx 0.1517 <span class="sc">\\</span></span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a>\hat{v}_3 &amp;= \frac{0.0000722}{1 - 0.997} = \frac{0.0000722}{0.003} \approx 0.02407 <span class="sc">\\</span></span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a>\theta_3 &amp;\approx 0.49803 - 0.001 \times \frac{0.1517}{\sqrt{0.02407}} \approx 0.49803 - 0.001 \times 0.978 \approx 0.49705</span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a>从这个例子可以观察到几个关键现象。首先，实际更新幅度（约 0.001）始终接近学习率 $\alpha$，这就是 Adam "自适应"的含义：无论梯度是 0.1 还是 0.2，Adam 都会自动调整为大约相同量级的更新步长。其次，偏差修正在前几步的影响非常显著——如果去掉偏差修正，$v_1$ 会被严重低估，导致更新幅度暴增。</span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a><span class="fu">#### AdamW：修正权重衰减的 bug</span></span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a>Adam 的普及也暴露了一个微妙的问题。在传统的 SGD 中，L2 正则化和权重衰减是**数学等价的**：</span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a>\text{L2 正则化: } \theta_{t+1} = \theta_t - \eta \left(\nabla f(\theta_t) + \lambda \theta_t\right) = (1 - \eta\lambda)\theta_t - \eta \nabla f(\theta_t)</span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a>\text{权重衰减: } \theta_{t+1} = (1 - \lambda')\theta_t - \eta \nabla f(\theta_t)</span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a>令 $\lambda' = \eta\lambda$，两者完全一样。但是在 Adam 中，这种等价性**被打破了**。</span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a>在 Adam + L2 正则化中，正则化项 $\lambda\theta_t$ 被加到梯度里，**一起经过了自适应缩放**：</span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a>\text{Adam + L2: } \quad g_t = \nabla f(\theta_t) + \lambda\theta_t, \quad \theta_{t+1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}</span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a>问题在于：正则化梯度 $\lambda\theta_t$ 被 $1/\sqrt{\hat{v}_t}$ 缩放了。对于那些历史梯度很大的参数（$\hat{v}_t$ 大），正则化的效果被**削弱**了；对于那些历史梯度很小的参数，正则化反而被**放大**了。这完全不是我们想要的——权重衰减的初衷是对所有参数施加统一的缩小压力，与它们的梯度历史无关。</span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a>AdamW 的修正很简单但意义重大：将权重衰减从梯度中**解耦**出来，直接作用于参数本身：</span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a>\text{AdamW: } \quad \theta_{t+1} = (1 - \lambda)\theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}</span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 2: AdamW — 关键差异 (Loshchilov &amp; Hutter, 2017)</span></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a><span class="in"># Adam + L2（错误的方式）:</span></span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a><span class="in">g_t ← ∇f(θ) + λ·θ          # 正则化进入梯度</span></span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a><span class="in">m_t, v_t ← 更新矩估计(g_t)    # 正则化被自适应缩放扭曲</span></span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a><span class="in">θ ← θ - α · m̂_t / (√v̂_t + ε) # 正则化效果依赖于梯度历史</span></span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a><span class="in"># AdamW（正确的方式）:</span></span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a><span class="in">g_t ← ∇f(θ)                  # 梯度不含正则化项</span></span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a><span class="in">m_t, v_t ← 更新矩估计(g_t)    # 自适应缩放只作用于真实梯度</span></span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a><span class="in">θ ← (1 - λ)·θ - α · m̂_t / (√v̂_t + ε)  # 权重衰减独立于梯度</span></span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a>*Source: Loshchilov, I. &amp; Hutter, F. (2017). "Decoupled Weight Decay Regularization". [arXiv:1711.05101](https://arxiv.org/abs/1711.05101), Algorithm 2*</span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a>Loshchilov 和 Hutter 的实验还揭示了一个重要的工程洞察：在 AdamW 中，最优学习率和最优权重衰减是**可分离的**——你可以独立调节它们，最优区域在超参数空间中形成一个矩形。而在 Adam + L2 中，两者是**耦合的**，最优区域是对角线形状，调一个就必须同时调另一个。这使得 AdamW 的超参数搜索效率大大提高。</span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a><span class="al">![AdamW 中学习率与权重衰减的最优区域呈**矩形**（左图），两者可以独立调节。在 Adam + L2 中，最优区域是对角线形状，调一个必须同时调另一个。热力图中暖色（红色）表示更好的验证性能，冷色（蓝色）表示较差性能。](figures/chapter-18/fig-adamw-heatmap.png)</span>{#fig-adamw-heatmap width=55%}</span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a>*Source: Loshchilov, I. &amp; Hutter, F. (2017). "Decoupled Weight Decay Regularization". [arXiv:1711.05101](https://arxiv.org/abs/1711.05101), Figure 2*</span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adafactor：当显存是瓶颈</span></span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a>Adam 的优势伴随着一个沉重的代价：它需要为每个参数维护两个状态变量（$m$ 和 $v$），这意味着**优化器状态的显存是模型参数的 2 倍**。对于一个 70B 参数的模型（FP32 存储需 280GB），Adam 的优化器状态额外需要 560GB——这在任何单张 GPU 上都是不可能的。</span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a>Shazeer 和 Stern 提出的 Adafactor 用一个巧妙的线性代数技巧大幅削减了这个开销。对于一个 $n \times m$ 的权重矩阵，Adam 需要存储完整的 $n \times m$ 二阶矩矩阵 $V$。Adafactor 观察到：与其存储完整的 $V$，不如只存储它的**行和**（$n$ 个值）和**列和**（$m$ 个值），然后通过外积重构完整矩阵：</span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a>\hat{V}_t = \frac{R_t \cdot C_t^{\top}}{\mathbf{1}_n^{\top} R_t}</span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a>其中 $R_t \in \mathbb{R}^n$ 是行和向量，$C_t \in \mathbb{R}^m$ 是列和向量。这将二阶矩的存储从 $O(nm)$ 降低到 $O(n + m)$。对于一个 $4096 \times 4096$ 的权重矩阵，存储量从 1600 万个值降低到 8192 个值——压缩了近 2000 倍。</span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a>Adafactor 还做出了几个额外的设计选择：去掉了一阶矩（momentum），改用更新裁剪（update clipping）来替代；使用相对步长（relative step size）而非绝对学习率；使用随时间衰减的 $\beta_2$。这些选择的共同效果是：**Adafactor 的优化器状态几乎不需要额外显存**，同时在 Transformer 翻译任务上的表现与 Adam 相当。T5 的训练就使用了 Adafactor。</span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Lion：进化搜索发现的优化器</span></span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a>2023 年，Google Brain 的 Chen 等人用一种完全不同的方法发现了新的优化器：他们定义了一个"数学程序"的搜索空间，用进化算法在这个空间中搜索最优的优化器——就像 NAS（Neural Architecture Search）搜索最优架构一样，但搜索对象换成了优化器的更新规则。</span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a>搜索发现的优化器被命名为 Lion（EvoLved Sign Momentum），它的更新规则出人意料地简单：</span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a><span class="fu">## Program 1: Lion (Chen et al., 2023)</span></span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a><span class="in">输入: 学习率 η, 权重衰减 λ, 衰减率 β₁ = 0.9, β₂ = 0.99</span></span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a><span class="in">while θ 未收敛:</span></span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a><span class="in">    g_t ← ∇_θ f_t(θ_{t-1})</span></span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a><span class="in">    # 计算更新方向（只取符号！）</span></span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a><span class="in">    update ← sign(β₁ · m_{t-1} + (1-β₁) · g_t)</span></span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a><span class="in">    # 应用权重衰减和更新</span></span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a><span class="in">    θ_t ← (1 - η·λ) · θ_{t-1} - η · update</span></span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a><span class="in">    # 更新动量（注意：用不同的 β₂）</span></span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a><span class="in">    m_t ← β₂ · m_{t-1} + (1-β₂) · g_t</span></span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a><span class="in">return θ_t</span></span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a>*Source: Chen, X. et al. (2023). "Symbolic Discovery of Optimization Algorithms". [arXiv:2302.06675](https://arxiv.org/abs/2302.06675), Program 1*</span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a>Lion 与 Adam 的核心区别在于 <span class="in">`sign()`</span> 操作——它将更新方向量化为 $<span class="sc">\{</span>-1, 0, +1<span class="sc">\}</span>$，产生**均匀幅度的更新**。这有几个深远的影响：</span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a>第一，Lion 只需要存储一个状态变量（动量 $m$），比 Adam 省一半的优化器显存。第二，因为 <span class="in">`sign()`</span> 的输出范数远大于 Adam 的自适应更新，Lion 需要**更小的学习率**（通常是 Adam 的 3-10 倍小）和**更大的权重衰减**来补偿。第三，`sign()` 操作对梯度的绝对值不敏感，这使得 Lion 在**大 batch 训练**时特别有优势——当 batch size 增大时，梯度的方差减小但方向变得更可靠，而 Lion 恰好只利用方向信息。</span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a><span class="al">![Lion vs AdamW 性能对比。左图：ImageNet 分类精度 vs 训练计算量，Lion 达到相同精度所需计算量约为 AdamW 的 1/3（~3x speedup）。右图：扩散模型的 FID 分数 vs 训练迭代数，Lion 同样展现约 2.3 倍加速。Lion 的优势在大规模训练中尤为明显。](figures/chapter-18/fig-lion-accuracy-vs-compute.png)</span>{#fig-lion-compute width=80%}</span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-340"><a href="#cb6-340" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-341"><a href="#cb6-341" aria-hidden="true" tabindex="-1"></a>*Source: Chen, X. et al. (2023). "Symbolic Discovery of Optimization Algorithms". [arXiv:2302.06675](https://arxiv.org/abs/2302.06675), Figure 1*</span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 8-bit Adam：量化优化器状态</span></span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a>Dettmers 等人提出了另一种减少优化器显存的思路：既然 Adam 的状态变量不需要完全精确，为什么不直接把它们从 32 位量化到 8 位呢？</span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a>核心技术是**分块量化**（block-wise quantization）：将每个状态张量切分成 2048 个元素一组的小块，每个块有自己独立的归一化常数（绝对最大值）。在每个块内部，值被归一化到 <span class="co">[</span><span class="ot">-1, 1</span><span class="co">]</span> 范围后再量化。这种分块设计有两个好处：异常大的值（outlier）只会影响它所在的块，不会"传染"到整个张量；而且每个块的量化和反量化可以在 GPU 上并行执行。</span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a>配合一种称为"动态树量化"的非线性量化方案（在不同幅度范围内自适应地分配量化精度），8-bit Adam 将优化器状态的显存压缩到约原来的四分之一，同时在训练效果上与 32-bit Adam **几乎完全一致**。这一方法已经通过 <span class="in">`bitsandbytes`</span> 库被广泛使用。</span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 优化器对比总结</span></span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 优化器 <span class="pp">|</span> 状态变量 <span class="pp">|</span> 每参数显存 <span class="pp">|</span> 核心特点 <span class="pp">|</span> 适用场景 <span class="pp">|</span></span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|----------|-----------|---------|---------|</span></span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SGD + Momentum <span class="pp">|</span> $m$ <span class="pp">|</span> 1× <span class="pp">|</span> 最简单，通用基线 <span class="pp">|</span> 小模型、CV任务 <span class="pp">|</span></span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Adam <span class="pp">|</span> $m, v$ <span class="pp">|</span> 2× <span class="pp">|</span> 自适应学习率 <span class="pp">|</span> 通用，Transformer标配 <span class="pp">|</span></span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> AdamW <span class="pp">|</span> $m, v$ <span class="pp">|</span> 2× <span class="pp">|</span> 正确的权重衰减 <span class="pp">|</span> Adam的严格上位替代 <span class="pp">|</span></span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Adafactor <span class="pp">|</span> $R, C$（分解） <span class="pp">|</span> ≈ 0× <span class="pp">|</span> 亚线性显存 <span class="pp">|</span> 超大模型（T5） <span class="pp">|</span></span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Lion <span class="pp">|</span> $m$ <span class="pp">|</span> 1× <span class="pp">|</span> 符号更新，进化搜索发现 <span class="pp">|</span> 大batch，视觉/扩散模型 <span class="pp">|</span></span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 8-bit Adam <span class="pp">|</span> $m, v$（量化） <span class="pp">|</span> ≈ 0.5× <span class="pp">|</span> 量化优化器状态 <span class="pp">|</span> 显存受限场景 <span class="pp">|</span></span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a><span class="fu">### 学习率策略</span></span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Warmup 的必要性：理论解释</span></span>
<span id="cb6-366"><a href="#cb6-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-367"><a href="#cb6-367" aria-hidden="true" tabindex="-1"></a>学习率 warmup 是大模型训练中最广泛使用的技巧之一，但长期以来它被视为一种"经验性的 hack"——大家都知道没有 warmup 训练会崩溃，却说不清楚为什么。Liu 等人（2019）的 RAdam 论文首次为 warmup 提供了严谨的理论解释。</span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a>核心分析如下：Adam 的自适应学习率是 $\alpha_t = \alpha / \sqrt{\hat{v}_t}$，其中 $\hat{v}_t$ 是二阶矩的偏差修正估计。问题在于，$\hat{v}_t$ 本身是一个随机变量——它是基于有限样本的估计。刘等人证明，$\hat{v}_t$ 的方差近似为：</span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a>\text{Var}\left<span class="co">[</span><span class="ot">\frac{1}{\hat{v}_t}\right</span><span class="co">]</span> \approx \frac{1}{\rho_t} \cdot \text{常数}</span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a>其中 $\rho_t$ 是"简单移动平均长度"（SMA length），表示二阶矩估计中有效参与的样本数量：</span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a>\rho_t = \rho_\infty - \frac{2t \cdot \beta_2^t}{1 - \beta_2^t}, \quad \rho_\infty = \frac{2}{1 - \beta_2} - 1</span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a>在训练初期（$t$ 很小），$\rho_t$ 很小，这意味着自适应学习率 $1/\sqrt{\hat{v}_t}$ 的方差极大——它可能在一步之间从 10 跳到 0.1，让参数更新变得完全不可预测。Warmup 的作用正是在这个高方差阶段使用小学习率来"兜底"。</span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a>RAdam 将这个分析自动化了——它计算一个方差修正系数 $r_t$，在 $\rho_t$ 足够大时使用完整的 Adam 更新，在 $\rho_t$ 过小时退化为普通 SGD。但 RAdam 的隐式 warmup 只有约 40 步（对于 $\beta_2 = 0.999$），对于需要数千步 warmup 的 LLM 训练来说远远不够——这说明 LLM 的 warmup 需求不仅来自 Adam 的方差问题，还有更深层的原因（如损失函数景观的条件数在早期极差）。</span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 余弦退火：从探索到收敛的平滑过渡</span></span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a>确定了 warmup 的必要性后，接下来的问题是：warmup 之后学习率如何衰减？</span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a>余弦退火（cosine annealing）的公式是：</span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a>\eta(t) = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T} \cdot \pi\right)\right)</span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a>其中 $\eta_{\max}$ 是峰值学习率，$\eta_{\min}$ 是最终学习率（通常为峰值的 10%），$T$ 是总训练步数。这个公式的形状恰好符合训练的需求：前半段学习率下降缓慢（保持高速探索），后半段学习率快速衰减（精确收敛）。</span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a>现代大模型训练的标准学习率配方因此是：</span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a>\eta(t) = \begin{cases}</span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a>\eta_{\max} \cdot \frac{t}{T_{\text{warmup}}} &amp; \text{if } t \leq T_{\text{warmup}} \quad \text{(线性 warmup)} <span class="sc">\\</span></span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a>\eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t - T_{\text{warmup}}}{T - T_{\text{warmup}}} \cdot \pi\right)\right) &amp; \text{if } t &gt; T_{\text{warmup}} \quad \text{(余弦衰减)}</span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a>GPT-2、GPT-3、LLaMA、PaLM、Chinchilla 几乎无一例外地使用这种"线性 warmup + 余弦衰减"的组合。Chinchilla 的实验还发现，调度的具体形状（余弦 vs 线性衰减）并不是特别关键——真正重要的是有 warmup 和最终衰减到接近 0 这两个要素。</span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a>一个值得注意的实际问题是：余弦退火要求**预先知道总训练步数**$T$。这在大多数训练中不是问题（你计划训练多久就设多少），但在需要灵活延长训练的场景下就很不方便。为此，最近出现了一种替代方案——**Warmup-Stable-Decay (WSD)** 策略：warmup 后学习率保持在峰值不变（stable 阶段），只在最后的一小段"冷却"（cooldown）阶段快速衰减。这允许在 stable 阶段的任何时刻做出"停止还是继续训练"的决定。</span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a><span class="al">![线性 Warmup + 余弦衰减的学习率调度。横轴为训练步数（共 30 个 epoch），纵轴为学习率。前段（约 5 个 epoch）学习率从 0 线性上升到峰值（warmup 阶段），之后按余弦函数平滑衰减。曲线在中期保持相对较高的学习率（高速探索），后期快速下降（精确收敛）。](figures/chapter-18/fig-cosine-schedule-warmup.svg)</span>{#fig-cosine-schedule width=60%}</span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a>*Source: D2L, Section 12.11 (Learning Rate Scheduling). [d2l.ai](https://d2l.ai/)*</span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a><span class="fu">### 混合精度训练</span></span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a><span class="fu">#### FP16、BF16 与 FP32 的位布局</span></span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a>理解混合精度训练，首先需要理解三种浮点格式的本质区别。一个浮点数由三部分组成：符号位（sign）、指数位（exponent）和尾数位（mantissa/fraction）。</span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a>\text{value} = (-1)^{\text{sign}} \times 2^{\text{exponent} - \text{bias}} \times (1 + \text{fraction})</span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 格式 <span class="pp">|</span> 总位数 <span class="pp">|</span> 符号 <span class="pp">|</span> 指数 <span class="pp">|</span> 尾数 <span class="pp">|</span> 动态范围 <span class="pp">|</span> 精度（有效数字） <span class="pp">|</span></span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|------|------|------|----------|----------------|</span></span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FP32 <span class="pp">|</span> 32 <span class="pp">|</span> 1 <span class="pp">|</span> 8 <span class="pp">|</span> 23 <span class="pp">|</span> $\pm 3.4 \times 10^{38}$ <span class="pp">|</span> ~7 位 <span class="pp">|</span></span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FP16 <span class="pp">|</span> 16 <span class="pp">|</span> 1 <span class="pp">|</span> 5 <span class="pp">|</span> 10 <span class="pp">|</span> $\pm 6.5 \times 10^{4}$ <span class="pp">|</span> ~3 位 <span class="pp">|</span></span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BF16 <span class="pp">|</span> 16 <span class="pp">|</span> 1 <span class="pp">|</span> 8 <span class="pp">|</span> 7 <span class="pp">|</span> $\pm 3.4 \times 10^{38}$ <span class="pp">|</span> ~2 位 <span class="pp">|</span></span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a>BF16 的设计哲学一目了然：它与 FP32 共享相同的 8 位指数，因此动态范围完全一致；代价是尾数只有 7 位（FP16 有 10 位），精度较低。对于深度学习训练来说，动态范围远比精度更重要——梯度的幅度可以跨越几十个数量级，但每个梯度值本身不需要高精度。</span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-434"><a href="#cb6-434" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：梯度下溢问题</span></span>
<span id="cb6-435"><a href="#cb6-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a>假设某个参数的梯度为 $g = 1.2 \times 10^{-6}$。在三种格式下：</span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP32**：可以精确表示（最小正数 $\approx 1.2 \times 10^{-38}$）</span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BF16**：可以表示（最小正数 $\approx 1.2 \times 10^{-38}$，与 FP32 相同）</span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP16**：**可以表示**，但精度只有 3 位有效数字</span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a>现在假设梯度更小，$g = 3.5 \times 10^{-8}$：</span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP32**：正常表示</span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BF16**：正常表示</span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP16**：$\approx 5.96 \times 10^{-8}$ 是 FP16 的最小正次正规数，$3.5 \times 10^{-8}$ 勉强可表示但精度极差</span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a>再小一点，$g = 1.0 \times 10^{-9}$：</span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP32 / BF16**：正常表示</span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP16**：**下溢为 0**！这个梯度被完全丢弃了。</span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a>Micikevicius 等人的实验显示，在 SSD（物体检测模型）的训练中，不做任何处理的 FP16 训练会让 **67% 的激活梯度变成零**。这解释了为什么 FP16 混合精度训练需要 loss scaling。</span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 混合精度训练的三大技术</span></span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a>混合精度训练并不是简单地"把所有数字换成 16 位"，而是一套精心设计的三重技术组合：</span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a>**技术一：FP32 主权重（Master Weights）**。参数的"权威版本"始终保存在 FP32 中。每次前向传播前将它们转换为 FP16/BF16 用于计算，反向传播结束后将 FP32 格式的梯度应用于 FP32 主权重。这保证了参数更新的累积精度不会受半精度的损害。</span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a>**技术二：损失缩放（Loss Scaling，仅 FP16 需要）**。在反向传播之前，将损失乘以一个大常数 $S$（如 1024 或 65536）。由于反向传播是线性的，所有梯度也会被放大 $S$ 倍，将那些原本会下溢为 0 的小梯度"提升"到 FP16 可表示的范围。优化器更新前再将梯度除以 $S$ 恢复原始尺度。$S$ 可以是固定值，也可以动态调整——从大值开始，遇到数值溢出就减半，一段时间没有溢出就翻倍。</span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a>**技术三：FP32 累加（Accumulation）**。某些运算（如矩阵乘法中的累加、BatchNorm/LayerNorm 中的均值和方差计算）对精度特别敏感。这些运算的中间结果用 FP32 累加，最终结果再转回 FP16/BF16。现代 GPU 的 Tensor Core 原生支持"FP16 输入、FP32 累加"的模式。</span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a>BF16 的出现大大简化了这个流程——由于 BF16 的动态范围与 FP32 相同，**loss scaling 通常不再需要**。这是 BF16 成为 LLM 训练标准的最重要原因之一：更少的工程复杂性，更少的潜在 bug。</span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a><span class="al">![混合精度训练的数据流。前向传播（FWD）和反向传播（BWD-Actv, BWD-Weight）均在 FP16 下执行以提高速度，只有权重更新（Weight Update）使用 FP32 的 Master Weights。`float2half` 将 FP32 主权重转换为 FP16 用于下一次前向传播。这种设计在保持训练精度的同时，将计算量减半并充分利用 Tensor Core。](figures/chapter-18/fig-mixed-precision-training-loop.png)</span>{#fig-mixed-precision width=80%}</span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a>*Source: Micikevicius, P. et al. (2017). "Mixed Precision Training". [arXiv:1710.03740](https://arxiv.org/abs/1710.03740), Figure 1*</span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练稳定性诊断</span></span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Loss Spike：原因与应对</span></span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a>Loss spike 是大模型训练中最令人头疼的问题之一。它的表现是损失函数在正常下降过程中突然飙升——有时飙升到训练初期的水平，有时甚至变成 NaN。</span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a>PaLM 540B 的训练提供了关于 loss spike 最详细的公开分析。Google 的团队发现：</span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a>第一，spike 的出现具有**不可预测性**。约 20 次 spike 发生在高度不规则的时间间隔，有时出现在训练的极晚期。使用相同的超参数在小模型上训练不会出现 spike——这表明 spike 是大规模独有的现象。</span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a>第二，spike 的原因是**数据与参数状态的混沌交互**。将引发 spike 的同一批数据从更早的 checkpoint 重新训练，spike 不会再次出现。这排除了"坏数据"假说，说明 spike 来自特定数据与特定参数状态的不幸组合。</span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a>第三，**有效的应对策略是重启 + 跳过**：回滚到 spike 前约 100 步的 checkpoint，跳过 spike 前后 200–500 个 batch 的数据，然后继续训练。这个策略虽然粗暴，但有效——在另一个参数状态下，这些数据不再引发 spike。</span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a><span class="al">![GLM-130B 训练过程中的实际学习率曲线。红色箭头标记了**人工干预降低学习率**的时刻——每一次骤降都对应一次训练不稳定事件。原计划的 warmup + 平滑衰减被频繁的手动干预打断，最终学习率呈不规则阶梯状下降。这张图生动展示了大模型训练的真实面貌：计划赶不上变化，人工值守不可或缺。](figures/chapter-18/fig-training-collapse-glm130b.png)</span>{#fig-lr-interventions width=75%}</span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a>*Source: Zeng et al. (2022). "GLM-130B: An Open Bilingual Pre-trained Model". [arXiv:2210.02414](https://arxiv.org/abs/2210.02414)*</span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a>OPT-175B 的训练日志补充了另一个视角：**动态 loss scalar 是不稳定性的先行指标**。当 loss scalar 反复崩溃到 0 时，往往预示着即将发生更严重的不稳定。团队学会了在选择重启 checkpoint 时，挑选 loss scalar 仍处于"健康"状态（≥ 1.0）的时间点。</span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 梯度范数监控</span></span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a>持续监控梯度的 L2 范数是诊断训练健康状况的最重要手段之一。正常训练中，梯度范数应该呈现一种特征模式：初期较大（模型离最优解远），中期逐渐减小并趋于稳定，后期随学习率衰减而进一步减小。</span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a>以下信号需要警觉：</span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**梯度范数频繁触及裁剪阈值**：说明学习率可能过大，或者模型在损失函数的"悬崖"附近。</span>
<span id="cb6-502"><a href="#cb6-502" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**梯度范数突然飙升**：可能是遇到了异常数据、数值溢出或模型参数进入了不稳定区域。</span>
<span id="cb6-503"><a href="#cb6-503" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**梯度范数持续为 0 或极小**：可能是梯度消失，或 loss scaling 的缩放因子过小导致所有梯度下溢。</span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**不同层的梯度范数差异扩大**：可能预示着训练不稳定性的积累。</span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 梯度裁剪</span></span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a>梯度裁剪（gradient clipping）是防止梯度爆炸的最基本防线。标准做法是全局范数裁剪（global norm clipping）：计算所有参数梯度的 L2 范数，如果超过阈值 $\theta$（通常为 1.0），则将所有梯度等比例缩小：</span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a>\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{<span class="sc">\|</span>\mathbf{g}<span class="sc">\|</span>_2}\right) \cdot \mathbf{g}</span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a>这个操作保证了梯度范数不超过 $\theta$，而不改变梯度的方向。但正如 PaLM 的经验所示，梯度裁剪是**必要但不充分的**——它可以防止最严重的梯度爆炸，但无法阻止更微妙的不稳定性。</span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 风险卡片：训练稳定性问题</span></span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> 内容 <span class="pp">|</span></span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **常见症状** <span class="pp">|</span> Loss spike、NaN、训练后期震荡、loss scalar 反复崩溃 <span class="pp">|</span></span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **典型原因** <span class="pp">|</span> 学习率过大、数值溢出（FP16 动态范围不足）、数据异常 batch、梯度爆炸、参数状态与数据的混沌交互 <span class="pp">|</span></span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **诊断方法** <span class="pp">|</span> 监控梯度范数、激活值分布、loss 曲线、loss scalar 状态 <span class="pp">|</span></span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **防护措施** <span class="pp">|</span> 梯度裁剪（norm 1.0）、warmup（≥2000 步）、BF16（避免 loss scaling）、频繁保存 checkpoint、数据清洗 <span class="pp">|</span></span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **应急策略** <span class="pp">|</span> 回滚到 spike 前 ~100 步 checkpoint + 跳过 200–500 batch 数据 <span class="pp">|</span></span>
<span id="cb6-525"><a href="#cb6-525" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **开放问题** <span class="pp">|</span> 为什么 spike 只在大规模出现？能否预测而非被动应对？自适应裁剪（AdaGC, AGGC）能否替代固定阈值？ <span class="pp">|</span></span>
<span id="cb6-526"><a href="#cb6-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a><span class="fu">### 现代 LLM 训练配方</span></span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a>下面展示一个将本章所有技术整合在一起的 PyTorch 训练配方：</span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler, autocast</span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 1. 优化器配置 ==========</span></span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ...  <span class="co"># 你的 Transformer 模型</span></span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW：正确的权重衰减方式</span></span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(</span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a>    model.parameters(),</span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">3e-4</span>,          <span class="co"># 峰值学习率</span></span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a>    betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.95</span>), <span class="co"># LLM 常用设置（β₂ 比默认的 0.999 小）</span></span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># 解耦的权重衰减</span></span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a>    eps<span class="op">=</span><span class="fl">1e-8</span></span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-558"><a href="#cb6-558" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 2. 学习率调度：线性 warmup + 余弦衰减 ==========</span></span>
<span id="cb6-559"><a href="#cb6-559" aria-hidden="true" tabindex="-1"></a>warmup_steps <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb6-560"><a href="#cb6-560" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="dv">300000</span></span>
<span id="cb6-561"><a href="#cb6-561" aria-hidden="true" tabindex="-1"></a>min_lr <span class="op">=</span> <span class="fl">3e-5</span>  <span class="co"># 峰值的 10%</span></span>
<span id="cb6-562"><a href="#cb6-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-563"><a href="#cb6-563" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lr(step):</span>
<span id="cb6-564"><a href="#cb6-564" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""线性 warmup + 余弦衰减"""</span></span>
<span id="cb6-565"><a href="#cb6-565" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">&lt;</span> warmup_steps:</span>
<span id="cb6-566"><a href="#cb6-566" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 线性 warmup</span></span>
<span id="cb6-567"><a href="#cb6-567" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">3e-4</span> <span class="op">*</span> step <span class="op">/</span> warmup_steps</span>
<span id="cb6-568"><a href="#cb6-568" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-569"><a href="#cb6-569" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 余弦衰减</span></span>
<span id="cb6-570"><a href="#cb6-570" aria-hidden="true" tabindex="-1"></a>        progress <span class="op">=</span> (step <span class="op">-</span> warmup_steps) <span class="op">/</span> (total_steps <span class="op">-</span> warmup_steps)</span>
<span id="cb6-571"><a href="#cb6-571" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> min_lr <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="fl">3e-4</span> <span class="op">-</span> min_lr) <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> math.cos(math.pi <span class="op">*</span> progress))</span>
<span id="cb6-572"><a href="#cb6-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-573"><a href="#cb6-573" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(</span>
<span id="cb6-574"><a href="#cb6-574" aria-hidden="true" tabindex="-1"></a>    optimizer, lr_lambda<span class="op">=</span><span class="kw">lambda</span> step: get_lr(step) <span class="op">/</span> <span class="fl">3e-4</span></span>
<span id="cb6-575"><a href="#cb6-575" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-576"><a href="#cb6-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-577"><a href="#cb6-577" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 3. 混合精度训练（BF16）==========</span></span>
<span id="cb6-578"><a href="#cb6-578" aria-hidden="true" tabindex="-1"></a><span class="co"># BF16 不需要 GradScaler（动态范围与 FP32 相同）</span></span>
<span id="cb6-579"><a href="#cb6-579" aria-hidden="true" tabindex="-1"></a>use_bf16 <span class="op">=</span> torch.cuda.is_bf16_supported()</span>
<span id="cb6-580"><a href="#cb6-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-581"><a href="#cb6-581" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 4. 训练循环 ==========</span></span>
<span id="cb6-582"><a href="#cb6-582" aria-hidden="true" tabindex="-1"></a>max_grad_norm <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># 梯度裁剪阈值</span></span>
<span id="cb6-583"><a href="#cb6-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-584"><a href="#cb6-584" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb6-585"><a href="#cb6-585" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 前向传播（BF16）</span></span>
<span id="cb6-586"><a href="#cb6-586" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> autocast(dtype<span class="op">=</span>torch.bfloat16 <span class="cf">if</span> use_bf16 <span class="cf">else</span> torch.float16):</span>
<span id="cb6-587"><a href="#cb6-587" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model(batch)</span>
<span id="cb6-588"><a href="#cb6-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-589"><a href="#cb6-589" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 反向传播</span></span>
<span id="cb6-590"><a href="#cb6-590" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb6-591"><a href="#cb6-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-592"><a href="#cb6-592" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 梯度裁剪</span></span>
<span id="cb6-593"><a href="#cb6-593" aria-hidden="true" tabindex="-1"></a>    grad_norm <span class="op">=</span> torch.nn.utils.clip_grad_norm_(</span>
<span id="cb6-594"><a href="#cb6-594" aria-hidden="true" tabindex="-1"></a>        model.parameters(), max_grad_norm</span>
<span id="cb6-595"><a href="#cb6-595" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-596"><a href="#cb6-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-597"><a href="#cb6-597" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 参数更新</span></span>
<span id="cb6-598"><a href="#cb6-598" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb6-599"><a href="#cb6-599" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb6-600"><a href="#cb6-600" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb6-601"><a href="#cb6-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-602"><a href="#cb6-602" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ========== 5. 训练监控 ==========</span></span>
<span id="cb6-603"><a href="#cb6-603" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-604"><a href="#cb6-604" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss"> | "</span></span>
<span id="cb6-605"><a href="#cb6-605" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"LR: </span><span class="sc">{</span>get_lr(step)<span class="sc">:.2e}</span><span class="ss"> | "</span></span>
<span id="cb6-606"><a href="#cb6-606" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Grad Norm: </span><span class="sc">{</span>grad_norm<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-607"><a href="#cb6-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-608"><a href="#cb6-608" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 检测异常</span></span>
<span id="cb6-609"><a href="#cb6-609" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.isnan(loss) <span class="kw">or</span> torch.isinf(loss):</span>
<span id="cb6-610"><a href="#cb6-610" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"⚠️ NaN/Inf detected at step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">! "</span></span>
<span id="cb6-611"><a href="#cb6-611" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Loading last checkpoint..."</span>)</span>
<span id="cb6-612"><a href="#cb6-612" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加载最近的 checkpoint 重启</span></span>
<span id="cb6-613"><a href="#cb6-613" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb6-614"><a href="#cb6-614" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-615"><a href="#cb6-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-616"><a href="#cb6-616" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键超参数速查</span></span>
<span id="cb6-617"><a href="#cb6-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-618"><a href="#cb6-618" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 超参数 <span class="pp">|</span> 典型值 <span class="pp">|</span> 来源/依据 <span class="pp">|</span></span>
<span id="cb6-619"><a href="#cb6-619" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|--------|----------|</span></span>
<span id="cb6-620"><a href="#cb6-620" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 峰值学习率 <span class="pp">|</span> $1 \times 10^{-4}$ 到 $3 \times 10^{-4}$ <span class="pp">|</span> Chinchilla, LLaMA <span class="pp">|</span></span>
<span id="cb6-621"><a href="#cb6-621" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\beta_1, \beta_2$ <span class="pp">|</span> 0.9, 0.95 <span class="pp">|</span> LLaMA（$\beta_2$ 比默认 0.999 小，有助于稳定性） <span class="pp">|</span></span>
<span id="cb6-622"><a href="#cb6-622" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 权重衰减 <span class="pp">|</span> 0.1 <span class="pp">|</span> GPT-3, LLaMA <span class="pp">|</span></span>
<span id="cb6-623"><a href="#cb6-623" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Warmup 步数 <span class="pp">|</span> 总步数的 0.1–1% <span class="pp">|</span> 通常 2000 步 <span class="pp">|</span></span>
<span id="cb6-624"><a href="#cb6-624" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 梯度裁剪阈值 <span class="pp">|</span> 1.0 <span class="pp">|</span> 几乎所有大模型 <span class="pp">|</span></span>
<span id="cb6-625"><a href="#cb6-625" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 最终学习率 <span class="pp">|</span> 峰值的 10% <span class="pp">|</span> Chinchilla <span class="pp">|</span></span>
<span id="cb6-626"><a href="#cb6-626" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Batch size <span class="pp">|</span> 逐步增大或固定 <span class="pp">|</span> GPT-3: 从 32K token 增到 3.2M token <span class="pp">|</span></span>
<span id="cb6-627"><a href="#cb6-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-628"><a href="#cb6-628" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现 Adam 优化器</span></span>
<span id="cb6-629"><a href="#cb6-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-630"><a href="#cb6-630" aria-hidden="true" tabindex="-1"></a>为了深入理解 Adam 的工作原理，下面给出一个从零实现的版本：</span>
<span id="cb6-631"><a href="#cb6-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-634"><a href="#cb6-634" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-635"><a href="#cb6-635" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-636"><a href="#cb6-636" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb6-637"><a href="#cb6-637" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Adam 优化器的完整实现（展开查看）"</span></span>
<span id="cb6-638"><a href="#cb6-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-639"><a href="#cb6-639" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdamFromScratch:</span>
<span id="cb6-640"><a href="#cb6-640" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-641"><a href="#cb6-641" aria-hidden="true" tabindex="-1"></a><span class="co">    从零实现的 Adam 优化器，用于理解内部机制。</span></span>
<span id="cb6-642"><a href="#cb6-642" aria-hidden="true" tabindex="-1"></a><span class="co">    生产环境请使用 torch.optim.AdamW。</span></span>
<span id="cb6-643"><a href="#cb6-643" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-644"><a href="#cb6-644" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr<span class="op">=</span><span class="fl">1e-3</span>, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.999</span>),</span>
<span id="cb6-645"><a href="#cb6-645" aria-hidden="true" tabindex="-1"></a>                 eps<span class="op">=</span><span class="fl">1e-8</span>, weight_decay<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb6-646"><a href="#cb6-646" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params <span class="op">=</span> <span class="bu">list</span>(params)</span>
<span id="cb6-647"><a href="#cb6-647" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb6-648"><a href="#cb6-648" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta1, <span class="va">self</span>.beta2 <span class="op">=</span> betas</span>
<span id="cb6-649"><a href="#cb6-649" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb6-650"><a href="#cb6-650" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_decay <span class="op">=</span> weight_decay</span>
<span id="cb6-651"><a href="#cb6-651" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-652"><a href="#cb6-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-653"><a href="#cb6-653" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 初始化状态</span></span>
<span id="cb6-654"><a href="#cb6-654" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m <span class="op">=</span> [torch.zeros_like(p) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params]  <span class="co"># 一阶矩</span></span>
<span id="cb6-655"><a href="#cb6-655" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> [torch.zeros_like(p) <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params]  <span class="co"># 二阶矩</span></span>
<span id="cb6-656"><a href="#cb6-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-657"><a href="#cb6-657" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb6-658"><a href="#cb6-658" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-659"><a href="#cb6-659" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-660"><a href="#cb6-660" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb6-661"><a href="#cb6-661" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb6-662"><a href="#cb6-662" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb6-663"><a href="#cb6-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-664"><a href="#cb6-664" aria-hidden="true" tabindex="-1"></a>                g <span class="op">=</span> p.grad</span>
<span id="cb6-665"><a href="#cb6-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-666"><a href="#cb6-666" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 更新一阶矩（动量）</span></span>
<span id="cb6-667"><a href="#cb6-667" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.m[i] <span class="op">=</span> <span class="va">self</span>.beta1 <span class="op">*</span> <span class="va">self</span>.m[i] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta1) <span class="op">*</span> g</span>
<span id="cb6-668"><a href="#cb6-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-669"><a href="#cb6-669" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 更新二阶矩（自适应项）</span></span>
<span id="cb6-670"><a href="#cb6-670" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.v[i] <span class="op">=</span> <span class="va">self</span>.beta2 <span class="op">*</span> <span class="va">self</span>.v[i] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta2) <span class="op">*</span> g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-671"><a href="#cb6-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-672"><a href="#cb6-672" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 偏差修正</span></span>
<span id="cb6-673"><a href="#cb6-673" aria-hidden="true" tabindex="-1"></a>                m_hat <span class="op">=</span> <span class="va">self</span>.m[i] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta1<span class="op">**</span><span class="va">self</span>.t)</span>
<span id="cb6-674"><a href="#cb6-674" aria-hidden="true" tabindex="-1"></a>                v_hat <span class="op">=</span> <span class="va">self</span>.v[i] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.beta2<span class="op">**</span><span class="va">self</span>.t)</span>
<span id="cb6-675"><a href="#cb6-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-676"><a href="#cb6-676" aria-hidden="true" tabindex="-1"></a>                <span class="co"># AdamW 风格：解耦权重衰减</span></span>
<span id="cb6-677"><a href="#cb6-677" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.weight_decay <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-678"><a href="#cb6-678" aria-hidden="true" tabindex="-1"></a>                    p.data <span class="op">*=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> <span class="va">self</span>.weight_decay)</span>
<span id="cb6-679"><a href="#cb6-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-680"><a href="#cb6-680" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 参数更新</span></span>
<span id="cb6-681"><a href="#cb6-681" aria-hidden="true" tabindex="-1"></a>                p.data <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> m_hat <span class="op">/</span> (torch.sqrt(v_hat) <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb6-682"><a href="#cb6-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-683"><a href="#cb6-683" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb6-684"><a href="#cb6-684" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb6-685"><a href="#cb6-685" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-686"><a href="#cb6-686" aria-hidden="true" tabindex="-1"></a>                p.grad.zero_()</span>
<span id="cb6-687"><a href="#cb6-687" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-688"><a href="#cb6-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-689"><a href="#cb6-689" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复现论文的关键细节</span></span>
<span id="cb6-690"><a href="#cb6-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-691"><a href="#cb6-691" aria-hidden="true" tabindex="-1"></a>在复现大模型训练论文时，以下细节经常在论文中被省略或一笔带过，但对复现结果至关重要：</span>
<span id="cb6-692"><a href="#cb6-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-693"><a href="#cb6-693" aria-hidden="true" tabindex="-1"></a>**$\beta_2$ 的选择**：默认的 $\beta_2 = 0.999$ 在大模型训练中可能过于保守，导致二阶矩估计对最近的梯度变化反应太慢。LLaMA 使用 $\beta_2 = 0.95$，PaLM 使用 $\beta_2 = 0.99$。更小的 $\beta_2$ 让优化器更"敏锐"，但也更容易受噪声影响。</span>
<span id="cb6-694"><a href="#cb6-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-695"><a href="#cb6-695" aria-hidden="true" tabindex="-1"></a>**权重衰减不作用于某些参数**：通常，bias 参数、LayerNorm 的 scale 和 shift 参数**不应用权重衰减**。这是因为这些参数的量级本身就不大，权重衰减会把它们压得过小，反而损害表达能力。</span>
<span id="cb6-696"><a href="#cb6-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-697"><a href="#cb6-697" aria-hidden="true" tabindex="-1"></a>**梯度累积的数值问题**：当使用梯度累积（gradient accumulation）来模拟大 batch 时，需要在累积完所有 micro-batch 后再做梯度裁剪和优化器更新——而不是在每个 micro-batch 后都做。这个顺序错误是常见的 bug，会导致有效学习率不一致。</span>
<span id="cb6-698"><a href="#cb6-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-699"><a href="#cb6-699" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-700"><a href="#cb6-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-701"><a href="#cb6-701" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb6-702"><a href="#cb6-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-703"><a href="#cb6-703" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么 Adam 特别适合 Transformer？</span></span>
<span id="cb6-704"><a href="#cb6-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-705"><a href="#cb6-705" aria-hidden="true" tabindex="-1"></a>Adam 之所以成为 Transformer 训练的事实标准，不仅仅是因为它"效果好"——背后有深层的原因。</span>
<span id="cb6-706"><a href="#cb6-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-707"><a href="#cb6-707" aria-hidden="true" tabindex="-1"></a>Transformer 架构中的 attention 层产生的梯度具有**高度稀疏性**。在 self-attention 中，每个 token 只"关注"序列中少数几个位置（即使 softmax 使所有位置都有非零权重，大部分权重实际上非常接近 0）。这导致 attention 权重矩阵的梯度在不同位置的量级差异极大。SGD 使用统一的学习率，要么对大梯度位置步子太大，要么对小梯度位置步子太小。Adam 的按参数自适应恰好解决了这个问题。</span>
<span id="cb6-708"><a href="#cb6-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-709"><a href="#cb6-709" aria-hidden="true" tabindex="-1"></a>此外，Transformer 中不同类型的参数——词嵌入、positional encoding、Q/K/V 投影、FFN 权重、LayerNorm 参数——它们的梯度尺度可以相差好几个数量级。Adam 的二阶矩自动为每种参数"校准"了合适的步长，而 SGD + 动量做不到这一点。</span>
<span id="cb6-710"><a href="#cb6-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-711"><a href="#cb6-711" aria-hidden="true" tabindex="-1"></a><span class="fu">### µP：超参数如何跨规模迁移</span></span>
<span id="cb6-712"><a href="#cb6-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-713"><a href="#cb6-713" aria-hidden="true" tabindex="-1"></a>训练大模型最昂贵的环节之一是**超参数调优**（hyperparameter tuning）。对于一个 7B 参数的模型，每次完整训练可能花费数十万美元。如果需要搜索 10 组超参数，成本就是数百万美元。能不能在小模型上调好超参数，然后**直接迁移到大模型**？</span>
<span id="cb6-714"><a href="#cb6-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-715"><a href="#cb6-715" aria-hidden="true" tabindex="-1"></a>在标准参数化（Standard Parameterization, SP）下，答案是否定的。Yang 等人（2022）证明，在 SP 下，最优学习率与模型宽度成反比——宽度翻倍，最优学习率就要减半。这意味着在 128 维宽度上找到的最优学习率，到 4096 维宽度时已经完全不适用了。</span>
<span id="cb6-716"><a href="#cb6-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-717"><a href="#cb6-717" aria-hidden="true" tabindex="-1"></a>**最大更新参数化**（Maximal Update Parameterization, µP）通过调整初始化尺度和学习率的缩放方式，使得每一层接收到的更新幅度**与模型宽度无关**。这样，最优学习率在不同宽度的模型之间保持**恒定**。</span>
<span id="cb6-718"><a href="#cb6-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-719"><a href="#cb6-719" aria-hidden="true" tabindex="-1"></a>µTransfer 的实际流程是：</span>
<span id="cb6-720"><a href="#cb6-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-721"><a href="#cb6-721" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>将目标模型用 µP 参数化</span>
<span id="cb6-722"><a href="#cb6-722" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>在一个小型代理模型（如宽度 256）上搜索最优超参数</span>
<span id="cb6-723"><a href="#cb6-723" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>将这些超参数直接迁移到全尺寸模型（如宽度 4096+），无需任何额外调优</span>
<span id="cb6-724"><a href="#cb6-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-725"><a href="#cb6-725" aria-hidden="true" tabindex="-1"></a>实证结果令人印象深刻：Yang 等人将超参数从 40M 参数的模型迁移到 GPT-3 6.7B（168 倍大），结果超过了 GPT-3 6.7B 的已发布性能，而超参数搜索的成本仅为完整预训练的 **7%**。</span>
<span id="cb6-726"><a href="#cb6-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-727"><a href="#cb6-727" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb6-728"><a href="#cb6-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-729"><a href="#cb6-729" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Loss Spike 的不可预测性</span></span>
<span id="cb6-730"><a href="#cb6-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-731"><a href="#cb6-731" aria-hidden="true" tabindex="-1"></a>目前所有的训练稳定性技术本质上都是**被动的**——它们在问题发生后做出响应（重启、裁剪、缩放），而不是在问题发生前预防。PaLM 的分析表明，loss spike 来自特定数据与特定参数状态的混沌交互，这意味着在理论上可能无法提前预测。</span>
<span id="cb6-732"><a href="#cb6-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-733"><a href="#cb6-733" aria-hidden="true" tabindex="-1"></a>Wortsman 等人（2023）的后续工作发现了一个有趣的现象：大规模模型的训练不稳定性可以在**小模型上用高学习率复现**。他们识别出两种关键的不稳定模式——attention logit 增长（导致 softmax 熵崩溃）和 output logit 发散——并证明 **qk-layernorm** 和 **z-loss 正则化** 等缓解措施在大小模型上都有效。这为"先在小规模上调试稳定性"提供了理论依据。</span>
<span id="cb6-734"><a href="#cb6-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-735"><a href="#cb6-735" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pre-LN vs Post-LN 与 Warmup 的关系</span></span>
<span id="cb6-736"><a href="#cb6-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-737"><a href="#cb6-737" aria-hidden="true" tabindex="-1"></a>原始 Transformer 采用 Post-LN（LayerNorm 在残差连接之后），后来的研究发现 Pre-LN（LayerNorm 在 attention/FFN 之前）更稳定。这背后的原因与 warmup 有关：Post-LN 结构中，梯度到达残差路径之前要经过 LayerNorm 的雅可比矩阵，这可能扭曲梯度方向。Pre-LN 保证了一条"干净"的残差路径，使梯度可以不经修改地直接回传。</span>
<span id="cb6-738"><a href="#cb6-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-739"><a href="#cb6-739" aria-hidden="true" tabindex="-1"></a>实际影响是：Pre-LN 结构通常**不需要 warmup**（或只需要很少的 warmup），而 Post-LN 结构必须使用 warmup 否则训练会发散。这也是为什么现代 LLM 几乎全部采用 Pre-LN 的原因之一。</span>
<span id="cb6-740"><a href="#cb6-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-741"><a href="#cb6-741" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 隐含假设与失效条件</span></span>
<span id="cb6-742"><a href="#cb6-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-743"><a href="#cb6-743" aria-hidden="true" tabindex="-1"></a>本章讨论的技术栈假设了几个条件：</span>
<span id="cb6-744"><a href="#cb6-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-745"><a href="#cb6-745" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**数据质量良好**：如果训练数据包含大量重复、损坏或极端分布的样本，再好的优化器也无法稳定训练。数据清洗和去重是稳定训练的前提。</span>
<span id="cb6-746"><a href="#cb6-746" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**模型架构合理**：某些架构选择（如极深的网络没有残差连接、attention 没有缩放）会使训练本质上不稳定，优化器层面的修补只能治标。</span>
<span id="cb6-747"><a href="#cb6-747" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**硬件正常运行**：大规模训练涉及数百甚至数千张 GPU，硬件故障（GPU 报错、网络抖动、NVLink 断开）是家常便饭。OPT-175B 训练中平均每天更换几台机器——这种级别的硬件不稳定需要系统层面的容错机制，超出了优化算法的范畴。</span>
<span id="cb6-748"><a href="#cb6-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-749"><a href="#cb6-749" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-750"><a href="#cb6-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-751"><a href="#cb6-751" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb6-752"><a href="#cb6-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-753"><a href="#cb6-753" aria-hidden="true" tabindex="-1"></a><span class="fu">### 被动而非预测</span></span>
<span id="cb6-754"><a href="#cb6-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-755"><a href="#cb6-755" aria-hidden="true" tabindex="-1"></a>当前的训练稳定性方法本质上是一套"保险措施"——warmup 防止早期崩溃、梯度裁剪防止梯度爆炸、BF16 防止数值溢出、checkpoint 重启防止 loss spike 后全部前功尽弃。但这些措施都不能**预测**问题何时发生，也不能**根除**问题的根源。我们缺少一个关于"训练何时以及为什么会变得不稳定"的完整理论。</span>
<span id="cb6-756"><a href="#cb6-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-757"><a href="#cb6-757" aria-hidden="true" tabindex="-1"></a><span class="fu">### 超参数仍然依赖经验</span></span>
<span id="cb6-758"><a href="#cb6-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-759"><a href="#cb6-759" aria-hidden="true" tabindex="-1"></a>尽管 µP 提供了一种跨规模迁移超参数的方法，但"第一组超参数"仍然需要在小模型上搜索。warmup 步数选 2000 而不是 1000 或 5000，梯度裁剪阈值选 1.0 而不是 0.5 或 2.0——这些选择在很大程度上仍然是经验性的，缺乏从第一性原理推导出的最优值。</span>
<span id="cb6-760"><a href="#cb6-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-761"><a href="#cb6-761" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从单卡到多卡：新的挑战</span></span>
<span id="cb6-762"><a href="#cb6-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-763"><a href="#cb6-763" aria-hidden="true" tabindex="-1"></a>即使在单张 GPU 上完美解决了训练稳定性，我们仍然面临一个残酷的现实：一个 70B 参数的模型（FP32 需要 280GB，即使 BF16 也需要 140GB），加上 Adam 的优化器状态（额外 560GB FP32），总共需要 840GB 显存——**没有任何单张 GPU 能装得下**。A100 有 80GB，H100 有 80GB，即使是最新的硬件也差了一个数量级。</span>
<span id="cb6-764"><a href="#cb6-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-765"><a href="#cb6-765" aria-hidden="true" tabindex="-1"></a>这就引出了下一章的核心问题：**如何将训练分布到多张 GPU、多台机器上？** 数据并行、模型并行、流水线并行、ZeRO——这些技术如何协同工作来解决"装不下"的问题，同时保持训练的稳定性和效率？</span>
<span id="cb6-766"><a href="#cb6-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-767"><a href="#cb6-767" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 本章解决了"如何在一张 GPU 上稳定训练"；下一章将解决"如何在一千张 GPU 上高效训练"。</span></span>
<span id="cb6-768"><a href="#cb6-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-769"><a href="#cb6-769" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-770"><a href="#cb6-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-771"><a href="#cb6-771" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb6-772"><a href="#cb6-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-773"><a href="#cb6-773" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb6-774"><a href="#cb6-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-775"><a href="#cb6-775" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：大模型训练面临损失函数景观崎岖、数值精度放大、训练时间累积风险三重挑战，PaLM 和 OPT 的训练经历生动说明了这些挑战的严重性。</span>
<span id="cb6-776"><a href="#cb6-776" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**优化器演进**：从 SGD 到 Adam（自适应学习率）→ AdamW（修正权重衰减）→ Adafactor（亚线性显存）→ Lion（符号更新，进化搜索发现），每一步都解决了前一步的具体痛点。</span>
<span id="cb6-777"><a href="#cb6-777" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**学习率策略**：线性 warmup（让 Adam 的二阶矩估计稳定）+ 余弦衰减（从探索到收敛的平滑过渡）是 LLM 训练的标准配方。</span>
<span id="cb6-778"><a href="#cb6-778" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**混合精度**：BF16 用与 FP32 相同的动态范围换取一半的显存和更高的吞吐量，避免了 FP16 的梯度下溢和 loss scaling 的复杂性。</span>
<span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**稳定性诊断**：梯度范数监控、loss spike 检测与 checkpoint 重启是必备的工程实践。</span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Adam 更新**: $\theta_t = \theta_{t-1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \varepsilon)$</span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AdamW 权重衰减**: $\theta_t = (1-\lambda)\theta_{t-1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \varepsilon)$</span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**偏差修正**: $\hat{m}_t = m_t / (1 - \beta_1^t)$，$\hat{v}_t = v_t / (1 - \beta_2^t)$</span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**余弦退火**: $\eta(t) = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi t / T))$</span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**梯度裁剪**: $\mathbf{g} \leftarrow \min(1, \theta / <span class="sc">\|</span>\mathbf{g}<span class="sc">\|</span>_2) \cdot \mathbf{g}$</span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**计算预算约束**: $C \approx 6ND$</span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么 AdamW 中权重衰减必须与自适应梯度缩放解耦？如果不解耦，对于梯度稳定很大的参数和梯度稳定很小的参数，正则化效果分别会怎样？</span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明 Adam 在第 $t$ 步的一阶矩期望为 $\mathbb{E}<span class="co">[</span><span class="ot">m_t</span><span class="co">]</span> = (1 - \beta_1^t) \cdot \mathbb{E}<span class="co">[</span><span class="ot">g</span><span class="co">]</span>$，其中 $g$ 是梯度的真实期望。（提示：将递推式展开。）</span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 假设你正在训练一个 13B 参数的模型，训练到第 50,000 步时 loss 突然飙升。请设计一个诊断和恢复流程：你会检查哪些指标？如何选择重启点？如何防止 spike 再次发生？</span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** Lion 优化器通过进化搜索发现，其更新规则出人意料地简单（只用 sign）。这是否意味着手工设计的优化器已经"够好了"，还是说搜索空间可能包含尚未发现的更好方案？如果你要设计一个优化器搜索实验，你会如何定义搜索空间和评估指标？</span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-806"><a href="#cb6-806" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kingma &amp; Ba (2014)** "Adam: A Method for Stochastic Optimization" — Adam 优化器的原始论文</span>
<span id="cb6-807"><a href="#cb6-807" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Algorithm 1（完整伪代码）、Section 3（偏差修正推导）</span>
<span id="cb6-808"><a href="#cb6-808" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>可跳过：Section 6（AdaMax 变体）</span>
<span id="cb6-809"><a href="#cb6-809" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Loshchilov &amp; Hutter (2017)** "Decoupled Weight Decay Regularization" — AdamW 论文</span>
<span id="cb6-810"><a href="#cb6-810" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2（L2 ≠ 权重衰减的分析）、Figure 2（超参数可分离性）</span>
<span id="cb6-811"><a href="#cb6-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-812"><a href="#cb6-812" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb6-813"><a href="#cb6-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-814"><a href="#cb6-814" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liu et al. (2019)** "On the Variance of the Adaptive Learning Rate and Beyond" — RAdam，对 warmup 必要性的理论解释</span>
<span id="cb6-815"><a href="#cb6-815" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Yang et al. (2022)** "Tensor Programs V" — µP / µTransfer，超参数跨规模迁移</span>
<span id="cb6-816"><a href="#cb6-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-817"><a href="#cb6-817" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练实战报告</span></span>
<span id="cb6-818"><a href="#cb6-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-819"><a href="#cb6-819" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chowdhery et al. (2022)** "PaLM" — Section 5.1 是关于训练不稳定性的最详细公开分析</span>
<span id="cb6-820"><a href="#cb6-820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zhang et al. (2022)** "OPT" — OPT-175B 训练日志是大模型训练工程的必读材料</span>
<span id="cb6-821"><a href="#cb6-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-822"><a href="#cb6-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb6-823"><a href="#cb6-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-824"><a href="#cb6-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chen et al. (2023)** "Symbolic Discovery of Optimization Algorithms" — Lion 优化器，进化搜索发现</span>
<span id="cb6-825"><a href="#cb6-825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wortsman et al. (2023)** "Small-scale proxies for large-scale Transformer training instabilities" — 在小模型上复现大模型的不稳定性（ICLR 2024 Oral）</span>
<span id="cb6-826"><a href="#cb6-826" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dettmers et al. (2021)** "8-bit Optimizers" — 量化优化器状态，<span class="in">`bitsandbytes`</span> 库</span>
<span id="cb6-827"><a href="#cb6-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-828"><a href="#cb6-828" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb6-829"><a href="#cb6-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-830"><a href="#cb6-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L Chapter 12** — 优化算法的系统教学，从 SGD 到 Adam</span>
<span id="cb6-831"><a href="#cb6-831" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**NVIDIA Mixed Precision Training Guide** — 混合精度训练的工程实践指南</span>
<span id="cb6-832"><a href="#cb6-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-833"><a href="#cb6-833" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-834"><a href="#cb6-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-835"><a href="#cb6-835" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb6-836"><a href="#cb6-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-837"><a href="#cb6-837" aria-hidden="true" tabindex="-1"></a>Adam 优化器论文是深度学习领域引用量最高的论文之一（截至 2025 年超过 20 万次引用），但它最初提交到 ICLR 2015 时只获得了"接受"而非"最佳论文"。事后看来，这篇论文可能是整个会议历史上影响力最大的工作。</span>
<span id="cb6-838"><a href="#cb6-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-839"><a href="#cb6-839" aria-hidden="true" tabindex="-1"></a>AdamW 的故事同样值得玩味。Loshchilov 和 Hutter 在 2017 年发现了 Adam 中 L2 正则化和权重衰减不等价的问题，但直到 2019 年这篇论文才在 ICLR 上正式发表。在此期间，整个社区都在使用"错误的" Adam + L2 组合训练模型，而且似乎也训练出了不错的结果（包括 BERT）。这提醒我们：深度学习中"能用"和"最优"之间可能存在巨大的差距，而我们常常意识不到自己正在使用次优的方法。</span>
<span id="cb6-840"><a href="#cb6-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-841"><a href="#cb6-841" aria-hidden="true" tabindex="-1"></a>OPT-175B 的训练日志是另一个历史性时刻。在此之前，大模型训练的困难和混乱都隐藏在科技公司的内部文档中。Meta 选择公开这份长达 100 页的原始日志——包括所有的错误、困惑、临时修补和凌晨三点的紧急干预——让整个社区第一次看到了"训练大模型到底是什么样的"。这份日志被很多研究者称为"大模型训练的《Surely You're Joking, Mr. Feynman》"——一部充满人性的工程冒险记录。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>