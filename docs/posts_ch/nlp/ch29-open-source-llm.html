<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="长期以来，最强大的语言模型一直是闭源的——GPT-4、Claude、Gemini的权重都不对外开放。研究者只能通过API窥见其能力，却无法复现、研究或改进。2023年2月，Meta发布LLaMA，打破了这一格局。从7B到65B参数，从LLaMA到Llama 3，从单一厂商到Mistral、Qwen、DeepSeek的多极化竞争——开源LLM不仅追上了闭源模型的性能，更构建了一个繁荣的研究与应用生态。本章将带你回顾这场’开源革命’的技术演进与生态变迁。">

<title>第29章：开源大模型的演进 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#为什么需要开源大模型" id="toc-为什么需要开源大模型" class="nav-link" data-scroll-target="#为什么需要开源大模型"><span class="header-section-number">2.1</span> 为什么需要开源大模型？</a></li>
  <li><a href="#开源模型面临的技术挑战" id="toc-开源模型面临的技术挑战" class="nav-link" data-scroll-target="#开源模型面临的技术挑战"><span class="header-section-number">2.2</span> 开源模型面临的技术挑战</a></li>
  </ul></li>
  <li><a href="#llama开源的里程碑" id="toc-llama开源的里程碑" class="nav-link" data-scroll-target="#llama开源的里程碑"><span class="header-section-number">3</span> LLaMA：开源的里程碑</a>
  <ul class="collapse">
  <li><a href="#发布背景" id="toc-发布背景" class="nav-link" data-scroll-target="#发布背景"><span class="header-section-number">3.1</span> 发布背景</a></li>
  <li><a href="#核心架构创新" id="toc-核心架构创新" class="nav-link" data-scroll-target="#核心架构创新"><span class="header-section-number">3.2</span> 核心架构创新</a></li>
  <li><a href="#llama的影响" id="toc-llama的影响" class="nav-link" data-scroll-target="#llama的影响"><span class="header-section-number">3.3</span> LLaMA的影响</a></li>
  </ul></li>
  <li><a href="#llama-2和llama-3规模化演进" id="toc-llama-2和llama-3规模化演进" class="nav-link" data-scroll-target="#llama-2和llama-3规模化演进"><span class="header-section-number">4</span> Llama 2和Llama 3：规模化演进</a>
  <ul class="collapse">
  <li><a href="#llama-2完全开源的转折点" id="toc-llama-2完全开源的转折点" class="nav-link" data-scroll-target="#llama-2完全开源的转折点"><span class="header-section-number">4.1</span> Llama 2：完全开源的转折点</a></li>
  <li><a href="#llama-3追平gpt-4" id="toc-llama-3追平gpt-4" class="nav-link" data-scroll-target="#llama-3追平gpt-4"><span class="header-section-number">4.2</span> Llama 3：追平GPT-4</a></li>
  <li><a href="#llama系列的技术演进总结" id="toc-llama系列的技术演进总结" class="nav-link" data-scroll-target="#llama系列的技术演进总结"><span class="header-section-number">4.3</span> Llama系列的技术演进总结</a></li>
  </ul></li>
  <li><a href="#mistral系列效率优先" id="toc-mistral系列效率优先" class="nav-link" data-scroll-target="#mistral系列效率优先"><span class="header-section-number">5</span> Mistral系列：效率优先</a>
  <ul class="collapse">
  <li><a href="#背景来自deepmind的创业" id="toc-背景来自deepmind的创业" class="nav-link" data-scroll-target="#背景来自deepmind的创业"><span class="header-section-number">5.1</span> 背景：来自DeepMind的创业</a></li>
  <li><a href="#mistral-7b小模型的逆袭" id="toc-mistral-7b小模型的逆袭" class="nav-link" data-scroll-target="#mistral-7b小模型的逆袭"><span class="header-section-number">5.2</span> Mistral 7B：小模型的逆袭</a></li>
  <li><a href="#mixtral-8x7b开源moe的标杆" id="toc-mixtral-8x7b开源moe的标杆" class="nav-link" data-scroll-target="#mixtral-8x7b开源moe的标杆"><span class="header-section-number">5.3</span> Mixtral 8x7B：开源MoE的标杆</a></li>
  <li><a href="#mistral的设计哲学" id="toc-mistral的设计哲学" class="nav-link" data-scroll-target="#mistral的设计哲学"><span class="header-section-number">5.4</span> Mistral的设计哲学</a></li>
  </ul></li>
  <li><a href="#多极化发展qwendeepseek与更多" id="toc-多极化发展qwendeepseek与更多" class="nav-link" data-scroll-target="#多极化发展qwendeepseek与更多"><span class="header-section-number">6</span> 多极化发展：Qwen、DeepSeek与更多</a>
  <ul class="collapse">
  <li><a href="#全球竞争格局" id="toc-全球竞争格局" class="nav-link" data-scroll-target="#全球竞争格局"><span class="header-section-number">6.1</span> 全球竞争格局</a></li>
  <li><a href="#qwen系列" id="toc-qwen系列" class="nav-link" data-scroll-target="#qwen系列"><span class="header-section-number">6.2</span> Qwen系列</a></li>
  <li><a href="#deepseek系列" id="toc-deepseek系列" class="nav-link" data-scroll-target="#deepseek系列"><span class="header-section-number">6.3</span> DeepSeek系列</a></li>
  </ul></li>
  <li><a href="#开源-vs-闭源的博弈" id="toc-开源-vs-闭源的博弈" class="nav-link" data-scroll-target="#开源-vs-闭源的博弈"><span class="header-section-number">7</span> 开源 vs 闭源的博弈</a>
  <ul class="collapse">
  <li><a href="#核心权衡" id="toc-核心权衡" class="nav-link" data-scroll-target="#核心权衡"><span class="header-section-number">7.1</span> 核心权衡</a></li>
  <li><a href="#趋势差距正在缩小" id="toc-趋势差距正在缩小" class="nav-link" data-scroll-target="#趋势差距正在缩小"><span class="header-section-number">7.2</span> 趋势：差距正在缩小</a></li>
  <li><a href="#混合策略" id="toc-混合策略" class="nav-link" data-scroll-target="#混合策略"><span class="header-section-number">7.3</span> 混合策略</a></li>
  </ul></li>
  <li><a href="#工程实践本地部署开源模型" id="toc-工程实践本地部署开源模型" class="nav-link" data-scroll-target="#工程实践本地部署开源模型"><span class="header-section-number">8</span> 工程实践：本地部署开源模型</a>
  <ul class="collapse">
  <li><a href="#选择合适的模型" id="toc-选择合适的模型" class="nav-link" data-scroll-target="#选择合适的模型"><span class="header-section-number">8.1</span> 选择合适的模型</a></li>
  <li><a href="#量化让大模型跑在小设备上" id="toc-量化让大模型跑在小设备上" class="nav-link" data-scroll-target="#量化让大模型跑在小设备上"><span class="header-section-number">8.2</span> 量化：让大模型跑在小设备上</a></li>
  <li><a href="#推理框架选择" id="toc-推理框架选择" class="nav-link" data-scroll-target="#推理框架选择"><span class="header-section-number">8.3</span> 推理框架选择</a></li>
  <li><a href="#快速开始示例" id="toc-快速开始示例" class="nav-link" data-scroll-target="#快速开始示例"><span class="header-section-number">8.4</span> 快速开始示例</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">9</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么开源模型能追上闭源" id="toc-为什么开源模型能追上闭源" class="nav-link" data-scroll-target="#为什么开源模型能追上闭源"><span class="header-section-number">9.1</span> 为什么开源模型能追上闭源？</a></li>
  <li><a href="#开放问题" id="toc-开放问题" class="nav-link" data-scroll-target="#开放问题"><span class="header-section-number">9.2</span> 开放问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">10</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#开源模型的局限" id="toc-开源模型的局限" class="nav-link" data-scroll-target="#开源模型的局限"><span class="header-section-number">10.1</span> 开源模型的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">10.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">11</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">11.1</span> 核心要点回顾</a></li>
  <li><a href="#关键技术速查" id="toc-关键技术速查" class="nav-link" data-scroll-target="#关键技术速查"><span class="header-section-number">11.2</span> 关键技术速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">11.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">12</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">12.1</span> 核心论文（必读）</a></li>
  <li><a href="#效率导向" id="toc-效率导向" class="nav-link" data-scroll-target="#效率导向"><span class="header-section-number">12.2</span> 效率导向</a></li>
  <li><a href="#综述与分析" id="toc-综述与分析" class="nav-link" data-scroll-target="#综述与分析"><span class="header-section-number">12.3</span> 综述与分析</a></li>
  <li><a href="#工具资源" id="toc-工具资源" class="nav-link" data-scroll-target="#工具资源"><span class="header-section-number">12.4</span> 工具资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">13</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第29章：开源大模型的演进</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">From LLaMA to the Open Frontier: The Democratization of Large Language Models</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">开源模型</div>
    <div class="quarto-category">LLaMA</div>
    <div class="quarto-category">Mistral</div>
    <div class="quarto-category">Qwen</div>
    <div class="quarto-category">DeepSeek</div>
  </div>
  </div>

<div>
  <div class="description">
    长期以来，最强大的语言模型一直是闭源的——GPT-4、Claude、Gemini的权重都不对外开放。研究者只能通过API窥见其能力，却无法复现、研究或改进。2023年2月，Meta发布LLaMA，打破了这一格局。从7B到65B参数，从LLaMA到Llama 3，从单一厂商到Mistral、Qwen、DeepSeek的多极化竞争——开源LLM不仅追上了闭源模型的性能，更构建了一个繁荣的研究与应用生态。本章将带你回顾这场’开源革命’的技术演进与生态变迁。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：开源大模型如何在技术上追赶甚至超越闭源模型？不同的开源模型在架构设计上有何异同？开源生态对LLM研究与应用意味着什么？</p>
<p><strong>历史坐标</strong>：2023–2025 | LLaMA (2023.02) → Llama 2 (2023.07) → Mistral 7B (2023.10) → Mixtral (2024.01) → Llama 3 (2024.04) → DeepSeek-V3 (2024.12) | 开源LLM的崛起与多极化</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Touvron et al.&nbsp;(2023a)</strong> “LLaMA: Open and Efficient Foundation Language Models” (<a href="https://arxiv.org/abs/2302.13971">arXiv:2302.13971</a>) — 参考了 Section 2 (架构设计)、Table 2 (模型规模)；开源LLM的里程碑</li>
<li><strong>Touvron et al.&nbsp;(2023b)</strong> “Llama 2: Open Foundation and Fine-Tuned Chat Models” (<a href="https://arxiv.org/abs/2307.09288">arXiv:2307.09288</a>) — 参考了 Section 2-5 (架构改进、安全对齐)；首个完全开源的Chat模型</li>
<li><strong>Llama Team (2024)</strong> “The Llama 3 Herd of Models” (<a href="https://arxiv.org/abs/2407.21783">arXiv:2407.21783</a>) — 参考了 Section 3 (15T训练数据)、Section 4 (后训练流程)；开源模型达到GPT-4水平</li>
<li><strong>Jiang et al.&nbsp;(2023)</strong> “Mistral 7B” (<a href="https://arxiv.org/abs/2310.06825">arXiv:2310.06825</a>) — 参考了 Section 2 (Sliding Window Attention)；效率优先的设计哲学</li>
<li><strong>Jiang et al.&nbsp;(2024)</strong> “Mixtral of Experts” (<a href="https://arxiv.org/abs/2401.04088">arXiv:2401.04088</a>) — 参考了 Section 2 (Sparse MoE架构)；开源MoE的标杆</li>
<li><strong>DeepSeek-AI (2024)</strong> “DeepSeek-V3 Technical Report” (<a href="https://arxiv.org/abs/2412.19437">arXiv:2412.19437</a>) — 参考了 Section 2-4 (MLA、DeepSeekMoE)；训练效率的极致优化</li>
</ul>
</section>
<section id="教材与博客" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材与博客"><span class="header-section-number">0.2</span> 教材与博客</h3>
<ul>
<li><strong>Cameron R. Wolfe</strong> “LLaMA-2 from the Ground Up” — 参考了LLaMA架构细节的解释</li>
<li><strong>Sebastian Raschka</strong> “The State of LLMs 2025” — 参考了开源模型生态的梳理</li>
</ul>
</section>
<section id="综述" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="综述"><span class="header-section-number">0.3</span> 综述</h3>
<ul>
<li><strong>arXiv:2510.12178</strong> “Evolution of Meta’s LLaMA Models: A Survey” — 参考了LLaMA系列演进脉络</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们探讨了状态空间模型（SSM）——一条不同于Transformer的序列建模路径。Mamba证明了<span class="math inline">\(O(n)\)</span>复杂度的模型可以在语言建模上与Transformer竞争，而Jamba等混合架构则展示了结合两种范式的可能性。</p>
<p>这些架构创新回答了一个技术问题：<strong>如何更高效地建模序列？</strong></p>
<p>然而，还有一个更实际的问题困扰着研究社区：<strong>即使知道了最先进的架构，普通研究者也无法真正实验它们。</strong></p>
<p>原因很简单：最强大的模型都是闭源的。</p>
<p>2022年底，ChatGPT横空出世，展示了语言模型的惊人能力。但OpenAI没有公开GPT-3.5或GPT-4的权重——研究者只能通过API调用，无法查看模型内部、无法复现实验、无法在其基础上改进。Claude、Gemini同样如此。这意味着：</p>
<ol type="1">
<li><strong>研究受限</strong>：无法进行消融实验、可解释性分析、安全性研究</li>
<li><strong>创新受限</strong>：无法基于现有模型进行微调或架构改进</li>
<li><strong>教育受限</strong>：学生只能学习过时的小模型，无法接触前沿</li>
<li><strong>应用受限</strong>：企业必须依赖API，面临成本、隐私、稳定性风险</li>
</ol>
<p>一个悖论出现了：<strong>深度学习是开放的学术传统催生的，但最先进的成果却被少数公司垄断。</strong></p>
<p>2023年2月，一切开始改变。</p>
<p>Meta（原Facebook）发布了LLaMA（Large Language Model Meta AI），一个从7B到65B参数的开源大语言模型家族。虽然最初的发布有使用限制，但权重文件很快在网上流传。研究者第一次可以在自己的机器上运行、分析、修改一个真正的大规模语言模型。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：LLaMA的发布不仅仅是一个模型的开源，而是开启了一个时代。它证明了开源模型可以达到商业闭源模型的水平，并催生了一个繁荣的开源生态——Alpaca、Vicuna证明了廉价微调的可行性；Mistral展示了小模型的效率潜力；Llama 3在多项基准上追平GPT-4。从”只有大公司才能做LLM”到”每个研究者都能实验LLM”，这是一场真正的民主化革命。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="为什么需要开源大模型" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="为什么需要开源大模型"><span class="header-section-number">2.1</span> 为什么需要开源大模型？</h3>
<p>在讨论具体技术之前，让我们先理解开源大模型解决的核心问题。</p>
<p><strong>科学可复现性的危机</strong></p>
<p>深度学习的一个核心原则是可复现性——论文中的实验应该能够被独立验证。但当GPT-4发表时，论文中几乎没有架构细节、训练数据描述或超参数设置。这不是”技术报告”，而是一篇”能力展示”。</p>
<p>没有可复现性，就没有真正的科学进步。我们只能相信OpenAI声称的性能数字，却无法验证它们是否在所有分布上都成立、是否有数据污染、是否有隐藏的故障模式。</p>
<p><strong>研究创新的瓶颈</strong></p>
<p>假设你有一个关于注意力机制的新想法，想验证它是否能提升大模型的推理能力。如果没有开源基础模型，你需要：</p>
<ol type="1">
<li>从头训练一个大模型（需要数百万美元的计算资源）</li>
<li>或者用小模型验证（但结果可能不能推广到大规模）</li>
<li>或者放弃这个想法</li>
</ol>
<p>开源模型改变了这个局面：你可以直接在Llama 3的基础上测试你的改进，只需消耗微调级别的计算资源。</p>
<p><strong>应用场景的限制</strong></p>
<p>对于企业应用，依赖闭源API存在多重风险：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>风险类型</th>
<th>具体问题</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>成本</strong></td>
<td>API调用按token计费，大规模应用成本高昂</td>
</tr>
<tr class="even">
<td><strong>隐私</strong></td>
<td>数据必须发送到第三方服务器，合规风险</td>
</tr>
<tr class="odd">
<td><strong>延迟</strong></td>
<td>网络往返增加响应时间</td>
</tr>
<tr class="even">
<td><strong>可用性</strong></td>
<td>服务可能中断、限流或下线</td>
</tr>
<tr class="odd">
<td><strong>定制</strong></td>
<td>无法针对特定领域进行深度微调</td>
</tr>
</tbody>
</table>
<p>开源模型可以本地部署，完全规避这些问题。</p>
</section>
<section id="开源模型面临的技术挑战" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="开源模型面临的技术挑战"><span class="header-section-number">2.2</span> 开源模型面临的技术挑战</h3>
<p>开源并不意味着”简单”。要构建一个有竞争力的开源大模型，需要解决几个核心技术挑战：</p>
<p><strong>挑战一：训练数据</strong></p>
<p>闭源模型的一个主要优势是对海量高质量数据的访问。OpenAI、Google拥有巨大的数据收集能力。开源模型必须依赖公开可用的数据，同时确保数据质量和多样性。</p>
<p><strong>挑战二：计算资源</strong></p>
<p>训练一个65B参数的模型需要数千个GPU运行数周。这不是个人研究者能承担的。开源模型的训练通常由大公司（Meta、Mistral AI）或充足资金的研究机构完成。</p>
<p><strong>挑战三：工程优化</strong></p>
<p>大模型训练涉及分布式系统、混合精度、通信优化等复杂工程。缺乏这些专业知识，即使有数据和算力也难以成功。</p>
<p><strong>挑战四：安全对齐</strong></p>
<p>仅仅训练一个能力强的模型不够——它还需要是安全的、有帮助的、诚实的。闭源模型有专门的红队和安全团队；开源模型如何实现可靠的对齐？</p>
<p>接下来，我们将看到LLaMA及其后继者如何应对这些挑战。</p>
<hr>
</section>
</section>
<section id="llama开源的里程碑" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="llama开源的里程碑"><span class="header-section-number">3</span> LLaMA：开源的里程碑</h2>
<section id="发布背景" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="发布背景"><span class="header-section-number">3.1</span> 发布背景</h3>
<p>2023年2月24日，Meta AI发布了LLaMA论文”LLaMA: Open and Efficient Foundation Language Models”。这是一个从7B到65B参数的模型家族，在多项基准测试上超越了GPT-3（175B）。</p>
<p>论文的核心论点是：<strong>通过更长时间地在更多数据上训练，较小的模型可以达到更大模型的性能。</strong> 这与当时流行的”越大越好”的观念形成对比，也呼应了后来Chinchilla论文的发现。</p>
<p>LLaMA的训练数据规模令人印象深刻：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>参数量</th>
<th>训练Token数</th>
<th>训练数据倍数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LLaMA-7B</td>
<td>7B</td>
<td>1.0T</td>
<td>143x</td>
</tr>
<tr class="even">
<td>LLaMA-13B</td>
<td>13B</td>
<td>1.0T</td>
<td>77x</td>
</tr>
<tr class="odd">
<td>LLaMA-33B</td>
<td>33B</td>
<td>1.4T</td>
<td>42x</td>
</tr>
<tr class="even">
<td>LLaMA-65B</td>
<td>65B</td>
<td>1.4T</td>
<td>22x</td>
</tr>
</tbody>
</table>
<p>对比：Chinchilla-optimal建议是约20x（即20倍于参数量的token数）。LLaMA选择了更长的训练，这在推理时是”免费”的——推理成本取决于参数量，而非训练数据量。</p>
</section>
<section id="核心架构创新" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="核心架构创新"><span class="header-section-number">3.2</span> 核心架构创新</h3>
<p>LLaMA的架构基于Transformer Decoder，但采用了多项当时最先进的技术改进。这些选择后来成为了开源模型的”标准配置”。</p>
<section id="pre-normalization-with-rmsnorm" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="pre-normalization-with-rmsnorm"><span class="header-section-number">3.2.1</span> Pre-Normalization with RMSNorm</h4>
<p>原始Transformer使用Post-Layer Normalization：先计算子层（Attention或FFN），再做LayerNorm。LLaMA改用Pre-Normalization：先做Normalization，再计算子层。</p>
<p>Pre-Norm的优势在于训练稳定性。从梯度流动的角度分析：Post-Norm结构中，梯度必须经过归一化层才能到达残差路径；而Pre-Norm保证了一条”干净”的残差路径，梯度可以不受干扰地直接回传。</p>
<p>此外，LLaMA用RMSNorm替代了标准的LayerNorm：</p>
<p><span class="math display">\[
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
\]</span></p>
<p>对比LayerNorm：</p>
<p><span class="math display">\[
\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
\]</span></p>
<p>RMSNorm省略了均值中心化（<span class="math inline">\(x - \mu\)</span>）和偏置项（<span class="math inline">\(\beta\)</span>）。实验表明这种简化不影响性能，但可以减少7%–64%的计算时间。</p>
</section>
<section id="swiglu激活函数" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="swiglu激活函数"><span class="header-section-number">3.2.2</span> SwiGLU激活函数</h4>
<p>原始Transformer的FFN使用ReLU激活：</p>
<p><span class="math display">\[
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
\]</span></p>
<p>LLaMA采用SwiGLU，这是Swish激活函数和门控线性单元（GLU）的结合：</p>
<p><span class="math display">\[
\text{SwiGLU}(x) = \text{Swish}(xW_1) \otimes (xW_2)
\]</span></p>
<p>其中<span class="math inline">\(\text{Swish}(x) = x \cdot \sigma(x)\)</span>，<span class="math inline">\(\sigma\)</span>是sigmoid函数，<span class="math inline">\(\otimes\)</span>表示逐元素乘法。</p>
<p>SwiGLU需要三个权重矩阵（<span class="math inline">\(W_1\)</span>, <span class="math inline">\(W_2\)</span>, <span class="math inline">\(W_3\)</span>），而标准FFN只需要两个。为了保持参数量不变，LLaMA调整了隐藏层维度：从标准的<span class="math inline">\(4d\)</span>变为<span class="math inline">\(\frac{2}{3} \times 4d \times 3 = 8d/3\)</span>。</p>
<p>为什么SwiGLU更好？直观上，门控机制允许网络学习性地”开关”某些特征通道，而Swish的平滑性避免了ReLU的”死神经元”问题。实验表明，SwiGLU在保持计算量不变的情况下能提升模型性能。</p>
</section>
<section id="rope位置编码" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="rope位置编码"><span class="header-section-number">3.2.3</span> RoPE位置编码</h4>
<p>LLaMA采用旋转位置编码（Rotary Position Embedding, RoPE），这是一种将位置信息注入Query和Key的方法。</p>
<p>RoPE的核心思想是：对于位置<span class="math inline">\(m\)</span>的Query向量<span class="math inline">\(q\)</span>和位置<span class="math inline">\(n\)</span>的Key向量<span class="math inline">\(k\)</span>，它们的点积应该只依赖于相对位置<span class="math inline">\(m-n\)</span>：</p>
<p><span class="math display">\[
\langle R_m q, R_n k \rangle = \langle R_{m-n} q, k \rangle
\]</span></p>
<p>这通过旋转矩阵实现。对于二维向量：</p>
<p><span class="math display">\[
R_\theta = \begin{pmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix}
\]</span></p>
<p>高维向量被分成多个二维子空间，每个子空间独立旋转。</p>
<p>RoPE的优势包括： - <strong>相对位置编码</strong>：天然编码了相对距离 - <strong>外推潜力</strong>：通过适当的缩放技术，可以扩展到比训练时更长的序列 - <strong>计算高效</strong>：旋转可以用简单的三角函数实现</p>
<p><strong>性能表现</strong></p>
<p>LLaMA在发布时展示了令人印象深刻的性能——较小的模型通过更充分的训练可以匹配甚至超越更大的模型：</p>
<div id="fig-llama-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llama-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-29/original/fig3-llama-training-loss.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llama-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: LLaMA在常识推理任务上的Zero-shot性能。LLaMA-65B在多项任务上超越了GPT-3（175B）和PaLM（540B），证明了”训练更充分的小模型”策略的有效性。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Touvron et al.&nbsp;(2023) “LLaMA: Open and Efficient Foundation Language Models”, Table 3. <a href="https://arxiv.org/abs/2302.13971">arXiv:2302.13971</a></em></p>
</div>
</section>
</section>
<section id="llama的影响" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="llama的影响"><span class="header-section-number">3.3</span> LLaMA的影响</h3>
<p>LLaMA的权重虽然最初限制用于研究目的，但很快在网上泄露。这引发了一场”意外的民主化”——世界各地的研究者开始基于LLaMA进行实验。</p>
<p>几周内，社区涌现了大量衍生工作：</p>
<ul>
<li><strong>Alpaca</strong>（Stanford）：用GPT-4生成的52K指令数据微调LLaMA-7B，成本不到$500</li>
<li><strong>Vicuna</strong>：用ShareGPT对话数据微调，据称达到ChatGPT 90%的能力</li>
<li><strong>Koala</strong>：用对话数据微调，强调研究可复现性</li>
<li><strong>GPT4All</strong>：量化后可在CPU上运行的版本</li>
</ul>
<p>这些工作证明了一个重要的事实：<strong>有了好的基础模型，即使是小团队也能以很低的成本创造有价值的应用。</strong></p>
<hr>
</section>
</section>
<section id="llama-2和llama-3规模化演进" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="llama-2和llama-3规模化演进"><span class="header-section-number">4</span> Llama 2和Llama 3：规模化演进</h2>
<section id="llama-2完全开源的转折点" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="llama-2完全开源的转折点"><span class="header-section-number">4.1</span> Llama 2：完全开源的转折点</h3>
<p>2023年7月，Meta发布Llama 2，这次是完全开源的——包括商业使用许可。这消除了LLaMA 1法律地位的模糊性，让企业可以放心地基于它构建产品。</p>
<p>Llama 2的主要改进包括：</p>
<p><strong>更大的训练规模</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>改进</th>
<th>LLaMA 1</th>
<th>Llama 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>训练Token数</td>
<td>1.0–1.4T</td>
<td>2.0T</td>
</tr>
<tr class="even">
<td>上下文长度</td>
<td>2,048</td>
<td>4,096</td>
</tr>
<tr class="odd">
<td>模型规模</td>
<td>7B, 13B, 33B, 65B</td>
<td>7B, 13B, 70B</td>
</tr>
</tbody>
</table>
<p>训练数据量翻倍，上下文窗口翻倍。</p>
<p><strong>Grouped Query Attention (GQA)</strong></p>
<p>LLaMA 1使用标准的Multi-Head Attention (MHA)，其中每个注意力头有独立的Query、Key、Value投影。Llama 2的70B模型引入了Grouped Query Attention (GQA)：多个Query头共享同一组Key-Value头。</p>
<p>具体来说，Llama 2-70B有64个Query头，但只有8个Key-Value头组（每组对应8个Query头）。这大幅减少了KV Cache的大小，提升了推理效率：</p>
<p><span class="math display">\[
\text{KV Cache大小} = 2 \times n_{layers} \times n_{kv\_heads} \times d_{head} \times seq\_len
\]</span></p>
<p>GQA将<span class="math inline">\(n_{kv\_heads}\)</span>从64减少到8，KV Cache减少为原来的1/8。</p>
<p><strong>安全对齐</strong></p>
<p>Llama 2最重要的贡献之一是详细公开了安全对齐流程。论文描述了完整的RLHF管线：</p>
<ol type="1">
<li><strong>SFT阶段</strong>：用约27K高质量对话数据监督微调</li>
<li><strong>Reward Model</strong>：用超过1M的人类偏好对比数据训练奖励模型</li>
<li><strong>RLHF阶段</strong>：用PPO算法优化，同时训练两个奖励模型（有用性和安全性）</li>
<li><strong>Red Teaming</strong>：350+人的红队测试，覆盖多种攻击场景</li>
</ol>
<p>这种透明度对安全研究意义重大——研究者第一次能够详细了解一个生产级模型的对齐流程。</p>
</section>
<section id="llama-3追平gpt-4" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="llama-3追平gpt-4"><span class="header-section-number">4.2</span> Llama 3：追平GPT-4</h3>
<p>2024年4月，Meta发布Llama 3，标志着开源模型正式进入”GPT-4级别”。</p>
<p><strong>数据规模的飞跃</strong></p>
<p>Llama 3在超过15T tokens上训练——是Llama 2的7.5倍。更重要的是，这15T tokens经过了极其严格的质量过滤：</p>
<ol type="1">
<li><strong>启发式过滤</strong>：去除低质量网页</li>
<li><strong>NSFW过滤</strong>：使用分类器过滤不安全内容</li>
<li><strong>语义去重</strong>：使用MinHash去除近似重复</li>
<li><strong>数据混合</strong>：精心平衡不同领域（网页、代码、数学等）</li>
</ol>
<p>论文指出一个反直觉的发现：即使8B模型的Chinchilla-optimal训练量约为200B tokens，但继续训练到15T tokens仍然能持续提升性能。这说明传统的scaling law可能低估了过度训练的价值。</p>
<p><strong>架构微调</strong></p>
<p>Llama 3的架构与Llama 2基本相同，但做了几处微调：</p>
<ul>
<li><strong>词表扩大</strong>：从32K增加到128K，提升多语言和代码能力</li>
<li><strong>GQA全面采用</strong>：8B和70B模型都使用GQA</li>
<li><strong>上下文长度</strong>：从4K扩展到8K，后续版本扩展到128K</li>
</ul>
<p><strong>后训练流程</strong></p>
<p>Llama 3的后训练比Llama 2更加复杂：</p>
<ol type="1">
<li><strong>多轮SFT</strong>：不是一次微调，而是多轮迭代</li>
<li><strong>合成数据</strong>：大量使用模型生成的数据进行训练</li>
<li><strong>DPO替代PPO</strong>：在部分阶段使用更稳定的DPO</li>
<li><strong>专项能力训练</strong>：针对代码、数学、推理、工具使用等分别优化</li>
</ol>
<p><strong>性能表现</strong></p>
<p>在多项基准测试上，Llama 3-405B与GPT-4相当或更好：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>基准</th>
<th>Llama 3 405B</th>
<th>GPT-4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MMLU</td>
<td>88.6%</td>
<td>86.4%</td>
</tr>
<tr class="even">
<td>HumanEval</td>
<td>89.0%</td>
<td>67.0%</td>
</tr>
<tr class="odd">
<td>GSM8K</td>
<td>96.8%</td>
<td>92.0%</td>
</tr>
<tr class="even">
<td>MATH</td>
<td>73.8%</td>
<td>52.9%</td>
</tr>
</tbody>
</table>
<p>这是开源模型首次在全面基准上达到闭源顶级模型的水平。</p>
</section>
<section id="llama系列的技术演进总结" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="llama系列的技术演进总结"><span class="header-section-number">4.3</span> Llama系列的技术演进总结</h3>
<p>让我们用一张表格总结LLaMA到Llama 3的技术演进：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>特性</th>
<th>LLaMA (2023.02)</th>
<th>Llama 2 (2023.07)</th>
<th>Llama 3 (2024.04)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>最大规模</strong></td>
<td>65B</td>
<td>70B</td>
<td>405B</td>
</tr>
<tr class="even">
<td><strong>训练数据</strong></td>
<td>1.0–1.4T tokens</td>
<td>2.0T tokens</td>
<td>15T+ tokens</td>
</tr>
<tr class="odd">
<td><strong>上下文</strong></td>
<td>2,048</td>
<td>4,096</td>
<td>8,192 → 128K</td>
</tr>
<tr class="even">
<td><strong>词表大小</strong></td>
<td>32K</td>
<td>32K</td>
<td>128K</td>
</tr>
<tr class="odd">
<td><strong>KV设计</strong></td>
<td>MHA</td>
<td>GQA (70B only)</td>
<td>GQA (全系列)</td>
</tr>
<tr class="even">
<td><strong>对齐方法</strong></td>
<td>无/基础</td>
<td>RLHF (PPO)</td>
<td>SFT + DPO + PPO</td>
</tr>
<tr class="odd">
<td><strong>开源许可</strong></td>
<td>研究限制</td>
<td>商业许可</td>
<td>商业许可</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="mistral系列效率优先" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="mistral系列效率优先"><span class="header-section-number">5</span> Mistral系列：效率优先</h2>
<section id="背景来自deepmind的创业" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="背景来自deepmind的创业"><span class="header-section-number">5.1</span> 背景：来自DeepMind的创业</h3>
<p>2023年，几位前DeepMind和Meta的研究者在巴黎创立了Mistral AI。他们的目标是构建高效、开放的语言模型。</p>
<p>与Meta的”大就是好”策略不同，Mistral专注于<strong>效率</strong>——用更少的参数实现更好的性能。</p>
</section>
<section id="mistral-7b小模型的逆袭" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="mistral-7b小模型的逆袭"><span class="header-section-number">5.2</span> Mistral 7B：小模型的逆袭</h3>
<p>2023年10月，Mistral发布了Mistral 7B，一个只有7B参数但性能超越Llama 2-13B的模型。</p>
<p><strong>核心创新一：Sliding Window Attention (SWA)</strong></p>
<p>标准Attention的复杂度是<span class="math inline">\(O(n^2)\)</span>，其中<span class="math inline">\(n\)</span>是序列长度。Mistral引入了滑动窗口注意力：每个位置只关注前<span class="math inline">\(W\)</span>个位置（<span class="math inline">\(W\)</span>是窗口大小），复杂度变为<span class="math inline">\(O(n \cdot W)\)</span>。</p>
<div id="fig-swa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-swa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-29/original/fig1-mistral-sliding-window.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-swa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Sliding Window Attention机制。左图：标准因果注意力（全部可见）；中图：滑动窗口注意力（只关注窗口内）；右图：多层堆叠后的有效感受野——通过逐层传播，信息可以覆盖更大范围。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Jiang et al.&nbsp;(2023) “Mistral 7B”, Figure 1. <a href="https://arxiv.org/abs/2310.06825">arXiv:2310.06825</a></em></p>
</div>
<p>但这不意味着长距离信息被丢弃。由于多层堆叠，信息可以逐层传播：</p>
<ul>
<li>第1层：位置<span class="math inline">\(i\)</span>能看到<span class="math inline">\([i-W, i]\)</span></li>
<li>第2层：位置<span class="math inline">\(i\)</span>能看到<span class="math inline">\([i-2W, i]\)</span>（通过中间位置传递）</li>
<li>第<span class="math inline">\(L\)</span>层：位置<span class="math inline">\(i\)</span>能看到<span class="math inline">\([i-L \cdot W, i]\)</span></li>
</ul>
<p>对于Mistral 7B，<span class="math inline">\(W = 4096\)</span>，<span class="math inline">\(L = 32\)</span>，有效感受野达到<span class="math inline">\(32 \times 4096 \approx 131K\)</span> tokens。</p>
<p><strong>核心创新二：高效KV Cache</strong></p>
<p>SWA的另一个优势是KV Cache可以被限制在窗口大小内。当序列长度超过窗口时，旧的KV可以被丢弃（Rolling Buffer Cache）：</p>
<pre><code>位置 0  1  2  3  4  5  6  7  8  ...
Cache         [3  4  5  6]       &lt;- 窗口大小=4，只保留最近4个</code></pre>
<p>这让推理时的内存占用保持恒定，非常适合长序列生成。</p>
<p><strong>性能对比</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>参数量</th>
<th>MMLU</th>
<th>HumanEval</th>
<th>数学推理</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Llama 2-7B</td>
<td>7B</td>
<td>45.3%</td>
<td>12.8%</td>
<td>13.5%</td>
</tr>
<tr class="even">
<td>Llama 2-13B</td>
<td>13B</td>
<td>54.8%</td>
<td>18.3%</td>
<td>22.1%</td>
</tr>
<tr class="odd">
<td><strong>Mistral 7B</strong></td>
<td><strong>7B</strong></td>
<td><strong>60.1%</strong></td>
<td><strong>30.5%</strong></td>
<td><strong>28.4%</strong></td>
</tr>
</tbody>
</table>
<p>用7B参数超越13B模型，这是Mistral “效率优先”理念的最佳证明。</p>
</section>
<section id="mixtral-8x7b开源moe的标杆" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="mixtral-8x7b开源moe的标杆"><span class="header-section-number">5.3</span> Mixtral 8x7B：开源MoE的标杆</h3>
<p>2024年1月，Mistral发布Mixtral 8x7B，一个基于Mixture of Experts (MoE)的模型。</p>
<p><strong>架构设计</strong></p>
<p>Mixtral的基础架构与Mistral 7B相同，但每个FFN层被替换为8个Expert：</p>
<div id="fig-mixtral-moe" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mixtral-moe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-29/original/fig2-mixtral-moe-architecture.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mixtral-moe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Mixtral的Mixture of Experts层架构。每个输入向量由Router分配给8个Expert中的2个，输出是两个选中Expert输出的加权和。这种设计使得总参数量达到46.7B，但每个token只激活12.9B参数。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Jiang et al.&nbsp;(2024) “Mixtral of Experts”, Figure 1. <a href="https://arxiv.org/abs/2401.04088">arXiv:2401.04088</a></em></p>
</div>
<ul>
<li><strong>总参数量</strong>：46.7B（8个Expert × 7B）</li>
<li><strong>激活参数量</strong>：12.9B（每个token激活2个Expert）</li>
<li><strong>Router</strong>：Top-2选择，即每个token选择得分最高的2个Expert</li>
</ul>
<p>这种设计结合了我们在第27章讨论的MoE思想：用稀疏激活实现参数量与计算量的解耦。</p>
<p><strong>性能表现</strong></p>
<p>Mixtral在多项任务上与Llama 2-70B相当或更好，但推理时只使用约1/5的计算量：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>激活参数</th>
<th>MMLU</th>
<th>MT-Bench</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Llama 2-70B</td>
<td>70B</td>
<td>68.9%</td>
<td>6.86</td>
</tr>
<tr class="even">
<td><strong>Mixtral 8x7B</strong></td>
<td><strong>12.9B</strong></td>
<td><strong>70.6%</strong></td>
<td><strong>8.30</strong></td>
</tr>
</tbody>
</table>
<p>特别值得注意的是MT-Bench分数——这是一个对话质量评测，Mixtral-Instruct达到8.30分，超过了当时的GPT-3.5-Turbo。</p>
</section>
<section id="mistral的设计哲学" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="mistral的设计哲学"><span class="header-section-number">5.4</span> Mistral的设计哲学</h3>
<p>从Mistral 7B到Mixtral，我们可以总结Mistral的核心设计哲学：</p>
<ol type="1">
<li><strong>效率第一</strong>：不追求最大参数量，而是追求参数效率</li>
<li><strong>实用主义</strong>：选择已验证有效的技术（GQA、SWA、MoE），而非追求新奇</li>
<li><strong>开放透明</strong>：Apache 2.0许可，完全商业可用</li>
</ol>
<hr>
</section>
</section>
<section id="多极化发展qwendeepseek与更多" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="多极化发展qwendeepseek与更多"><span class="header-section-number">6</span> 多极化发展：Qwen、DeepSeek与更多</h2>
<section id="全球竞争格局" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="全球竞争格局"><span class="header-section-number">6.1</span> 全球竞争格局</h3>
<p>2024年，开源大模型进入多极化时代。除了Meta和Mistral，中国的阿里、DeepSeek、百川等也加入竞争：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>厂商</th>
<th>代表模型</th>
<th>最大规模</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Meta</td>
<td>Llama 3</td>
<td>405B</td>
<td>全面基准领先</td>
</tr>
<tr class="even">
<td>Mistral AI</td>
<td>Mixtral</td>
<td>46.7B (12.9B active)</td>
<td>效率标杆</td>
</tr>
<tr class="odd">
<td>阿里云</td>
<td>Qwen2.5</td>
<td>72B + Max (MoE)</td>
<td>多语言、代码</td>
</tr>
<tr class="even">
<td>DeepSeek</td>
<td>DeepSeek-V3</td>
<td>671B (37B active)</td>
<td>训练效率极致</td>
</tr>
<tr class="odd">
<td>01.AI</td>
<td>Yi-1.5</td>
<td>34B</td>
<td>中英双语</td>
</tr>
<tr class="even">
<td>Stability AI</td>
<td>StableLM</td>
<td>12B</td>
<td>边缘部署</td>
</tr>
</tbody>
</table>
</section>
<section id="qwen系列" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="qwen系列"><span class="header-section-number">6.2</span> Qwen系列</h3>
<p>阿里云的Qwen（通义千问）系列是中国开源LLM的代表。</p>
<p><strong>Qwen2.5的技术亮点</strong></p>
<p>Qwen2.5于2024年9月发布，覆盖0.5B到72B多个规模：</p>
<ul>
<li><strong>多语言能力</strong>：支持29种语言，中英文尤其强大</li>
<li><strong>长上下文</strong>：支持128K上下文</li>
<li><strong>代码能力</strong>：专门的Qwen2.5-Coder变体</li>
<li><strong>数学能力</strong>：专门的Qwen2.5-Math变体</li>
</ul>
<p><strong>Qwen2.5-Max</strong></p>
<p>2025年1月，阿里发布Qwen2.5-Max，一个大规模MoE模型。据报道，它在Arena-Hard、LiveBench等基准上超越了DeepSeek-V3和GPT-4o，展示了中国模型的快速进步。</p>
</section>
<section id="deepseek系列" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="deepseek系列"><span class="header-section-number">6.3</span> DeepSeek系列</h3>
<p>DeepSeek是一家中国AI初创公司，以极致的训练效率著称。</p>
<p><strong>DeepSeek-V3的技术创新</strong></p>
<p>2024年12月发布的DeepSeek-V3是一个技术上极具创新性的模型：</p>
<p><strong>Multi-head Latent Attention (MLA)</strong></p>
<p>这是DeepSeek-V2引入的高效注意力变体。MLA的核心思想是：将Key和Value压缩到一个低维的”隐空间”，然后从这个隐空间恢复完整的KV。</p>
<p><span class="math display">\[
\text{传统GQA:} \quad K = W_K h, \quad V = W_V h
\]</span></p>
<p><span class="math display">\[
\text{MLA:} \quad c = W_C h, \quad K = W_{KD} c, \quad V = W_{VD} c
\]</span></p>
<p>其中<span class="math inline">\(c\)</span>是一个低维的压缩向量。这进一步减少了KV Cache的大小。</p>
<p><strong>DeepSeekMoE</strong></p>
<p>DeepSeek使用细粒度的Expert设计：</p>
<ul>
<li><strong>共享Expert</strong>：所有token都使用的专家，捕获通用模式</li>
<li><strong>路由Expert</strong>：根据token内容动态选择的专家</li>
</ul>
<p>结合这两种Expert，DeepSeek-V3实现了更好的负载均衡和更稳定的训练。</p>
<p><strong>训练效率的极致</strong></p>
<p>DeepSeek-V3最惊人的是其训练成本：</p>
<ul>
<li><strong>总参数</strong>：671B（37B激活）</li>
<li><strong>训练Token</strong>：14.8T</li>
<li><strong>训练成本</strong>：仅2.788M H800 GPU hours</li>
</ul>
<p>对比：Llama 3-405B的训练成本据估计超过30M GPU hours。DeepSeek用不到1/10的成本达到了相近的性能水平。</p>
<p>这种效率来自于： 1. MLA大幅减少内存带宽需求 2. 辅助损失免费(auxiliary-loss-free)的负载均衡 3. FP8混合精度训练 4. 极致的工程优化</p>
<hr>
</section>
</section>
<section id="开源-vs-闭源的博弈" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="开源-vs-闭源的博弈"><span class="header-section-number">7</span> 开源 vs 闭源的博弈</h2>
<section id="核心权衡" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="核心权衡"><span class="header-section-number">7.1</span> 核心权衡</h3>
<p>开源和闭源大模型各有优劣，选择取决于具体需求：</p>
<p><strong>开源模型的优势</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>优势</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>成本</strong></td>
<td>自托管避免API费用；可根据需求选择模型规模</td>
</tr>
<tr class="even">
<td><strong>定制</strong></td>
<td>可以针对特定领域微调；可以修改架构</td>
</tr>
<tr class="odd">
<td><strong>隐私</strong></td>
<td>数据不离开本地；满足合规要求</td>
</tr>
<tr class="even">
<td><strong>透明</strong></td>
<td>可以审计模型行为；可以进行安全研究</td>
</tr>
<tr class="odd">
<td><strong>研究</strong></td>
<td>可以复现实验；可以进行消融分析</td>
</tr>
<tr class="even">
<td><strong>长期</strong></td>
<td>不受供应商策略变化影响</td>
</tr>
</tbody>
</table>
<p><strong>闭源模型的优势</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>优势</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>性能</strong></td>
<td>顶级闭源模型（GPT-4、Claude）仍领先</td>
</tr>
<tr class="even">
<td><strong>支持</strong></td>
<td>专业团队提供技术支持</td>
</tr>
<tr class="odd">
<td><strong>简单</strong></td>
<td>无需管理基础设施</td>
</tr>
<tr class="even">
<td><strong>安全</strong></td>
<td>持续的安全监控和更新</td>
</tr>
<tr class="odd">
<td><strong>合规</strong></td>
<td>SOC 2、HIPAA等企业级合规</td>
</tr>
</tbody>
</table>
</section>
<section id="趋势差距正在缩小" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="趋势差距正在缩小"><span class="header-section-number">7.2</span> 趋势：差距正在缩小</h3>
<p>2024–2025年的数据显示，开源模型正在快速追赶：</p>
<pre><code>2023初: GPT-4 &gt;&gt;&gt; 开源最佳 (差距巨大)
2023中: GPT-4 &gt;&gt; Llama 2-70B (显著差距)
2024初: GPT-4 &gt; Mixtral/Llama 3 (差距缩小)
2024末: GPT-4 ≈ Llama 3-405B/DeepSeek-V3 (接近持平)</code></pre>
<p>根据a16z的调研，41%的企业计划增加开源模型使用，另外41%表示如果开源性能追上会考虑切换。</p>
</section>
<section id="混合策略" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="混合策略"><span class="header-section-number">7.3</span> 混合策略</h3>
<p>实践中，许多组织采用混合策略：</p>
<ol type="1">
<li><strong>研发阶段</strong>：用开源模型快速原型和实验</li>
<li><strong>生产阶段</strong>：根据具体需求选择
<ul>
<li>通用对话 → 可能继续用开源</li>
<li>复杂推理 → 可能调用GPT-4</li>
<li>隐私敏感 → 必须用开源自托管</li>
</ul></li>
<li><strong>长期规划</strong>：随着开源能力提升，逐步迁移</li>
</ol>
<hr>
</section>
</section>
<section id="工程实践本地部署开源模型" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="工程实践本地部署开源模型"><span class="header-section-number">8</span> 工程实践：本地部署开源模型</h2>
<section id="选择合适的模型" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="选择合适的模型"><span class="header-section-number">8.1</span> 选择合适的模型</h3>
<p>部署前需要考虑几个因素：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>因素</th>
<th>考虑</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>任务复杂度</strong></td>
<td>简单任务用小模型；复杂推理用大模型</td>
</tr>
<tr class="even">
<td><strong>硬件限制</strong></td>
<td>GPU显存、CPU内存、存储空间</td>
</tr>
<tr class="odd">
<td><strong>延迟要求</strong></td>
<td>实时应用需要快速响应</td>
</tr>
<tr class="even">
<td><strong>精度要求</strong></td>
<td>是否可接受量化损失</td>
</tr>
<tr class="odd">
<td><strong>语言需求</strong></td>
<td>中文优先考虑Qwen/DeepSeek</td>
</tr>
</tbody>
</table>
</section>
<section id="量化让大模型跑在小设备上" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="量化让大模型跑在小设备上"><span class="header-section-number">8.2</span> 量化：让大模型跑在小设备上</h3>
<p>量化将模型权重从FP16（16位）压缩到更低精度：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>量化方式</th>
<th>显存需求(7B)</th>
<th>质量损失</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FP16</td>
<td>~14GB</td>
<td>无</td>
<td>有高端GPU</td>
</tr>
<tr class="even">
<td>INT8</td>
<td>~7GB</td>
<td>极小</td>
<td>消费级GPU</td>
</tr>
<tr class="odd">
<td>INT4 (GPTQ/AWQ)</td>
<td>~4GB</td>
<td>小</td>
<td>移动/边缘</td>
</tr>
<tr class="even">
<td>GGUF (llama.cpp)</td>
<td>~4GB</td>
<td>小</td>
<td>CPU推理</td>
</tr>
</tbody>
</table>
</section>
<section id="推理框架选择" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="推理框架选择"><span class="header-section-number">8.3</span> 推理框架选择</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>框架</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>vLLM</strong></td>
<td>PagedAttention，高吞吐</td>
<td>高并发服务</td>
</tr>
<tr class="even">
<td><strong>TGI</strong></td>
<td>HuggingFace官方，易用</td>
<td>快速部署</td>
</tr>
<tr class="odd">
<td><strong>llama.cpp</strong></td>
<td>纯CPU/Metal支持</td>
<td>边缘设备</td>
</tr>
<tr class="even">
<td><strong>Ollama</strong></td>
<td>一键安装，极简</td>
<td>个人使用</td>
</tr>
<tr class="odd">
<td><strong>TensorRT-LLM</strong></td>
<td>NVIDIA优化</td>
<td>A100/H100</td>
</tr>
</tbody>
</table>
</section>
<section id="快速开始示例" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="快速开始示例"><span class="header-section-number">8.4</span> 快速开始示例</h3>
<p><strong>使用Ollama（最简单）</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装Ollama</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 运行Llama 3</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama3</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 运行Mistral</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run mistral</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>使用vLLM（高性能服务）</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm <span class="im">import</span> LLM, SamplingParams</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(model<span class="op">=</span><span class="st">"meta-llama/Llama-3.1-8B-Instruct"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>sampling_params <span class="op">=</span> SamplingParams(temperature<span class="op">=</span><span class="fl">0.7</span>, top_p<span class="op">=</span><span class="fl">0.9</span>, max_tokens<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm.generate([<span class="st">"What is the capital of France?"</span>], sampling_params)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>].outputs[<span class="dv">0</span>].text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>使用Transformers + BitsAndBytes（量化）</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 4-bit量化配置</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>quantization_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">"nf4"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载量化模型</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"meta-llama/Llama-3.1-8B-Instruct"</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>quantization_config,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"meta-llama/Llama-3.1-8B-Instruct"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"Hello, how are you?"</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, max_new_tokens<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">9</span> 深入理解</h2>
<section id="为什么开源模型能追上闭源" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="为什么开源模型能追上闭源"><span class="header-section-number">9.1</span> 为什么开源模型能追上闭源？</h3>
<p>从技术角度，开源模型的追赶得益于几个因素：</p>
<p><strong>因素一：架构已经收敛</strong></p>
<p>2024年的大模型架构基本统一：Transformer Decoder + RMSNorm + SwiGLU + RoPE + GQA。这意味着架构不再是差异化因素——开源模型可以直接采用最佳实践。</p>
<p><strong>因素二：训练技术的开放</strong></p>
<p>关键的训练技术（如RLHF、DPO、合成数据生成）已经通过论文和开源代码公开。虽然细节仍有差异，但核心方法论是共享的。</p>
<p><strong>因素三：规模效应</strong></p>
<p>当Meta、Mistral、阿里投入大量资源训练开源模型时，它们实际上在与闭源厂商进行同等规模的竞争。DeepSeek-V3的14.8T训练数据量与GPT-4在同一数量级。</p>
<p><strong>因素四：社区放大效应</strong></p>
<p>开源模型有一个独特优势：社区贡献。当Llama发布后，成百上千的研究者在其基础上实验、改进、反馈。这种分布式创新是闭源模型无法复制的。</p>
</section>
<section id="开放问题" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="开放问题"><span class="header-section-number">9.2</span> 开放问题</h3>
<p><strong>问题一：开源的边界在哪里？</strong></p>
<p>“开源”的定义本身就有争议：</p>
<ul>
<li>权重开源 ≠ 训练代码开源 ≠ 数据开源</li>
<li>Llama 3的许可证禁止用于”10亿月活用户以上的产品”</li>
<li>这还算真正的”开源”吗？</li>
</ul>
<p><strong>问题二：安全与开放的张力</strong></p>
<p>开源模型可以被任何人修改——包括移除安全对齐。这带来了双重使用风险：</p>
<ul>
<li>正面：安全研究者可以测试防御</li>
<li>负面：恶意行为者可以创建有害变体</li>
</ul>
<p>如何平衡开放性和安全性？目前没有共识。</p>
<p><strong>问题三：可持续性</strong></p>
<p>训练一个前沿模型需要数千万美元。Meta可以承担（作为开源的战略投入），但这种模式是否可持续？开源模型是否会永远依赖少数大公司的”善意”？</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">10</span> 局限性与未解决的问题</h2>
<section id="开源模型的局限" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="开源模型的局限"><span class="header-section-number">10.1</span> 开源模型的局限</h3>
<p><strong>能力边界</strong></p>
<p>尽管Llama 3-405B在许多基准上追平GPT-4，但在某些任务上仍有差距：</p>
<ul>
<li>极复杂的多步推理</li>
<li>最新知识（训练数据截止）</li>
<li>多模态理解（图像、音频、视频）</li>
</ul>
<p><strong>工程复杂度</strong></p>
<p>部署开源模型需要： - GPU基础设施 - 运维能力 - 安全监控 - 持续更新</p>
<p>这对小团队是显著的负担。</p>
<p><strong>碎片化</strong></p>
<p>开源生态的碎片化也是问题： - 模型太多，难以选择 - 版本迭代快，难以跟踪 - 不同框架之间兼容性问题</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">10.2</span> 这些局限导向了什么？</h3>
<p>开源模型的局限指向几个发展方向：</p>
<ol type="1">
<li><strong>更易用的部署工具</strong>：Ollama、Replicate等正在降低使用门槛</li>
<li><strong>模型选择指导</strong>：benchmark聚合、自动选择服务</li>
<li><strong>多模态开源</strong>：LLaVA、Qwen-VL等正在补齐能力</li>
<li><strong>持续对齐</strong>：社区驱动的安全研究和红队测试</li>
</ol>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">11</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">11.1</span> 核心要点回顾</h3>
<ol type="1">
<li><p><strong>问题</strong>：最先进的语言模型长期被少数公司垄断，限制了研究、创新和应用。</p></li>
<li><p><strong>突破</strong>：LLaMA（2023.02）的发布打破了这一格局，证明开源模型可以达到商业级水平。</p></li>
<li><p><strong>技术演进</strong>：</p>
<ul>
<li>LLaMA确立了”标准配置”：RMSNorm + SwiGLU + RoPE</li>
<li>Llama 2引入GQA、完善安全对齐</li>
<li>Llama 3通过规模化训练达到GPT-4水平</li>
<li>Mistral展示了效率优先的可能</li>
<li>DeepSeek证明了极致的训练效率</li>
</ul></li>
<li><p><strong>意义</strong>：开源模型的繁荣改变了LLM生态——研究更开放、创新更民主、应用更自由。</p></li>
</ol>
</section>
<section id="关键技术速查" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="关键技术速查"><span class="header-section-number">11.2</span> 关键技术速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>技术</th>
<th>作用</th>
<th>首次大规模采用</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RMSNorm</td>
<td>简化归一化，加速训练</td>
<td>LLaMA</td>
</tr>
<tr class="even">
<td>SwiGLU</td>
<td>更好的FFN激活函数</td>
<td>LLaMA</td>
</tr>
<tr class="odd">
<td>RoPE</td>
<td>相对位置编码，支持外推</td>
<td>LLaMA</td>
</tr>
<tr class="even">
<td>GQA</td>
<td>减少KV Cache，加速推理</td>
<td>Llama 2-70B</td>
</tr>
<tr class="odd">
<td>SWA</td>
<td>滑动窗口注意力，长序列</td>
<td>Mistral 7B</td>
</tr>
<tr class="even">
<td>MoE</td>
<td>稀疏激活，扩大容量</td>
<td>Mixtral 8x7B</td>
</tr>
<tr class="odd">
<td>MLA</td>
<td>压缩KV，极致效率</td>
<td>DeepSeek-V2</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">11.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么SwiGLU需要三个权重矩阵而不是两个？这如何影响模型的参数效率设计？</p></li>
<li><p><strong>[对比分析]</strong> Mistral 7B如何用7B参数超越Llama 2-13B？滑动窗口注意力在其中起了什么作用？</p></li>
<li><p><strong>[工程实践]</strong> 选择一个开源模型（如Llama 3-8B），使用vLLM或Ollama部署本地服务，测量不同量化精度下的推理延迟和生成质量。</p></li>
<li><p><strong>[开放思考]</strong> 开源模型的”开源”程度差异很大（仅权重 vs 完整训练流程 vs 包含数据）。你认为什么程度的开放才算”真正的开源”？这对研究和应用分别意味着什么？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">12</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">12.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>Touvron et al.&nbsp;(2023)</strong> “LLaMA: Open and Efficient Foundation Language Models”
<ul>
<li>重点读：Section 2 (架构设计)、Table 2 (模型配置)</li>
<li>开源LLM的开山之作</li>
</ul></li>
<li><strong>Touvron et al.&nbsp;(2023)</strong> “Llama 2: Open Foundation and Fine-Tuned Chat Models”
<ul>
<li>重点读：Section 3 (RLHF)、Section 4 (安全对齐)</li>
<li>完整的安全对齐流程公开</li>
</ul></li>
<li><strong>Llama Team (2024)</strong> “The Llama 3 Herd of Models”
<ul>
<li>重点读：Section 3 (数据)、Section 4 (后训练)</li>
<li>开源模型达到GPT-4水平的技术细节</li>
</ul></li>
</ul>
</section>
<section id="效率导向" class="level3" data-number="12.2">
<h3 data-number="12.2" class="anchored" data-anchor-id="效率导向"><span class="header-section-number">12.2</span> 效率导向</h3>
<ul>
<li><strong>Jiang et al.&nbsp;(2023)</strong> “Mistral 7B”
<ul>
<li>滑动窗口注意力、高效推理设计</li>
</ul></li>
<li><strong>Jiang et al.&nbsp;(2024)</strong> “Mixtral of Experts”
<ul>
<li>开源MoE的标杆实现</li>
</ul></li>
<li><strong>DeepSeek-AI (2024)</strong> “DeepSeek-V3 Technical Report”
<ul>
<li>MLA、训练效率的极致优化</li>
</ul></li>
</ul>
</section>
<section id="综述与分析" class="level3" data-number="12.3">
<h3 data-number="12.3" class="anchored" data-anchor-id="综述与分析"><span class="header-section-number">12.3</span> 综述与分析</h3>
<ul>
<li><strong>Cameron Wolfe</strong> “LLaMA-2 from the Ground Up” — LLaMA架构的详细解释</li>
<li><strong>Sebastian Raschka</strong> “The State of LLMs 2025” — 开源生态的全面梳理</li>
</ul>
</section>
<section id="工具资源" class="level3" data-number="12.4">
<h3 data-number="12.4" class="anchored" data-anchor-id="工具资源"><span class="header-section-number">12.4</span> 工具资源</h3>
<ul>
<li><strong>Hugging Face Hub</strong> — 开源模型权重和微调版本</li>
<li><strong>vLLM</strong> — 高性能推理框架</li>
<li><strong>Ollama</strong> — 一键本地运行</li>
<li><strong>llama.cpp</strong> — CPU/边缘推理</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">13</span> 历史注脚</h2>
<p>LLaMA的发布是一个充满戏剧性的事件。</p>
<p>2023年2月24日，Meta发布了LLaMA论文和权重。最初，权重仅对学术研究者开放，需要填写申请表。但在发布后不到一周，权重文件就在网上泄露，通过torrent在研究社区中广泛传播。</p>
<p>Meta面临一个微妙的处境：是追究泄露者，还是默许这种”意外的开源”？最终，Meta选择了后者。事实上，这次泄露可能是LLM历史上最具影响力的”事故”——它让全世界的研究者在同一起跑线上开始实验。</p>
<p>泄露后的几周内，社区的创造力爆发了。斯坦福的Alpaca项目证明了用不到$500的成本就能微调出一个像样的指令模型；Vicuna声称达到了ChatGPT 90%的能力；各种针对特定任务的微调版本层出不穷。</p>
<p>有趣的是，LLaMA的成功部分归功于它的”恰到好处”。65B参数够大，能够展示涌现能力；但又不像GPT-3的175B那么不可企及——研究者可以在几张A100上进行微调实验。</p>
<p>Meta显然从LLaMA的经历中学到了教训。Llama 2的发布采用了完全不同的策略——从一开始就开放商业许可，主动拥抱开源社区。这种策略转变被认为是Meta在AI时代寻找自身定位的表现：既然无法与OpenAI在闭源产品上竞争，不如通过开源建立生态影响力。</p>
<p>Mistral的故事则代表了另一种可能。这家由DeepMind和Meta校友创立的法国初创公司，选择了”效率优先”的路线。Mistral 7B用7B参数超越了Llama 2-13B，证明了”小而美”也是一条可行的路径。更有意思的是，Mistral在商业上也取得了成功——仅用一年就估值超过60亿美元。</p>
<p>中国的开源模型阵营同样不可忽视。阿里的Qwen、DeepSeek、百川、智谱等公司相继发布了有竞争力的开源模型。DeepSeek-V3尤其引人注目——它用不到1/10的训练成本达到了接近GPT-4的水平，展示了工程优化可以在多大程度上弥补资源差距。</p>
<p>回顾这段历史，开源大模型的崛起不是一个单一事件，而是多方力量共同作用的结果：Meta的战略选择、社区的创造力、学术界的需求、以及多个国家在AI领域的竞争意识。</p>
<p>未来会怎样？一个可能的方向是：开源模型继续追赶，直到与闭源模型的差距变得微不足道。到那时，闭源模型将主要依靠服务质量、集成便利性和企业支持来维持竞争力，而核心能力将成为”公共品”。</p>
<p>这或许是深度学习最初的承诺：知识应该是开放的，创新应该是共享的。LLaMA可能只是这个故事的开始。</p>
<hr>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li>
<li><a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
<li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></li>
<li><a href="https://arxiv.org/abs/2310.06825">Mistral 7B</a></li>
<li><a href="https://arxiv.org/abs/2401.04088">Mixtral of Experts</a></li>
<li><a href="https://arxiv.org/abs/2412.19437">DeepSeek-V3 Technical Report</a></li>
<li><a href="https://mbrenndoerfer.com/writing/llama-components-rmsnorm-swiglu-rope">LLaMA Components: RMSNorm, SwiGLU, and RoPE</a></li>
<li><a href="https://blog.spheron.network/choosing-the-right-llm-2024-comparison-of-open-source-vs-closed-source-llms">2024 Comparison of Open-Source Vs Closed-Source LLMs</a></li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第29章：开源大模型的演进"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "From LLaMA to the Open Frontier: The Democratization of Large Language Models"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, 开源模型, LLaMA, Mistral, Qwen, DeepSeek]</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [LLaMA, Llama 2, Llama 3, Mistral, Mixtral, Qwen, DeepSeek, 开源LLM, RMSNorm, SwiGLU, RoPE, GQA]</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "长期以来，最强大的语言模型一直是闭源的——GPT-4、Claude、Gemini的权重都不对外开放。研究者只能通过API窥见其能力，却无法复现、研究或改进。2023年2月，Meta发布LLaMA，打破了这一格局。从7B到65B参数，从LLaMA到Llama 3，从单一厂商到Mistral、Qwen、DeepSeek的多极化竞争——开源LLM不仅追上了闭源模型的性能，更构建了一个繁荣的研究与应用生态。本章将带你回顾这场'开源革命'的技术演进与生态变迁。"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-29/original/fig1-mistral-sliding-window.png"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：开源大模型如何在技术上追赶甚至超越闭源模型？不同的开源模型在架构设计上有何异同？开源生态对LLM研究与应用意味着什么？</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2023–2025 </span><span class="pp">|</span><span class="at"> LLaMA (2023.02) → Llama 2 (2023.07) → Mistral 7B (2023.10) → Mixtral (2024.01) → Llama 3 (2024.04) → DeepSeek-V3 (2024.12) </span><span class="pp">|</span><span class="at"> 开源LLM的崛起与多极化</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Touvron et al. (2023a)** "LLaMA: Open and Efficient Foundation Language Models" (<span class="co">[</span><span class="ot">arXiv:2302.13971</span><span class="co">](https://arxiv.org/abs/2302.13971)</span>) — 参考了 Section 2 (架构设计)、Table 2 (模型规模)；开源LLM的里程碑</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Touvron et al. (2023b)** "Llama 2: Open Foundation and Fine-Tuned Chat Models" (<span class="co">[</span><span class="ot">arXiv:2307.09288</span><span class="co">](https://arxiv.org/abs/2307.09288)</span>) — 参考了 Section 2-5 (架构改进、安全对齐)；首个完全开源的Chat模型</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Llama Team (2024)** "The Llama 3 Herd of Models" (<span class="co">[</span><span class="ot">arXiv:2407.21783</span><span class="co">](https://arxiv.org/abs/2407.21783)</span>) — 参考了 Section 3 (15T训练数据)、Section 4 (后训练流程)；开源模型达到GPT-4水平</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Jiang et al. (2023)** "Mistral 7B" (<span class="co">[</span><span class="ot">arXiv:2310.06825</span><span class="co">](https://arxiv.org/abs/2310.06825)</span>) — 参考了 Section 2 (Sliding Window Attention)；效率优先的设计哲学</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Jiang et al. (2024)** "Mixtral of Experts" (<span class="co">[</span><span class="ot">arXiv:2401.04088</span><span class="co">](https://arxiv.org/abs/2401.04088)</span>) — 参考了 Section 2 (Sparse MoE架构)；开源MoE的标杆</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepSeek-AI (2024)** "DeepSeek-V3 Technical Report" (<span class="co">[</span><span class="ot">arXiv:2412.19437</span><span class="co">](https://arxiv.org/abs/2412.19437)</span>) — 参考了 Section 2-4 (MLA、DeepSeekMoE)；训练效率的极致优化</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材与博客</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cameron R. Wolfe** "LLaMA-2 from the Ground Up" — 参考了LLaMA架构细节的解释</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sebastian Raschka** "The State of LLMs 2025" — 参考了开源模型生态的梳理</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**arXiv:2510.12178** "Evolution of Meta's LLaMA Models: A Survey" — 参考了LLaMA系列演进脉络</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>上一章我们探讨了状态空间模型（SSM）——一条不同于Transformer的序列建模路径。Mamba证明了$O(n)$复杂度的模型可以在语言建模上与Transformer竞争，而Jamba等混合架构则展示了结合两种范式的可能性。</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>这些架构创新回答了一个技术问题：**如何更高效地建模序列？**</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>然而，还有一个更实际的问题困扰着研究社区：**即使知道了最先进的架构，普通研究者也无法真正实验它们。**</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>原因很简单：最强大的模型都是闭源的。</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>2022年底，ChatGPT横空出世，展示了语言模型的惊人能力。但OpenAI没有公开GPT-3.5或GPT-4的权重——研究者只能通过API调用，无法查看模型内部、无法复现实验、无法在其基础上改进。Claude、Gemini同样如此。这意味着：</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**研究受限**：无法进行消融实验、可解释性分析、安全性研究</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**创新受限**：无法基于现有模型进行微调或架构改进</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**教育受限**：学生只能学习过时的小模型，无法接触前沿</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**应用受限**：企业必须依赖API，面临成本、隐私、稳定性风险</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>一个悖论出现了：**深度学习是开放的学术传统催生的，但最先进的成果却被少数公司垄断。**</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>2023年2月，一切开始改变。</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>Meta（原Facebook）发布了LLaMA（Large Language Model Meta AI），一个从7B到65B参数的开源大语言模型家族。虽然最初的发布有使用限制，但权重文件很快在网上流传。研究者第一次可以在自己的机器上运行、分析、修改一个真正的大规模语言模型。</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：LLaMA的发布不仅仅是一个模型的开源，而是开启了一个时代。它证明了开源模型可以达到商业闭源模型的水平，并催生了一个繁荣的开源生态——Alpaca、Vicuna证明了廉价微调的可行性；Mistral展示了小模型的效率潜力；Llama 3在多项基准上追平GPT-4。从"只有大公司才能做LLM"到"每个研究者都能实验LLM"，这是一场真正的民主化革命。</span></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么需要开源大模型？</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>在讨论具体技术之前，让我们先理解开源大模型解决的核心问题。</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>**科学可复现性的危机**</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>深度学习的一个核心原则是可复现性——论文中的实验应该能够被独立验证。但当GPT-4发表时，论文中几乎没有架构细节、训练数据描述或超参数设置。这不是"技术报告"，而是一篇"能力展示"。</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>没有可复现性，就没有真正的科学进步。我们只能相信OpenAI声称的性能数字，却无法验证它们是否在所有分布上都成立、是否有数据污染、是否有隐藏的故障模式。</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>**研究创新的瓶颈**</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>假设你有一个关于注意力机制的新想法，想验证它是否能提升大模型的推理能力。如果没有开源基础模型，你需要：</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>从头训练一个大模型（需要数百万美元的计算资源）</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>或者用小模型验证（但结果可能不能推广到大规模）</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>或者放弃这个想法</span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>开源模型改变了这个局面：你可以直接在Llama 3的基础上测试你的改进，只需消耗微调级别的计算资源。</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>**应用场景的限制**</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>对于企业应用，依赖闭源API存在多重风险：</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 风险类型 <span class="pp">|</span> 具体问题 <span class="pp">|</span></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|----------|</span></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **成本** <span class="pp">|</span> API调用按token计费，大规模应用成本高昂 <span class="pp">|</span></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **隐私** <span class="pp">|</span> 数据必须发送到第三方服务器，合规风险 <span class="pp">|</span></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **延迟** <span class="pp">|</span> 网络往返增加响应时间 <span class="pp">|</span></span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **可用性** <span class="pp">|</span> 服务可能中断、限流或下线 <span class="pp">|</span></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **定制** <span class="pp">|</span> 无法针对特定领域进行深度微调 <span class="pp">|</span></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>开源模型可以本地部署，完全规避这些问题。</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开源模型面临的技术挑战</span></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>开源并不意味着"简单"。要构建一个有竞争力的开源大模型，需要解决几个核心技术挑战：</span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>**挑战一：训练数据**</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a>闭源模型的一个主要优势是对海量高质量数据的访问。OpenAI、Google拥有巨大的数据收集能力。开源模型必须依赖公开可用的数据，同时确保数据质量和多样性。</span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>**挑战二：计算资源**</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>训练一个65B参数的模型需要数千个GPU运行数周。这不是个人研究者能承担的。开源模型的训练通常由大公司（Meta、Mistral AI）或充足资金的研究机构完成。</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>**挑战三：工程优化**</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>大模型训练涉及分布式系统、混合精度、通信优化等复杂工程。缺乏这些专业知识，即使有数据和算力也难以成功。</span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>**挑战四：安全对齐**</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a>仅仅训练一个能力强的模型不够——它还需要是安全的、有帮助的、诚实的。闭源模型有专门的红队和安全团队；开源模型如何实现可靠的对齐？</span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>接下来，我们将看到LLaMA及其后继者如何应对这些挑战。</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLaMA：开源的里程碑</span></span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a><span class="fu">### 发布背景</span></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>2023年2月24日，Meta AI发布了LLaMA论文"LLaMA: Open and Efficient Foundation Language Models"。这是一个从7B到65B参数的模型家族，在多项基准测试上超越了GPT-3（175B）。</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>论文的核心论点是：**通过更长时间地在更多数据上训练，较小的模型可以达到更大模型的性能。** 这与当时流行的"越大越好"的观念形成对比，也呼应了后来Chinchilla论文的发现。</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>LLaMA的训练数据规模令人印象深刻：</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 参数量 <span class="pp">|</span> 训练Token数 <span class="pp">|</span> 训练数据倍数 <span class="pp">|</span></span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|-------------|--------------|</span></span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LLaMA-7B <span class="pp">|</span> 7B <span class="pp">|</span> 1.0T <span class="pp">|</span> 143x <span class="pp">|</span></span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LLaMA-13B <span class="pp">|</span> 13B <span class="pp">|</span> 1.0T <span class="pp">|</span> 77x <span class="pp">|</span></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LLaMA-33B <span class="pp">|</span> 33B <span class="pp">|</span> 1.4T <span class="pp">|</span> 42x <span class="pp">|</span></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LLaMA-65B <span class="pp">|</span> 65B <span class="pp">|</span> 1.4T <span class="pp">|</span> 22x <span class="pp">|</span></span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a>对比：Chinchilla-optimal建议是约20x（即20倍于参数量的token数）。LLaMA选择了更长的训练，这在推理时是"免费"的——推理成本取决于参数量，而非训练数据量。</span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心架构创新</span></span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>LLaMA的架构基于Transformer Decoder，但采用了多项当时最先进的技术改进。这些选择后来成为了开源模型的"标准配置"。</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pre-Normalization with RMSNorm</span></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>原始Transformer使用Post-Layer Normalization：先计算子层（Attention或FFN），再做LayerNorm。LLaMA改用Pre-Normalization：先做Normalization，再计算子层。</span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>Pre-Norm的优势在于训练稳定性。从梯度流动的角度分析：Post-Norm结构中，梯度必须经过归一化层才能到达残差路径；而Pre-Norm保证了一条"干净"的残差路径，梯度可以不受干扰地直接回传。</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>此外，LLaMA用RMSNorm替代了标准的LayerNorm：</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a>对比LayerNorm：</span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a>\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta</span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a>RMSNorm省略了均值中心化（$x - \mu$）和偏置项（$\beta$）。实验表明这种简化不影响性能，但可以减少7%–64%的计算时间。</span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SwiGLU激活函数</span></span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a>原始Transformer的FFN使用ReLU激活：</span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2</span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a>LLaMA采用SwiGLU，这是Swish激活函数和门控线性单元（GLU）的结合：</span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a>\text{SwiGLU}(x) = \text{Swish}(xW_1) \otimes (xW_2)</span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a>其中$\text{Swish}(x) = x \cdot \sigma(x)$，$\sigma$是sigmoid函数，$\otimes$表示逐元素乘法。</span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>SwiGLU需要三个权重矩阵（$W_1$, $W_2$, $W_3$），而标准FFN只需要两个。为了保持参数量不变，LLaMA调整了隐藏层维度：从标准的$4d$变为$\frac{2}{3} \times 4d \times 3 = 8d/3$。</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a>为什么SwiGLU更好？直观上，门控机制允许网络学习性地"开关"某些特征通道，而Swish的平滑性避免了ReLU的"死神经元"问题。实验表明，SwiGLU在保持计算量不变的情况下能提升模型性能。</span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RoPE位置编码</span></span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a>LLaMA采用旋转位置编码（Rotary Position Embedding, RoPE），这是一种将位置信息注入Query和Key的方法。</span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a>RoPE的核心思想是：对于位置$m$的Query向量$q$和位置$n$的Key向量$k$，它们的点积应该只依赖于相对位置$m-n$：</span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a>\langle R_m q, R_n k \rangle = \langle R_{m-n} q, k \rangle</span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a>这通过旋转矩阵实现。对于二维向量：</span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a>R_\theta = \begin{pmatrix} \cos\theta &amp; -\sin\theta <span class="sc">\\</span> \sin\theta &amp; \cos\theta \end{pmatrix}</span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a>高维向量被分成多个二维子空间，每个子空间独立旋转。</span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a>RoPE的优势包括：</span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**相对位置编码**：天然编码了相对距离</span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**外推潜力**：通过适当的缩放技术，可以扩展到比训练时更长的序列</span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**计算高效**：旋转可以用简单的三角函数实现</span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a>**性能表现**</span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a>LLaMA在发布时展示了令人印象深刻的性能——较小的模型通过更充分的训练可以匹配甚至超越更大的模型：</span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a><span class="al">![LLaMA在常识推理任务上的Zero-shot性能。LLaMA-65B在多项任务上超越了GPT-3（175B）和PaLM（540B），证明了"训练更充分的小模型"策略的有效性。](figures/chapter-29/original/fig3-llama-training-loss.png)</span>{#fig-llama-performance width=90%}</span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a>*Source: Touvron et al. (2023) "LLaMA: Open and Efficient Foundation Language Models", Table 3. [arXiv:2302.13971](https://arxiv.org/abs/2302.13971)*</span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### LLaMA的影响</span></span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a>LLaMA的权重虽然最初限制用于研究目的，但很快在网上泄露。这引发了一场"意外的民主化"——世界各地的研究者开始基于LLaMA进行实验。</span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a>几周内，社区涌现了大量衍生工作：</span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Alpaca**（Stanford）：用GPT-4生成的52K指令数据微调LLaMA-7B，成本不到$500</span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vicuna**：用ShareGPT对话数据微调，据称达到ChatGPT 90%的能力</span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Koala**：用对话数据微调，强调研究可复现性</span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GPT4All**：量化后可在CPU上运行的版本</span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a>这些工作证明了一个重要的事实：**有了好的基础模型，即使是小团队也能以很低的成本创造有价值的应用。**</span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a><span class="fu">## Llama 2和Llama 3：规模化演进</span></span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a><span class="fu">### Llama 2：完全开源的转折点</span></span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a>2023年7月，Meta发布Llama 2，这次是完全开源的——包括商业使用许可。这消除了LLaMA 1法律地位的模糊性，让企业可以放心地基于它构建产品。</span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a>Llama 2的主要改进包括：</span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a>**更大的训练规模**</span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 改进 <span class="pp">|</span> LLaMA 1 <span class="pp">|</span> Llama 2 <span class="pp">|</span></span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|---------|</span></span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 训练Token数 <span class="pp">|</span> 1.0–1.4T <span class="pp">|</span> 2.0T <span class="pp">|</span></span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 上下文长度 <span class="pp">|</span> 2,048 <span class="pp">|</span> 4,096 <span class="pp">|</span></span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型规模 <span class="pp">|</span> 7B, 13B, 33B, 65B <span class="pp">|</span> 7B, 13B, 70B <span class="pp">|</span></span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a>训练数据量翻倍，上下文窗口翻倍。</span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a>**Grouped Query Attention (GQA)**</span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a>LLaMA 1使用标准的Multi-Head Attention (MHA)，其中每个注意力头有独立的Query、Key、Value投影。Llama 2的70B模型引入了Grouped Query Attention (GQA)：多个Query头共享同一组Key-Value头。</span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a>具体来说，Llama 2-70B有64个Query头，但只有8个Key-Value头组（每组对应8个Query头）。这大幅减少了KV Cache的大小，提升了推理效率：</span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a>\text{KV Cache大小} = 2 \times n_{layers} \times n_{kv<span class="sc">\_</span>heads} \times d_{head} \times seq<span class="sc">\_</span>len</span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a>GQA将$n_{kv<span class="sc">\_</span>heads}$从64减少到8，KV Cache减少为原来的1/8。</span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a>**安全对齐**</span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a>Llama 2最重要的贡献之一是详细公开了安全对齐流程。论文描述了完整的RLHF管线：</span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**SFT阶段**：用约27K高质量对话数据监督微调</span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Reward Model**：用超过1M的人类偏好对比数据训练奖励模型</span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**RLHF阶段**：用PPO算法优化，同时训练两个奖励模型（有用性和安全性）</span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Red Teaming**：350+人的红队测试，覆盖多种攻击场景</span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a>这种透明度对安全研究意义重大——研究者第一次能够详细了解一个生产级模型的对齐流程。</span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a><span class="fu">### Llama 3：追平GPT-4</span></span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a>2024年4月，Meta发布Llama 3，标志着开源模型正式进入"GPT-4级别"。</span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a>**数据规模的飞跃**</span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a>Llama 3在超过15T tokens上训练——是Llama 2的7.5倍。更重要的是，这15T tokens经过了极其严格的质量过滤：</span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**启发式过滤**：去除低质量网页</span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**NSFW过滤**：使用分类器过滤不安全内容</span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**语义去重**：使用MinHash去除近似重复</span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**数据混合**：精心平衡不同领域（网页、代码、数学等）</span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a>论文指出一个反直觉的发现：即使8B模型的Chinchilla-optimal训练量约为200B tokens，但继续训练到15T tokens仍然能持续提升性能。这说明传统的scaling law可能低估了过度训练的价值。</span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a>**架构微调**</span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a>Llama 3的架构与Llama 2基本相同，但做了几处微调：</span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**词表扩大**：从32K增加到128K，提升多语言和代码能力</span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GQA全面采用**：8B和70B模型都使用GQA</span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**上下文长度**：从4K扩展到8K，后续版本扩展到128K</span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a>**后训练流程**</span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a>Llama 3的后训练比Llama 2更加复杂：</span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**多轮SFT**：不是一次微调，而是多轮迭代</span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**合成数据**：大量使用模型生成的数据进行训练</span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**DPO替代PPO**：在部分阶段使用更稳定的DPO</span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**专项能力训练**：针对代码、数学、推理、工具使用等分别优化</span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a>**性能表现**</span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a>在多项基准测试上，Llama 3-405B与GPT-4相当或更好：</span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 基准 <span class="pp">|</span> Llama 3 405B <span class="pp">|</span> GPT-4 <span class="pp">|</span></span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------------|-------|</span></span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MMLU <span class="pp">|</span> 88.6% <span class="pp">|</span> 86.4% <span class="pp">|</span></span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> HumanEval <span class="pp">|</span> 89.0% <span class="pp">|</span> 67.0% <span class="pp">|</span></span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GSM8K <span class="pp">|</span> 96.8% <span class="pp">|</span> 92.0% <span class="pp">|</span></span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MATH <span class="pp">|</span> 73.8% <span class="pp">|</span> 52.9% <span class="pp">|</span></span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a>这是开源模型首次在全面基准上达到闭源顶级模型的水平。</span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a><span class="fu">### Llama系列的技术演进总结</span></span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a>让我们用一张表格总结LLaMA到Llama 3的技术演进：</span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特性 <span class="pp">|</span> LLaMA (2023.02) <span class="pp">|</span> Llama 2 (2023.07) <span class="pp">|</span> Llama 3 (2024.04) <span class="pp">|</span></span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------------|-------------------|-------------------|</span></span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **最大规模** <span class="pp">|</span> 65B <span class="pp">|</span> 70B <span class="pp">|</span> 405B <span class="pp">|</span></span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **训练数据** <span class="pp">|</span> 1.0–1.4T tokens <span class="pp">|</span> 2.0T tokens <span class="pp">|</span> 15T+ tokens <span class="pp">|</span></span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **上下文** <span class="pp">|</span> 2,048 <span class="pp">|</span> 4,096 <span class="pp">|</span> 8,192 → 128K <span class="pp">|</span></span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **词表大小** <span class="pp">|</span> 32K <span class="pp">|</span> 32K <span class="pp">|</span> 128K <span class="pp">|</span></span>
<span id="cb6-340"><a href="#cb6-340" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **KV设计** <span class="pp">|</span> MHA <span class="pp">|</span> GQA (70B only) <span class="pp">|</span> GQA (全系列) <span class="pp">|</span></span>
<span id="cb6-341"><a href="#cb6-341" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **对齐方法** <span class="pp">|</span> 无/基础 <span class="pp">|</span> RLHF (PPO) <span class="pp">|</span> SFT + DPO + PPO <span class="pp">|</span></span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **开源许可** <span class="pp">|</span> 研究限制 <span class="pp">|</span> 商业许可 <span class="pp">|</span> 商业许可 <span class="pp">|</span></span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mistral系列：效率优先</span></span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a><span class="fu">### 背景：来自DeepMind的创业</span></span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a>2023年，几位前DeepMind和Meta的研究者在巴黎创立了Mistral AI。他们的目标是构建高效、开放的语言模型。</span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a>与Meta的"大就是好"策略不同，Mistral专注于**效率**——用更少的参数实现更好的性能。</span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mistral 7B：小模型的逆袭</span></span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a>2023年10月，Mistral发布了Mistral 7B，一个只有7B参数但性能超越Llama 2-13B的模型。</span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a>**核心创新一：Sliding Window Attention (SWA)**</span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a>标准Attention的复杂度是$O(n^2)$，其中$n$是序列长度。Mistral引入了滑动窗口注意力：每个位置只关注前$W$个位置（$W$是窗口大小），复杂度变为$O(n \cdot W)$。</span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a><span class="al">![Sliding Window Attention机制。左图：标准因果注意力（全部可见）；中图：滑动窗口注意力（只关注窗口内）；右图：多层堆叠后的有效感受野——通过逐层传播，信息可以覆盖更大范围。](figures/chapter-29/original/fig1-mistral-sliding-window.png)</span>{#fig-swa width=90%}</span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a>*Source: Jiang et al. (2023) "Mistral 7B", Figure 1. [arXiv:2310.06825](https://arxiv.org/abs/2310.06825)*</span>
<span id="cb6-366"><a href="#cb6-366" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-367"><a href="#cb6-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a>但这不意味着长距离信息被丢弃。由于多层堆叠，信息可以逐层传播：</span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>第1层：位置$i$能看到$<span class="co">[</span><span class="ot">i-W, i</span><span class="co">]</span>$</span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>第2层：位置$i$能看到$<span class="co">[</span><span class="ot">i-2W, i</span><span class="co">]</span>$（通过中间位置传递）</span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>第$L$层：位置$i$能看到$<span class="co">[</span><span class="ot">i-L \cdot W, i</span><span class="co">]</span>$</span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a>对于Mistral 7B，$W = 4096$，$L = 32$，有效感受野达到$32 \times 4096 \approx 131K$ tokens。</span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a>**核心创新二：高效KV Cache**</span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a>SWA的另一个优势是KV Cache可以被限制在窗口大小内。当序列长度超过窗口时，旧的KV可以被丢弃（Rolling Buffer Cache）：</span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a><span class="in">位置 0  1  2  3  4  5  6  7  8  ...</span></span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a><span class="in">Cache         [3  4  5  6]       &lt;- 窗口大小=4，只保留最近4个</span></span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a>这让推理时的内存占用保持恒定，非常适合长序列生成。</span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a>**性能对比**</span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 参数量 <span class="pp">|</span> MMLU <span class="pp">|</span> HumanEval <span class="pp">|</span> 数学推理 <span class="pp">|</span></span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|------|-----------|----------|</span></span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Llama 2-7B <span class="pp">|</span> 7B <span class="pp">|</span> 45.3% <span class="pp">|</span> 12.8% <span class="pp">|</span> 13.5% <span class="pp">|</span></span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Llama 2-13B <span class="pp">|</span> 13B <span class="pp">|</span> 54.8% <span class="pp">|</span> 18.3% <span class="pp">|</span> 22.1% <span class="pp">|</span></span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Mistral 7B** | **7B** | **60.1%** | **30.5%** | **28.4%** <span class="pp">|</span></span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a>用7B参数超越13B模型，这是Mistral "效率优先"理念的最佳证明。</span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mixtral 8x7B：开源MoE的标杆</span></span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a>2024年1月，Mistral发布Mixtral 8x7B，一个基于Mixture of Experts (MoE)的模型。</span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a>**架构设计**</span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a>Mixtral的基础架构与Mistral 7B相同，但每个FFN层被替换为8个Expert：</span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a><span class="al">![Mixtral的Mixture of Experts层架构。每个输入向量由Router分配给8个Expert中的2个，输出是两个选中Expert输出的加权和。这种设计使得总参数量达到46.7B，但每个token只激活12.9B参数。](figures/chapter-29/original/fig2-mixtral-moe-architecture.png)</span>{#fig-mixtral-moe width=85%}</span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a>*Source: Jiang et al. (2024) "Mixtral of Experts", Figure 1. [arXiv:2401.04088](https://arxiv.org/abs/2401.04088)*</span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数量**：46.7B（8个Expert × 7B）</span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**激活参数量**：12.9B（每个token激活2个Expert）</span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Router**：Top-2选择，即每个token选择得分最高的2个Expert</span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a>这种设计结合了我们在第27章讨论的MoE思想：用稀疏激活实现参数量与计算量的解耦。</span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a>**性能表现**</span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a>Mixtral在多项任务上与Llama 2-70B相当或更好，但推理时只使用约1/5的计算量：</span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 激活参数 <span class="pp">|</span> MMLU <span class="pp">|</span> MT-Bench <span class="pp">|</span></span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|------|----------|</span></span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Llama 2-70B <span class="pp">|</span> 70B <span class="pp">|</span> 68.9% <span class="pp">|</span> 6.86 <span class="pp">|</span></span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Mixtral 8x7B** | **12.9B** | **70.6%** | **8.30** <span class="pp">|</span></span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a>特别值得注意的是MT-Bench分数——这是一个对话质量评测，Mixtral-Instruct达到8.30分，超过了当时的GPT-3.5-Turbo。</span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mistral的设计哲学</span></span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a>从Mistral 7B到Mixtral，我们可以总结Mistral的核心设计哲学：</span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**效率第一**：不追求最大参数量，而是追求参数效率</span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**实用主义**：选择已验证有效的技术（GQA、SWA、MoE），而非追求新奇</span>
<span id="cb6-434"><a href="#cb6-434" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**开放透明**：Apache 2.0许可，完全商业可用</span>
<span id="cb6-435"><a href="#cb6-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a><span class="fu">## 多极化发展：Qwen、DeepSeek与更多</span></span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### 全球竞争格局</span></span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a>2024年，开源大模型进入多极化时代。除了Meta和Mistral，中国的阿里、DeepSeek、百川等也加入竞争：</span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 厂商 <span class="pp">|</span> 代表模型 <span class="pp">|</span> 最大规模 <span class="pp">|</span> 特点 <span class="pp">|</span></span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|----------|------|</span></span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Meta <span class="pp">|</span> Llama 3 <span class="pp">|</span> 405B <span class="pp">|</span> 全面基准领先 <span class="pp">|</span></span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Mistral AI <span class="pp">|</span> Mixtral <span class="pp">|</span> 46.7B (12.9B active) <span class="pp">|</span> 效率标杆 <span class="pp">|</span></span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 阿里云 <span class="pp">|</span> Qwen2.5 <span class="pp">|</span> 72B + Max (MoE) <span class="pp">|</span> 多语言、代码 <span class="pp">|</span></span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> DeepSeek <span class="pp">|</span> DeepSeek-V3 <span class="pp">|</span> 671B (37B active) <span class="pp">|</span> 训练效率极致 <span class="pp">|</span></span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 01.AI <span class="pp">|</span> Yi-1.5 <span class="pp">|</span> 34B <span class="pp">|</span> 中英双语 <span class="pp">|</span></span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Stability AI <span class="pp">|</span> StableLM <span class="pp">|</span> 12B <span class="pp">|</span> 边缘部署 <span class="pp">|</span></span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a><span class="fu">### Qwen系列</span></span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a>阿里云的Qwen（通义千问）系列是中国开源LLM的代表。</span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a>**Qwen2.5的技术亮点**</span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a>Qwen2.5于2024年9月发布，覆盖0.5B到72B多个规模：</span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**多语言能力**：支持29种语言，中英文尤其强大</span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**长上下文**：支持128K上下文</span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**代码能力**：专门的Qwen2.5-Coder变体</span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**数学能力**：专门的Qwen2.5-Math变体</span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a>**Qwen2.5-Max**</span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a>2025年1月，阿里发布Qwen2.5-Max，一个大规模MoE模型。据报道，它在Arena-Hard、LiveBench等基准上超越了DeepSeek-V3和GPT-4o，展示了中国模型的快速进步。</span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a><span class="fu">### DeepSeek系列</span></span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a>DeepSeek是一家中国AI初创公司，以极致的训练效率著称。</span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a>**DeepSeek-V3的技术创新**</span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a>2024年12月发布的DeepSeek-V3是一个技术上极具创新性的模型：</span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a>**Multi-head Latent Attention (MLA)**</span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a>这是DeepSeek-V2引入的高效注意力变体。MLA的核心思想是：将Key和Value压缩到一个低维的"隐空间"，然后从这个隐空间恢复完整的KV。</span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a>\text{传统GQA:} \quad K = W_K h, \quad V = W_V h</span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a>\text{MLA:} \quad c = W_C h, \quad K = W_{KD} c, \quad V = W_{VD} c</span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a>其中$c$是一个低维的压缩向量。这进一步减少了KV Cache的大小。</span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a>**DeepSeekMoE**</span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a>DeepSeek使用细粒度的Expert设计：</span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**共享Expert**：所有token都使用的专家，捕获通用模式</span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**路由Expert**：根据token内容动态选择的专家</span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a>结合这两种Expert，DeepSeek-V3实现了更好的负载均衡和更稳定的训练。</span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a>**训练效率的极致**</span>
<span id="cb6-502"><a href="#cb6-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-503"><a href="#cb6-503" aria-hidden="true" tabindex="-1"></a>DeepSeek-V3最惊人的是其训练成本：</span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数**：671B（37B激活）</span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**训练Token**：14.8T</span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**训练成本**：仅2.788M H800 GPU hours</span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a>对比：Llama 3-405B的训练成本据估计超过30M GPU hours。DeepSeek用不到1/10的成本达到了相近的性能水平。</span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a>这种效率来自于：</span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>MLA大幅减少内存带宽需求</span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>辅助损失免费(auxiliary-loss-free)的负载均衡</span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>FP8混合精度训练</span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>极致的工程优化</span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a><span class="fu">## 开源 vs 闭源的博弈</span></span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心权衡</span></span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a>开源和闭源大模型各有优劣，选择取决于具体需求：</span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-525"><a href="#cb6-525" aria-hidden="true" tabindex="-1"></a>**开源模型的优势**</span>
<span id="cb6-526"><a href="#cb6-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 优势 <span class="pp">|</span> 说明 <span class="pp">|</span></span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **成本** <span class="pp">|</span> 自托管避免API费用；可根据需求选择模型规模 <span class="pp">|</span></span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **定制** <span class="pp">|</span> 可以针对特定领域微调；可以修改架构 <span class="pp">|</span></span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **隐私** <span class="pp">|</span> 数据不离开本地；满足合规要求 <span class="pp">|</span></span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **透明** <span class="pp">|</span> 可以审计模型行为；可以进行安全研究 <span class="pp">|</span></span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **研究** <span class="pp">|</span> 可以复现实验；可以进行消融分析 <span class="pp">|</span></span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **长期** <span class="pp">|</span> 不受供应商策略变化影响 <span class="pp">|</span></span>
<span id="cb6-535"><a href="#cb6-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-536"><a href="#cb6-536" aria-hidden="true" tabindex="-1"></a>**闭源模型的优势**</span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 优势 <span class="pp">|</span> 说明 <span class="pp">|</span></span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **性能** <span class="pp">|</span> 顶级闭源模型（GPT-4、Claude）仍领先 <span class="pp">|</span></span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **支持** <span class="pp">|</span> 专业团队提供技术支持 <span class="pp">|</span></span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **简单** <span class="pp">|</span> 无需管理基础设施 <span class="pp">|</span></span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **安全** <span class="pp">|</span> 持续的安全监控和更新 <span class="pp">|</span></span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **合规** <span class="pp">|</span> SOC 2、HIPAA等企业级合规 <span class="pp">|</span></span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a><span class="fu">### 趋势：差距正在缩小</span></span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a>2024–2025年的数据显示，开源模型正在快速追赶：</span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a><span class="in">2023初: GPT-4 &gt;&gt;&gt; 开源最佳 (差距巨大)</span></span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a><span class="in">2023中: GPT-4 &gt;&gt; Llama 2-70B (显著差距)</span></span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a><span class="in">2024初: GPT-4 &gt; Mixtral/Llama 3 (差距缩小)</span></span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a><span class="in">2024末: GPT-4 ≈ Llama 3-405B/DeepSeek-V3 (接近持平)</span></span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a>根据a16z的调研，41%的企业计划增加开源模型使用，另外41%表示如果开源性能追上会考虑切换。</span>
<span id="cb6-558"><a href="#cb6-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-559"><a href="#cb6-559" aria-hidden="true" tabindex="-1"></a><span class="fu">### 混合策略</span></span>
<span id="cb6-560"><a href="#cb6-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-561"><a href="#cb6-561" aria-hidden="true" tabindex="-1"></a>实践中，许多组织采用混合策略：</span>
<span id="cb6-562"><a href="#cb6-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-563"><a href="#cb6-563" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**研发阶段**：用开源模型快速原型和实验</span>
<span id="cb6-564"><a href="#cb6-564" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**生产阶段**：根据具体需求选择</span>
<span id="cb6-565"><a href="#cb6-565" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>通用对话 → 可能继续用开源</span>
<span id="cb6-566"><a href="#cb6-566" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>复杂推理 → 可能调用GPT-4</span>
<span id="cb6-567"><a href="#cb6-567" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>隐私敏感 → 必须用开源自托管</span>
<span id="cb6-568"><a href="#cb6-568" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**长期规划**：随着开源能力提升，逐步迁移</span>
<span id="cb6-569"><a href="#cb6-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-570"><a href="#cb6-570" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-571"><a href="#cb6-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-572"><a href="#cb6-572" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：本地部署开源模型</span></span>
<span id="cb6-573"><a href="#cb6-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-574"><a href="#cb6-574" aria-hidden="true" tabindex="-1"></a><span class="fu">### 选择合适的模型</span></span>
<span id="cb6-575"><a href="#cb6-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-576"><a href="#cb6-576" aria-hidden="true" tabindex="-1"></a>部署前需要考虑几个因素：</span>
<span id="cb6-577"><a href="#cb6-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-578"><a href="#cb6-578" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 因素 <span class="pp">|</span> 考虑 <span class="pp">|</span></span>
<span id="cb6-579"><a href="#cb6-579" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb6-580"><a href="#cb6-580" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **任务复杂度** <span class="pp">|</span> 简单任务用小模型；复杂推理用大模型 <span class="pp">|</span></span>
<span id="cb6-581"><a href="#cb6-581" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **硬件限制** <span class="pp">|</span> GPU显存、CPU内存、存储空间 <span class="pp">|</span></span>
<span id="cb6-582"><a href="#cb6-582" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **延迟要求** <span class="pp">|</span> 实时应用需要快速响应 <span class="pp">|</span></span>
<span id="cb6-583"><a href="#cb6-583" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **精度要求** <span class="pp">|</span> 是否可接受量化损失 <span class="pp">|</span></span>
<span id="cb6-584"><a href="#cb6-584" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **语言需求** <span class="pp">|</span> 中文优先考虑Qwen/DeepSeek <span class="pp">|</span></span>
<span id="cb6-585"><a href="#cb6-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-586"><a href="#cb6-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### 量化：让大模型跑在小设备上</span></span>
<span id="cb6-587"><a href="#cb6-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-588"><a href="#cb6-588" aria-hidden="true" tabindex="-1"></a>量化将模型权重从FP16（16位）压缩到更低精度：</span>
<span id="cb6-589"><a href="#cb6-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-590"><a href="#cb6-590" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 量化方式 <span class="pp">|</span> 显存需求(7B) <span class="pp">|</span> 质量损失 <span class="pp">|</span> 适用场景 <span class="pp">|</span></span>
<span id="cb6-591"><a href="#cb6-591" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|--------------|----------|----------|</span></span>
<span id="cb6-592"><a href="#cb6-592" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FP16 <span class="pp">|</span> ~14GB <span class="pp">|</span> 无 <span class="pp">|</span> 有高端GPU <span class="pp">|</span></span>
<span id="cb6-593"><a href="#cb6-593" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> INT8 <span class="pp">|</span> ~7GB <span class="pp">|</span> 极小 <span class="pp">|</span> 消费级GPU <span class="pp">|</span></span>
<span id="cb6-594"><a href="#cb6-594" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> INT4 (GPTQ/AWQ) <span class="pp">|</span> ~4GB <span class="pp">|</span> 小 <span class="pp">|</span> 移动/边缘 <span class="pp">|</span></span>
<span id="cb6-595"><a href="#cb6-595" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GGUF (llama.cpp) <span class="pp">|</span> ~4GB <span class="pp">|</span> 小 <span class="pp">|</span> CPU推理 <span class="pp">|</span></span>
<span id="cb6-596"><a href="#cb6-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-597"><a href="#cb6-597" aria-hidden="true" tabindex="-1"></a><span class="fu">### 推理框架选择</span></span>
<span id="cb6-598"><a href="#cb6-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-599"><a href="#cb6-599" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 框架 <span class="pp">|</span> 特点 <span class="pp">|</span> 适用场景 <span class="pp">|</span></span>
<span id="cb6-600"><a href="#cb6-600" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|----------|</span></span>
<span id="cb6-601"><a href="#cb6-601" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **vLLM** <span class="pp">|</span> PagedAttention，高吞吐 <span class="pp">|</span> 高并发服务 <span class="pp">|</span></span>
<span id="cb6-602"><a href="#cb6-602" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **TGI** <span class="pp">|</span> HuggingFace官方，易用 <span class="pp">|</span> 快速部署 <span class="pp">|</span></span>
<span id="cb6-603"><a href="#cb6-603" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **llama.cpp** <span class="pp">|</span> 纯CPU/Metal支持 <span class="pp">|</span> 边缘设备 <span class="pp">|</span></span>
<span id="cb6-604"><a href="#cb6-604" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Ollama** <span class="pp">|</span> 一键安装，极简 <span class="pp">|</span> 个人使用 <span class="pp">|</span></span>
<span id="cb6-605"><a href="#cb6-605" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **TensorRT-LLM** <span class="pp">|</span> NVIDIA优化 <span class="pp">|</span> A100/H100 <span class="pp">|</span></span>
<span id="cb6-606"><a href="#cb6-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-607"><a href="#cb6-607" aria-hidden="true" tabindex="-1"></a><span class="fu">### 快速开始示例</span></span>
<span id="cb6-608"><a href="#cb6-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-609"><a href="#cb6-609" aria-hidden="true" tabindex="-1"></a>**使用Ollama（最简单）**</span>
<span id="cb6-610"><a href="#cb6-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-611"><a href="#cb6-611" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb6-612"><a href="#cb6-612" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装Ollama</span></span>
<span id="cb6-613"><a href="#cb6-613" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span>
<span id="cb6-614"><a href="#cb6-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-615"><a href="#cb6-615" aria-hidden="true" tabindex="-1"></a><span class="co"># 运行Llama 3</span></span>
<span id="cb6-616"><a href="#cb6-616" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama3</span>
<span id="cb6-617"><a href="#cb6-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-618"><a href="#cb6-618" aria-hidden="true" tabindex="-1"></a><span class="co"># 运行Mistral</span></span>
<span id="cb6-619"><a href="#cb6-619" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run mistral</span>
<span id="cb6-620"><a href="#cb6-620" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-621"><a href="#cb6-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-622"><a href="#cb6-622" aria-hidden="true" tabindex="-1"></a>**使用vLLM（高性能服务）**</span>
<span id="cb6-623"><a href="#cb6-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-624"><a href="#cb6-624" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-625"><a href="#cb6-625" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm <span class="im">import</span> LLM, SamplingParams</span>
<span id="cb6-626"><a href="#cb6-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-627"><a href="#cb6-627" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb6-628"><a href="#cb6-628" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(model<span class="op">=</span><span class="st">"meta-llama/Llama-3.1-8B-Instruct"</span>)</span>
<span id="cb6-629"><a href="#cb6-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-630"><a href="#cb6-630" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成</span></span>
<span id="cb6-631"><a href="#cb6-631" aria-hidden="true" tabindex="-1"></a>sampling_params <span class="op">=</span> SamplingParams(temperature<span class="op">=</span><span class="fl">0.7</span>, top_p<span class="op">=</span><span class="fl">0.9</span>, max_tokens<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb6-632"><a href="#cb6-632" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm.generate([<span class="st">"What is the capital of France?"</span>], sampling_params)</span>
<span id="cb6-633"><a href="#cb6-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-634"><a href="#cb6-634" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>].outputs[<span class="dv">0</span>].text)</span>
<span id="cb6-635"><a href="#cb6-635" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-636"><a href="#cb6-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-637"><a href="#cb6-637" aria-hidden="true" tabindex="-1"></a>**使用Transformers + BitsAndBytes（量化）**</span>
<span id="cb6-638"><a href="#cb6-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-639"><a href="#cb6-639" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-640"><a href="#cb6-640" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig</span>
<span id="cb6-641"><a href="#cb6-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-642"><a href="#cb6-642" aria-hidden="true" tabindex="-1"></a><span class="co"># 4-bit量化配置</span></span>
<span id="cb6-643"><a href="#cb6-643" aria-hidden="true" tabindex="-1"></a>quantization_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb6-644"><a href="#cb6-644" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-645"><a href="#cb6-645" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb6-646"><a href="#cb6-646" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">"nf4"</span>,</span>
<span id="cb6-647"><a href="#cb6-647" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-648"><a href="#cb6-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-649"><a href="#cb6-649" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载量化模型</span></span>
<span id="cb6-650"><a href="#cb6-650" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-651"><a href="#cb6-651" aria-hidden="true" tabindex="-1"></a>    <span class="st">"meta-llama/Llama-3.1-8B-Instruct"</span>,</span>
<span id="cb6-652"><a href="#cb6-652" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>quantization_config,</span>
<span id="cb6-653"><a href="#cb6-653" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb6-654"><a href="#cb6-654" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-655"><a href="#cb6-655" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"meta-llama/Llama-3.1-8B-Instruct"</span>)</span>
<span id="cb6-656"><a href="#cb6-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-657"><a href="#cb6-657" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成</span></span>
<span id="cb6-658"><a href="#cb6-658" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(<span class="st">"Hello, how are you?"</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb6-659"><a href="#cb6-659" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, max_new_tokens<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb6-660"><a href="#cb6-660" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>]))</span>
<span id="cb6-661"><a href="#cb6-661" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-662"><a href="#cb6-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-663"><a href="#cb6-663" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-664"><a href="#cb6-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-665"><a href="#cb6-665" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb6-666"><a href="#cb6-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-667"><a href="#cb6-667" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么开源模型能追上闭源？</span></span>
<span id="cb6-668"><a href="#cb6-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-669"><a href="#cb6-669" aria-hidden="true" tabindex="-1"></a>从技术角度，开源模型的追赶得益于几个因素：</span>
<span id="cb6-670"><a href="#cb6-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-671"><a href="#cb6-671" aria-hidden="true" tabindex="-1"></a>**因素一：架构已经收敛**</span>
<span id="cb6-672"><a href="#cb6-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-673"><a href="#cb6-673" aria-hidden="true" tabindex="-1"></a>2024年的大模型架构基本统一：Transformer Decoder + RMSNorm + SwiGLU + RoPE + GQA。这意味着架构不再是差异化因素——开源模型可以直接采用最佳实践。</span>
<span id="cb6-674"><a href="#cb6-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-675"><a href="#cb6-675" aria-hidden="true" tabindex="-1"></a>**因素二：训练技术的开放**</span>
<span id="cb6-676"><a href="#cb6-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-677"><a href="#cb6-677" aria-hidden="true" tabindex="-1"></a>关键的训练技术（如RLHF、DPO、合成数据生成）已经通过论文和开源代码公开。虽然细节仍有差异，但核心方法论是共享的。</span>
<span id="cb6-678"><a href="#cb6-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-679"><a href="#cb6-679" aria-hidden="true" tabindex="-1"></a>**因素三：规模效应**</span>
<span id="cb6-680"><a href="#cb6-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-681"><a href="#cb6-681" aria-hidden="true" tabindex="-1"></a>当Meta、Mistral、阿里投入大量资源训练开源模型时，它们实际上在与闭源厂商进行同等规模的竞争。DeepSeek-V3的14.8T训练数据量与GPT-4在同一数量级。</span>
<span id="cb6-682"><a href="#cb6-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-683"><a href="#cb6-683" aria-hidden="true" tabindex="-1"></a>**因素四：社区放大效应**</span>
<span id="cb6-684"><a href="#cb6-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-685"><a href="#cb6-685" aria-hidden="true" tabindex="-1"></a>开源模型有一个独特优势：社区贡献。当Llama发布后，成百上千的研究者在其基础上实验、改进、反馈。这种分布式创新是闭源模型无法复制的。</span>
<span id="cb6-686"><a href="#cb6-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-687"><a href="#cb6-687" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放问题</span></span>
<span id="cb6-688"><a href="#cb6-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-689"><a href="#cb6-689" aria-hidden="true" tabindex="-1"></a>**问题一：开源的边界在哪里？**</span>
<span id="cb6-690"><a href="#cb6-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-691"><a href="#cb6-691" aria-hidden="true" tabindex="-1"></a>"开源"的定义本身就有争议：</span>
<span id="cb6-692"><a href="#cb6-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-693"><a href="#cb6-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>权重开源 ≠ 训练代码开源 ≠ 数据开源</span>
<span id="cb6-694"><a href="#cb6-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Llama 3的许可证禁止用于"10亿月活用户以上的产品"</span>
<span id="cb6-695"><a href="#cb6-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>这还算真正的"开源"吗？</span>
<span id="cb6-696"><a href="#cb6-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-697"><a href="#cb6-697" aria-hidden="true" tabindex="-1"></a>**问题二：安全与开放的张力**</span>
<span id="cb6-698"><a href="#cb6-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-699"><a href="#cb6-699" aria-hidden="true" tabindex="-1"></a>开源模型可以被任何人修改——包括移除安全对齐。这带来了双重使用风险：</span>
<span id="cb6-700"><a href="#cb6-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-701"><a href="#cb6-701" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>正面：安全研究者可以测试防御</span>
<span id="cb6-702"><a href="#cb6-702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>负面：恶意行为者可以创建有害变体</span>
<span id="cb6-703"><a href="#cb6-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-704"><a href="#cb6-704" aria-hidden="true" tabindex="-1"></a>如何平衡开放性和安全性？目前没有共识。</span>
<span id="cb6-705"><a href="#cb6-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-706"><a href="#cb6-706" aria-hidden="true" tabindex="-1"></a>**问题三：可持续性**</span>
<span id="cb6-707"><a href="#cb6-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-708"><a href="#cb6-708" aria-hidden="true" tabindex="-1"></a>训练一个前沿模型需要数千万美元。Meta可以承担（作为开源的战略投入），但这种模式是否可持续？开源模型是否会永远依赖少数大公司的"善意"？</span>
<span id="cb6-709"><a href="#cb6-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-710"><a href="#cb6-710" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-711"><a href="#cb6-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-712"><a href="#cb6-712" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb6-713"><a href="#cb6-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-714"><a href="#cb6-714" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开源模型的局限</span></span>
<span id="cb6-715"><a href="#cb6-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-716"><a href="#cb6-716" aria-hidden="true" tabindex="-1"></a>**能力边界**</span>
<span id="cb6-717"><a href="#cb6-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-718"><a href="#cb6-718" aria-hidden="true" tabindex="-1"></a>尽管Llama 3-405B在许多基准上追平GPT-4，但在某些任务上仍有差距：</span>
<span id="cb6-719"><a href="#cb6-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-720"><a href="#cb6-720" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>极复杂的多步推理</span>
<span id="cb6-721"><a href="#cb6-721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>最新知识（训练数据截止）</span>
<span id="cb6-722"><a href="#cb6-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>多模态理解（图像、音频、视频）</span>
<span id="cb6-723"><a href="#cb6-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-724"><a href="#cb6-724" aria-hidden="true" tabindex="-1"></a>**工程复杂度**</span>
<span id="cb6-725"><a href="#cb6-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-726"><a href="#cb6-726" aria-hidden="true" tabindex="-1"></a>部署开源模型需要：</span>
<span id="cb6-727"><a href="#cb6-727" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>GPU基础设施</span>
<span id="cb6-728"><a href="#cb6-728" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>运维能力</span>
<span id="cb6-729"><a href="#cb6-729" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>安全监控</span>
<span id="cb6-730"><a href="#cb6-730" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>持续更新</span>
<span id="cb6-731"><a href="#cb6-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-732"><a href="#cb6-732" aria-hidden="true" tabindex="-1"></a>这对小团队是显著的负担。</span>
<span id="cb6-733"><a href="#cb6-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-734"><a href="#cb6-734" aria-hidden="true" tabindex="-1"></a>**碎片化**</span>
<span id="cb6-735"><a href="#cb6-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-736"><a href="#cb6-736" aria-hidden="true" tabindex="-1"></a>开源生态的碎片化也是问题：</span>
<span id="cb6-737"><a href="#cb6-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>模型太多，难以选择</span>
<span id="cb6-738"><a href="#cb6-738" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>版本迭代快，难以跟踪</span>
<span id="cb6-739"><a href="#cb6-739" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>不同框架之间兼容性问题</span>
<span id="cb6-740"><a href="#cb6-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-741"><a href="#cb6-741" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb6-742"><a href="#cb6-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-743"><a href="#cb6-743" aria-hidden="true" tabindex="-1"></a>开源模型的局限指向几个发展方向：</span>
<span id="cb6-744"><a href="#cb6-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-745"><a href="#cb6-745" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**更易用的部署工具**：Ollama、Replicate等正在降低使用门槛</span>
<span id="cb6-746"><a href="#cb6-746" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**模型选择指导**：benchmark聚合、自动选择服务</span>
<span id="cb6-747"><a href="#cb6-747" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**多模态开源**：LLaVA、Qwen-VL等正在补齐能力</span>
<span id="cb6-748"><a href="#cb6-748" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**持续对齐**：社区驱动的安全研究和红队测试</span>
<span id="cb6-749"><a href="#cb6-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-750"><a href="#cb6-750" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-751"><a href="#cb6-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-752"><a href="#cb6-752" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb6-753"><a href="#cb6-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-754"><a href="#cb6-754" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb6-755"><a href="#cb6-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-756"><a href="#cb6-756" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：最先进的语言模型长期被少数公司垄断，限制了研究、创新和应用。</span>
<span id="cb6-757"><a href="#cb6-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-758"><a href="#cb6-758" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**突破**：LLaMA（2023.02）的发布打破了这一格局，证明开源模型可以达到商业级水平。</span>
<span id="cb6-759"><a href="#cb6-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-760"><a href="#cb6-760" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**技术演进**：</span>
<span id="cb6-761"><a href="#cb6-761" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>LLaMA确立了"标准配置"：RMSNorm + SwiGLU + RoPE</span>
<span id="cb6-762"><a href="#cb6-762" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Llama 2引入GQA、完善安全对齐</span>
<span id="cb6-763"><a href="#cb6-763" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Llama 3通过规模化训练达到GPT-4水平</span>
<span id="cb6-764"><a href="#cb6-764" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Mistral展示了效率优先的可能</span>
<span id="cb6-765"><a href="#cb6-765" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>DeepSeek证明了极致的训练效率</span>
<span id="cb6-766"><a href="#cb6-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-767"><a href="#cb6-767" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**意义**：开源模型的繁荣改变了LLM生态——研究更开放、创新更民主、应用更自由。</span>
<span id="cb6-768"><a href="#cb6-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-769"><a href="#cb6-769" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键技术速查</span></span>
<span id="cb6-770"><a href="#cb6-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-771"><a href="#cb6-771" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 技术 <span class="pp">|</span> 作用 <span class="pp">|</span> 首次大规模采用 <span class="pp">|</span></span>
<span id="cb6-772"><a href="#cb6-772" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|----------------|</span></span>
<span id="cb6-773"><a href="#cb6-773" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RMSNorm <span class="pp">|</span> 简化归一化，加速训练 <span class="pp">|</span> LLaMA <span class="pp">|</span></span>
<span id="cb6-774"><a href="#cb6-774" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SwiGLU <span class="pp">|</span> 更好的FFN激活函数 <span class="pp">|</span> LLaMA <span class="pp">|</span></span>
<span id="cb6-775"><a href="#cb6-775" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RoPE <span class="pp">|</span> 相对位置编码，支持外推 <span class="pp">|</span> LLaMA <span class="pp">|</span></span>
<span id="cb6-776"><a href="#cb6-776" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GQA <span class="pp">|</span> 减少KV Cache，加速推理 <span class="pp">|</span> Llama 2-70B <span class="pp">|</span></span>
<span id="cb6-777"><a href="#cb6-777" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SWA <span class="pp">|</span> 滑动窗口注意力，长序列 <span class="pp">|</span> Mistral 7B <span class="pp">|</span></span>
<span id="cb6-778"><a href="#cb6-778" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MoE <span class="pp">|</span> 稀疏激活，扩大容量 <span class="pp">|</span> Mixtral 8x7B <span class="pp">|</span></span>
<span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MLA <span class="pp">|</span> 压缩KV，极致效率 <span class="pp">|</span> DeepSeek-V2 <span class="pp">|</span></span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么SwiGLU需要三个权重矩阵而不是两个？这如何影响模型的参数效率设计？</span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[对比分析]** Mistral 7B如何用7B参数超越Llama 2-13B？滑动窗口注意力在其中起了什么作用？</span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 选择一个开源模型（如Llama 3-8B），使用vLLM或Ollama部署本地服务，测量不同量化精度下的推理延迟和生成质量。</span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** 开源模型的"开源"程度差异很大（仅权重 vs 完整训练流程 vs 包含数据）。你认为什么程度的开放才算"真正的开源"？这对研究和应用分别意味着什么？</span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Touvron et al. (2023)** "LLaMA: Open and Efficient Foundation Language Models"</span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2 (架构设计)、Table 2 (模型配置)</span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>开源LLM的开山之作</span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Touvron et al. (2023)** "Llama 2: Open Foundation and Fine-Tuned Chat Models"</span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3 (RLHF)、Section 4 (安全对齐)</span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>完整的安全对齐流程公开</span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Llama Team (2024)** "The Llama 3 Herd of Models"</span>
<span id="cb6-806"><a href="#cb6-806" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3 (数据)、Section 4 (后训练)</span>
<span id="cb6-807"><a href="#cb6-807" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>开源模型达到GPT-4水平的技术细节</span>
<span id="cb6-808"><a href="#cb6-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-809"><a href="#cb6-809" aria-hidden="true" tabindex="-1"></a><span class="fu">### 效率导向</span></span>
<span id="cb6-810"><a href="#cb6-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-811"><a href="#cb6-811" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Jiang et al. (2023)** "Mistral 7B"</span>
<span id="cb6-812"><a href="#cb6-812" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>滑动窗口注意力、高效推理设计</span>
<span id="cb6-813"><a href="#cb6-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-814"><a href="#cb6-814" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Jiang et al. (2024)** "Mixtral of Experts"</span>
<span id="cb6-815"><a href="#cb6-815" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>开源MoE的标杆实现</span>
<span id="cb6-816"><a href="#cb6-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-817"><a href="#cb6-817" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepSeek-AI (2024)** "DeepSeek-V3 Technical Report"</span>
<span id="cb6-818"><a href="#cb6-818" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>MLA、训练效率的极致优化</span>
<span id="cb6-819"><a href="#cb6-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-820"><a href="#cb6-820" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与分析</span></span>
<span id="cb6-821"><a href="#cb6-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-822"><a href="#cb6-822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cameron Wolfe** "LLaMA-2 from the Ground Up" — LLaMA架构的详细解释</span>
<span id="cb6-823"><a href="#cb6-823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sebastian Raschka** "The State of LLMs 2025" — 开源生态的全面梳理</span>
<span id="cb6-824"><a href="#cb6-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-825"><a href="#cb6-825" aria-hidden="true" tabindex="-1"></a><span class="fu">### 工具资源</span></span>
<span id="cb6-826"><a href="#cb6-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-827"><a href="#cb6-827" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Hub** — 开源模型权重和微调版本</span>
<span id="cb6-828"><a href="#cb6-828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**vLLM** — 高性能推理框架</span>
<span id="cb6-829"><a href="#cb6-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ollama** — 一键本地运行</span>
<span id="cb6-830"><a href="#cb6-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**llama.cpp** — CPU/边缘推理</span>
<span id="cb6-831"><a href="#cb6-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-832"><a href="#cb6-832" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-833"><a href="#cb6-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-834"><a href="#cb6-834" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb6-835"><a href="#cb6-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-836"><a href="#cb6-836" aria-hidden="true" tabindex="-1"></a>LLaMA的发布是一个充满戏剧性的事件。</span>
<span id="cb6-837"><a href="#cb6-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-838"><a href="#cb6-838" aria-hidden="true" tabindex="-1"></a>2023年2月24日，Meta发布了LLaMA论文和权重。最初，权重仅对学术研究者开放，需要填写申请表。但在发布后不到一周，权重文件就在网上泄露，通过torrent在研究社区中广泛传播。</span>
<span id="cb6-839"><a href="#cb6-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-840"><a href="#cb6-840" aria-hidden="true" tabindex="-1"></a>Meta面临一个微妙的处境：是追究泄露者，还是默许这种"意外的开源"？最终，Meta选择了后者。事实上，这次泄露可能是LLM历史上最具影响力的"事故"——它让全世界的研究者在同一起跑线上开始实验。</span>
<span id="cb6-841"><a href="#cb6-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-842"><a href="#cb6-842" aria-hidden="true" tabindex="-1"></a>泄露后的几周内，社区的创造力爆发了。斯坦福的Alpaca项目证明了用不到$500的成本就能微调出一个像样的指令模型；Vicuna声称达到了ChatGPT 90%的能力；各种针对特定任务的微调版本层出不穷。</span>
<span id="cb6-843"><a href="#cb6-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-844"><a href="#cb6-844" aria-hidden="true" tabindex="-1"></a>有趣的是，LLaMA的成功部分归功于它的"恰到好处"。65B参数够大，能够展示涌现能力；但又不像GPT-3的175B那么不可企及——研究者可以在几张A100上进行微调实验。</span>
<span id="cb6-845"><a href="#cb6-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-846"><a href="#cb6-846" aria-hidden="true" tabindex="-1"></a>Meta显然从LLaMA的经历中学到了教训。Llama 2的发布采用了完全不同的策略——从一开始就开放商业许可，主动拥抱开源社区。这种策略转变被认为是Meta在AI时代寻找自身定位的表现：既然无法与OpenAI在闭源产品上竞争，不如通过开源建立生态影响力。</span>
<span id="cb6-847"><a href="#cb6-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-848"><a href="#cb6-848" aria-hidden="true" tabindex="-1"></a>Mistral的故事则代表了另一种可能。这家由DeepMind和Meta校友创立的法国初创公司，选择了"效率优先"的路线。Mistral 7B用7B参数超越了Llama 2-13B，证明了"小而美"也是一条可行的路径。更有意思的是，Mistral在商业上也取得了成功——仅用一年就估值超过60亿美元。</span>
<span id="cb6-849"><a href="#cb6-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-850"><a href="#cb6-850" aria-hidden="true" tabindex="-1"></a>中国的开源模型阵营同样不可忽视。阿里的Qwen、DeepSeek、百川、智谱等公司相继发布了有竞争力的开源模型。DeepSeek-V3尤其引人注目——它用不到1/10的训练成本达到了接近GPT-4的水平，展示了工程优化可以在多大程度上弥补资源差距。</span>
<span id="cb6-851"><a href="#cb6-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-852"><a href="#cb6-852" aria-hidden="true" tabindex="-1"></a>回顾这段历史，开源大模型的崛起不是一个单一事件，而是多方力量共同作用的结果：Meta的战略选择、社区的创造力、学术界的需求、以及多个国家在AI领域的竞争意识。</span>
<span id="cb6-853"><a href="#cb6-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-854"><a href="#cb6-854" aria-hidden="true" tabindex="-1"></a>未来会怎样？一个可能的方向是：开源模型继续追赶，直到与闭源模型的差距变得微不足道。到那时，闭源模型将主要依靠服务质量、集成便利性和企业支持来维持竞争力，而核心能力将成为"公共品"。</span>
<span id="cb6-855"><a href="#cb6-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-856"><a href="#cb6-856" aria-hidden="true" tabindex="-1"></a>这或许是深度学习最初的承诺：知识应该是开放的，创新应该是共享的。LLaMA可能只是这个故事的开始。</span>
<span id="cb6-857"><a href="#cb6-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-858"><a href="#cb6-858" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-859"><a href="#cb6-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-860"><a href="#cb6-860" aria-hidden="true" tabindex="-1"></a>**Sources:**</span>
<span id="cb6-861"><a href="#cb6-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-862"><a href="#cb6-862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">LLaMA: Open and Efficient Foundation Language Models</span><span class="co">](https://arxiv.org/abs/2302.13971)</span></span>
<span id="cb6-863"><a href="#cb6-863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Llama 2: Open Foundation and Fine-Tuned Chat Models</span><span class="co">](https://arxiv.org/abs/2307.09288)</span></span>
<span id="cb6-864"><a href="#cb6-864" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">The Llama 3 Herd of Models</span><span class="co">](https://arxiv.org/abs/2407.21783)</span></span>
<span id="cb6-865"><a href="#cb6-865" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Mistral 7B</span><span class="co">](https://arxiv.org/abs/2310.06825)</span></span>
<span id="cb6-866"><a href="#cb6-866" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Mixtral of Experts</span><span class="co">](https://arxiv.org/abs/2401.04088)</span></span>
<span id="cb6-867"><a href="#cb6-867" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DeepSeek-V3 Technical Report</span><span class="co">](https://arxiv.org/abs/2412.19437)</span></span>
<span id="cb6-868"><a href="#cb6-868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">LLaMA Components: RMSNorm, SwiGLU, and RoPE</span><span class="co">](https://mbrenndoerfer.com/writing/llama-components-rmsnorm-swiglu-rope)</span></span>
<span id="cb6-869"><a href="#cb6-869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">2024 Comparison of Open-Source Vs Closed-Source LLMs</span><span class="co">](https://blog.spheron.network/choosing-the-right-llm-2024-comparison-of-open-source-vs-closed-source-llms)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>