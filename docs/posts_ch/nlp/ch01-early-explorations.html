<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-23">
<meta name="description" content="前深度学习时代的NLP技术演进：从规则系统到统计学习，从N-gram到CRF，以及为什么特征工程成为了整个领域的天花板。">

<title>第1章：语言理解的早期探索 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从第0章说起" id="toc-从第0章说起" class="nav-link active" data-scroll-target="#从第0章说起"><span class="header-section-number">1</span> 从第0章说起</a></li>
  <li><a href="#符号主义的辉煌与幻灭" id="toc-符号主义的辉煌与幻灭" class="nav-link" data-scroll-target="#符号主义的辉煌与幻灭"><span class="header-section-number">2</span> 符号主义的辉煌与幻灭</a>
  <ul class="collapse">
  <li><a href="#早期的梦想让机器理解语言" id="toc-早期的梦想让机器理解语言" class="nav-link" data-scroll-target="#早期的梦想让机器理解语言"><span class="header-section-number">2.1</span> 早期的梦想：让机器理解语言</a></li>
  <li><a href="#规则系统的成就" id="toc-规则系统的成就" class="nav-link" data-scroll-target="#规则系统的成就"><span class="header-section-number">2.2</span> 规则系统的成就</a></li>
  <li><a href="#规则系统的根本问题" id="toc-规则系统的根本问题" class="nav-link" data-scroll-target="#规则系统的根本问题"><span class="header-section-number">2.3</span> 规则系统的根本问题</a></li>
  </ul></li>
  <li><a href="#统计革命从规则到概率" id="toc-统计革命从规则到概率" class="nav-link" data-scroll-target="#统计革命从规则到概率"><span class="header-section-number">3</span> 统计革命：从规则到概率</a>
  <ul class="collapse">
  <li><a href="#数据驱动的范式转变" id="toc-数据驱动的范式转变" class="nav-link" data-scroll-target="#数据驱动的范式转变"><span class="header-section-number">3.1</span> 数据驱动的范式转变</a></li>
  <li><a href="#统计方法的基本思路" id="toc-统计方法的基本思路" class="nav-link" data-scroll-target="#统计方法的基本思路"><span class="header-section-number">3.2</span> 统计方法的基本思路</a></li>
  </ul></li>
  <li><a href="#n-gram与语言模型基础" id="toc-n-gram与语言模型基础" class="nav-link" data-scroll-target="#n-gram与语言模型基础"><span class="header-section-number">4</span> N-gram与语言模型基础</a>
  <ul class="collapse">
  <li><a href="#什么是语言模型" id="toc-什么是语言模型" class="nav-link" data-scroll-target="#什么是语言模型"><span class="header-section-number">4.1</span> 什么是语言模型？</a></li>
  <li><a href="#n-gram简单而有效" id="toc-n-gram简单而有效" class="nav-link" data-scroll-target="#n-gram简单而有效"><span class="header-section-number">4.2</span> N-gram：简单而有效</a></li>
  <li><a href="#n-gram的局限" id="toc-n-gram的局限" class="nav-link" data-scroll-target="#n-gram的局限"><span class="header-section-number">4.3</span> N-gram的局限</a></li>
  </ul></li>
  <li><a href="#序列标注从hmm到crf的演进" id="toc-序列标注从hmm到crf的演进" class="nav-link" data-scroll-target="#序列标注从hmm到crf的演进"><span class="header-section-number">5</span> 序列标注：从HMM到CRF的演进</a>
  <ul class="collapse">
  <li><a href="#序列标注问题" id="toc-序列标注问题" class="nav-link" data-scroll-target="#序列标注问题"><span class="header-section-number">5.1</span> 序列标注问题</a></li>
  <li><a href="#隐马尔可夫模型hmm" id="toc-隐马尔可夫模型hmm" class="nav-link" data-scroll-target="#隐马尔可夫模型hmm"><span class="header-section-number">5.2</span> 隐马尔可夫模型（HMM）</a></li>
  <li><a href="#条件随机场crf" id="toc-条件随机场crf" class="nav-link" data-scroll-target="#条件随机场crf"><span class="header-section-number">5.3</span> 条件随机场（CRF）</a></li>
  </ul></li>
  <li><a href="#核心痛点特征工程的诅咒" id="toc-核心痛点特征工程的诅咒" class="nav-link" data-scroll-target="#核心痛点特征工程的诅咒"><span class="header-section-number">6</span> 核心痛点：特征工程的诅咒</a>
  <ul class="collapse">
  <li><a href="#什么是特征工程" id="toc-什么是特征工程" class="nav-link" data-scroll-target="#什么是特征工程"><span class="header-section-number">6.1</span> 什么是特征工程？</a></li>
  <li><a href="#特征工程的困境" id="toc-特征工程的困境" class="nav-link" data-scroll-target="#特征工程的困境"><span class="header-section-number">6.2</span> 特征工程的困境</a></li>
  <li><a href="#一个具体的例子命名实体识别" id="toc-一个具体的例子命名实体识别" class="nav-link" data-scroll-target="#一个具体的例子命名实体识别"><span class="header-section-number">6.3</span> 一个具体的例子：命名实体识别</a></li>
  <li><a href="#瓶颈的本质" id="toc-瓶颈的本质" class="nav-link" data-scroll-target="#瓶颈的本质"><span class="header-section-number">6.4</span> 瓶颈的本质</a></li>
  </ul></li>
  <li><a href="#工程实践传统nlp-pipeline体验" id="toc-工程实践传统nlp-pipeline体验" class="nav-link" data-scroll-target="#工程实践传统nlp-pipeline体验"><span class="header-section-number">7</span> 工程实践：传统NLP Pipeline体验</a>
  <ul class="collapse">
  <li><a href="#n-gram语言模型" id="toc-n-gram语言模型" class="nav-link" data-scroll-target="#n-gram语言模型"><span class="header-section-number">7.1</span> N-gram语言模型</a></li>
  <li><a href="#使用crf做命名实体识别" id="toc-使用crf做命名实体识别" class="nav-link" data-scroll-target="#使用crf做命名实体识别"><span class="header-section-number">7.2</span> 使用CRF做命名实体识别</a></li>
  <li><a href="#体会特征工程的痛苦" id="toc-体会特征工程的痛苦" class="nav-link" data-scroll-target="#体会特征工程的痛苦"><span class="header-section-number">7.3</span> 体会特征工程的痛苦</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">8</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么统计方法最终胜出" id="toc-为什么统计方法最终胜出" class="nav-link" data-scroll-target="#为什么统计方法最终胜出"><span class="header-section-number">8.1</span> 为什么统计方法最终胜出？</a></li>
  <li><a href="#马尔可夫假设的边界" id="toc-马尔可夫假设的边界" class="nav-link" data-scroll-target="#马尔可夫假设的边界"><span class="header-section-number">8.2</span> 马尔可夫假设的边界</a></li>
  <li><a href="#生成模型-vs-判别模型" id="toc-生成模型-vs-判别模型" class="nav-link" data-scroll-target="#生成模型-vs-判别模型"><span class="header-section-number">8.3</span> 生成模型 vs 判别模型</a></li>
  <li><a href="#开放问题与反思" id="toc-开放问题与反思" class="nav-link" data-scroll-target="#开放问题与反思"><span class="header-section-number">8.4</span> 开放问题与反思</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#特征工程不可扩展" id="toc-特征工程不可扩展" class="nav-link" data-scroll-target="#特征工程不可扩展"><span class="header-section-number">9.1</span> 特征工程不可扩展</a></li>
  <li><a href="#表示的离散性" id="toc-表示的离散性" class="nav-link" data-scroll-target="#表示的离散性"><span class="header-section-number">9.2</span> 表示的离散性</a></li>
  <li><a href="#任务特定性" id="toc-任务特定性" class="nav-link" data-scroll-target="#任务特定性"><span class="header-section-number">9.3</span> 任务特定性</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">9.4</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">10</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</a></li>
  <li><a href="#关键概念速查" id="toc-关键概念速查" class="nav-link" data-scroll-target="#关键概念速查"><span class="header-section-number">10.2</span> 关键概念速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">10.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">11</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#经典教材" id="toc-经典教材" class="nav-link" data-scroll-target="#经典教材"><span class="header-section-number">11.1</span> 经典教材</a></li>
  <li><a href="#历史论文" id="toc-历史论文" class="nav-link" data-scroll-target="#历史论文"><span class="header-section-number">11.2</span> 历史论文</a></li>
  <li><a href="#回顾与反思" id="toc-回顾与反思" class="nav-link" data-scroll-target="#回顾与反思"><span class="header-section-number">11.3</span> 回顾与反思</a></li>
  <li><a href="#工具与代码" id="toc-工具与代码" class="nav-link" data-scroll-target="#工具与代码"><span class="header-section-number">11.4</span> 工具与代码</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">12</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第1章：语言理解的早期探索</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">从符号到统计的范式转变</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">History</div>
    <div class="quarto-category">N-gram</div>
    <div class="quarto-category">HMM</div>
    <div class="quarto-category">CRF</div>
    <div class="quarto-category">Feature Engineering</div>
  </div>
  </div>

<div>
  <div class="description">
    前深度学习时代的NLP技术演进：从规则系统到统计学习，从N-gram到CRF，以及为什么特征工程成为了整个领域的天花板。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 23, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：在深度学习之前，人们是如何让计算机理解和处理自然语言的？这些方法取得了什么成就，又遭遇了什么根本性的困难？</p>
<p><strong>历史坐标</strong>：1950s-2010s | 从ELIZA到CRF | 符号主义→统计学习的演进</p>
</blockquote>
<hr>
<section id="从第0章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从第0章说起"><span class="header-section-number">1</span> 从第0章说起</h2>
<p>在上一章中，我们讨论了如何阅读NLP研究。现在，让我们正式开始NLP的技术之旅。</p>
<p>但在进入深度学习的世界之前，我们需要先回顾一段历史。这不是为了怀旧，而是因为理解”过去的问题”是理解”现在的解决方案”的前提。深度学习在NLP中的成功并非偶然——它解决了困扰这个领域几十年的根本性问题。如果你不知道那些问题是什么，你就无法真正理解为什么Word2Vec是革命性的，为什么Transformer如此重要，为什么大语言模型改变了一切。</p>
<p>这一章将快速回顾前深度学习时代的NLP。我们会看到两次重要的范式转变：从符号主义到统计方法，以及统计方法如何在”特征工程的诅咒”中逐渐走向瓶颈。这个瓶颈，正是第2章”表示学习”登场的舞台。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：传统NLP的核心困境是”特征工程”——需要人工设计特征来表示语言现象。这不仅耗费大量专家知识，而且无法扩展到复杂任务。深度学习的革命性在于它”自动学习特征”，彻底打破了这个瓶颈。</p>
</blockquote>
<hr>
</section>
<section id="符号主义的辉煌与幻灭" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="符号主义的辉煌与幻灭"><span class="header-section-number">2</span> 符号主义的辉煌与幻灭</h2>
<section id="早期的梦想让机器理解语言" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="早期的梦想让机器理解语言"><span class="header-section-number">2.1</span> 早期的梦想：让机器理解语言</h3>
<p>1950年代，人工智能刚刚诞生，研究者们对让机器理解语言充满了乐观。当时主流的思路是”符号主义”（Symbolic AI）：把语言看作符号的操作，用逻辑规则来处理。</p>
<p>这个思路直觉上很自然。人类语言有语法规则，句子可以分析成主语、谓语、宾语。如果我们把这些规则编码成程序，机器不就能”理解”语言了吗？</p>
<p>1966年，MIT的Joseph Weizenbaum创建了ELIZA，一个模拟心理治疗师的程序。ELIZA没有任何真正的语言理解能力，它只是用简单的模式匹配和替换规则来响应用户输入。比如，如果用户说”I am sad”，ELIZA会匹配到”I am X”模式，然后回复”Why are you sad?“。这个简单的技巧却让很多用户相信他们在和一个有智能的存在交流。</p>
<p>ELIZA的成功（和它揭示的人类错觉）引发了一波对”聊天机器人”的热情。但更重要的是，它展示了符号主义方法的基本范式：人工编写规则，让机器按规则处理语言。</p>
</section>
<section id="规则系统的成就" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="规则系统的成就"><span class="header-section-number">2.2</span> 规则系统的成就</h3>
<p>在接下来的几十年里，基于规则的NLP系统取得了一些令人印象深刻的成就。</p>
<p><strong>句法分析器（Parser）</strong>在理解句子结构方面做得相当不错。通过上下文无关文法（Context-Free Grammar）和更复杂的形式语法，研究者可以解析出句子的语法树。比如，“The cat sat on the mat”可以被分析为：[NP [Det The] [N cat]] [VP [V sat] [PP [P on] [NP [Det the] [N mat]]]]。这种分析对于问答系统、信息提取等任务是有用的。</p>
<p><strong>机器翻译</strong>也在规则驱动下取得了进展。早期的机器翻译系统依赖双语词典和转换规则。IBM和其他公司投入了大量资源开发这类系统。对于结构相似的语言对（如西班牙语和葡萄牙语），这些系统能产生可用的翻译。</p>
<p><strong>专家系统</strong>在特定领域展示了符号AI的威力。比如MYCIN系统可以诊断细菌感染并推荐抗生素，它用数百条规则编码了医学专家的知识。虽然MYCIN处理的是医学知识而非自然语言，但它代表了”用规则编码专家知识”这一范式的巅峰。</p>
</section>
<section id="规则系统的根本问题" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="规则系统的根本问题"><span class="header-section-number">2.3</span> 规则系统的根本问题</h3>
<p>然而，规则系统很快就遇到了难以逾越的障碍。</p>
<p>第一个问题是<strong>规则爆炸</strong>。自然语言的复杂性远超预期。一个简单的句子可能有多种合理的解析方式，需要上下文、常识、甚至语用知识来消歧。每增加一个现象的处理，就需要增加大量规则，而这些规则之间可能相互冲突。研究者发现，一个覆盖面广的语法系统可能有成千上万条规则，维护成本极其高昂。</p>
<p>第二个问题是<strong>脆弱性</strong>。规则系统对输入的变化非常敏感。一个句子稍微换一种说法，或者有一个拼写错误，系统就可能完全失效。真实世界的语言充满了非标准表达、省略、口语化用法，这些都是规则系统的噩梦。</p>
<p>第三个问题是<strong>知识获取瓶颈</strong>。编写规则需要语言学专家，而专家的时间是有限的。更糟糕的是，很多语言知识是隐性的（tacit knowledge）——母语者能轻松判断一个句子是否自然，但无法说清具体遵循了什么规则。这些知识无法被显式编码。</p>
<p>到了1980年代末，符号主义NLP陷入了困境。研究者开始意识到，靠人工编写规则来覆盖自然语言的复杂性可能是不可行的。这个领域需要一种全新的思路。</p>
<hr>
</section>
</section>
<section id="统计革命从规则到概率" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="统计革命从规则到概率"><span class="header-section-number">3</span> 统计革命：从规则到概率</h2>
<section id="数据驱动的范式转变" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="数据驱动的范式转变"><span class="header-section-number">3.1</span> 数据驱动的范式转变</h3>
<p>1990年代，NLP经历了一次根本性的范式转变：从”规则驱动”到”数据驱动”，从”符号处理”到”统计学习”。</p>
<p>这次转变的核心洞察是：与其让专家编写规则，不如让机器从大量数据中自动学习规律。语言中的规律可能太复杂、太隐晦，无法被人类显式编码，但它们确实存在于语言数据中——只要我们有足够的数据和正确的统计工具。</p>
<p>这个转变有几个推动力。首先是数据的可得性。随着数字化文本的增加，研究者可以获得越来越大的语料库。Penn Treebank（1992）提供了大量人工标注的句法树，Brown Corpus提供了标注好的词性数据。这些标注数据使得监督学习成为可能。</p>
<p>其次是计算能力的提升。统计方法往往需要大量计算，1990年代的计算机终于能够处理实用规模的问题了。</p>
<p>最关键的是统计机器翻译的成功。IBM的研究团队在1990年代初提出了一系列基于统计的机器翻译模型（IBM Model 1-5），效果显著优于规则系统。Fred Jelinek有一句著名的话：“Every time I fire a linguist, the performance of the speech recognizer goes up.”（每解雇一个语言学家，语音识别器的性能就提升一些。）这句话虽然有些夸张，但它捕捉到了当时的时代精神：数据和统计可能比语言学规则更重要。</p>
</section>
<section id="统计方法的基本思路" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="统计方法的基本思路"><span class="header-section-number">3.2</span> 统计方法的基本思路</h3>
<p>统计NLP的核心思想可以用一个公式概括：</p>
<p><span class="math display">\[
\hat{y} = \arg\max_y P(y|x)
\]</span></p>
<p>给定输入<span class="math inline">\(x\)</span>（比如一个英文句子），找到最可能的输出<span class="math inline">\(y\)</span>（比如对应的中文翻译）。这个框架可以套用到几乎所有NLP任务：词性标注是给定句子预测每个词的词性，命名实体识别是给定句子预测哪些片段是人名/地名/机构名，情感分析是给定评论预测正面/负面。</p>
<p>实现这个框架需要解决两个问题。第一是如何定义<span class="math inline">\(P(y|x)\)</span>——用什么模型来表示这个概率分布？第二是如何从数据中学习模型的参数——这就是机器学习的任务。</p>
<p>在深度学习之前，主流的做法是：人工设计特征函数<span class="math inline">\(\phi(x)\)</span>来表示输入，然后用经典的机器学习模型（如逻辑回归、SVM、CRF）来学习从特征到输出的映射。这就是”特征工程+经典分类器”的范式。</p>
<hr>
</section>
</section>
<section id="n-gram与语言模型基础" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="n-gram与语言模型基础"><span class="header-section-number">4</span> N-gram与语言模型基础</h2>
<section id="什么是语言模型" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="什么是语言模型"><span class="header-section-number">4.1</span> 什么是语言模型？</h3>
<p>在深入讨论”特征工程”之前，我们先介绍一个贯穿整个NLP历史的核心概念：语言模型（Language Model）。</p>
<p>语言模型的目标是给文本分配概率——评估一段文本在给定语言中有多”自然”。形式化地，给定一个词序列<span class="math inline">\(w_1, w_2, \ldots, w_n\)</span>，语言模型计算：</p>
<p><span class="math display">\[
P(w_1, w_2, \ldots, w_n)
\]</span></p>
<p>这个看似简单的目标有深远的应用。在机器翻译中，语言模型帮助选择更流畅的译文。在语音识别中，语言模型帮助消除声学歧义。在拼写纠错中，语言模型判断哪个候选词更合理。可以说，语言模型是NLP的”基础设施”。</p>
</section>
<section id="n-gram简单而有效" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="n-gram简单而有效"><span class="header-section-number">4.2</span> N-gram：简单而有效</h3>
<p>如何估计<span class="math inline">\(P(w_1, w_2, \ldots, w_n)\)</span>？直接估计是不可行的——可能的词序列数量随长度指数增长，任何语料库都无法覆盖所有可能的句子。</p>
<p>N-gram语言模型提供了一个简洁的近似。核心假设是<strong>马尔可夫假设</strong>：一个词的出现只依赖于它前面的<span class="math inline">\(n-1\)</span>个词。用链式法则展开联合概率：</p>
<p><span class="math display">\[
P(w_1, \ldots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, \ldots, w_{i-1})
\]</span></p>
<p>然后用马尔可夫假设简化：</p>
<p><span class="math display">\[
P(w_i | w_1, \ldots, w_{i-1}) \approx P(w_i | w_{i-n+1}, \ldots, w_{i-1})
\]</span></p>
<p>当<span class="math inline">\(n=2\)</span>时，称为bigram模型：<span class="math inline">\(P(w_i | w_{i-1})\)</span>。当<span class="math inline">\(n=3\)</span>时，称为trigram模型：<span class="math inline">\(P(w_i | w_{i-2}, w_{i-1})\)</span>。</p>
<p>这些条件概率如何估计？最简单的方法是最大似然估计（MLE），也就是计数：</p>
<p><span class="math display">\[
P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
\]</span></p>
<p>其中<span class="math inline">\(C(\cdot)\)</span>是语料库中的计数。这种方法直观、高效，在足够大的语料库上可以得到不错的估计。</p>
<section id="数值示例bigram-概率计算" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="数值示例bigram-概率计算"><span class="header-section-number">4.2.1</span> 数值示例：Bigram 概率计算</h4>
<p>让我们用一个极简的例子来具体理解 N-gram 是如何工作的。</p>
<p><strong>语料库</strong>（3个句子）：</p>
<ul>
<li>“I love cats”</li>
<li>“I love dogs”</li>
<li>“cats love fish”</li>
</ul>
<p><strong>Step 1: 统计 Bigram 计数</strong></p>
<p>首先为每个句子加上起始符号 <code>&lt;s&gt;</code> 和结束符号 <code>&lt;/s&gt;</code>，然后统计所有相邻词对的出现次数：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Bigram</th>
<th>计数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>&lt;s&gt;</code> I</td>
<td>2</td>
</tr>
<tr class="even">
<td>I love</td>
<td>2</td>
</tr>
<tr class="odd">
<td>love cats</td>
<td>1</td>
</tr>
<tr class="even">
<td>love dogs</td>
<td>1</td>
</tr>
<tr class="odd">
<td>love fish</td>
<td>1</td>
</tr>
<tr class="even">
<td>cats <code>&lt;/s&gt;</code></td>
<td>1</td>
</tr>
<tr class="odd">
<td>dogs <code>&lt;/s&gt;</code></td>
<td>1</td>
</tr>
<tr class="even">
<td>fish <code>&lt;/s&gt;</code></td>
<td>1</td>
</tr>
<tr class="odd">
<td>cats love</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>同时统计每个词作为”前一个词”出现的总次数：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>词</th>
<th>作为前词的总次数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>&lt;s&gt;</code></td>
<td>3</td>
</tr>
<tr class="even">
<td>I</td>
<td>2</td>
</tr>
<tr class="odd">
<td>love</td>
<td>3</td>
</tr>
<tr class="even">
<td>cats</td>
<td>2</td>
</tr>
<tr class="odd">
<td>dogs</td>
<td>1</td>
</tr>
<tr class="even">
<td>fish</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><strong>Step 2: 计算条件概率</strong></p>
<p>使用 MLE 公式 <span class="math inline">\(P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}\)</span>：</p>
<p><span class="math display">\[
P(\text{love} | \text{I}) = \frac{C(\text{I love})}{C(\text{I})} = \frac{2}{2} = 1.0
\]</span></p>
<p><span class="math display">\[
P(\text{cats} | \text{love}) = \frac{C(\text{love cats})}{C(\text{love})} = \frac{1}{3} \approx 0.33
\]</span></p>
<p><span class="math display">\[
P(\text{dogs} | \text{love}) = \frac{C(\text{love dogs})}{C(\text{love})} = \frac{1}{3} \approx 0.33
\]</span></p>
<p><strong>Step 3: 计算句子概率</strong></p>
<p>现在计算一个新句子 “I love fish” 的概率：</p>
<p><span class="math display">\[
P(\text{I love fish}) = P(\text{I}|\text{&lt;s&gt;}) \times P(\text{love}|\text{I}) \times P(\text{fish}|\text{love}) \times P(\text{&lt;/s&gt;}|\text{fish})
\]</span></p>
<p><span class="math display">\[
= \frac{2}{3} \times \frac{2}{2} \times \frac{1}{3} \times \frac{1}{1} = \frac{2}{3} \times 1 \times \frac{1}{3} \times 1 = \frac{2}{9} \approx 0.22
\]</span></p>
<p><strong>Step 4: 计算困惑度（Perplexity）</strong></p>
<p>困惑度是语言模型的标准评估指标，定义为：</p>
<p><span class="math display">\[
\text{PPL} = P(w_1, \ldots, w_n)^{-1/n} = \exp\left(-\frac{1}{n}\sum_{i=1}^{n} \log P(w_i|w_{i-1})\right)
\]</span></p>
<p>对于 “I love fish”（4个token，含<code>&lt;/s&gt;</code>）：</p>
<p><span class="math display">\[
\text{PPL} = \left(\frac{2}{9}\right)^{-1/4} = \left(\frac{9}{2}\right)^{1/4} \approx 1.46
\]</span></p>
<p>困惑度越低，说明模型对句子的”惊讶程度”越低，即句子越符合训练数据的分布。</p>
<p><strong>关键观察</strong>：</p>
<ul>
<li>“I love fish” 虽然在训练数据中没有出现过，但因为 “love fish” 这个 bigram 出现过，所以概率不为零</li>
<li>如果我们问 “I hate cats” 的概率，由于 “I hate” 从未出现，MLE 会给出 <span class="math inline">\(P = 0\)</span>——这就是稀疏性问题</li>
</ul>
</section>
</section>
<section id="n-gram的局限" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="n-gram的局限"><span class="header-section-number">4.3</span> N-gram的局限</h3>
<p>N-gram模型虽然简单有效，但有几个根本性的局限。</p>
<p>第一是<strong>稀疏性问题</strong>。即使在大型语料库中，大多数n-gram组合也从未出现过。如果<span class="math inline">\(C(w_{i-1}, w_i) = 0\)</span>，MLE会给出<span class="math inline">\(P(w_i|w_{i-1}) = 0\)</span>，这显然不合理——一个从未见过的词序列不一定是不可能的。研究者发明了各种”平滑”技术来解决这个问题（加一平滑、Kneser-Ney平滑、Good-Turing估计），但这本质上是在”打补丁”。</p>
<p>第二是<strong>上下文窗口固定</strong>。马尔可夫假设是一个很强的假设：词只依赖于前面的<span class="math inline">\(n-1\)</span>个词。但语言中的依赖关系可以跨越很长的距离。考虑这个例子：“The keys to the cabinet are on the table.”主语”keys”和动词”are”之间隔了四个词。trigram模型在预测”are”时只能看到”cabinet are”，完全看不到”keys”，因此会倾向于预测单数形式”is”。增大<span class="math inline">\(n\)</span>可以部分缓解这个问题，但参数数量会指数增长，很快变得不可行。</p>
<p>第三是<strong>没有泛化能力</strong>。N-gram完全依赖表面形式的计数，不理解词的含义。“The cat sat on the mat”和”The dog sat on the mat”对N-gram来说是完全不同的序列，即使人类知道”cat”和”dog”都是动物、都能坐在垫子上。</p>
<p>这些局限预示了后面章节要解决的问题：如何表示词的”含义”（第2章：Word2Vec），如何建模长距离依赖（第4章：LSTM），如何不受固定窗口限制（第5-8章：Attention和Transformer）。</p>
<hr>
</section>
</section>
<section id="序列标注从hmm到crf的演进" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="序列标注从hmm到crf的演进"><span class="header-section-number">5</span> 序列标注：从HMM到CRF的演进</h2>
<section id="序列标注问题" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="序列标注问题"><span class="header-section-number">5.1</span> 序列标注问题</h3>
<p>NLP中有一大类任务属于”序列标注”（Sequence Labeling）：给定一个词序列，为每个词分配一个标签。经典例子包括词性标注（POS tagging）——判断每个词是名词、动词还是形容词，命名实体识别（NER）——识别出人名、地名、机构名等，以及分词（对于中文等没有明确词边界的语言）。</p>
<p>序列标注的挑战在于标签之间有依赖关系。比如在词性标注中，冠词后面通常是名词或形容词，而不是动词。在NER中，一个人名可能由多个词组成（如”New York”是一个整体）。独立地预测每个词的标签会忽略这些依赖，效果不佳。</p>
</section>
<section id="隐马尔可夫模型hmm" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="隐马尔可夫模型hmm"><span class="header-section-number">5.2</span> 隐马尔可夫模型（HMM）</h3>
<p>隐马尔可夫模型是序列标注的经典工具。它假设存在一个隐藏的状态序列（标签），生成了我们观察到的词序列。</p>
<p>HMM有两个核心假设。第一是马尔可夫假设：当前状态只依赖于前一个状态，<span class="math inline">\(P(y_t | y_1, \ldots, y_{t-1}) = P(y_t | y_{t-1})\)</span>。第二是输出独立性假设：当前观察只依赖于当前状态，<span class="math inline">\(P(x_t | y_1, \ldots, y_t, x_1, \ldots, x_{t-1}) = P(x_t | y_t)\)</span>。</p>
<p>基于这两个假设，HMM定义了序列的联合概率：</p>
<p><span class="math display">\[
P(x, y) = \prod_{t=1}^{T} P(y_t | y_{t-1}) \cdot P(x_t | y_t)
\]</span></p>
<p>其中<span class="math inline">\(P(y_t | y_{t-1})\)</span>是转移概率（transition probability），<span class="math inline">\(P(x_t | y_t)\)</span>是发射概率（emission probability）。</p>
<p>给定一个观察序列<span class="math inline">\(x\)</span>，如何找到最可能的标签序列<span class="math inline">\(y\)</span>？这是一个动态规划问题，可以用Viterbi算法高效求解，复杂度是<span class="math inline">\(O(TK^2)\)</span>，其中<span class="math inline">\(T\)</span>是序列长度，<span class="math inline">\(K\)</span>是标签数量。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Viterbi 解码 (Rabiner, 1989)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>：观察序列 <span class="math inline">\(O = (o_1, o_2, \ldots, o_T)\)</span>，HMM 参数 <span class="math inline">\(\lambda = (\pi, A, B)\)</span></p>
<p><strong>输出</strong>：最优状态序列 <span class="math inline">\(Q^* = (q_1^*, q_2^*, \ldots, q_T^*)\)</span></p>
<p><strong>1. 初始化</strong> (<span class="math inline">\(t = 1\)</span>)： <span class="math display">\[
\delta_1(i) = \pi_i \cdot b_i(o_1), \quad 1 \leq i \leq N
\]</span> <span class="math display">\[
\psi_1(i) = 0
\]</span></p>
<p><strong>2. 递归</strong> (<span class="math inline">\(t = 2, 3, \ldots, T\)</span>)： <span class="math display">\[
\delta_t(j) = \max_{1 \leq i \leq N} \left[ \delta_{t-1}(i) \cdot a_{ij} \right] \cdot b_j(o_t), \quad 1 \leq j \leq N
\]</span> <span class="math display">\[
\psi_t(j) = \arg\max_{1 \leq i \leq N} \left[ \delta_{t-1}(i) \cdot a_{ij} \right], \quad 1 \leq j \leq N
\]</span></p>
<p><strong>3. 终止</strong>： <span class="math display">\[
P^* = \max_{1 \leq i \leq N} \delta_T(i)
\]</span> <span class="math display">\[
q_T^* = \arg\max_{1 \leq i \leq N} \delta_T(i)
\]</span></p>
<p><strong>4. 回溯</strong> (<span class="math inline">\(t = T-1, T-2, \ldots, 1\)</span>)： <span class="math display">\[
q_t^* = \psi_{t+1}(q_{t+1}^*)
\]</span></p>
<p>其中： - <span class="math inline">\(\delta_t(i)\)</span>：在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span>，且经过<strong>最优路径</strong>到达的概率 - <span class="math inline">\(\psi_t(j)\)</span>：回溯指针，记录到达状态 <span class="math inline">\(j\)</span> 的最优前驱状态 - <span class="math inline">\(\pi_i\)</span>：初始状态概率，<span class="math inline">\(a_{ij}\)</span>：转移概率，<span class="math inline">\(b_j(o_t)\)</span>：发射概率</p>
<p><em>Source: Rabiner, L. (1989). “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition”. Proceedings of the IEEE, 77(2), 257-286. <a href="https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf">PDF</a></em></p>
</div>
</div>
<section id="数值示例viterbi-解码" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="数值示例viterbi-解码"><span class="header-section-number">5.2.1</span> 数值示例：Viterbi 解码</h4>
<p>让我们用一个具体的例子来理解 Viterbi 算法是如何找到最优标签序列的。</p>
<p><strong>问题设定</strong>：词性标注任务</p>
<ul>
<li><strong>状态（标签）</strong>：N（名词）、V（动词），共2种</li>
<li><strong>观察（词）</strong>：句子 “fish can swim”</li>
<li><strong>目标</strong>：找到最可能的词性序列</li>
</ul>
<p><strong>模型参数</strong>：</p>
<p><strong>初始概率</strong> <span class="math inline">\(\pi\)</span>（句子以某个词性开始的概率）：</p>
<p><span class="math display">\[
\pi(N) = 0.6, \quad \pi(V) = 0.4
\]</span></p>
<p><strong>转移概率</strong> <span class="math inline">\(A\)</span>（从一个词性转移到另一个词性）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(P(y_t \| y_{t-1})\)</span></th>
<th>→ N</th>
<th>→ V</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N →</td>
<td>0.3</td>
<td>0.7</td>
</tr>
<tr class="even">
<td>V →</td>
<td>0.8</td>
<td>0.2</td>
</tr>
</tbody>
</table>
<p>解读：名词后面 70% 是动词，动词后面 80% 是名词。</p>
<p><strong>发射概率</strong> <span class="math inline">\(B\)</span>（某个词性生成某个词）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(P(x \| y)\)</span></th>
<th>“fish”</th>
<th>“can”</th>
<th>“swim”</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N</td>
<td>0.4</td>
<td>0.3</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>V</td>
<td>0.1</td>
<td>0.2</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<p>解读：“fish” 更可能是名词（0.4 vs 0.1），“swim” 更可能是动词（0.5 vs 0.1）。</p>
<p><strong>Viterbi 算法逐步计算</strong></p>
<p>Viterbi 使用动态规划，定义 <span class="math inline">\(\delta_t(j)\)</span> 为在时刻 <span class="math inline">\(t\)</span>、状态为 <span class="math inline">\(j\)</span> 时，所有路径中的最大概率。</p>
<p><strong><span class="math inline">\(t=1\)</span>：处理 “fish”</strong></p>
<p><span class="math display">\[
\delta_1(N) = \pi(N) \times P(\text{fish}|N) = 0.6 \times 0.4 = 0.24
\]</span></p>
<p><span class="math display">\[
\delta_1(V) = \pi(V) \times P(\text{fish}|V) = 0.4 \times 0.1 = 0.04
\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>状态</th>
<th><span class="math inline">\(\delta_1\)</span></th>
<th>回溯指针</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N</td>
<td><strong>0.24</strong></td>
<td>-</td>
</tr>
<tr class="even">
<td>V</td>
<td>0.04</td>
<td>-</td>
</tr>
</tbody>
</table>
<p><strong><span class="math inline">\(t=2\)</span>：处理 “can”</strong></p>
<p>对于每个当前状态，考虑所有可能的前一状态，取最大值：</p>
<p><span class="math display">\[
\delta_2(N) = \max\begin{cases}
\delta_1(N) \times P(N|N) \times P(\text{can}|N) = 0.24 \times 0.3 \times 0.3 = 0.0216 \\
\delta_1(V) \times P(N|V) \times P(\text{can}|N) = 0.04 \times 0.8 \times 0.3 = 0.0096
\end{cases}
\]</span></p>
<p>最大值是 0.0216，来自 N→N 路径。</p>
<p><span class="math display">\[
\delta_2(V) = \max\begin{cases}
\delta_1(N) \times P(V|N) \times P(\text{can}|V) = 0.24 \times 0.7 \times 0.2 = 0.0336 \\
\delta_1(V) \times P(V|V) \times P(\text{can}|V) = 0.04 \times 0.2 \times 0.2 = 0.0016
\end{cases}
\]</span></p>
<p>最大值是 0.0336，来自 N→V 路径。</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>状态</th>
<th><span class="math inline">\(\delta_2\)</span></th>
<th>回溯指针</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N</td>
<td>0.0216</td>
<td>← N</td>
</tr>
<tr class="even">
<td>V</td>
<td><strong>0.0336</strong></td>
<td>← N</td>
</tr>
</tbody>
</table>
<p><strong><span class="math inline">\(t=3\)</span>：处理 “swim”</strong></p>
<p><span class="math display">\[
\delta_3(N) = \max\begin{cases}
\delta_2(N) \times P(N|N) \times P(\text{swim}|N) = 0.0216 \times 0.3 \times 0.1 = 0.000648 \\
\delta_2(V) \times P(N|V) \times P(\text{swim}|N) = 0.0336 \times 0.8 \times 0.1 = 0.002688
\end{cases}
\]</span></p>
<p>最大值是 0.002688，来自 V→N 路径。</p>
<p><span class="math display">\[
\delta_3(V) = \max\begin{cases}
\delta_2(N) \times P(V|N) \times P(\text{swim}|V) = 0.0216 \times 0.7 \times 0.5 = 0.00756 \\
\delta_2(V) \times P(V|V) \times P(\text{swim}|V) = 0.0336 \times 0.2 \times 0.5 = 0.00336
\end{cases}
\]</span></p>
<p>最大值是 0.00756，来自 N→V 路径。</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>状态</th>
<th><span class="math inline">\(\delta_3\)</span></th>
<th>回溯指针</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N</td>
<td>0.002688</td>
<td>← V</td>
</tr>
<tr class="even">
<td>V</td>
<td><strong>0.00756</strong></td>
<td>← N</td>
</tr>
</tbody>
</table>
<p><strong>回溯找最优路径</strong></p>
<ol type="1">
<li><span class="math inline">\(t=3\)</span>：最大的是 <span class="math inline">\(\delta_3(V) = 0.00756\)</span>，所以 <span class="math inline">\(y_3 = V\)</span></li>
<li><span class="math inline">\(t=2\)</span>：<span class="math inline">\(y_3 = V\)</span> 的回溯指针指向 N，所以 <span class="math inline">\(y_2 = N\)</span></li>
<li><span class="math inline">\(t=1\)</span>：<span class="math inline">\(y_2 = N\)</span> 的回溯指针指向 N，所以 <span class="math inline">\(y_1 = N\)</span></li>
</ol>
<p><strong>最终结果</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>词</th>
<th>fish</th>
<th>can</th>
<th>swim</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>词性</td>
<td><strong>N</strong></td>
<td><strong>N</strong></td>
<td><strong>V</strong></td>
</tr>
</tbody>
</table>
<p>解读：“fish” 是名词（鱼），“can” 是名词（罐头），“swim” 是动词。这个结果符合直觉！</p>
<div id="fig-viterbi-trellis" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-viterbi-trellis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-1/fig-viterbi-trellis.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-viterbi-trellis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Viterbi 解码过程全景：从左到右，算法在每个时间步为每个状态计算最大累积概率δ，并记录最优前驱。橙色高亮路径 N→N→V 是全局最优解。注意交叉箭头上的转移概率如何影响路径选择——即使”can”作为动词（B(V)=0.2）的发射概率低于名词（B(N)=0.3），N→V 路径的高转移概率（a=0.7）仍然使得 δ₂(V) &gt; δ₂(N)。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Python生成，基于 Rabiner (1989) Viterbi 算法</em></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>为什么不是 N-V-V？
</div>
</div>
<div class="callout-body-container callout-body">
<p>你可能会问：“can” 不是也可以当动词（能够）吗？为什么 Viterbi 选了 N-N-V 而不是 N-V-V？</p>
<p>让我们比较这两条路径的概率：</p>
<ul>
<li><strong>N-N-V</strong>：<span class="math inline">\(0.6 \times 0.4 \times 0.3 \times 0.3 \times 0.7 \times 0.5 = 0.00756\)</span></li>
<li><strong>N-V-V</strong>：<span class="math inline">\(0.6 \times 0.4 \times 0.7 \times 0.2 \times 0.2 \times 0.5 = 0.00336\)</span></li>
</ul>
<p>N-N-V 更优！原因是：虽然 “can” 作动词合理，但 V→V 的转移概率只有 0.2（连续两个动词不常见），而 N→V 的转移概率是 0.7。这个例子展示了 HMM 如何利用<strong>转移概率</strong>来消歧。</p>
</div>
</div>
<p>HMM在词性标注等任务上取得了不错的效果，长期是主流方法。但它有一个根本性的局限：生成模型的约束。HMM是一个生成模型，它建模的是<span class="math inline">\(P(x, y)\)</span>而非<span class="math inline">\(P(y|x)\)</span>。为了使模型tractable，它必须做很强的独立性假设（发射概率只依赖当前状态）。这意味着它很难利用丰富的输入特征。</p>
</section>
</section>
<section id="条件随机场crf" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="条件随机场crf"><span class="header-section-number">5.3</span> 条件随机场（CRF）</h3>
<p>条件随机场（Conditional Random Field）是对HMM的一个根本性改进。它是判别模型，直接建模<span class="math inline">\(P(y|x)\)</span>，因此不需要对输入<span class="math inline">\(x\)</span>做任何假设。</p>
<p>线性链CRF的形式是：</p>
<p><span class="math display">\[
P(y|x) = \frac{1}{Z(x)} \exp\left(\sum_{t=1}^{T} \sum_k \lambda_k f_k(y_{t-1}, y_t, x, t)\right)
\]</span></p>
<p>这里<span class="math inline">\(f_k\)</span>是特征函数，<span class="math inline">\(\lambda_k\)</span>是要学习的权重，<span class="math inline">\(Z(x)\)</span>是归一化常数。</p>
<p>CRF的关键优势是特征的灵活性。特征函数<span class="math inline">\(f_k\)</span>可以依赖于整个输入序列<span class="math inline">\(x\)</span>，而不只是当前位置。比如，可以定义特征”如果当前词以-ing结尾，且标签是VBG（动词进行时），则特征为1”，或者”如果前一个词是冠词，且当前标签是NN（名词），则特征为1”。这种灵活性让CRF可以利用丰富的语言学知识和上下文信息。</p>
<p>CRF在2000年代成为序列标注的标准方法，在很多任务上显著超越了HMM。然而，它也面临一个问题：特征需要人工设计。这就是下一节要讨论的”特征工程的诅咒”。</p>
<hr>
</section>
</section>
<section id="核心痛点特征工程的诅咒" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="核心痛点特征工程的诅咒"><span class="header-section-number">6</span> 核心痛点：特征工程的诅咒</h2>
<section id="什么是特征工程" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="什么是特征工程"><span class="header-section-number">6.1</span> 什么是特征工程？</h3>
<p>在深度学习之前，几乎所有NLP系统都遵循同一个范式：</p>
<pre><code>原始文本 → 特征提取 → 经典分类器 → 预测结果</code></pre>
<p>“特征提取”这一步是关键，也是最耗费人力的部分。特征（feature）是对输入的一种数值表示，它应该捕获与任务相关的信息。</p>
<p>以情感分析为例。如何表示一条评论，让分类器能判断它是正面还是负面？研究者需要设计各种特征。最简单的是词袋特征（Bag of Words），统计每个词出现的次数，形成一个高维向量。然后可以加入N-gram特征，不只看单词，还看词组，如”not good”作为一个整体。还可以设计否定词处理特征，检测”not”“never”等否定词，翻转后续词的极性。可以利用情感词典，统计正面词和负面词的数量。句法特征也有用，比如主语是什么，动词是什么，形容词修饰什么…</p>
<p>这个过程就是”特征工程”：由领域专家（语言学家、NLP研究者）根据任务需求和语言学知识，手工设计特征。</p>
</section>
<section id="特征工程的困境" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="特征工程的困境"><span class="header-section-number">6.2</span> 特征工程的困境</h3>
<p>特征工程有几个根本性的问题。</p>
<p>第一是<strong>耗费专家知识</strong>。设计好的特征需要深厚的语言学背景和任务理解。不同任务需要不同的特征，一个情感分析的专家设计的特征可能对问答系统毫无用处。专家的时间是昂贵的、有限的。</p>
<p>第二是<strong>无法捕获复杂模式</strong>。很多语言现象太复杂、太隐晦，人类专家无法想到合适的特征。比如，讽刺（irony）的检测需要理解字面意思和说话者意图的不一致，这种”不一致”如何用特征表示？隐喻（metaphor）的理解需要概念之间的类比关系，这如何显式编码？</p>
<p>第三是<strong>特征之间的交互</strong>。很多有用的模式是特征的组合，而不是单个特征。比如，“not bad”表达的是轻微正面情感，需要同时考虑”not”和”bad”的交互。手工设计所有可能的交互是不可行的——特征数量会组合爆炸。</p>
<p>第四是<strong>任务迁移困难</strong>。为一个任务设计的特征往往不能直接用于另一个任务。每换一个任务，特征工程就要从头做起。这与人类的学习方式形成鲜明对比——人类学会阅读后，可以轻松迁移到各种阅读理解任务。</p>
</section>
<section id="一个具体的例子命名实体识别" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="一个具体的例子命名实体识别"><span class="header-section-number">6.3</span> 一个具体的例子：命名实体识别</h3>
<p>让我用命名实体识别（NER）来具体说明特征工程的复杂性。</p>
<p>NER的任务是识别文本中的人名、地名、机构名等实体。一个典型的句子可能是：“Barack Obama was born in Hawaii.”我们需要识别出”Barack Obama”是人名（PER），“Hawaii”是地名（LOC）。</p>
<p>为了训练一个CRF模型做NER，研究者可能设计这些特征：</p>
<p><strong>词级特征</strong>：当前词本身、当前词的小写形式、当前词的前缀（前3个字符）、当前词的后缀（后3个字符）。</p>
<p><strong>正字法特征</strong>（orthographic）：词是否全大写、词是否首字母大写、词是否包含数字、词是否包含标点。</p>
<p><strong>上下文特征</strong>：前一个词、后一个词、前两个词、后两个词，以及它们的各种组合。</p>
<p><strong>词典特征</strong>：词是否出现在人名词典中、是否出现在地名词典中、是否出现在机构名词典中。</p>
<p><strong>句法特征</strong>：词的词性标签、词在句法树中的位置、依存关系。</p>
<p><strong>聚类特征</strong>：词在Brown Clustering中的cluster ID（一种无监督的词聚类方法）。</p>
<p>一个工业级的NER系统可能有成百上千个这样的特征。研究者需要逐一设计、实验、调优。如果换一个领域（比如从新闻变成生物医学文本），很多特征需要重新设计，词典需要重新收集。</p>
<p>这就是”特征工程的诅咒”：它work，但它不scale。</p>
</section>
<section id="瓶颈的本质" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="瓶颈的本质"><span class="header-section-number">6.4</span> 瓶颈的本质</h3>
<p>如果我们抽象一下，特征工程的本质问题是：<strong>表示问题</strong>。</p>
<p>我们需要把原始文本（一个符号序列）转换成机器学习模型可以处理的数值表示（向量）。传统方法依赖人工设计这个转换过程。但自然语言太复杂、太多变，人工设计无法覆盖所有情况。</p>
<p>这就引出了一个自然的问题：能不能让机器自己学习如何表示语言？</p>
<p>这个问题的答案，就是下一章的主题：表示学习，从Word2Vec开始的”词向量”革命。</p>
<hr>
</section>
</section>
<section id="工程实践传统nlp-pipeline体验" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="工程实践传统nlp-pipeline体验"><span class="header-section-number">7</span> 工程实践：传统NLP Pipeline体验</h2>
<p>为了让你对传统NLP有更直观的感受，让我们用Python实现一个简单的N-gram语言模型和一个基于CRF的NER系统。</p>
<section id="n-gram语言模型" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="n-gram语言模型"><span class="header-section-number">7.1</span> N-gram语言模型</h3>
<div id="2c9116d1" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, Counter</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NgramLanguageModel:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    简单的N-gram语言模型</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    使用加一平滑（Laplace smoothing）处理稀疏性</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ngram_counts <span class="op">=</span> defaultdict(Counter)  <span class="co"># context -&gt; {word: count}</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_counts <span class="op">=</span> Counter()  <span class="co"># context -&gt; total count</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tokenize(<span class="va">self</span>, text):</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""简单的分词：按空格分割，加上起始和结束标记"""</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> text.lower().split()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="st">'&lt;s&gt;'</span>] <span class="op">*</span> (<span class="va">self</span>.n <span class="op">-</span> <span class="dv">1</span>) <span class="op">+</span> tokens <span class="op">+</span> [<span class="st">'&lt;/s&gt;'</span>]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, corpus):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""从语料库训练模型"""</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> corpus:</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> <span class="va">self</span>.tokenize(sentence)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.vocab.update(tokens)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n <span class="op">-</span> <span class="dv">1</span>, <span class="bu">len</span>(tokens)):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                context <span class="op">=</span> <span class="bu">tuple</span>(tokens[i <span class="op">-</span> <span class="va">self</span>.n <span class="op">+</span> <span class="dv">1</span> : i])  <span class="co"># 前n-1个词</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                word <span class="op">=</span> tokens[i]</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.ngram_counts[context][word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.context_counts[context] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> probability(<span class="va">self</span>, word, context):</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""计算P(word|context)，使用加一平滑"""</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> <span class="bu">tuple</span>(context[<span class="op">-</span>(<span class="va">self</span>.n<span class="op">-</span><span class="dv">1</span>):])  <span class="co"># 只取最后n-1个词</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> <span class="va">self</span>.ngram_counts[context][word]</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="va">self</span>.context_counts[context]</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        vocab_size <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.vocab)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加一平滑：(count + 1) / (total + vocab_size)</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (count <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (total <span class="op">+</span> vocab_size)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> perplexity(<span class="va">self</span>, sentence):</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""计算句子的困惑度（perplexity）"""</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> <span class="va">self</span>.tokenize(sentence)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        log_prob <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n <span class="op">-</span> <span class="dv">1</span>, <span class="bu">len</span>(tokens)):</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> tokens[i <span class="op">-</span> <span class="va">self</span>.n <span class="op">+</span> <span class="dv">1</span> : i]</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>            word <span class="op">=</span> tokens[i]</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>            prob <span class="op">=</span> <span class="va">self</span>.probability(word, context)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>            log_prob <span class="op">+=</span> math.log(prob)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perplexity = exp(-average log probability)</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.exp(<span class="op">-</span>log_prob <span class="op">/</span> count)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, context, max_length<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""给定上下文，生成后续文本"""</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> random</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> <span class="bu">list</span>(context)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">=</span> []</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>            ctx <span class="op">=</span> <span class="bu">tuple</span>(context[<span class="op">-</span>(<span class="va">self</span>.n<span class="op">-</span><span class="dv">1</span>):])</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ctx <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.ngram_counts:</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 按概率采样下一个词</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>            words <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.ngram_counts[ctx].keys())</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>            weights <span class="op">=</span> [<span class="va">self</span>.ngram_counts[ctx][w] <span class="cf">for</span> w <span class="kw">in</span> words]</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>            next_word <span class="op">=</span> random.choices(words, weights<span class="op">=</span>weights)[<span class="dv">0</span>]</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> next_word <span class="op">==</span> <span class="st">'&lt;/s&gt;'</span>:</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>            generated.append(next_word)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>            context.append(next_word)</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">' '</span>.join(generated)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例使用</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat sat on the mat"</span>,</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The dog sat on the floor"</span>,</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat chased the mouse"</span>,</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The dog chased the cat"</span>,</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A cat is a small animal"</span>,</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A dog is a loyal animal"</span>,</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NgramLanguageModel(n<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>model.train(corpus)</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算困惑度</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>test_sentences <span class="op">=</span> [</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat sat on the floor"</span>,  <span class="co"># 类似训练数据</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The elephant flew to the moon"</span>,  <span class="co"># 完全不同</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> test_sentences:</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>    ppl <span class="op">=</span> model.perplexity(sent)</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Perplexity of '</span><span class="sc">{</span>sent<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>ppl<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成文本</span></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Generated text:"</span>)</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting with 'The cat':"</span>, model.generate([<span class="st">'the'</span>, <span class="st">'cat'</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="使用crf做命名实体识别" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="使用crf做命名实体识别"><span class="header-section-number">7.2</span> 使用CRF做命名实体识别</h3>
<div id="56f242f3" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 需要安装: pip install sklearn-crfsuite</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn_crfsuite</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn_crfsuite <span class="im">import</span> metrics</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> word2features(sentence, i):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    为句子中第i个词提取特征</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    这就是"特征工程"——人工设计的特征</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    word <span class="op">=</span> sentence[i]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> {</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 基本特征</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.lower'</span>: word.lower(),</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.isupper'</span>: word.isupper(),</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.istitle'</span>: word.istitle(),</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.isdigit'</span>: word.isdigit(),</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 词的形态特征</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.prefix2'</span>: word[:<span class="dv">2</span>].lower(),</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.prefix3'</span>: word[:<span class="dv">3</span>].lower(),</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.suffix2'</span>: word[<span class="op">-</span><span class="dv">2</span>:].lower(),</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.suffix3'</span>: word[<span class="op">-</span><span class="dv">3</span>:].lower(),</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 词长</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.length'</span>: <span class="bu">len</span>(word),</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 上下文特征：前一个词</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        prev_word <span class="op">=</span> sentence[i<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        features.update({</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">'-1:word.lower'</span>: prev_word.lower(),</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">'-1:word.istitle'</span>: prev_word.istitle(),</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        features[<span class="st">'BOS'</span>] <span class="op">=</span> <span class="va">True</span>  <span class="co"># Beginning of Sentence</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 上下文特征：后一个词</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(sentence) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        next_word <span class="op">=</span> sentence[i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        features.update({</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            <span class="st">'+1:word.lower'</span>: next_word.lower(),</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            <span class="st">'+1:word.istitle'</span>: next_word.istitle(),</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        features[<span class="st">'EOS'</span>] <span class="op">=</span> <span class="va">True</span>  <span class="co"># End of Sentence</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sent2features(sentence):</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""为整个句子提取特征"""</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [word2features(sentence, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sentence))]</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例数据（简化的NER数据）</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> [</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (句子, 标签)</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    ([<span class="st">"Barack"</span>, <span class="st">"Obama"</span>, <span class="st">"was"</span>, <span class="st">"born"</span>, <span class="st">"in"</span>, <span class="st">"Hawaii"</span>, <span class="st">"."</span>],</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>     [<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-LOC"</span>, <span class="st">"O"</span>]),</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>    ([<span class="st">"Apple"</span>, <span class="st">"Inc"</span>, <span class="st">"is"</span>, <span class="st">"located"</span>, <span class="st">"in"</span>, <span class="st">"California"</span>, <span class="st">"."</span>],</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>     [<span class="st">"B-ORG"</span>, <span class="st">"I-ORG"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-LOC"</span>, <span class="st">"O"</span>]),</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    ([<span class="st">"John"</span>, <span class="st">"Smith"</span>, <span class="st">"works"</span>, <span class="st">"at"</span>, <span class="st">"Google"</span>, <span class="st">"."</span>],</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>     [<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-ORG"</span>, <span class="st">"O"</span>]),</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备训练数据</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> [sent2features(sent) <span class="cf">for</span> sent, _ <span class="kw">in</span> train_data]</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> [labels <span class="cf">for</span> _, labels <span class="kw">in</span> train_data]</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练CRF模型</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>crf <span class="op">=</span> sklearn_crfsuite.CRF(</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>    algorithm<span class="op">=</span><span class="st">'lbfgs'</span>,</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>    c1<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># L1正则化</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    c2<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># L2正则化</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>    max_iterations<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>crf.fit(X_train, y_train)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>test_sentence <span class="op">=</span> [<span class="st">"Elon"</span>, <span class="st">"Musk"</span>, <span class="st">"founded"</span>, <span class="st">"Tesla"</span>, <span class="st">"in"</span>, <span class="st">"Palo"</span>, <span class="st">"Alto"</span>, <span class="st">"."</span>]</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> [sent2features(test_sentence)]</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> crf.predict(X_test)[<span class="dv">0</span>]</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"NER结果："</span>)</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, label <span class="kw">in</span> <span class="bu">zip</span>(test_sentence, y_pred):</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:12}</span><span class="ss"> -&gt; </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="体会特征工程的痛苦" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="体会特征工程的痛苦"><span class="header-section-number">7.3</span> 体会特征工程的痛苦</h3>
<p>上面的CRF代码中，<code>word2features</code>函数就是特征工程的缩影。注意我们需要手工定义每一个特征：词的大小写、前缀后缀、上下文词…</p>
<p>现在想象一下，如果你需要处理一个新任务（比如关系抽取），或者换一个领域（比如生物医学文本），你需要重新设计整套特征。词典需要重新收集，词性标注可能不够用，需要加入领域特定的知识…</p>
<p>这就是为什么深度学习的”自动特征学习”如此革命性——它让我们摆脱了这个瓶颈。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">8</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者视角</strong>：这一节探讨传统NLP方法的理论基础和历史意义</p>
</blockquote>
<section id="为什么统计方法最终胜出" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="为什么统计方法最终胜出"><span class="header-section-number">8.1</span> 为什么统计方法最终胜出？</h3>
<p>符号主义和统计主义的争论贯穿了AI历史。在NLP领域，统计方法最终占据了主导，但这不意味着符号主义完全失败了。</p>
<p>统计方法胜出的原因可以从几个角度理解。首先是鲁棒性。规则系统对噪声和变体非常敏感，而统计方法天然具有”平均化”的能力，对噪声更鲁棒。其次是可扩展性。人工编写规则的速度无法跟上数据增长的速度，而统计模型可以自动从更多数据中受益。第三是客观评估。统计方法自然地引入了定量评估（准确率、F1等），使得不同方法可以公平比较，推动了领域的科学化。</p>
<p>然而，符号主义的一些洞察仍然有价值。语言确实有结构，语法规则确实存在，推理需要符号操作。现代的混合方法（neural-symbolic）试图结合两者的优势。</p>
</section>
<section id="马尔可夫假设的边界" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="马尔可夫假设的边界"><span class="header-section-number">8.2</span> 马尔可夫假设的边界</h3>
<p>N-gram和HMM都依赖马尔可夫假设。这个假设在什么时候合理，什么时候失效？</p>
<p>马尔可夫假设在局部依赖主导的情况下效果不错。比如词性标注中，当前词的词性主要由相邻的1-2个词决定。对于这类任务，HMM和N-gram可以取得很好的效果。</p>
<p>但语言中有大量长距离依赖，马尔可夫假设在这些情况下就失效了。主谓一致可能跨越很多词。代词指代可能需要回溯很多句子。篇章连贯性需要理解整个段落甚至文档。对于这些任务，固定窗口的模型天然受限。</p>
<p>这个局限性直接导向了第4章的RNN和第8章的Transformer——它们试图打破固定窗口的限制。</p>
</section>
<section id="生成模型-vs-判别模型" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="生成模型-vs-判别模型"><span class="header-section-number">8.3</span> 生成模型 vs 判别模型</h3>
<p>HMM是生成模型，CRF是判别模型。这两种范式的差异是什么？</p>
<p>生成模型建模联合分布<span class="math inline">\(P(X, Y)\)</span>，然后用贝叶斯定理得到<span class="math inline">\(P(Y|X)\)</span>。它对输入<span class="math inline">\(X\)</span>的分布做了假设（如HMM的发射概率假设）。这带来了一些优势：可以生成新样本，可以处理缺失数据，有时更容易解释。但也有劣势：模型假设可能与现实不符，难以引入复杂特征。</p>
<p>判别模型直接建模<span class="math inline">\(P(Y|X)\)</span>，不对<span class="math inline">\(X\)</span>的分布做假设。这带来了灵活性：可以引入任意特征，不需要担心<span class="math inline">\(X\)</span>的分布。在实践中，判别模型在分类任务上通常优于生成模型，这就是CRF超越HMM的原因。</p>
<p>深度学习模型大多是判别模型（如BERT做分类）。但生成模型的思想在语言模型（GPT系列）中得到了复兴——它们通过自回归方式”生成”文本。</p>
</section>
<section id="开放问题与反思" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="开放问题与反思"><span class="header-section-number">8.4</span> 开放问题与反思</h3>
<p>回顾这段历史，有几个开放问题值得思考。</p>
<p>第一，特征工程真的一无是处吗？深度学习确实可以自动学习特征，但人工设计的特征有时仍然有用。特别是在数据有限的情况下，好的特征可以引入有用的归纳偏置。如何将领域知识与端到端学习结合，仍然是一个活跃的研究方向。</p>
<p>第二，小模型有没有价值？N-gram和CRF模型非常轻量，可以在边缘设备上运行。在大模型时代，小而专的模型是否还有一席之地？</p>
<p>第三，可解释性的代价是什么？传统方法往往更可解释——你可以检查特征权重，理解模型为什么做出某个预测。深度学习模型是”黑箱”。这个trade-off在高风险应用中尤其重要。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</h2>
<section id="特征工程不可扩展" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="特征工程不可扩展"><span class="header-section-number">9.1</span> 特征工程不可扩展</h3>
<p>传统方法的最大局限是特征工程的瓶颈。这不只是”麻烦”的问题，而是”不可能”的问题——对于足够复杂的任务（如开放域问答、自由对话），没有人能设计出足够好的特征。</p>
</section>
<section id="表示的离散性" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="表示的离散性"><span class="header-section-number">9.2</span> 表示的离散性</h3>
<p>传统方法把词当作离散符号处理。“cat”和”dog”只是两个不同的ID，模型不知道它们都是动物、都是宠物、都有四条腿。这种离散表示无法捕获语义相似性，导致泛化困难。</p>
</section>
<section id="任务特定性" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="任务特定性"><span class="header-section-number">9.3</span> 任务特定性</h3>
<p>每个任务需要独立设计特征、独立训练模型。知识无法在任务之间迁移。这与人类的学习方式形成鲜明对比——人类学会一种语言技能后，可以轻松迁移到相关任务。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">9.4</span> 这些局限导向了什么？</h3>
<p>上述问题共同指向一个核心需求：学习词的分布式表示（distributed representation），让语义相似的词有相似的表示，让这种表示可以在任务之间共享和迁移。</p>
<p>这正是下一章的主题：表示学习，从Word2Vec开启的词向量革命。</p>
<blockquote class="blockquote">
<p>下一章预告：第2章将介绍Word2Vec——一个简单但革命性的想法，它让机器第一次能够”理解”词的含义。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">10</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</h3>
<p>这一章我们快速回顾了深度学习之前的NLP历史。</p>
<p>符号主义尝试用规则处理语言，在有限领域取得了成功，但无法扩展到复杂的真实世界任务。统计革命带来了数据驱动的范式，用概率模型从数据中学习规律。N-gram语言模型和HMM/CRF序列标注模型是这个时代的代表性技术。</p>
<p>但统计方法有一个致命弱点：依赖人工设计特征。这个”特征工程的诅咒”成为了整个领域的瓶颈，直到深度学习带来的”表示学习”革命才被打破。</p>
</section>
<section id="关键概念速查" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="关键概念速查"><span class="header-section-number">10.2</span> 关键概念速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>概念</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N-gram</td>
<td>基于马尔可夫假设的语言模型，用前n-1个词预测下一个词</td>
</tr>
<tr class="even">
<td>HMM</td>
<td>隐马尔可夫模型，用于序列标注的生成模型</td>
</tr>
<tr class="odd">
<td>CRF</td>
<td>条件随机场，用于序列标注的判别模型</td>
</tr>
<tr class="even">
<td>特征工程</td>
<td>人工设计将原始输入转换为数值表示的过程</td>
</tr>
<tr class="odd">
<td>生成模型 vs 判别模型</td>
<td>前者建模P(X,Y)，后者直接建模P(Y</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">10.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么说马尔可夫假设是一个”强假设”？举一个马尔可夫假设失效的具体例子。</p></li>
<li><p><strong>[对比分析]</strong> 比较HMM和CRF在建模假设上的区别。为什么CRF在序列标注任务上通常优于HMM？</p></li>
<li><p><strong>[工程实践]</strong> 为一个情感分析任务设计特征。你会设计哪些特征？这些特征能否捕获讽刺（irony）的情况？</p></li>
<li><p><strong>[研究思考]</strong> 如果深度学习没有出现，你认为NLP领域会如何发展？特征工程是否有其他的突破口？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">11</span> 延伸阅读</h2>
<section id="经典教材" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="经典教材"><span class="header-section-number">11.1</span> 经典教材</h3>
<p><strong>“Speech and Language Processing” (Jurafsky &amp; Martin)</strong>：NLP领域的”圣经”，第1-6章详细介绍了本章涉及的内容。第3版在线免费，是很好的参考资料。</p>
</section>
<section id="历史论文" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="历史论文"><span class="header-section-number">11.2</span> 历史论文</h3>
<p><strong>“A Maximum Entropy Model for Part-of-Speech Tagging” (Ratnaparkhi, 1996)</strong>：最大熵模型在NLP中的经典应用，展示了特征工程的典型范式。</p>
<p><strong>“Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data” (Lafferty et al., 2001)</strong>：CRF的原始论文，开创了判别式序列标注的时代。</p>
</section>
<section id="回顾与反思" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="回顾与反思"><span class="header-section-number">11.3</span> 回顾与反思</h3>
<p><strong>“The Last 10 Meters of AI” (Knight, 2019)</strong>：一篇讨论符号主义与神经网络融合的文章，提供了对这段历史的反思。</p>
</section>
<section id="工具与代码" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="工具与代码"><span class="header-section-number">11.4</span> 工具与代码</h3>
<p><strong>NLTK</strong> (Natural Language Toolkit)：Python的经典NLP库，包含大量传统方法的实现。适合学习和原型开发。</p>
<p><strong>sklearn-crfsuite</strong>：CRF的Python实现，本章实践部分使用了这个库。</p>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">12</span> 历史注脚</h2>
<p>“Every time I fire a linguist, the performance goes up”这句话经常被引用来说明统计方法的胜利，但它的完整语境更加微妙。Fred Jelinek并不是真的反对语言学——IBM的语音识别团队中也有语言学家。他的观点是：纯粹基于语言学规则的方法不如数据驱动的方法有效，但语言学知识仍然可以作为特征融入统计模型。</p>
<p>有趣的是，深度学习时代的某些发展似乎又在”回归”某种结构。Transformer的Self-Attention可以学习出类似语法依赖的模式；预训练模型似乎”知道”一些语法规则。也许符号主义与统计/神经方法的争论不是”谁对谁错”，而是”如何结合”。</p>
<p>另一个有趣的历史细节是：很多深度学习时代的”新想法”其实有很早的先驱。Word2Vec的思想可以追溯到1980年代的分布式表示（distributed representation）。注意力机制的雏形可以在传统的alignment model中找到。历史常常以螺旋的方式前进。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第1章：语言理解的早期探索"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "从符号到统计的范式转变"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-23"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, History, N-gram, HMM, CRF, Feature Engineering]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [符号主义, 统计NLP, 语言模型, 序列标注, 特征工程, Viterbi]</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "前深度学习时代的NLP技术演进：从规则系统到统计学习，从N-gram到CRF，以及为什么特征工程成为了整个领域的天花板。"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-1/early-nlp-banner.png"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：在深度学习之前，人们是如何让计算机理解和处理自然语言的？这些方法取得了什么成就，又遭遇了什么根本性的困难？</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：1950s-2010s </span><span class="pp">|</span><span class="at"> 从ELIZA到CRF </span><span class="pp">|</span><span class="at"> 符号主义→统计学习的演进</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从第0章说起</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>在上一章中，我们讨论了如何阅读NLP研究。现在，让我们正式开始NLP的技术之旅。</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>但在进入深度学习的世界之前，我们需要先回顾一段历史。这不是为了怀旧，而是因为理解"过去的问题"是理解"现在的解决方案"的前提。深度学习在NLP中的成功并非偶然——它解决了困扰这个领域几十年的根本性问题。如果你不知道那些问题是什么，你就无法真正理解为什么Word2Vec是革命性的，为什么Transformer如此重要，为什么大语言模型改变了一切。</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>这一章将快速回顾前深度学习时代的NLP。我们会看到两次重要的范式转变：从符号主义到统计方法，以及统计方法如何在"特征工程的诅咒"中逐渐走向瓶颈。这个瓶颈，正是第2章"表示学习"登场的舞台。</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：传统NLP的核心困境是"特征工程"——需要人工设计特征来表示语言现象。这不仅耗费大量专家知识，而且无法扩展到复杂任务。深度学习的革命性在于它"自动学习特征"，彻底打破了这个瓶颈。</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="fu">## 符号主义的辉煌与幻灭</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### 早期的梦想：让机器理解语言</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>1950年代，人工智能刚刚诞生，研究者们对让机器理解语言充满了乐观。当时主流的思路是"符号主义"（Symbolic AI）：把语言看作符号的操作，用逻辑规则来处理。</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>这个思路直觉上很自然。人类语言有语法规则，句子可以分析成主语、谓语、宾语。如果我们把这些规则编码成程序，机器不就能"理解"语言了吗？</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>1966年，MIT的Joseph Weizenbaum创建了ELIZA，一个模拟心理治疗师的程序。ELIZA没有任何真正的语言理解能力，它只是用简单的模式匹配和替换规则来响应用户输入。比如，如果用户说"I am sad"，ELIZA会匹配到"I am X"模式，然后回复"Why are you sad?"。这个简单的技巧却让很多用户相信他们在和一个有智能的存在交流。</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>ELIZA的成功（和它揭示的人类错觉）引发了一波对"聊天机器人"的热情。但更重要的是，它展示了符号主义方法的基本范式：人工编写规则，让机器按规则处理语言。</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="fu">### 规则系统的成就</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>在接下来的几十年里，基于规则的NLP系统取得了一些令人印象深刻的成就。</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>**句法分析器（Parser）**在理解句子结构方面做得相当不错。通过上下文无关文法（Context-Free Grammar）和更复杂的形式语法，研究者可以解析出句子的语法树。比如，"The cat sat on the mat"可以被分析为：<span class="co">[</span><span class="ot">NP [Det The] [N cat]</span><span class="co">] [VP [V sat]</span> <span class="co">[</span><span class="ot">PP [P on</span><span class="co">] [NP [Det the]</span> <span class="co">[</span><span class="ot">N mat</span><span class="co">]</span>]]]。这种分析对于问答系统、信息提取等任务是有用的。</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>**机器翻译**也在规则驱动下取得了进展。早期的机器翻译系统依赖双语词典和转换规则。IBM和其他公司投入了大量资源开发这类系统。对于结构相似的语言对（如西班牙语和葡萄牙语），这些系统能产生可用的翻译。</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>**专家系统**在特定领域展示了符号AI的威力。比如MYCIN系统可以诊断细菌感染并推荐抗生素，它用数百条规则编码了医学专家的知识。虽然MYCIN处理的是医学知识而非自然语言，但它代表了"用规则编码专家知识"这一范式的巅峰。</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### 规则系统的根本问题</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>然而，规则系统很快就遇到了难以逾越的障碍。</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>第一个问题是**规则爆炸**。自然语言的复杂性远超预期。一个简单的句子可能有多种合理的解析方式，需要上下文、常识、甚至语用知识来消歧。每增加一个现象的处理，就需要增加大量规则，而这些规则之间可能相互冲突。研究者发现，一个覆盖面广的语法系统可能有成千上万条规则，维护成本极其高昂。</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>第二个问题是**脆弱性**。规则系统对输入的变化非常敏感。一个句子稍微换一种说法，或者有一个拼写错误，系统就可能完全失效。真实世界的语言充满了非标准表达、省略、口语化用法，这些都是规则系统的噩梦。</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>第三个问题是**知识获取瓶颈**。编写规则需要语言学专家，而专家的时间是有限的。更糟糕的是，很多语言知识是隐性的（tacit knowledge）——母语者能轻松判断一个句子是否自然，但无法说清具体遵循了什么规则。这些知识无法被显式编码。</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>到了1980年代末，符号主义NLP陷入了困境。研究者开始意识到，靠人工编写规则来覆盖自然语言的复杂性可能是不可行的。这个领域需要一种全新的思路。</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="fu">## 统计革命：从规则到概率</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据驱动的范式转变</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>1990年代，NLP经历了一次根本性的范式转变：从"规则驱动"到"数据驱动"，从"符号处理"到"统计学习"。</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>这次转变的核心洞察是：与其让专家编写规则，不如让机器从大量数据中自动学习规律。语言中的规律可能太复杂、太隐晦，无法被人类显式编码，但它们确实存在于语言数据中——只要我们有足够的数据和正确的统计工具。</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>这个转变有几个推动力。首先是数据的可得性。随着数字化文本的增加，研究者可以获得越来越大的语料库。Penn Treebank（1992）提供了大量人工标注的句法树，Brown Corpus提供了标注好的词性数据。这些标注数据使得监督学习成为可能。</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>其次是计算能力的提升。统计方法往往需要大量计算，1990年代的计算机终于能够处理实用规模的问题了。</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>最关键的是统计机器翻译的成功。IBM的研究团队在1990年代初提出了一系列基于统计的机器翻译模型（IBM Model 1-5），效果显著优于规则系统。Fred Jelinek有一句著名的话："Every time I fire a linguist, the performance of the speech recognizer goes up."（每解雇一个语言学家，语音识别器的性能就提升一些。）这句话虽然有些夸张，但它捕捉到了当时的时代精神：数据和统计可能比语言学规则更重要。</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### 统计方法的基本思路</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>统计NLP的核心思想可以用一个公式概括：</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>\hat{y} = \arg\max_y P(y|x)</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>给定输入$x$（比如一个英文句子），找到最可能的输出$y$（比如对应的中文翻译）。这个框架可以套用到几乎所有NLP任务：词性标注是给定句子预测每个词的词性，命名实体识别是给定句子预测哪些片段是人名/地名/机构名，情感分析是给定评论预测正面/负面。</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>实现这个框架需要解决两个问题。第一是如何定义$P(y|x)$——用什么模型来表示这个概率分布？第二是如何从数据中学习模型的参数——这就是机器学习的任务。</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>在深度学习之前，主流的做法是：人工设计特征函数$\phi(x)$来表示输入，然后用经典的机器学习模型（如逻辑回归、SVM、CRF）来学习从特征到输出的映射。这就是"特征工程+经典分类器"的范式。</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="fu">## N-gram与语言模型基础</span></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="fu">### 什么是语言模型？</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>在深入讨论"特征工程"之前，我们先介绍一个贯穿整个NLP历史的核心概念：语言模型（Language Model）。</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>语言模型的目标是给文本分配概率——评估一段文本在给定语言中有多"自然"。形式化地，给定一个词序列$w_1, w_2, \ldots, w_n$，语言模型计算：</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>P(w_1, w_2, \ldots, w_n)</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>这个看似简单的目标有深远的应用。在机器翻译中，语言模型帮助选择更流畅的译文。在语音识别中，语言模型帮助消除声学歧义。在拼写纠错中，语言模型判断哪个候选词更合理。可以说，语言模型是NLP的"基础设施"。</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a><span class="fu">### N-gram：简单而有效</span></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>如何估计$P(w_1, w_2, \ldots, w_n)$？直接估计是不可行的——可能的词序列数量随长度指数增长，任何语料库都无法覆盖所有可能的句子。</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>N-gram语言模型提供了一个简洁的近似。核心假设是**马尔可夫假设**：一个词的出现只依赖于它前面的$n-1$个词。用链式法则展开联合概率：</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>P(w_1, \ldots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, \ldots, w_{i-1})</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>然后用马尔可夫假设简化：</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>P(w_i | w_1, \ldots, w_{i-1}) \approx P(w_i | w_{i-n+1}, \ldots, w_{i-1})</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>当$n=2$时，称为bigram模型：$P(w_i | w_{i-1})$。当$n=3$时，称为trigram模型：$P(w_i | w_{i-2}, w_{i-1})$。</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>这些条件概率如何估计？最简单的方法是最大似然估计（MLE），也就是计数：</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>其中$C(\cdot)$是语料库中的计数。这种方法直观、高效，在足够大的语料库上可以得到不错的估计。</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：Bigram 概率计算</span></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>让我们用一个极简的例子来具体理解 N-gram 是如何工作的。</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>**语料库**（3个句子）：</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I love cats"</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I love dogs"</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"cats love fish"</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>**Step 1: 统计 Bigram 计数**</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>首先为每个句子加上起始符号 <span class="in">`&lt;s&gt;`</span> 和结束符号 <span class="in">`&lt;/s&gt;`</span>，然后统计所有相邻词对的出现次数：</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Bigram <span class="pp">|</span> 计数 <span class="pp">|</span></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|------|</span></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`&lt;s&gt;`</span> I <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> I love <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> love cats <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> love dogs <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> love fish <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> cats <span class="in">`&lt;/s&gt;`</span> <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> dogs <span class="in">`&lt;/s&gt;`</span> <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> fish <span class="in">`&lt;/s&gt;`</span> <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> cats love <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>同时统计每个词作为"前一个词"出现的总次数：</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词 <span class="pp">|</span> 作为前词的总次数 <span class="pp">|</span></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a><span class="pp">|----|-----------------|</span></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`&lt;s&gt;`</span> <span class="pp">|</span> 3 <span class="pp">|</span></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> I <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> love <span class="pp">|</span> 3 <span class="pp">|</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> cats <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> dogs <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> fish <span class="pp">|</span> 1 <span class="pp">|</span></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>**Step 2: 计算条件概率**</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>使用 MLE 公式 $P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$：</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>P(\text{love} | \text{I}) = \frac{C(\text{I love})}{C(\text{I})} = \frac{2}{2} = 1.0</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>P(\text{cats} | \text{love}) = \frac{C(\text{love cats})}{C(\text{love})} = \frac{1}{3} \approx 0.33</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>P(\text{dogs} | \text{love}) = \frac{C(\text{love dogs})}{C(\text{love})} = \frac{1}{3} \approx 0.33</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>**Step 3: 计算句子概率**</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>现在计算一个新句子 "I love fish" 的概率：</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>P(\text{I love fish}) = P(\text{I}|\text{<span class="dt">&lt;</span><span class="kw">s</span><span class="dt">&gt;</span>}) \times P(\text{love}|\text{I}) \times P(\text{fish}|\text{love}) \times P(\text{<span class="dt">&lt;/</span><span class="kw">s</span><span class="dt">&gt;</span>}|\text{fish})</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>= \frac{2}{3} \times \frac{2}{2} \times \frac{1}{3} \times \frac{1}{1} = \frac{2}{3} \times 1 \times \frac{1}{3} \times 1 = \frac{2}{9} \approx 0.22</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>**Step 4: 计算困惑度（Perplexity）**</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>困惑度是语言模型的标准评估指标，定义为：</span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>\text{PPL} = P(w_1, \ldots, w_n)^{-1/n} = \exp\left(-\frac{1}{n}\sum_{i=1}^{n} \log P(w_i|w_{i-1})\right)</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>对于 "I love fish"（4个token，含<span class="in">`&lt;/s&gt;`</span>）：</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>\text{PPL} = \left(\frac{2}{9}\right)^{-1/4} = \left(\frac{9}{2}\right)^{1/4} \approx 1.46</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>困惑度越低，说明模型对句子的"惊讶程度"越低，即句子越符合训练数据的分布。</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>**关键观察**：</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I love fish" 虽然在训练数据中没有出现过，但因为 "love fish" 这个 bigram 出现过，所以概率不为零</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>如果我们问 "I hate cats" 的概率，由于 "I hate" 从未出现，MLE 会给出 $P = 0$——这就是稀疏性问题</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### N-gram的局限</span></span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>N-gram模型虽然简单有效，但有几个根本性的局限。</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>第一是**稀疏性问题**。即使在大型语料库中，大多数n-gram组合也从未出现过。如果$C(w_{i-1}, w_i) = 0$，MLE会给出$P(w_i|w_{i-1}) = 0$，这显然不合理——一个从未见过的词序列不一定是不可能的。研究者发明了各种"平滑"技术来解决这个问题（加一平滑、Kneser-Ney平滑、Good-Turing估计），但这本质上是在"打补丁"。</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>第二是**上下文窗口固定**。马尔可夫假设是一个很强的假设：词只依赖于前面的$n-1$个词。但语言中的依赖关系可以跨越很长的距离。考虑这个例子："The keys to the cabinet are on the table."主语"keys"和动词"are"之间隔了四个词。trigram模型在预测"are"时只能看到"cabinet are"，完全看不到"keys"，因此会倾向于预测单数形式"is"。增大$n$可以部分缓解这个问题，但参数数量会指数增长，很快变得不可行。</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>第三是**没有泛化能力**。N-gram完全依赖表面形式的计数，不理解词的含义。"The cat sat on the mat"和"The dog sat on the mat"对N-gram来说是完全不同的序列，即使人类知道"cat"和"dog"都是动物、都能坐在垫子上。</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>这些局限预示了后面章节要解决的问题：如何表示词的"含义"（第2章：Word2Vec），如何建模长距离依赖（第4章：LSTM），如何不受固定窗口限制（第5-8章：Attention和Transformer）。</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a><span class="fu">## 序列标注：从HMM到CRF的演进</span></span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a><span class="fu">### 序列标注问题</span></span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>NLP中有一大类任务属于"序列标注"（Sequence Labeling）：给定一个词序列，为每个词分配一个标签。经典例子包括词性标注（POS tagging）——判断每个词是名词、动词还是形容词，命名实体识别（NER）——识别出人名、地名、机构名等，以及分词（对于中文等没有明确词边界的语言）。</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>序列标注的挑战在于标签之间有依赖关系。比如在词性标注中，冠词后面通常是名词或形容词，而不是动词。在NER中，一个人名可能由多个词组成（如"New York"是一个整体）。独立地预测每个词的标签会忽略这些依赖，效果不佳。</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### 隐马尔可夫模型（HMM）</span></span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>隐马尔可夫模型是序列标注的经典工具。它假设存在一个隐藏的状态序列（标签），生成了我们观察到的词序列。</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>HMM有两个核心假设。第一是马尔可夫假设：当前状态只依赖于前一个状态，$P(y_t | y_1, \ldots, y_{t-1}) = P(y_t | y_{t-1})$。第二是输出独立性假设：当前观察只依赖于当前状态，$P(x_t | y_1, \ldots, y_t, x_1, \ldots, x_{t-1}) = P(x_t | y_t)$。</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>基于这两个假设，HMM定义了序列的联合概率：</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>P(x, y) = \prod_{t=1}^{T} P(y_t | y_{t-1}) \cdot P(x_t | y_t)</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>其中$P(y_t | y_{t-1})$是转移概率（transition probability），$P(x_t | y_t)$是发射概率（emission probability）。</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>给定一个观察序列$x$，如何找到最可能的标签序列$y$？这是一个动态规划问题，可以用Viterbi算法高效求解，复杂度是$O(TK^2)$，其中$T$是序列长度，$K$是标签数量。</span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Viterbi 解码 (Rabiner, 1989)</span></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>**输入**：观察序列 $O = (o_1, o_2, \ldots, o_T)$，HMM 参数 $\lambda = (\pi, A, B)$</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>**输出**：最优状态序列 $Q^* = (q_1^*, q_2^*, \ldots, q_T^*)$</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>**1. 初始化** ($t = 1$)：</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>\delta_1(i) = \pi_i \cdot b_i(o_1), \quad 1 \leq i \leq N</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>\psi_1(i) = 0</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>**2. 递归** ($t = 2, 3, \ldots, T$)：</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>\delta_t(j) = \max_{1 \leq i \leq N} \left<span class="co">[</span><span class="ot"> \delta_{t-1}(i) \cdot a_{ij} \right</span><span class="co">]</span> \cdot b_j(o_t), \quad 1 \leq j \leq N</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>\psi_t(j) = \arg\max_{1 \leq i \leq N} \left<span class="co">[</span><span class="ot"> \delta_{t-1}(i) \cdot a_{ij} \right</span><span class="co">]</span>, \quad 1 \leq j \leq N</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>**3. 终止**：</span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>P^* = \max_{1 \leq i \leq N} \delta_T(i)</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>q_T^* = \arg\max_{1 \leq i \leq N} \delta_T(i)</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>**4. 回溯** ($t = T-1, T-2, \ldots, 1$)：</span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>q_t^* = \psi_{t+1}(q_{t+1}^*)</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>其中：</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\delta_t(i)$：在时刻 $t$ 状态为 $i$，且经过**最优路径**到达的概率</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\psi_t(j)$：回溯指针，记录到达状态 $j$ 的最优前驱状态</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\pi_i$：初始状态概率，$a_{ij}$：转移概率，$b_j(o_t)$：发射概率</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>*Source: Rabiner, L. (1989). "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition". Proceedings of the IEEE, 77(2), 257-286. [PDF](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf)*</span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：Viterbi 解码</span></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的例子来理解 Viterbi 算法是如何找到最优标签序列的。</span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>**问题设定**：词性标注任务</span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**状态（标签）**：N（名词）、V（动词），共2种</span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**观察（词）**：句子 "fish can swim"</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**目标**：找到最可能的词性序列</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>**模型参数**：</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>**初始概率** $\pi$（句子以某个词性开始的概率）：</span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>\pi(N) = 0.6, \quad \pi(V) = 0.4</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a>**转移概率** $A$（从一个词性转移到另一个词性）：</span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P(y_t <span class="sc">\|</span> y_{t-1})$ <span class="pp">|</span> → N <span class="pp">|</span> → V <span class="pp">|</span></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a><span class="pp">|---------------------|-----|-----|</span></span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> N → <span class="pp">|</span> 0.3 <span class="pp">|</span> 0.7 <span class="pp">|</span></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> V → <span class="pp">|</span> 0.8 <span class="pp">|</span> 0.2 <span class="pp">|</span></span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>解读：名词后面 70% 是动词，动词后面 80% 是名词。</span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>**发射概率** $B$（某个词性生成某个词）：</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P(x <span class="sc">\|</span> y)$ <span class="pp">|</span> "fish" <span class="pp">|</span> "can" <span class="pp">|</span> "swim" <span class="pp">|</span></span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------------|--------|-------|--------|</span></span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> N <span class="pp">|</span> 0.4 <span class="pp">|</span> 0.3 <span class="pp">|</span> 0.1 <span class="pp">|</span></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> V <span class="pp">|</span> 0.1 <span class="pp">|</span> 0.2 <span class="pp">|</span> 0.5 <span class="pp">|</span></span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>解读："fish" 更可能是名词（0.4 vs 0.1），"swim" 更可能是动词（0.5 vs 0.1）。</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>**Viterbi 算法逐步计算**</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a>Viterbi 使用动态规划，定义 $\delta_t(j)$ 为在时刻 $t$、状态为 $j$ 时，所有路径中的最大概率。</span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a>**$t=1$：处理 "fish"**</span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a>\delta_1(N) = \pi(N) \times P(\text{fish}|N) = 0.6 \times 0.4 = 0.24</span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a>\delta_1(V) = \pi(V) \times P(\text{fish}|V) = 0.4 \times 0.1 = 0.04</span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 状态 <span class="pp">|</span> $\delta_1$ <span class="pp">|</span> 回溯指针 <span class="pp">|</span></span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------------|---------|</span></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> N <span class="pp">|</span> **0.24** <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> V <span class="pp">|</span> 0.04 <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a>**$t=2$：处理 "can"**</span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a>对于每个当前状态，考虑所有可能的前一状态，取最大值：</span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a>\delta_2(N) = \max\begin{cases}</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a>\delta_1(N) \times P(N|N) \times P(\text{can}|N) = 0.24 \times 0.3 \times 0.3 = 0.0216 <span class="sc">\\</span></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>\delta_1(V) \times P(N|V) \times P(\text{can}|N) = 0.04 \times 0.8 \times 0.3 = 0.0096</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>最大值是 0.0216，来自 N→N 路径。</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a>\delta_2(V) = \max\begin{cases}</span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>\delta_1(N) \times P(V|N) \times P(\text{can}|V) = 0.24 \times 0.7 \times 0.2 = 0.0336 <span class="sc">\\</span></span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a>\delta_1(V) \times P(V|V) \times P(\text{can}|V) = 0.04 \times 0.2 \times 0.2 = 0.0016</span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a>最大值是 0.0336，来自 N→V 路径。</span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 状态 <span class="pp">|</span> $\delta_2$ <span class="pp">|</span> 回溯指针 <span class="pp">|</span></span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------------|---------|</span></span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> N <span class="pp">|</span> 0.0216 <span class="pp">|</span> ← N <span class="pp">|</span></span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> V <span class="pp">|</span> **0.0336** <span class="pp">|</span> ← N <span class="pp">|</span></span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>**$t=3$：处理 "swim"**</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a>\delta_3(N) = \max\begin{cases}</span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>\delta_2(N) \times P(N|N) \times P(\text{swim}|N) = 0.0216 \times 0.3 \times 0.1 = 0.000648 <span class="sc">\\</span></span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a>\delta_2(V) \times P(N|V) \times P(\text{swim}|N) = 0.0336 \times 0.8 \times 0.1 = 0.002688</span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a>最大值是 0.002688，来自 V→N 路径。</span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>\delta_3(V) = \max\begin{cases}</span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>\delta_2(N) \times P(V|N) \times P(\text{swim}|V) = 0.0216 \times 0.7 \times 0.5 = 0.00756 <span class="sc">\\</span></span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>\delta_2(V) \times P(V|V) \times P(\text{swim}|V) = 0.0336 \times 0.2 \times 0.5 = 0.00336</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>最大值是 0.00756，来自 N→V 路径。</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 状态 <span class="pp">|</span> $\delta_3$ <span class="pp">|</span> 回溯指针 <span class="pp">|</span></span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------------|---------|</span></span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> N <span class="pp">|</span> 0.002688 <span class="pp">|</span> ← V <span class="pp">|</span></span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> V <span class="pp">|</span> **0.00756** <span class="pp">|</span> ← N <span class="pp">|</span></span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a>**回溯找最优路径**</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$t=3$：最大的是 $\delta_3(V) = 0.00756$，所以 $y_3 = V$</span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$t=2$：$y_3 = V$ 的回溯指针指向 N，所以 $y_2 = N$</span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$t=1$：$y_2 = N$ 的回溯指针指向 N，所以 $y_1 = N$</span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a>**最终结果**：</span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词 <span class="pp">|</span> fish <span class="pp">|</span> can <span class="pp">|</span> swim <span class="pp">|</span></span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a><span class="pp">|----|------|-----|------|</span></span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词性 <span class="pp">|</span> **N** | **N** | **V** <span class="pp">|</span></span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>解读："fish" 是名词（鱼），"can" 是名词（罐头），"swim" 是动词。这个结果符合直觉！</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a><span class="al">![Viterbi 解码过程全景：从左到右，算法在每个时间步为每个状态计算最大累积概率δ，并记录最优前驱。橙色高亮路径 N→N→V 是全局最优解。注意交叉箭头上的转移概率如何影响路径选择——即使"can"作为动词（B(V)=0.2）的发射概率低于名词（B(N)=0.3），N→V 路径的高转移概率（a=0.7）仍然使得 δ₂(V) &gt; δ₂(N)。](figures/chapter-1/fig-viterbi-trellis.png)</span>{#fig-viterbi-trellis}</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>*Python生成，基于 Rabiner (1989) Viterbi 算法*</span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a><span class="fu">## 为什么不是 N-V-V？</span></span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a>你可能会问："can" 不是也可以当动词（能够）吗？为什么 Viterbi 选了 N-N-V 而不是 N-V-V？</span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>让我们比较这两条路径的概率：</span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**N-N-V**：$0.6 \times 0.4 \times 0.3 \times 0.3 \times 0.7 \times 0.5 = 0.00756$</span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**N-V-V**：$0.6 \times 0.4 \times 0.7 \times 0.2 \times 0.2 \times 0.5 = 0.00336$</span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>N-N-V 更优！原因是：虽然 "can" 作动词合理，但 V→V 的转移概率只有 0.2（连续两个动词不常见），而 N→V 的转移概率是 0.7。这个例子展示了 HMM 如何利用**转移概率**来消歧。</span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a>HMM在词性标注等任务上取得了不错的效果，长期是主流方法。但它有一个根本性的局限：生成模型的约束。HMM是一个生成模型，它建模的是$P(x, y)$而非$P(y|x)$。为了使模型tractable，它必须做很强的独立性假设（发射概率只依赖当前状态）。这意味着它很难利用丰富的输入特征。</span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a><span class="fu">### 条件随机场（CRF）</span></span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>条件随机场（Conditional Random Field）是对HMM的一个根本性改进。它是判别模型，直接建模$P(y|x)$，因此不需要对输入$x$做任何假设。</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a>线性链CRF的形式是：</span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a>P(y|x) = \frac{1}{Z(x)} \exp\left(\sum_{t=1}^{T} \sum_k \lambda_k f_k(y_{t-1}, y_t, x, t)\right)</span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a>这里$f_k$是特征函数，$\lambda_k$是要学习的权重，$Z(x)$是归一化常数。</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>CRF的关键优势是特征的灵活性。特征函数$f_k$可以依赖于整个输入序列$x$，而不只是当前位置。比如，可以定义特征"如果当前词以-ing结尾，且标签是VBG（动词进行时），则特征为1"，或者"如果前一个词是冠词，且当前标签是NN（名词），则特征为1"。这种灵活性让CRF可以利用丰富的语言学知识和上下文信息。</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a>CRF在2000年代成为序列标注的标准方法，在很多任务上显著超越了HMM。然而，它也面临一个问题：特征需要人工设计。这就是下一节要讨论的"特征工程的诅咒"。</span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心痛点：特征工程的诅咒</span></span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a><span class="fu">### 什么是特征工程？</span></span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a>在深度学习之前，几乎所有NLP系统都遵循同一个范式：</span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a><span class="in">原始文本 → 特征提取 → 经典分类器 → 预测结果</span></span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a>"特征提取"这一步是关键，也是最耗费人力的部分。特征（feature）是对输入的一种数值表示，它应该捕获与任务相关的信息。</span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a>以情感分析为例。如何表示一条评论，让分类器能判断它是正面还是负面？研究者需要设计各种特征。最简单的是词袋特征（Bag of Words），统计每个词出现的次数，形成一个高维向量。然后可以加入N-gram特征，不只看单词，还看词组，如"not good"作为一个整体。还可以设计否定词处理特征，检测"not""never"等否定词，翻转后续词的极性。可以利用情感词典，统计正面词和负面词的数量。句法特征也有用，比如主语是什么，动词是什么，形容词修饰什么...</span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a>这个过程就是"特征工程"：由领域专家（语言学家、NLP研究者）根据任务需求和语言学知识，手工设计特征。</span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a><span class="fu">### 特征工程的困境</span></span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a>特征工程有几个根本性的问题。</span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a>第一是**耗费专家知识**。设计好的特征需要深厚的语言学背景和任务理解。不同任务需要不同的特征，一个情感分析的专家设计的特征可能对问答系统毫无用处。专家的时间是昂贵的、有限的。</span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-495"><a href="#cb4-495" aria-hidden="true" tabindex="-1"></a>第二是**无法捕获复杂模式**。很多语言现象太复杂、太隐晦，人类专家无法想到合适的特征。比如，讽刺（irony）的检测需要理解字面意思和说话者意图的不一致，这种"不一致"如何用特征表示？隐喻（metaphor）的理解需要概念之间的类比关系，这如何显式编码？</span>
<span id="cb4-496"><a href="#cb4-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a>第三是**特征之间的交互**。很多有用的模式是特征的组合，而不是单个特征。比如，"not bad"表达的是轻微正面情感，需要同时考虑"not"和"bad"的交互。手工设计所有可能的交互是不可行的——特征数量会组合爆炸。</span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a>第四是**任务迁移困难**。为一个任务设计的特征往往不能直接用于另一个任务。每换一个任务，特征工程就要从头做起。这与人类的学习方式形成鲜明对比——人类学会阅读后，可以轻松迁移到各种阅读理解任务。</span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个具体的例子：命名实体识别</span></span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a>让我用命名实体识别（NER）来具体说明特征工程的复杂性。</span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a>NER的任务是识别文本中的人名、地名、机构名等实体。一个典型的句子可能是："Barack Obama was born in Hawaii."我们需要识别出"Barack Obama"是人名（PER），"Hawaii"是地名（LOC）。</span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-507"><a href="#cb4-507" aria-hidden="true" tabindex="-1"></a>为了训练一个CRF模型做NER，研究者可能设计这些特征：</span>
<span id="cb4-508"><a href="#cb4-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a>**词级特征**：当前词本身、当前词的小写形式、当前词的前缀（前3个字符）、当前词的后缀（后3个字符）。</span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a>**正字法特征**（orthographic）：词是否全大写、词是否首字母大写、词是否包含数字、词是否包含标点。</span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a>**上下文特征**：前一个词、后一个词、前两个词、后两个词，以及它们的各种组合。</span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a>**词典特征**：词是否出现在人名词典中、是否出现在地名词典中、是否出现在机构名词典中。</span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a>**句法特征**：词的词性标签、词在句法树中的位置、依存关系。</span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a>**聚类特征**：词在Brown Clustering中的cluster ID（一种无监督的词聚类方法）。</span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a>一个工业级的NER系统可能有成百上千个这样的特征。研究者需要逐一设计、实验、调优。如果换一个领域（比如从新闻变成生物医学文本），很多特征需要重新设计，词典需要重新收集。</span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a>这就是"特征工程的诅咒"：它work，但它不scale。</span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a><span class="fu">### 瓶颈的本质</span></span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a>如果我们抽象一下，特征工程的本质问题是：**表示问题**。</span>
<span id="cb4-528"><a href="#cb4-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-529"><a href="#cb4-529" aria-hidden="true" tabindex="-1"></a>我们需要把原始文本（一个符号序列）转换成机器学习模型可以处理的数值表示（向量）。传统方法依赖人工设计这个转换过程。但自然语言太复杂、太多变，人工设计无法覆盖所有情况。</span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a>这就引出了一个自然的问题：能不能让机器自己学习如何表示语言？</span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a>这个问题的答案，就是下一章的主题：表示学习，从Word2Vec开始的"词向量"革命。</span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-535"><a href="#cb4-535" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-536"><a href="#cb4-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-537"><a href="#cb4-537" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：传统NLP Pipeline体验</span></span>
<span id="cb4-538"><a href="#cb4-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-539"><a href="#cb4-539" aria-hidden="true" tabindex="-1"></a>为了让你对传统NLP有更直观的感受，让我们用Python实现一个简单的N-gram语言模型和一个基于CRF的NER系统。</span>
<span id="cb4-540"><a href="#cb4-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-541"><a href="#cb4-541" aria-hidden="true" tabindex="-1"></a><span class="fu">### N-gram语言模型</span></span>
<span id="cb4-542"><a href="#cb4-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-545"><a href="#cb4-545" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-546"><a href="#cb4-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb4-547"><a href="#cb4-547" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-548"><a href="#cb4-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-549"><a href="#cb4-549" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, Counter</span>
<span id="cb4-550"><a href="#cb4-550" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-551"><a href="#cb4-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-552"><a href="#cb4-552" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NgramLanguageModel:</span>
<span id="cb4-553"><a href="#cb4-553" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-554"><a href="#cb4-554" aria-hidden="true" tabindex="-1"></a><span class="co">    简单的N-gram语言模型</span></span>
<span id="cb4-555"><a href="#cb4-555" aria-hidden="true" tabindex="-1"></a><span class="co">    使用加一平滑（Laplace smoothing）处理稀疏性</span></span>
<span id="cb4-556"><a href="#cb4-556" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-557"><a href="#cb4-557" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb4-558"><a href="#cb4-558" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb4-559"><a href="#cb4-559" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ngram_counts <span class="op">=</span> defaultdict(Counter)  <span class="co"># context -&gt; {word: count}</span></span>
<span id="cb4-560"><a href="#cb4-560" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_counts <span class="op">=</span> Counter()  <span class="co"># context -&gt; total count</span></span>
<span id="cb4-561"><a href="#cb4-561" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb4-562"><a href="#cb4-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-563"><a href="#cb4-563" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tokenize(<span class="va">self</span>, text):</span>
<span id="cb4-564"><a href="#cb4-564" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""简单的分词：按空格分割，加上起始和结束标记"""</span></span>
<span id="cb4-565"><a href="#cb4-565" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> text.lower().split()</span>
<span id="cb4-566"><a href="#cb4-566" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="st">'&lt;s&gt;'</span>] <span class="op">*</span> (<span class="va">self</span>.n <span class="op">-</span> <span class="dv">1</span>) <span class="op">+</span> tokens <span class="op">+</span> [<span class="st">'&lt;/s&gt;'</span>]</span>
<span id="cb4-567"><a href="#cb4-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-568"><a href="#cb4-568" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, corpus):</span>
<span id="cb4-569"><a href="#cb4-569" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""从语料库训练模型"""</span></span>
<span id="cb4-570"><a href="#cb4-570" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> corpus:</span>
<span id="cb4-571"><a href="#cb4-571" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> <span class="va">self</span>.tokenize(sentence)</span>
<span id="cb4-572"><a href="#cb4-572" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.vocab.update(tokens)</span>
<span id="cb4-573"><a href="#cb4-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-574"><a href="#cb4-574" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n <span class="op">-</span> <span class="dv">1</span>, <span class="bu">len</span>(tokens)):</span>
<span id="cb4-575"><a href="#cb4-575" aria-hidden="true" tabindex="-1"></a>                context <span class="op">=</span> <span class="bu">tuple</span>(tokens[i <span class="op">-</span> <span class="va">self</span>.n <span class="op">+</span> <span class="dv">1</span> : i])  <span class="co"># 前n-1个词</span></span>
<span id="cb4-576"><a href="#cb4-576" aria-hidden="true" tabindex="-1"></a>                word <span class="op">=</span> tokens[i]</span>
<span id="cb4-577"><a href="#cb4-577" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.ngram_counts[context][word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-578"><a href="#cb4-578" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.context_counts[context] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-579"><a href="#cb4-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-580"><a href="#cb4-580" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> probability(<span class="va">self</span>, word, context):</span>
<span id="cb4-581"><a href="#cb4-581" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""计算P(word|context)，使用加一平滑"""</span></span>
<span id="cb4-582"><a href="#cb4-582" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> <span class="bu">tuple</span>(context[<span class="op">-</span>(<span class="va">self</span>.n<span class="op">-</span><span class="dv">1</span>):])  <span class="co"># 只取最后n-1个词</span></span>
<span id="cb4-583"><a href="#cb4-583" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> <span class="va">self</span>.ngram_counts[context][word]</span>
<span id="cb4-584"><a href="#cb4-584" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="va">self</span>.context_counts[context]</span>
<span id="cb4-585"><a href="#cb4-585" aria-hidden="true" tabindex="-1"></a>        vocab_size <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.vocab)</span>
<span id="cb4-586"><a href="#cb4-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-587"><a href="#cb4-587" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加一平滑：(count + 1) / (total + vocab_size)</span></span>
<span id="cb4-588"><a href="#cb4-588" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (count <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (total <span class="op">+</span> vocab_size)</span>
<span id="cb4-589"><a href="#cb4-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-590"><a href="#cb4-590" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> perplexity(<span class="va">self</span>, sentence):</span>
<span id="cb4-591"><a href="#cb4-591" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""计算句子的困惑度（perplexity）"""</span></span>
<span id="cb4-592"><a href="#cb4-592" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> <span class="va">self</span>.tokenize(sentence)</span>
<span id="cb4-593"><a href="#cb4-593" aria-hidden="true" tabindex="-1"></a>        log_prob <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-594"><a href="#cb4-594" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-595"><a href="#cb4-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-596"><a href="#cb4-596" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n <span class="op">-</span> <span class="dv">1</span>, <span class="bu">len</span>(tokens)):</span>
<span id="cb4-597"><a href="#cb4-597" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> tokens[i <span class="op">-</span> <span class="va">self</span>.n <span class="op">+</span> <span class="dv">1</span> : i]</span>
<span id="cb4-598"><a href="#cb4-598" aria-hidden="true" tabindex="-1"></a>            word <span class="op">=</span> tokens[i]</span>
<span id="cb4-599"><a href="#cb4-599" aria-hidden="true" tabindex="-1"></a>            prob <span class="op">=</span> <span class="va">self</span>.probability(word, context)</span>
<span id="cb4-600"><a href="#cb4-600" aria-hidden="true" tabindex="-1"></a>            log_prob <span class="op">+=</span> math.log(prob)</span>
<span id="cb4-601"><a href="#cb4-601" aria-hidden="true" tabindex="-1"></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-602"><a href="#cb4-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-603"><a href="#cb4-603" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perplexity = exp(-average log probability)</span></span>
<span id="cb4-604"><a href="#cb4-604" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.exp(<span class="op">-</span>log_prob <span class="op">/</span> count)</span>
<span id="cb4-605"><a href="#cb4-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-606"><a href="#cb4-606" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, context, max_length<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb4-607"><a href="#cb4-607" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""给定上下文，生成后续文本"""</span></span>
<span id="cb4-608"><a href="#cb4-608" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> random</span>
<span id="cb4-609"><a href="#cb4-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-610"><a href="#cb4-610" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> <span class="bu">list</span>(context)</span>
<span id="cb4-611"><a href="#cb4-611" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">=</span> []</span>
<span id="cb4-612"><a href="#cb4-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-613"><a href="#cb4-613" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb4-614"><a href="#cb4-614" aria-hidden="true" tabindex="-1"></a>            ctx <span class="op">=</span> <span class="bu">tuple</span>(context[<span class="op">-</span>(<span class="va">self</span>.n<span class="op">-</span><span class="dv">1</span>):])</span>
<span id="cb4-615"><a href="#cb4-615" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ctx <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.ngram_counts:</span>
<span id="cb4-616"><a href="#cb4-616" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb4-617"><a href="#cb4-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-618"><a href="#cb4-618" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 按概率采样下一个词</span></span>
<span id="cb4-619"><a href="#cb4-619" aria-hidden="true" tabindex="-1"></a>            words <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.ngram_counts[ctx].keys())</span>
<span id="cb4-620"><a href="#cb4-620" aria-hidden="true" tabindex="-1"></a>            weights <span class="op">=</span> [<span class="va">self</span>.ngram_counts[ctx][w] <span class="cf">for</span> w <span class="kw">in</span> words]</span>
<span id="cb4-621"><a href="#cb4-621" aria-hidden="true" tabindex="-1"></a>            next_word <span class="op">=</span> random.choices(words, weights<span class="op">=</span>weights)[<span class="dv">0</span>]</span>
<span id="cb4-622"><a href="#cb4-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-623"><a href="#cb4-623" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> next_word <span class="op">==</span> <span class="st">'&lt;/s&gt;'</span>:</span>
<span id="cb4-624"><a href="#cb4-624" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb4-625"><a href="#cb4-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-626"><a href="#cb4-626" aria-hidden="true" tabindex="-1"></a>            generated.append(next_word)</span>
<span id="cb4-627"><a href="#cb4-627" aria-hidden="true" tabindex="-1"></a>            context.append(next_word)</span>
<span id="cb4-628"><a href="#cb4-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-629"><a href="#cb4-629" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">' '</span>.join(generated)</span>
<span id="cb4-630"><a href="#cb4-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-631"><a href="#cb4-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-632"><a href="#cb4-632" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例使用</span></span>
<span id="cb4-633"><a href="#cb4-633" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb4-634"><a href="#cb4-634" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat sat on the mat"</span>,</span>
<span id="cb4-635"><a href="#cb4-635" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The dog sat on the floor"</span>,</span>
<span id="cb4-636"><a href="#cb4-636" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat chased the mouse"</span>,</span>
<span id="cb4-637"><a href="#cb4-637" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The dog chased the cat"</span>,</span>
<span id="cb4-638"><a href="#cb4-638" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A cat is a small animal"</span>,</span>
<span id="cb4-639"><a href="#cb4-639" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A dog is a loyal animal"</span>,</span>
<span id="cb4-640"><a href="#cb4-640" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-641"><a href="#cb4-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-642"><a href="#cb4-642" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NgramLanguageModel(n<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb4-643"><a href="#cb4-643" aria-hidden="true" tabindex="-1"></a>model.train(corpus)</span>
<span id="cb4-644"><a href="#cb4-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-645"><a href="#cb4-645" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算困惑度</span></span>
<span id="cb4-646"><a href="#cb4-646" aria-hidden="true" tabindex="-1"></a>test_sentences <span class="op">=</span> [</span>
<span id="cb4-647"><a href="#cb4-647" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat sat on the floor"</span>,  <span class="co"># 类似训练数据</span></span>
<span id="cb4-648"><a href="#cb4-648" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The elephant flew to the moon"</span>,  <span class="co"># 完全不同</span></span>
<span id="cb4-649"><a href="#cb4-649" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-650"><a href="#cb4-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-651"><a href="#cb4-651" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> test_sentences:</span>
<span id="cb4-652"><a href="#cb4-652" aria-hidden="true" tabindex="-1"></a>    ppl <span class="op">=</span> model.perplexity(sent)</span>
<span id="cb4-653"><a href="#cb4-653" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Perplexity of '</span><span class="sc">{</span>sent<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>ppl<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb4-654"><a href="#cb4-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-655"><a href="#cb4-655" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成文本</span></span>
<span id="cb4-656"><a href="#cb4-656" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Generated text:"</span>)</span>
<span id="cb4-657"><a href="#cb4-657" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting with 'The cat':"</span>, model.generate([<span class="st">'the'</span>, <span class="st">'cat'</span>]))</span>
<span id="cb4-658"><a href="#cb4-658" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-659"><a href="#cb4-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-660"><a href="#cb4-660" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用CRF做命名实体识别</span></span>
<span id="cb4-661"><a href="#cb4-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-664"><a href="#cb4-664" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-665"><a href="#cb4-665" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb4-666"><a href="#cb4-666" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-667"><a href="#cb4-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-668"><a href="#cb4-668" aria-hidden="true" tabindex="-1"></a><span class="co"># 需要安装: pip install sklearn-crfsuite</span></span>
<span id="cb4-669"><a href="#cb4-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-670"><a href="#cb4-670" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn_crfsuite</span>
<span id="cb4-671"><a href="#cb4-671" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn_crfsuite <span class="im">import</span> metrics</span>
<span id="cb4-672"><a href="#cb4-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-673"><a href="#cb4-673" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> word2features(sentence, i):</span>
<span id="cb4-674"><a href="#cb4-674" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-675"><a href="#cb4-675" aria-hidden="true" tabindex="-1"></a><span class="co">    为句子中第i个词提取特征</span></span>
<span id="cb4-676"><a href="#cb4-676" aria-hidden="true" tabindex="-1"></a><span class="co">    这就是"特征工程"——人工设计的特征</span></span>
<span id="cb4-677"><a href="#cb4-677" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-678"><a href="#cb4-678" aria-hidden="true" tabindex="-1"></a>    word <span class="op">=</span> sentence[i]</span>
<span id="cb4-679"><a href="#cb4-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-680"><a href="#cb4-680" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> {</span>
<span id="cb4-681"><a href="#cb4-681" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 基本特征</span></span>
<span id="cb4-682"><a href="#cb4-682" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.lower'</span>: word.lower(),</span>
<span id="cb4-683"><a href="#cb4-683" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.isupper'</span>: word.isupper(),</span>
<span id="cb4-684"><a href="#cb4-684" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.istitle'</span>: word.istitle(),</span>
<span id="cb4-685"><a href="#cb4-685" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.isdigit'</span>: word.isdigit(),</span>
<span id="cb4-686"><a href="#cb4-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-687"><a href="#cb4-687" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 词的形态特征</span></span>
<span id="cb4-688"><a href="#cb4-688" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.prefix2'</span>: word[:<span class="dv">2</span>].lower(),</span>
<span id="cb4-689"><a href="#cb4-689" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.prefix3'</span>: word[:<span class="dv">3</span>].lower(),</span>
<span id="cb4-690"><a href="#cb4-690" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.suffix2'</span>: word[<span class="op">-</span><span class="dv">2</span>:].lower(),</span>
<span id="cb4-691"><a href="#cb4-691" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.suffix3'</span>: word[<span class="op">-</span><span class="dv">3</span>:].lower(),</span>
<span id="cb4-692"><a href="#cb4-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-693"><a href="#cb4-693" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 词长</span></span>
<span id="cb4-694"><a href="#cb4-694" aria-hidden="true" tabindex="-1"></a>        <span class="st">'word.length'</span>: <span class="bu">len</span>(word),</span>
<span id="cb4-695"><a href="#cb4-695" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-696"><a href="#cb4-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-697"><a href="#cb4-697" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 上下文特征：前一个词</span></span>
<span id="cb4-698"><a href="#cb4-698" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb4-699"><a href="#cb4-699" aria-hidden="true" tabindex="-1"></a>        prev_word <span class="op">=</span> sentence[i<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-700"><a href="#cb4-700" aria-hidden="true" tabindex="-1"></a>        features.update({</span>
<span id="cb4-701"><a href="#cb4-701" aria-hidden="true" tabindex="-1"></a>            <span class="st">'-1:word.lower'</span>: prev_word.lower(),</span>
<span id="cb4-702"><a href="#cb4-702" aria-hidden="true" tabindex="-1"></a>            <span class="st">'-1:word.istitle'</span>: prev_word.istitle(),</span>
<span id="cb4-703"><a href="#cb4-703" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb4-704"><a href="#cb4-704" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-705"><a href="#cb4-705" aria-hidden="true" tabindex="-1"></a>        features[<span class="st">'BOS'</span>] <span class="op">=</span> <span class="va">True</span>  <span class="co"># Beginning of Sentence</span></span>
<span id="cb4-706"><a href="#cb4-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-707"><a href="#cb4-707" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 上下文特征：后一个词</span></span>
<span id="cb4-708"><a href="#cb4-708" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(sentence) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb4-709"><a href="#cb4-709" aria-hidden="true" tabindex="-1"></a>        next_word <span class="op">=</span> sentence[i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb4-710"><a href="#cb4-710" aria-hidden="true" tabindex="-1"></a>        features.update({</span>
<span id="cb4-711"><a href="#cb4-711" aria-hidden="true" tabindex="-1"></a>            <span class="st">'+1:word.lower'</span>: next_word.lower(),</span>
<span id="cb4-712"><a href="#cb4-712" aria-hidden="true" tabindex="-1"></a>            <span class="st">'+1:word.istitle'</span>: next_word.istitle(),</span>
<span id="cb4-713"><a href="#cb4-713" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb4-714"><a href="#cb4-714" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-715"><a href="#cb4-715" aria-hidden="true" tabindex="-1"></a>        features[<span class="st">'EOS'</span>] <span class="op">=</span> <span class="va">True</span>  <span class="co"># End of Sentence</span></span>
<span id="cb4-716"><a href="#cb4-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-717"><a href="#cb4-717" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features</span>
<span id="cb4-718"><a href="#cb4-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-719"><a href="#cb4-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-720"><a href="#cb4-720" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sent2features(sentence):</span>
<span id="cb4-721"><a href="#cb4-721" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""为整个句子提取特征"""</span></span>
<span id="cb4-722"><a href="#cb4-722" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [word2features(sentence, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sentence))]</span>
<span id="cb4-723"><a href="#cb4-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-724"><a href="#cb4-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-725"><a href="#cb4-725" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例数据（简化的NER数据）</span></span>
<span id="cb4-726"><a href="#cb4-726" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> [</span>
<span id="cb4-727"><a href="#cb4-727" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (句子, 标签)</span></span>
<span id="cb4-728"><a href="#cb4-728" aria-hidden="true" tabindex="-1"></a>    ([<span class="st">"Barack"</span>, <span class="st">"Obama"</span>, <span class="st">"was"</span>, <span class="st">"born"</span>, <span class="st">"in"</span>, <span class="st">"Hawaii"</span>, <span class="st">"."</span>],</span>
<span id="cb4-729"><a href="#cb4-729" aria-hidden="true" tabindex="-1"></a>     [<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-LOC"</span>, <span class="st">"O"</span>]),</span>
<span id="cb4-730"><a href="#cb4-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-731"><a href="#cb4-731" aria-hidden="true" tabindex="-1"></a>    ([<span class="st">"Apple"</span>, <span class="st">"Inc"</span>, <span class="st">"is"</span>, <span class="st">"located"</span>, <span class="st">"in"</span>, <span class="st">"California"</span>, <span class="st">"."</span>],</span>
<span id="cb4-732"><a href="#cb4-732" aria-hidden="true" tabindex="-1"></a>     [<span class="st">"B-ORG"</span>, <span class="st">"I-ORG"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-LOC"</span>, <span class="st">"O"</span>]),</span>
<span id="cb4-733"><a href="#cb4-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-734"><a href="#cb4-734" aria-hidden="true" tabindex="-1"></a>    ([<span class="st">"John"</span>, <span class="st">"Smith"</span>, <span class="st">"works"</span>, <span class="st">"at"</span>, <span class="st">"Google"</span>, <span class="st">"."</span>],</span>
<span id="cb4-735"><a href="#cb4-735" aria-hidden="true" tabindex="-1"></a>     [<span class="st">"B-PER"</span>, <span class="st">"I-PER"</span>, <span class="st">"O"</span>, <span class="st">"O"</span>, <span class="st">"B-ORG"</span>, <span class="st">"O"</span>]),</span>
<span id="cb4-736"><a href="#cb4-736" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-737"><a href="#cb4-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-738"><a href="#cb4-738" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备训练数据</span></span>
<span id="cb4-739"><a href="#cb4-739" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> [sent2features(sent) <span class="cf">for</span> sent, _ <span class="kw">in</span> train_data]</span>
<span id="cb4-740"><a href="#cb4-740" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> [labels <span class="cf">for</span> _, labels <span class="kw">in</span> train_data]</span>
<span id="cb4-741"><a href="#cb4-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-742"><a href="#cb4-742" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练CRF模型</span></span>
<span id="cb4-743"><a href="#cb4-743" aria-hidden="true" tabindex="-1"></a>crf <span class="op">=</span> sklearn_crfsuite.CRF(</span>
<span id="cb4-744"><a href="#cb4-744" aria-hidden="true" tabindex="-1"></a>    algorithm<span class="op">=</span><span class="st">'lbfgs'</span>,</span>
<span id="cb4-745"><a href="#cb4-745" aria-hidden="true" tabindex="-1"></a>    c1<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># L1正则化</span></span>
<span id="cb4-746"><a href="#cb4-746" aria-hidden="true" tabindex="-1"></a>    c2<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># L2正则化</span></span>
<span id="cb4-747"><a href="#cb4-747" aria-hidden="true" tabindex="-1"></a>    max_iterations<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb4-748"><a href="#cb4-748" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-749"><a href="#cb4-749" aria-hidden="true" tabindex="-1"></a>crf.fit(X_train, y_train)</span>
<span id="cb4-750"><a href="#cb4-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-751"><a href="#cb4-751" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试</span></span>
<span id="cb4-752"><a href="#cb4-752" aria-hidden="true" tabindex="-1"></a>test_sentence <span class="op">=</span> [<span class="st">"Elon"</span>, <span class="st">"Musk"</span>, <span class="st">"founded"</span>, <span class="st">"Tesla"</span>, <span class="st">"in"</span>, <span class="st">"Palo"</span>, <span class="st">"Alto"</span>, <span class="st">"."</span>]</span>
<span id="cb4-753"><a href="#cb4-753" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> [sent2features(test_sentence)]</span>
<span id="cb4-754"><a href="#cb4-754" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> crf.predict(X_test)[<span class="dv">0</span>]</span>
<span id="cb4-755"><a href="#cb4-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-756"><a href="#cb4-756" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"NER结果："</span>)</span>
<span id="cb4-757"><a href="#cb4-757" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, label <span class="kw">in</span> <span class="bu">zip</span>(test_sentence, y_pred):</span>
<span id="cb4-758"><a href="#cb4-758" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:12}</span><span class="ss"> -&gt; </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-759"><a href="#cb4-759" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-760"><a href="#cb4-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-761"><a href="#cb4-761" aria-hidden="true" tabindex="-1"></a><span class="fu">### 体会特征工程的痛苦</span></span>
<span id="cb4-762"><a href="#cb4-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-763"><a href="#cb4-763" aria-hidden="true" tabindex="-1"></a>上面的CRF代码中，<span class="in">`word2features`</span>函数就是特征工程的缩影。注意我们需要手工定义每一个特征：词的大小写、前缀后缀、上下文词...</span>
<span id="cb4-764"><a href="#cb4-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-765"><a href="#cb4-765" aria-hidden="true" tabindex="-1"></a>现在想象一下，如果你需要处理一个新任务（比如关系抽取），或者换一个领域（比如生物医学文本），你需要重新设计整套特征。词典需要重新收集，词性标注可能不够用，需要加入领域特定的知识...</span>
<span id="cb4-766"><a href="#cb4-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-767"><a href="#cb4-767" aria-hidden="true" tabindex="-1"></a>这就是为什么深度学习的"自动特征学习"如此革命性——它让我们摆脱了这个瓶颈。</span>
<span id="cb4-768"><a href="#cb4-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-769"><a href="#cb4-769" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-770"><a href="#cb4-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-771"><a href="#cb4-771" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb4-772"><a href="#cb4-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-773"><a href="#cb4-773" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者视角**：这一节探讨传统NLP方法的理论基础和历史意义</span></span>
<span id="cb4-774"><a href="#cb4-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-775"><a href="#cb4-775" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么统计方法最终胜出？</span></span>
<span id="cb4-776"><a href="#cb4-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-777"><a href="#cb4-777" aria-hidden="true" tabindex="-1"></a>符号主义和统计主义的争论贯穿了AI历史。在NLP领域，统计方法最终占据了主导，但这不意味着符号主义完全失败了。</span>
<span id="cb4-778"><a href="#cb4-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-779"><a href="#cb4-779" aria-hidden="true" tabindex="-1"></a>统计方法胜出的原因可以从几个角度理解。首先是鲁棒性。规则系统对噪声和变体非常敏感，而统计方法天然具有"平均化"的能力，对噪声更鲁棒。其次是可扩展性。人工编写规则的速度无法跟上数据增长的速度，而统计模型可以自动从更多数据中受益。第三是客观评估。统计方法自然地引入了定量评估（准确率、F1等），使得不同方法可以公平比较，推动了领域的科学化。</span>
<span id="cb4-780"><a href="#cb4-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-781"><a href="#cb4-781" aria-hidden="true" tabindex="-1"></a>然而，符号主义的一些洞察仍然有价值。语言确实有结构，语法规则确实存在，推理需要符号操作。现代的混合方法（neural-symbolic）试图结合两者的优势。</span>
<span id="cb4-782"><a href="#cb4-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-783"><a href="#cb4-783" aria-hidden="true" tabindex="-1"></a><span class="fu">### 马尔可夫假设的边界</span></span>
<span id="cb4-784"><a href="#cb4-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-785"><a href="#cb4-785" aria-hidden="true" tabindex="-1"></a>N-gram和HMM都依赖马尔可夫假设。这个假设在什么时候合理，什么时候失效？</span>
<span id="cb4-786"><a href="#cb4-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-787"><a href="#cb4-787" aria-hidden="true" tabindex="-1"></a>马尔可夫假设在局部依赖主导的情况下效果不错。比如词性标注中，当前词的词性主要由相邻的1-2个词决定。对于这类任务，HMM和N-gram可以取得很好的效果。</span>
<span id="cb4-788"><a href="#cb4-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-789"><a href="#cb4-789" aria-hidden="true" tabindex="-1"></a>但语言中有大量长距离依赖，马尔可夫假设在这些情况下就失效了。主谓一致可能跨越很多词。代词指代可能需要回溯很多句子。篇章连贯性需要理解整个段落甚至文档。对于这些任务，固定窗口的模型天然受限。</span>
<span id="cb4-790"><a href="#cb4-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-791"><a href="#cb4-791" aria-hidden="true" tabindex="-1"></a>这个局限性直接导向了第4章的RNN和第8章的Transformer——它们试图打破固定窗口的限制。</span>
<span id="cb4-792"><a href="#cb4-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-793"><a href="#cb4-793" aria-hidden="true" tabindex="-1"></a><span class="fu">### 生成模型 vs 判别模型</span></span>
<span id="cb4-794"><a href="#cb4-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-795"><a href="#cb4-795" aria-hidden="true" tabindex="-1"></a>HMM是生成模型，CRF是判别模型。这两种范式的差异是什么？</span>
<span id="cb4-796"><a href="#cb4-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-797"><a href="#cb4-797" aria-hidden="true" tabindex="-1"></a>生成模型建模联合分布$P(X, Y)$，然后用贝叶斯定理得到$P(Y|X)$。它对输入$X$的分布做了假设（如HMM的发射概率假设）。这带来了一些优势：可以生成新样本，可以处理缺失数据，有时更容易解释。但也有劣势：模型假设可能与现实不符，难以引入复杂特征。</span>
<span id="cb4-798"><a href="#cb4-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-799"><a href="#cb4-799" aria-hidden="true" tabindex="-1"></a>判别模型直接建模$P(Y|X)$，不对$X$的分布做假设。这带来了灵活性：可以引入任意特征，不需要担心$X$的分布。在实践中，判别模型在分类任务上通常优于生成模型，这就是CRF超越HMM的原因。</span>
<span id="cb4-800"><a href="#cb4-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-801"><a href="#cb4-801" aria-hidden="true" tabindex="-1"></a>深度学习模型大多是判别模型（如BERT做分类）。但生成模型的思想在语言模型（GPT系列）中得到了复兴——它们通过自回归方式"生成"文本。</span>
<span id="cb4-802"><a href="#cb4-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-803"><a href="#cb4-803" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放问题与反思</span></span>
<span id="cb4-804"><a href="#cb4-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-805"><a href="#cb4-805" aria-hidden="true" tabindex="-1"></a>回顾这段历史，有几个开放问题值得思考。</span>
<span id="cb4-806"><a href="#cb4-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-807"><a href="#cb4-807" aria-hidden="true" tabindex="-1"></a>第一，特征工程真的一无是处吗？深度学习确实可以自动学习特征，但人工设计的特征有时仍然有用。特别是在数据有限的情况下，好的特征可以引入有用的归纳偏置。如何将领域知识与端到端学习结合，仍然是一个活跃的研究方向。</span>
<span id="cb4-808"><a href="#cb4-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-809"><a href="#cb4-809" aria-hidden="true" tabindex="-1"></a>第二，小模型有没有价值？N-gram和CRF模型非常轻量，可以在边缘设备上运行。在大模型时代，小而专的模型是否还有一席之地？</span>
<span id="cb4-810"><a href="#cb4-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-811"><a href="#cb4-811" aria-hidden="true" tabindex="-1"></a>第三，可解释性的代价是什么？传统方法往往更可解释——你可以检查特征权重，理解模型为什么做出某个预测。深度学习模型是"黑箱"。这个trade-off在高风险应用中尤其重要。</span>
<span id="cb4-812"><a href="#cb4-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-813"><a href="#cb4-813" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-814"><a href="#cb4-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-815"><a href="#cb4-815" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb4-816"><a href="#cb4-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-817"><a href="#cb4-817" aria-hidden="true" tabindex="-1"></a><span class="fu">### 特征工程不可扩展</span></span>
<span id="cb4-818"><a href="#cb4-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-819"><a href="#cb4-819" aria-hidden="true" tabindex="-1"></a>传统方法的最大局限是特征工程的瓶颈。这不只是"麻烦"的问题，而是"不可能"的问题——对于足够复杂的任务（如开放域问答、自由对话），没有人能设计出足够好的特征。</span>
<span id="cb4-820"><a href="#cb4-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-821"><a href="#cb4-821" aria-hidden="true" tabindex="-1"></a><span class="fu">### 表示的离散性</span></span>
<span id="cb4-822"><a href="#cb4-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-823"><a href="#cb4-823" aria-hidden="true" tabindex="-1"></a>传统方法把词当作离散符号处理。"cat"和"dog"只是两个不同的ID，模型不知道它们都是动物、都是宠物、都有四条腿。这种离散表示无法捕获语义相似性，导致泛化困难。</span>
<span id="cb4-824"><a href="#cb4-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-825"><a href="#cb4-825" aria-hidden="true" tabindex="-1"></a><span class="fu">### 任务特定性</span></span>
<span id="cb4-826"><a href="#cb4-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-827"><a href="#cb4-827" aria-hidden="true" tabindex="-1"></a>每个任务需要独立设计特征、独立训练模型。知识无法在任务之间迁移。这与人类的学习方式形成鲜明对比——人类学会一种语言技能后，可以轻松迁移到相关任务。</span>
<span id="cb4-828"><a href="#cb4-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-829"><a href="#cb4-829" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb4-830"><a href="#cb4-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-831"><a href="#cb4-831" aria-hidden="true" tabindex="-1"></a>上述问题共同指向一个核心需求：学习词的分布式表示（distributed representation），让语义相似的词有相似的表示，让这种表示可以在任务之间共享和迁移。</span>
<span id="cb4-832"><a href="#cb4-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-833"><a href="#cb4-833" aria-hidden="true" tabindex="-1"></a>这正是下一章的主题：表示学习，从Word2Vec开启的词向量革命。</span>
<span id="cb4-834"><a href="#cb4-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-835"><a href="#cb4-835" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第2章将介绍Word2Vec——一个简单但革命性的想法，它让机器第一次能够"理解"词的含义。</span></span>
<span id="cb4-836"><a href="#cb4-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-837"><a href="#cb4-837" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-838"><a href="#cb4-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-839"><a href="#cb4-839" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb4-840"><a href="#cb4-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-841"><a href="#cb4-841" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb4-842"><a href="#cb4-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-843"><a href="#cb4-843" aria-hidden="true" tabindex="-1"></a>这一章我们快速回顾了深度学习之前的NLP历史。</span>
<span id="cb4-844"><a href="#cb4-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-845"><a href="#cb4-845" aria-hidden="true" tabindex="-1"></a>符号主义尝试用规则处理语言，在有限领域取得了成功，但无法扩展到复杂的真实世界任务。统计革命带来了数据驱动的范式，用概率模型从数据中学习规律。N-gram语言模型和HMM/CRF序列标注模型是这个时代的代表性技术。</span>
<span id="cb4-846"><a href="#cb4-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-847"><a href="#cb4-847" aria-hidden="true" tabindex="-1"></a>但统计方法有一个致命弱点：依赖人工设计特征。这个"特征工程的诅咒"成为了整个领域的瓶颈，直到深度学习带来的"表示学习"革命才被打破。</span>
<span id="cb4-848"><a href="#cb4-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-849"><a href="#cb4-849" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键概念速查</span></span>
<span id="cb4-850"><a href="#cb4-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-851"><a href="#cb4-851" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 概念 <span class="pp">|</span> 定义 <span class="pp">|</span></span>
<span id="cb4-852"><a href="#cb4-852" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb4-853"><a href="#cb4-853" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> N-gram <span class="pp">|</span> 基于马尔可夫假设的语言模型，用前n-1个词预测下一个词 <span class="pp">|</span></span>
<span id="cb4-854"><a href="#cb4-854" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> HMM <span class="pp">|</span> 隐马尔可夫模型，用于序列标注的生成模型 <span class="pp">|</span></span>
<span id="cb4-855"><a href="#cb4-855" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CRF <span class="pp">|</span> 条件随机场，用于序列标注的判别模型 <span class="pp">|</span></span>
<span id="cb4-856"><a href="#cb4-856" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特征工程 <span class="pp">|</span> 人工设计将原始输入转换为数值表示的过程 <span class="pp">|</span></span>
<span id="cb4-857"><a href="#cb4-857" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 生成模型 vs 判别模型 <span class="pp">|</span> 前者建模P(X,Y)，后者直接建模P(Y<span class="pp">|</span>X) <span class="pp">|</span></span>
<span id="cb4-858"><a href="#cb4-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-859"><a href="#cb4-859" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb4-860"><a href="#cb4-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-861"><a href="#cb4-861" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么说马尔可夫假设是一个"强假设"？举一个马尔可夫假设失效的具体例子。</span>
<span id="cb4-862"><a href="#cb4-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-863"><a href="#cb4-863" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[对比分析]** 比较HMM和CRF在建模假设上的区别。为什么CRF在序列标注任务上通常优于HMM？</span>
<span id="cb4-864"><a href="#cb4-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-865"><a href="#cb4-865" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 为一个情感分析任务设计特征。你会设计哪些特征？这些特征能否捕获讽刺（irony）的情况？</span>
<span id="cb4-866"><a href="#cb4-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-867"><a href="#cb4-867" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** 如果深度学习没有出现，你认为NLP领域会如何发展？特征工程是否有其他的突破口？</span>
<span id="cb4-868"><a href="#cb4-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-869"><a href="#cb4-869" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-870"><a href="#cb4-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-871"><a href="#cb4-871" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb4-872"><a href="#cb4-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-873"><a href="#cb4-873" aria-hidden="true" tabindex="-1"></a><span class="fu">### 经典教材</span></span>
<span id="cb4-874"><a href="#cb4-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-875"><a href="#cb4-875" aria-hidden="true" tabindex="-1"></a>**"Speech and Language Processing" (Jurafsky &amp; Martin)**：NLP领域的"圣经"，第1-6章详细介绍了本章涉及的内容。第3版在线免费，是很好的参考资料。</span>
<span id="cb4-876"><a href="#cb4-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-877"><a href="#cb4-877" aria-hidden="true" tabindex="-1"></a><span class="fu">### 历史论文</span></span>
<span id="cb4-878"><a href="#cb4-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-879"><a href="#cb4-879" aria-hidden="true" tabindex="-1"></a>**"A Maximum Entropy Model for Part-of-Speech Tagging" (Ratnaparkhi, 1996)**：最大熵模型在NLP中的经典应用，展示了特征工程的典型范式。</span>
<span id="cb4-880"><a href="#cb4-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-881"><a href="#cb4-881" aria-hidden="true" tabindex="-1"></a>**"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data" (Lafferty et al., 2001)**：CRF的原始论文，开创了判别式序列标注的时代。</span>
<span id="cb4-882"><a href="#cb4-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-883"><a href="#cb4-883" aria-hidden="true" tabindex="-1"></a><span class="fu">### 回顾与反思</span></span>
<span id="cb4-884"><a href="#cb4-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-885"><a href="#cb4-885" aria-hidden="true" tabindex="-1"></a>**"The Last 10 Meters of AI" (Knight, 2019)**：一篇讨论符号主义与神经网络融合的文章，提供了对这段历史的反思。</span>
<span id="cb4-886"><a href="#cb4-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-887"><a href="#cb4-887" aria-hidden="true" tabindex="-1"></a><span class="fu">### 工具与代码</span></span>
<span id="cb4-888"><a href="#cb4-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-889"><a href="#cb4-889" aria-hidden="true" tabindex="-1"></a>**NLTK** (Natural Language Toolkit)：Python的经典NLP库，包含大量传统方法的实现。适合学习和原型开发。</span>
<span id="cb4-890"><a href="#cb4-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-891"><a href="#cb4-891" aria-hidden="true" tabindex="-1"></a>**sklearn-crfsuite**：CRF的Python实现，本章实践部分使用了这个库。</span>
<span id="cb4-892"><a href="#cb4-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-893"><a href="#cb4-893" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-894"><a href="#cb4-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-895"><a href="#cb4-895" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb4-896"><a href="#cb4-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-897"><a href="#cb4-897" aria-hidden="true" tabindex="-1"></a>"Every time I fire a linguist, the performance goes up"这句话经常被引用来说明统计方法的胜利，但它的完整语境更加微妙。Fred Jelinek并不是真的反对语言学——IBM的语音识别团队中也有语言学家。他的观点是：纯粹基于语言学规则的方法不如数据驱动的方法有效，但语言学知识仍然可以作为特征融入统计模型。</span>
<span id="cb4-898"><a href="#cb4-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-899"><a href="#cb4-899" aria-hidden="true" tabindex="-1"></a>有趣的是，深度学习时代的某些发展似乎又在"回归"某种结构。Transformer的Self-Attention可以学习出类似语法依赖的模式；预训练模型似乎"知道"一些语法规则。也许符号主义与统计/神经方法的争论不是"谁对谁错"，而是"如何结合"。</span>
<span id="cb4-900"><a href="#cb4-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-901"><a href="#cb4-901" aria-hidden="true" tabindex="-1"></a>另一个有趣的历史细节是：很多深度学习时代的"新想法"其实有很早的先驱。Word2Vec的思想可以追溯到1980年代的分布式表示（distributed representation）。注意力机制的雏形可以在传统的alignment model中找到。历史常常以螺旋的方式前进。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>