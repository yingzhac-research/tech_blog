<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-29">
<meta name="description" content="当大语言模型从训练走向部署，推理成本成为新的瓶颈——一个70B模型的FP16推理需要140GB显存，远超单卡容量；自回归生成的token-by-token方式让GPU利用率仅有10-20%。本章系统梳理推理优化的三条主线：量化技术（GPTQ、AWQ）用精度换内存，将70B模型压缩到单卡运行；投机解码用小模型猜测、大模型验证的方式打破自回归瓶颈，实现2-3倍无损加速；持续批处理和PagedAttention通过动态调度最大化硬件利用率。这些技术可以叠加使用，将推理成本降低10倍以上，让’只有大厂用得起’变成’人人可部署’。">

<title>第31章：推理优化 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#推理的成本结构" id="toc-推理的成本结构" class="nav-link" data-scroll-target="#推理的成本结构"><span class="header-section-number">2.1</span> 推理的成本结构</a></li>
  <li><a href="#自回归解码的瓶颈分析" id="toc-自回归解码的瓶颈分析" class="nav-link" data-scroll-target="#自回归解码的瓶颈分析"><span class="header-section-number">2.2</span> 自回归解码的瓶颈分析</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#量化的直觉有损压缩" id="toc-量化的直觉有损压缩" class="nav-link" data-scroll-target="#量化的直觉有损压缩"><span class="header-section-number">3.1</span> 量化的直觉：有损压缩</a></li>
  <li><a href="#投机解码的直觉草稿-审稿流程" id="toc-投机解码的直觉草稿-审稿流程" class="nav-link" data-scroll-target="#投机解码的直觉草稿-审稿流程"><span class="header-section-number">3.2</span> 投机解码的直觉：草稿-审稿流程</a></li>
  <li><a href="#持续批处理的直觉餐厅动态座位安排" id="toc-持续批处理的直觉餐厅动态座位安排" class="nav-link" data-scroll-target="#持续批处理的直觉餐厅动态座位安排"><span class="header-section-number">3.3</span> 持续批处理的直觉：餐厅动态座位安排</a></li>
  <li><a href="#pagedattention的直觉操作系统的内存分页" id="toc-pagedattention的直觉操作系统的内存分页" class="nav-link" data-scroll-target="#pagedattention的直觉操作系统的内存分页"><span class="header-section-number">3.4</span> PagedAttention的直觉：操作系统的内存分页</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#量化技术" id="toc-量化技术" class="nav-link" data-scroll-target="#量化技术"><span class="header-section-number">4.1</span> 量化技术</a></li>
  <li><a href="#投机解码speculative-decoding" id="toc-投机解码speculative-decoding" class="nav-link" data-scroll-target="#投机解码speculative-decoding"><span class="header-section-number">4.2</span> 投机解码（Speculative Decoding）</a></li>
  <li><a href="#持续批处理continuous-batching" id="toc-持续批处理continuous-batching" class="nav-link" data-scroll-target="#持续批处理continuous-batching"><span class="header-section-number">4.3</span> 持续批处理（Continuous Batching）</a></li>
  <li><a href="#模型并行推理" id="toc-模型并行推理" class="nav-link" data-scroll-target="#模型并行推理"><span class="header-section-number">4.4</span> 模型并行推理</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#使用-gptq-量化模型" id="toc-使用-gptq-量化模型" class="nav-link" data-scroll-target="#使用-gptq-量化模型"><span class="header-section-number">5.1</span> 使用 GPTQ 量化模型</a></li>
  <li><a href="#使用-awq-量化模型" id="toc-使用-awq-量化模型" class="nav-link" data-scroll-target="#使用-awq-量化模型"><span class="header-section-number">5.2</span> 使用 AWQ 量化模型</a></li>
  <li><a href="#vllm-部署示例" id="toc-vllm-部署示例" class="nav-link" data-scroll-target="#vllm-部署示例"><span class="header-section-number">5.3</span> vLLM 部署示例</a></li>
  <li><a href="#tgi-部署docker" id="toc-tgi-部署docker" class="nav-link" data-scroll-target="#tgi-部署docker"><span class="header-section-number">5.4</span> TGI 部署（Docker）</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#边界条件与失效模式" id="toc-边界条件与失效模式" class="nav-link" data-scroll-target="#边界条件与失效模式"><span class="header-section-number">6.2</span> 边界条件与失效模式</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#本方法的局限" id="toc-本方法的局限" class="nav-link" data-scroll-target="#本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#方法对比速查" id="toc-方法对比速查" class="nav-link" data-scroll-target="#方法对比速查"><span class="header-section-number">8.3</span> 方法对比速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.4</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">9.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.3</span> 后续发展</a></li>
  <li><a href="#代码资源-1" id="toc-代码资源-1" class="nav-link" data-scroll-target="#代码资源-1"><span class="header-section-number">9.4</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第31章：推理优化</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">From Training to Deployment: The Art of Making Giants Fast and Cheap</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">Inference</div>
    <div class="quarto-category">Quantization</div>
    <div class="quarto-category">Speculative Decoding</div>
  </div>
  </div>

<div>
  <div class="description">
    当大语言模型从训练走向部署，推理成本成为新的瓶颈——一个70B模型的FP16推理需要140GB显存，远超单卡容量；自回归生成的token-by-token方式让GPU利用率仅有10-20%。本章系统梳理推理优化的三条主线：量化技术（GPTQ、AWQ）用精度换内存，将70B模型压缩到单卡运行；投机解码用小模型猜测、大模型验证的方式打破自回归瓶颈，实现2-3倍无损加速；持续批处理和PagedAttention通过动态调度最大化硬件利用率。这些技术可以叠加使用，将推理成本降低10倍以上，让’只有大厂用得起’变成’人人可部署’。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 29, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：当大语言模型从训练走向部署，如何在成本、延迟、质量三者之间取得最优平衡？</p>
<p><strong>历史坐标</strong>：2022–2024 | LLM.int8() (Dettmers, 2022), GPTQ (Frantar, 2022), SmoothQuant (Xiao, 2022), Speculative Decoding (Leviathan &amp; Chen, 2023), AWQ (Lin, 2023), vLLM (Kwon, 2023) | 从算法到系统的推理优化全栈</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Dettmers et al.&nbsp;(2022)</strong> “LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale” (<a href="https://arxiv.org/abs/2208.07339">arXiv:2208.07339</a>) — 参考了 outlier 特征的发现和混合精度分解方法</li>
<li><strong>Frantar et al.&nbsp;(2022)</strong> “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers” (<a href="https://arxiv.org/abs/2210.17323">arXiv:2210.17323</a>) — 参考了基于 Hessian 的逐层量化方法（ICLR 2023）</li>
<li><strong>Xiao et al.&nbsp;(2022)</strong> “SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models” (<a href="https://arxiv.org/abs/2211.10438">arXiv:2211.10438</a>) — 参考了激活平滑技术</li>
<li><strong>Lin et al.&nbsp;(2023)</strong> “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration” (<a href="https://arxiv.org/abs/2306.00978">arXiv:2306.00978</a>) — 参考了激活感知量化方法（MLSys 2024 最佳论文）；从论文提取了1张架构图</li>
<li><strong>Leviathan et al.&nbsp;(2023)</strong> “Fast Inference from Transformers via Speculative Decoding” (<a href="https://arxiv.org/abs/2211.17192">arXiv:2211.17192</a>) — 参考了投机解码的基本框架和数学证明</li>
<li><strong>Chen et al.&nbsp;(2023)</strong> “Accelerating Large Language Model Decoding with Speculative Sampling” (<a href="https://arxiv.org/abs/2302.01318">arXiv:2302.01318</a>) — 参考了 Google 的投机采样实现</li>
<li><strong>Kwon et al.&nbsp;(2023)</strong> “Efficient Memory Management for Large Language Model Serving with PagedAttention” (<a href="https://arxiv.org/abs/2309.06180">arXiv:2309.06180</a>) — 参考了 PagedAttention 和 vLLM 系统设计（SOSP 2023）</li>
<li><strong>Yu et al.&nbsp;(2022)</strong> “Orca: A Distributed Serving System for Transformer-Based Generative Models” (OSDI 2022) — 参考了迭代级调度和持续批处理</li>
<li><strong>Ainslie et al.&nbsp;(2023)</strong> “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints” (<a href="https://arxiv.org/abs/2305.13245">arXiv:2305.13245</a>) — 参考了 Grouped-Query Attention 的设计</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.2</span> 课程</h3>
<ul>
<li><strong>Stanford CS229S</strong> Systems for Machine Learning (2023) — 参考了推理系统优化的框架</li>
<li><strong>UC Berkeley CS294</strong> Machine Learning Systems (2024) — 参考了量化技术的教学组织</li>
</ul>
</section>
<section id="代码资源" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">0.3</span> 代码资源</h3>
<ul>
<li><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a> — vLLM 高性能推理引擎</li>
<li><a href="https://github.com/huggingface/text-generation-inference">huggingface/text-generation-inference</a> — TGI 推理服务</li>
<li><a href="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq</a> — GPTQ 官方实现</li>
<li><a href="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq</a> — AWQ 官方实现</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了参数高效微调（PEFT）的革命——从 Adapter 的层间插入，到 LoRA 的低秩分解，再到 QLoRA 的 4-bit 量化微调。这些技术将大模型微调从”需要一个集群”变成”一张消费级 GPU”。70B 参数的 LLaMA 现在可以在单张 24GB 的 RTX 4090 上完成微调，成本从数万美元降到几百美元。</p>
<p>但微调只是模型生命周期的<strong>一次性成本</strong>。一旦模型上线，真正的成本来自<strong>推理</strong>——每一次用户请求都需要消耗计算资源。对于一个有百万日活用户的应用，推理成本可能是微调成本的数百倍。让我们做一个简单的算术：</p>
<p>假设你运营一个基于 LLaMA-70B 的 AI 助手：</p>
<ul>
<li><strong>微调成本</strong>（一次性）：QLoRA 在单 GPU 上训练，约 $200（云 GPU 租用）</li>
<li><strong>推理成本</strong>（持续）：
<ul>
<li>每次请求平均 500 token（输入 + 输出）</li>
<li>每天 100 万次请求</li>
<li>70B 模型 FP16 推理需要约 140GB 显存，需要 2 张 A100 80GB</li>
<li>A100 租用成本约 $2/小时 × 2 = $4/小时 = <strong>$96/天</strong></li>
<li>每月推理成本：<strong>$2,880</strong></li>
</ul></li>
</ul>
<p>一年下来，推理成本接近 <strong>$35,000</strong>，是微调成本的 175 倍！</p>
<p>更糟糕的是，这还是在理想情况下的估算。实际部署中，由于自回归生成的顺序特性，GPU 的利用率通常只有 10–30%。大部分时间，昂贵的 A100 GPU 都在”等待”——等待上一个 token 生成完毕，才能开始下一个 token 的计算。这是一种巨大的浪费。</p>
<p>推理优化面临三大核心挑战：</p>
<p><strong>挑战一：显存瓶颈</strong></p>
<p>一个 70B 模型的 FP16 参数占用 140GB，这还没算 KV Cache。在长上下文场景下（比如 128K token），KV Cache 可能占用数百 GB。即使是 8 张 A100 组成的服务器，也可能被显存卡住。</p>
<p><strong>挑战二：延迟瓶颈</strong></p>
<p>自回归生成是天然顺序的——每个 token 都依赖前面所有 token。这意味着生成 100 个 token 需要进行 100 次完整的前向传播。用户看到的是”逐字蹦出”的体验，延迟感明显。</p>
<p><strong>挑战三：吞吐瓶颈</strong></p>
<p>传统的静态批处理要求所有请求同时开始、同时结束。但实际场景中，用户请求长度参差不齐——有的问”今天天气”，有的要求写一篇长文。短请求被迫等待长请求完成，系统吞吐量大打折扣。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：推理优化的本质是在<strong>精度、速度、成本</strong>三角形中寻找帕累托最优。量化技术通过降低数值精度换取内存和带宽——INT4 量化可以将 70B 模型压缩到 35GB，单卡可运行。投机解码通过”猜测-验证”的并行化打破自回归的顺序瓶颈——小模型一次猜 4 个 token，大模型一次验证全部，实现 2–3 倍无损加速。持续批处理通过迭代级调度最大化硬件利用率——新请求可以随时加入，完成的请求立即退出，不再”陪跑”。这些技术可以叠加使用：一个部署 INT4 量化 + 投机解码 + PagedAttention + 持续批处理的系统，相比 naive 实现可以将成本降低 <strong>10 倍以上</strong>。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="推理的成本结构" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="推理的成本结构"><span class="header-section-number">2.1</span> 推理的成本结构</h3>
<p>要优化推理，首先要理解成本从何而来。大模型推理的资源消耗可以分解为三大部分：</p>
<p><strong>模型参数</strong></p>
<p>这是最直观的部分。70B 参数在 FP16 下占用 <span class="math inline">\(70 \times 10^9 \times 2 = 140\)</span> GB。参数需要从显存读取到计算单元（CUDA Core 或 Tensor Core），这是推理的第一个瓶颈。</p>
<p><strong>KV Cache</strong></p>
<p>这是容易被忽视但至关重要的部分。在自回归生成中，为了避免重复计算历史 token 的 Key 和 Value，模型会将它们缓存下来。KV Cache 的大小随序列长度线性增长：</p>
<p><span class="math display">\[
\text{KV Cache} = 2 \times L \times n \times d \times \text{sizeof(dtype)}
\]</span></p>
<p>其中 <span class="math inline">\(L\)</span> 是层数，<span class="math inline">\(n\)</span> 是序列长度，<span class="math inline">\(d\)</span> 是隐藏维度，因子 2 来自 K 和 V 两个矩阵。对于 LLaMA-70B（<span class="math inline">\(L=80\)</span>，<span class="math inline">\(d=8192\)</span>），128K 上下文的 KV Cache 占用：</p>
<p><span class="math display">\[
2 \times 80 \times 128000 \times 8192 \times 2 = 335 \text{ GB (FP16)}
\]</span></p>
<p>这比模型参数本身还大！在长上下文场景，KV Cache 是显存的主要消耗者。</p>
<p><strong>激活值</strong></p>
<p>前向传播过程中的中间结果。相比参数和 KV Cache，激活值占用较小（batch size 为 1 时通常只有几个 GB），但在大 batch 场景下也不可忽视。</p>
</section>
<section id="自回归解码的瓶颈分析" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="自回归解码的瓶颈分析"><span class="header-section-number">2.2</span> 自回归解码的瓶颈分析</h3>
<p>为什么大模型推理这么慢？要理解这一点，我们需要深入分析自回归解码的计算特性。</p>
<p>在生成第 <span class="math inline">\(t\)</span> 个 token 时，模型需要：</p>
<ol type="1">
<li>读取所有模型参数（140GB for 70B model）</li>
<li>读取 KV Cache（随 <span class="math inline">\(t\)</span> 线性增长）</li>
<li>计算注意力和 FFN</li>
<li>输出一个 token 的 logits</li>
</ol>
<p>关键观察是：<strong>生成单个 token 只需要很少的计算（相对于参数量），但需要读取大量数据（所有参数）</strong>。这让推理变成了一个 <strong>内存带宽受限</strong>（memory-bound）而非<strong>计算受限</strong>（compute-bound）的任务。</p>
<p>让我们算一下。A100 GPU 的规格是：</p>
<ul>
<li>内存带宽：2 TB/s</li>
<li>FP16 算力：312 TFLOPS</li>
</ul>
<p>生成一个 token 需要读取 140GB 参数，按 2 TB/s 带宽计算：</p>
<p><span class="math display">\[
\text{读取时间} = \frac{140 \text{ GB}}{2000 \text{ GB/s}} = 70 \text{ ms}
\]</span></p>
<p>而 70B 模型的 FP16 计算量约为 <span class="math inline">\(2 \times 70 \times 10^9 = 140\)</span> GFLOPs（每个参数大约 2 次运算），按 312 TFLOPS 计算：</p>
<p><span class="math display">\[
\text{计算时间} = \frac{140 \text{ GFLOPs}}{312 \text{ TFLOPs}} = 0.45 \text{ ms}
\]</span></p>
<p>计算只需要 0.45ms，但读取参数需要 70ms——<strong>实际利用率不到 1%</strong>！GPU 的强大算力几乎完全浪费在等待数据传输上。</p>
<p>这就是为什么单纯增加 batch size 可以提升吞吐：多个请求共享同一份参数读取，摊薄了内存带宽的开销。但 batch size 受限于显存（更多请求 = 更多 KV Cache），不能无限增大。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>理解了问题的本质，解决方案的方向就清晰了：</p>
<ol type="1">
<li><strong>减少需要读取的数据量</strong> → <strong>量化</strong>：把 FP16 参数压缩成 INT8 或 INT4，读取量减半或减到四分之一</li>
<li><strong>打破顺序依赖</strong> → <strong>投机解码</strong>：一次生成多个 token，减少前向传播次数</li>
<li><strong>最大化硬件利用率</strong> → <strong>持续批处理</strong>：动态调度请求，让 GPU 时刻保持忙碌</li>
<li><strong>高效管理 KV Cache</strong> → <strong>PagedAttention</strong>：像操作系统管理内存一样管理 KV Cache</li>
</ol>
<p>这四类技术相互独立、可以叠加。一个现代推理系统（如 vLLM）会同时使用它们。接下来，我们逐一深入探讨每项技术的原理和实现。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<p>在进入技术细节之前，让我们先用直觉建立对各项技术的理解。</p>
<section id="量化的直觉有损压缩" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="量化的直觉有损压缩"><span class="header-section-number">3.1</span> 量化的直觉：有损压缩</h3>
<p>想象你要通过一条窄带宽的网络传输一张高清照片。原图是 24-bit RGB，每个像素 3 个字节。如果带宽不够，你会怎么做？最直接的方法是<strong>压缩</strong>——JPEG 就是这样做的，它用 8-bit 甚至更低的精度存储像素，肉眼几乎看不出差别。</p>
<p>量化对神经网络做的事情完全类似。预训练的权重是 FP16（16-bit 浮点数），包含了大量”冗余”的精度。研究发现，大模型的权重分布高度集中——绝大多数权重的绝对值都很小，只有极少数”异常值”（outliers）比较大。这种分布意味着我们可以用更少的 bit 来表示大部分权重，而几乎不损失信息。</p>
<p>INT8 量化将 16-bit 压缩到 8-bit，参数量减半；INT4 量化更激进，压缩到四分之一。代价是轻微的精度损失——就像 JPEG 压缩会引入一些”噪点”，但通常在可接受范围内。</p>
</section>
<section id="投机解码的直觉草稿-审稿流程" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="投机解码的直觉草稿-审稿流程"><span class="header-section-number">3.2</span> 投机解码的直觉：草稿-审稿流程</h3>
<p>想象你是一位繁忙的主编，需要审阅大量稿件。你可以亲自一字一句地写每篇文章，但这太慢了。一个更高效的流程是：让一位初级编辑先写草稿，然后你快速审阅——接受好的部分，修改不满意的部分。</p>
<p>投机解码（Speculative Decoding）正是这个思路。大模型（target model）太慢，每生成一个 token 都要 70ms。我们引入一个小模型（draft model），它虽然不如大模型准确，但生成速度快得多（比如只需 7ms）。流程变成：</p>
<ol type="1">
<li>小模型快速生成 4 个候选 token：[“The”, “cat”, “sat”, “on”]</li>
<li>大模型<strong>一次性</strong>验证这 4 个 token 是否可接受</li>
<li>如果前 3 个被接受，第 4 个被拒绝，则保留前 3 个，从拒绝位置重新开始</li>
</ol>
<p>关键洞察是：大模型验证 4 个 token 只需要<strong>一次</strong>前向传播（约 70ms），而如果大模型自己生成 4 个 token 需要<strong>四次</strong>前向传播（约 280ms）。只要小模型的预测足够准确（被接受的比例够高），整体速度就能提升。</p>
<p>更妙的是，通过精心设计的接受-拒绝机制，投机解码可以保证最终输出分布与直接使用大模型<strong>完全相同</strong>——这是数学上可证明的无损加速！</p>
</section>
<section id="持续批处理的直觉餐厅动态座位安排" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="持续批处理的直觉餐厅动态座位安排"><span class="header-section-number">3.3</span> 持续批处理的直觉：餐厅动态座位安排</h3>
<p>想象一家繁忙的餐厅。传统的做法是”静态批处理”：服务员等凑齐 8 位客人，一起安排到一张大桌子；所有人必须等最慢的那位吃完，才能一起离席。如果有人只点了一杯咖啡，他也得陪着吃牛排的人坐到最后。</p>
<p>持续批处理（Continuous Batching）更像现代快餐店：客人随到随坐，吃完就走，座位立刻分配给下一位。这样，快速的请求不用等待慢速的请求，系统吞吐量大幅提升。</p>
<p>在 LLM 推理中，不同请求的输出长度差异巨大——有的回答只需 10 个 token，有的需要 1000 个。静态批处理让短请求”陪跑”长请求，GPU 利用率极低。持续批处理允许在每个 iteration（生成一个 token）的粒度上调度：完成的请求立即退出，新请求可以随时加入。</p>
</section>
<section id="pagedattention的直觉操作系统的内存分页" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="pagedattention的直觉操作系统的内存分页"><span class="header-section-number">3.4</span> PagedAttention的直觉：操作系统的内存分页</h3>
<p>传统的 KV Cache 管理是”连续分配”的：为每个请求预留一大块连续显存，足以容纳其最大可能长度。这就像早期操作系统的内存管理——为每个程序分配一整块连续内存，导致严重的碎片化。</p>
<p>PagedAttention（分页注意力）借鉴了现代操作系统的<strong>虚拟内存</strong>技术：将 KV Cache 切分成固定大小的”页”（blocks），按需分配，不要求连续。这样，不同请求可以共享物理显存，碎片化问题迎刃而解。</p>
<p>一个额外的好处是：当多个请求共享相同的 prompt（比如系统提示），它们的 KV Cache 可以共享同一份物理内存，进一步节省空间。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="量化技术" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="量化技术"><span class="header-section-number">4.1</span> 量化技术</h3>
<section id="量化基础" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="量化基础"><span class="header-section-number">4.1.1</span> 量化基础</h4>
<p>量化的本质是用离散值近似连续值。给定一个浮点数权重 <span class="math inline">\(w\)</span>，量化过程可以表示为：</p>
<p><span class="math display">\[
w_q = \text{round}\left(\frac{w - z}{s}\right), \quad w_{dq} = w_q \cdot s + z
\]</span></p>
<p>其中 <span class="math inline">\(s\)</span> 是<strong>缩放因子</strong>（scale），<span class="math inline">\(z\)</span> 是<strong>零点</strong>（zero point），<span class="math inline">\(w_q\)</span> 是量化后的整数，<span class="math inline">\(w_{dq}\)</span> 是反量化后的近似值。</p>
<p>量化有几个关键维度：</p>
<p><strong>对称 vs 非对称</strong></p>
<ul>
<li><strong>对称量化</strong>：假设权重分布关于 0 对称，<span class="math inline">\(z = 0\)</span>。只需要存储 <span class="math inline">\(s\)</span>。</li>
<li><strong>非对称量化</strong>：允许任意分布，需要同时存储 <span class="math inline">\(s\)</span> 和 <span class="math inline">\(z\)</span>。更灵活但开销稍大。</li>
</ul>
<p><strong>量化粒度</strong></p>
<ul>
<li><strong>Per-tensor</strong>：整个权重矩阵共享一组 <span class="math inline">\((s, z)\)</span>。最节省空间，但精度最低。</li>
<li><strong>Per-channel</strong>：每个输出通道有独立的 <span class="math inline">\((s, z)\)</span>。平衡了精度和开销。</li>
<li><strong>Per-group</strong>：每 <span class="math inline">\(g\)</span> 个元素共享 <span class="math inline">\((s, z)\)</span>，比如每 128 个元素一组。GPTQ 和 AWQ 常用。</li>
</ul>
</section>
<section id="完整数值示例fp16-int4-量化" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="完整数值示例fp16-int4-量化"><span class="header-section-number">4.1.2</span> 完整数值示例：FP16 → INT4 量化</h4>
<p>让我们通过一个具体的例子，手把手走过量化的完整流程。</p>
<p><strong>设定</strong>：一个 <span class="math inline">\(4 \times 4\)</span> 的权重矩阵，原始值为 FP16。</p>
<p><span class="math display">\[
W = \begin{bmatrix}
-2.1 &amp; 0.3 &amp; 1.5 &amp; -0.8 \\
0.5 &amp; 2.7 &amp; -1.2 &amp; 0.1 \\
1.8 &amp; -0.4 &amp; 0.9 &amp; 2.3 \\
-1.5 &amp; 1.1 &amp; -0.2 &amp; 0.7
\end{bmatrix}
\]</span></p>
<p><strong>Step 1: 计算量化参数</strong></p>
<p>首先找到权重的范围： <span class="math display">\[
w_{\min} = -2.1, \quad w_{\max} = 2.7
\]</span></p>
<p>INT4 可以表示 <span class="math inline">\(2^4 = 16\)</span> 个值，范围是 <span class="math inline">\([0, 15]\)</span>（无符号）或 <span class="math inline">\([-8, 7]\)</span>（有符号）。我们使用无符号 INT4，范围 <span class="math inline">\([0, 15]\)</span>。</p>
<p>计算缩放因子和零点： <span class="math display">\[
s = \frac{w_{\max} - w_{\min}}{2^4 - 1} = \frac{2.7 - (-2.1)}{15} = \frac{4.8}{15} = 0.32
\]</span></p>
<p><span class="math display">\[
z = \text{round}\left(-\frac{w_{\min}}{s}\right) = \text{round}\left(\frac{2.1}{0.32}\right) = \text{round}(6.56) = 7
\]</span></p>
<p><strong>Step 2: 量化</strong></p>
<p>对每个权重 <span class="math inline">\(w\)</span>，计算量化值： <span class="math display">\[
w_q = \text{clamp}\left(\text{round}\left(\frac{w}{s}\right) + z, \, 0, \, 15\right)
\]</span></p>
<p>以 <span class="math inline">\(w = 1.5\)</span> 为例： <span class="math display">\[
w_q = \text{round}\left(\frac{1.5}{0.32}\right) + 7 = \text{round}(4.69) + 7 = 5 + 7 = 12
\]</span></p>
<p>完整的量化矩阵： <span class="math display">\[
W_q = \begin{bmatrix}
0 &amp; 8 &amp; 12 &amp; 4 \\
9 &amp; 15 &amp; 3 &amp; 7 \\
13 &amp; 6 &amp; 10 &amp; 14 \\
2 &amp; 10 &amp; 6 &amp; 9
\end{bmatrix}
\]</span></p>
<p><strong>Step 3: 反量化与误差分析</strong></p>
<p>反量化公式：<span class="math inline">\(w_{dq} = (w_q - z) \cdot s\)</span></p>
<p>以 <span class="math inline">\(w_q = 12\)</span>（原始 <span class="math inline">\(w = 1.5\)</span>）为例： <span class="math display">\[
w_{dq} = (12 - 7) \cdot 0.32 = 5 \cdot 0.32 = 1.60
\]</span></p>
<p>量化误差：<span class="math inline">\(|1.5 - 1.60| = 0.10\)</span></p>
<p><strong>误差分析</strong>：INT4 量化的最大误差为 <span class="math inline">\(s/2 = 0.16\)</span>。平均误差通常为最大误差的一半左右，即 <span class="math inline">\(\sim 0.08\)</span>。相比原始权重的典型大小（<span class="math inline">\(\sim 1\)</span>），相对误差约 8%。</p>
<p><strong>Step 4: 存储空间对比</strong></p>
<ul>
<li><strong>FP16</strong>：<span class="math inline">\(16 \times 2 = 32\)</span> bytes</li>
<li><strong>INT4</strong>：<span class="math inline">\(16 \times 0.5 + 2 \times 2 = 8 + 4 = 12\)</span> bytes（权重 + scale &amp; zero）</li>
<li><strong>压缩比</strong>：<span class="math inline">\(32 / 12 \approx 2.7\times\)</span></li>
</ul>
<p>对于更大的矩阵，使用 per-group 量化（如每 128 个元素一组），压缩比可以接近理论值 <span class="math inline">\(4\times\)</span>。</p>
</section>
<section id="llm.int8发现异常值" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="llm.int8发现异常值"><span class="header-section-number">4.1.3</span> LLM.int8()：发现异常值</h4>
<p>2022 年，Dettmers 等人在尝试直接对 LLM 进行 INT8 量化时，遇到了一个令人惊讶的现象：模型性能<strong>断崖式下降</strong>，远超预期。经过深入分析，他们发现了问题的根源：<strong>异常值特征</strong>（emergent outliers）。</p>
<p>在大模型的某些维度上，激活值会出现极端的异常值——比正常值大 10–100 倍。这些异常值只占全部特征的约 0.1%，但对模型输出至关重要。直接量化会严重损害这些异常值的精度，导致模型崩溃。</p>
<p>LLM.int8() 的解决方案是<strong>混合精度分解</strong>：</p>
<ol type="1">
<li>检测异常值维度（绝对值超过阈值的维度）</li>
<li>异常值维度保持 FP16 精度</li>
<li>其他维度使用 INT8 量化</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: LLM.int8() 混合精度矩阵乘法 (Dettmers et al., 2022)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>： - 激活矩阵 <span class="math inline">\(X \in \mathbb{R}^{b \times d}\)</span> - 权重矩阵 <span class="math inline">\(W \in \mathbb{R}^{d \times k}\)</span> - 异常值阈值 <span class="math inline">\(\alpha\)</span>（通常为 6.0）</p>
<p><strong>输出</strong>：<span class="math inline">\(Y = XW\)</span></p>
<pre><code>1. 检测异常值维度:
   O ← {i : max|X[:, i]| &gt; α}  // 异常值维度集合

2. 分解输入:
   X_outlier ← X[:, O]         // 异常值部分 (FP16)
   X_normal ← X[:, ~O]         // 正常部分

3. 分解权重:
   W_outlier ← W[O, :]         // 对应异常值的权重 (FP16)
   W_normal ← W[~O, :]         // 其他权重

4. 量化正常部分:
   X_q, s_x ← quantize_int8(X_normal)
   W_q, s_w ← quantize_int8(W_normal)

5. 混合精度计算:
   Y_outlier ← matmul_fp16(X_outlier, W_outlier)
   Y_normal ← matmul_int8(X_q, W_q) * s_x * s_w

6. return Y_outlier + Y_normal</code></pre>
<p><strong>关键设计</strong>： - 约 0.1% 的维度是异常值，99.9% 的计算仍在 INT8 - 内存占用与纯 INT8 相近，推理速度略有下降（约 15–20%）</p>
<p><em>Source: Dettmers, T. et al.&nbsp;(2022). “LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale”. <a href="https://arxiv.org/abs/2208.07339">arXiv:2208.07339</a></em></p>
</div>
</div>
<p>LLM.int8() 的重要贡献不仅是方法本身，更是对异常值现象的发现和分析。这一发现启发了后续的 SmoothQuant 和 AWQ 等工作。</p>
</section>
<section id="gptq基于-hessian-的最优量化" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="gptq基于-hessian-的最优量化"><span class="header-section-number">4.1.4</span> GPTQ：基于 Hessian 的最优量化</h4>
<p>GPTQ（Frantar et al., 2022）采用了完全不同的思路：与其简单地四舍五入，不如找到<strong>最优</strong>的量化方式，使量化误差对输出的影响最小化。</p>
<p>核心想法来自经典的<strong>最优脑外科</strong>（Optimal Brain Surgeon）方法：给定一个训练好的网络，如何”手术式”地移除一些参数（或降低精度），同时最小化对输出的影响？</p>
<p>数学上，我们希望最小化量化引入的重构误差：</p>
<p><span class="math display">\[
\arg\min_{W_q} \|WX - W_q X\|_2^2
\]</span></p>
<p>其中 <span class="math inline">\(X\)</span> 是校准数据的激活值。利用二阶 Taylor 展开，这个优化问题可以近似为：</p>
<p><span class="math display">\[
\arg\min_{\delta W} \delta W^\top H \delta W
\]</span></p>
<p>其中 <span class="math inline">\(H = 2XX^\top\)</span> 是 Hessian 矩阵（或 Fisher 信息矩阵）。直觉是：Hessian 的特征值反映了参数的”敏感度”——特征值大的方向，参数变化会显著影响输出；特征值小的方向，参数变化影响较小。</p>
<p>GPTQ 的关键创新是<strong>逐列量化</strong>：不是一次量化整个矩阵，而是一列一列地量化，每量化一列后，用 Hessian 信息更新剩余列，补偿已引入的误差。这种贪心策略在实践中效果极好。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: GPTQ 逐列量化 (Frantar et al., 2022)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>： - 权重矩阵 <span class="math inline">\(W \in \mathbb{R}^{d_{row} \times d_{col}}\)</span> - Hessian 矩阵 <span class="math inline">\(H = 2XX^\top \in \mathbb{R}^{d_{col} \times d_{col}}\)</span> - 目标量化位宽 <span class="math inline">\(b\)</span>（如 4-bit） - 分组大小 <span class="math inline">\(B\)</span>（如 128）</p>
<p><strong>输出</strong>：量化后的权重 <span class="math inline">\(\hat{W}\)</span></p>
<pre><code>1. Cholesky 分解:
   H ← Cholesky(H)^{-1}     // 高效求逆

2. 初始化:
   E ← 0                     // 累计误差
   Q ← empty                 // 量化结果

3. for i = 0 to d_col - 1 in steps of B:
      // 当前块的列索引
      cols ← [i, i+1, ..., min(i+B-1, d_col-1)]

      // 量化当前块
      for j in cols:
         q_j ← quantize(W[:, j], b)   // 标准量化
         Q[:, j] ← q_j
         err ← (W[:, j] - q_j) / H[j, j]

         // 更新后续列（误差补偿）
         W[:, j+1:] ← W[:, j+1:] - err ⊗ H[j, j+1:]

4. return Q</code></pre>
<p><strong>关键设计</strong>： - 逐列量化 + 误差补偿，利用 Hessian 信息指导更新 - 分块处理，平衡精度和效率 - 时间复杂度 <span class="math inline">\(O(d^2 n)\)</span>，约 4 小时可量化 175B 模型</p>
<p><em>Source: Frantar, E. et al.&nbsp;(2022). “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers”. ICLR 2023. <a href="https://arxiv.org/abs/2210.17323">arXiv:2210.17323</a></em></p>
</div>
</div>
<p>GPTQ 的实验结果令人印象深刻：4-bit 量化后，175B 参数的 OPT-175B 模型的困惑度（perplexity）仅上升 0.5，几乎无损。这使得在单张 A100 上运行 175B 模型成为可能。</p>
</section>
<section id="awq激活感知的权重重要性" class="level4" data-number="4.1.5">
<h4 data-number="4.1.5" class="anchored" data-anchor-id="awq激活感知的权重重要性"><span class="header-section-number">4.1.5</span> AWQ：激活感知的权重重要性</h4>
<p>AWQ（Lin et al., 2023）提出了一个简洁而深刻的观察：<strong>不是所有权重都同等重要</strong>。</p>
<div id="fig-awq-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-awq-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-31/original/fig-awq-overview.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-awq-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: AWQ的核心思想：保护重要的权重通道
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Lin et al.&nbsp;(2023) “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”, Figure 1. MLSys 2024 Best Paper.</em></p>
</div>
<p>如上图所示，AWQ 的关键洞察是：权重的重要性应该由<strong>激活值</strong>来衡量，而非权重本身的大小。具体来说，如果某个输入通道的激活值很大，那么对应的权重列就很重要——即使这些权重本身数值不大，它们乘以大激活后对输出影响很大。</p>
<p>AWQ 的策略是：</p>
<ol type="1">
<li>用校准数据统计每个输入通道的平均激活幅度</li>
<li>对于重要通道（激活大的），放大权重后再量化</li>
<li>放大系数存下来，推理时对输入做相应的缩小（数学等价变换）</li>
</ol>
<p>数学上，原始计算是 <span class="math inline">\(Y = XW\)</span>。AWQ 引入对角缩放矩阵 <span class="math inline">\(S\)</span>：</p>
<p><span class="math display">\[
Y = XW = (XS^{-1})(SW)
\]</span></p>
<p>通过选择合适的 <span class="math inline">\(S\)</span>，使 <span class="math inline">\(SW\)</span> 更容易量化。具体地，对于重要通道 <span class="math inline">\(i\)</span>，设置 <span class="math inline">\(S_{ii} &gt; 1\)</span>，这会放大 <span class="math inline">\(W\)</span> 中对应的列，使其在量化时保留更高的精度。</p>
<p>AWQ 在 MLSys 2024 获得最佳论文奖，其简洁的设计和出色的效果代表了量化技术的前沿水平。</p>
</section>
<section id="量化技术对比" class="level4" data-number="4.1.6">
<h4 data-number="4.1.6" class="anchored" data-anchor-id="量化技术对比"><span class="header-section-number">4.1.6</span> 量化技术对比</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>位宽</th>
<th>核心思想</th>
<th>校准数据</th>
<th>速度</th>
<th>精度损失</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LLM.int8()</td>
<td>W8A8</td>
<td>混合精度分解</td>
<td>无</td>
<td>+50%</td>
<td>~0</td>
</tr>
<tr class="even">
<td>SmoothQuant</td>
<td>W8A8</td>
<td>平滑激活分布</td>
<td>~1000 样本</td>
<td>+80%</td>
<td>~0</td>
</tr>
<tr class="odd">
<td>GPTQ</td>
<td>W4A16</td>
<td>Hessian 最优量化</td>
<td>~128 样本</td>
<td>+200%</td>
<td>0.1-0.5</td>
</tr>
<tr class="even">
<td>AWQ</td>
<td>W4A16</td>
<td>激活感知重要性</td>
<td>~128 样本</td>
<td>+200%</td>
<td>0.1-0.3</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="投机解码speculative-decoding" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="投机解码speculative-decoding"><span class="header-section-number">4.2</span> 投机解码（Speculative Decoding）</h3>
<section id="基本原理" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="基本原理"><span class="header-section-number">4.2.1</span> 基本原理</h4>
<p>投机解码的核心思想惊人地简单：用一个快速但不那么准确的小模型（draft model）生成候选 token 序列，然后用目标大模型（target model）<strong>并行验证</strong>这些候选。</p>
<p>传统自回归生成：</p>
<pre><code>Token 1 → Token 2 → Token 3 → Token 4
   70ms      70ms      70ms      70ms
Total: 280ms</code></pre>
<p>投机解码：</p>
<pre><code>Draft: Token 1,2,3,4 (并行生成)  →  Target: 验证全部 (一次前向)
           28ms                          70ms
Total: ~100ms (假设全部接受)</code></pre>
<p>关键问题是：如果 draft 的预测被拒绝怎么办？投机解码使用一种精心设计的<strong>接受-拒绝采样</strong>机制，保证最终输出分布与直接使用 target model 完全相同。</p>
</section>
<section id="数学原理为什么能保证无损" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="数学原理为什么能保证无损"><span class="header-section-number">4.2.2</span> 数学原理：为什么能保证无损？</h4>
<p>设 <span class="math inline">\(p(x)\)</span> 是 target model 在某位置的概率分布，<span class="math inline">\(q(x)\)</span> 是 draft model 的分布。投机解码的接受规则是：</p>
<p><span class="math display">\[
\text{Accept } x \text{ with probability } \min\left(1, \frac{p(x)}{q(x)}\right)
\]</span></p>
<p>如果 <span class="math inline">\(x\)</span> 被接受，输出就是 <span class="math inline">\(x\)</span>。如果被拒绝，从修正分布 <span class="math inline">\(p'(x)\)</span> 中重新采样：</p>
<p><span class="math display">\[
p'(x) = \text{normalize}\left(\max(0, p(x) - q(x))\right)
\]</span></p>
<p><strong>定理</strong>：在上述接受-拒绝规则下，最终输出的分布恰好是 <span class="math inline">\(p(x)\)</span>。</p>
<p><strong>证明思路</strong>： 输出 <span class="math inline">\(x\)</span> 的总概率 = (由 draft 生成 <span class="math inline">\(x\)</span> 的概率) × (接受 <span class="math inline">\(x\)</span> 的概率) + (从修正分布采样到 <span class="math inline">\(x\)</span> 的概率)</p>
<p><span class="math display">\[
P(\text{output}=x) = q(x) \cdot \min\left(1, \frac{p(x)}{q(x)}\right) + P(\text{reject}) \cdot p'(x)
\]</span></p>
<p>经过仔细计算，可以验证这确实等于 <span class="math inline">\(p(x)\)</span>。关键洞察是：当 <span class="math inline">\(q(x) &gt; p(x)\)</span> 时，我们”过度采样”了 <span class="math inline">\(x\)</span>，需要通过拒绝来修正；当 <span class="math inline">\(q(x) &lt; p(x)\)</span> 时，我们”欠采样”了 <span class="math inline">\(x\)</span>，通过修正分布来补充。</p>
<p>这个结果非常优雅：它意味着投机解码在数学上是<strong>无损</strong>的，不会改变输出质量，只会提高速度。</p>
</section>
<section id="完整数值示例draft-verify-流程" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="完整数值示例draft-verify-流程"><span class="header-section-number">4.2.3</span> 完整数值示例：Draft-Verify 流程</h4>
<p>让我们通过一个具体例子，完整走过投机解码的一轮验证。</p>
<p><strong>设定</strong>： - Target model：70B 参数 - Draft model：7B 参数（同系列的小模型） - 猜测长度 <span class="math inline">\(K = 4\)</span> - 当前已生成的前缀：“The weather today is”</p>
<p><strong>Step 1: Draft 生成</strong></p>
<p>Draft model 并行生成 4 个候选 token 及其概率：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>位置</th>
<th>Token</th>
<th>Draft 概率 <span class="math inline">\(q(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>“very”</td>
<td>0.60</td>
</tr>
<tr class="even">
<td>2</td>
<td>“nice”</td>
<td>0.55</td>
</tr>
<tr class="odd">
<td>3</td>
<td>“and”</td>
<td>0.40</td>
</tr>
<tr class="even">
<td>4</td>
<td>“warm”</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<p><strong>Step 2: Target 验证（一次前向传播）</strong></p>
<p>Target model 对序列 “The weather today is very nice and warm” 计算条件概率。这是<strong>单次</strong>前向传播，但输出每个位置的概率分布：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>位置</th>
<th>Token</th>
<th>Target 概率 <span class="math inline">\(p(x)\)</span></th>
<th>Draft 概率 <span class="math inline">\(q(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>“very”</td>
<td>0.70</td>
<td>0.60</td>
</tr>
<tr class="even">
<td>2</td>
<td>“nice”</td>
<td>0.45</td>
<td>0.55</td>
</tr>
<tr class="odd">
<td>3</td>
<td>“and”</td>
<td>0.50</td>
<td>0.40</td>
</tr>
<tr class="even">
<td>4</td>
<td>“warm”</td>
<td>0.30</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<p><strong>Step 3: 逐位置接受/拒绝</strong></p>
<p>对每个位置，计算接受概率 <span class="math inline">\(\min(1, p/q)\)</span> 并掷骰子：</p>
<p><strong>位置 1 (“very”)</strong>： <span class="math display">\[
P(\text{accept}) = \min\left(1, \frac{0.70}{0.60}\right) = \min(1, 1.17) = 1.0
\]</span> 因为 <span class="math inline">\(p &gt; q\)</span>，target 比 draft 更喜欢这个 token，一定接受。✅</p>
<p><strong>位置 2 (“nice”)</strong>： <span class="math display">\[
P(\text{accept}) = \min\left(1, \frac{0.45}{0.55}\right) = 0.82
\]</span> 掷骰子得到 0.65 &lt; 0.82，接受。✅</p>
<p><strong>位置 3 (“and”)</strong>： <span class="math display">\[
P(\text{accept}) = \min\left(1, \frac{0.50}{0.40}\right) = 1.0
\]</span> 一定接受。✅</p>
<p><strong>位置 4 (“warm”)</strong>： <span class="math display">\[
P(\text{accept}) = \min\left(1, \frac{0.30}{0.35}\right) = 0.86
\]</span> 掷骰子得到 0.91 &gt; 0.86，拒绝。❌</p>
<p><strong>Step 4: 处理拒绝</strong></p>
<p>位置 4 被拒绝，需要从修正分布 <span class="math inline">\(p'\)</span> 中重新采样。假设 target 在位置 4 的完整分布是：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Token</th>
<th><span class="math inline">\(p(x)\)</span></th>
<th><span class="math inline">\(q(x)\)</span></th>
<th><span class="math inline">\(\max(0, p-q)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“warm”</td>
<td>0.30</td>
<td>0.35</td>
<td>0.00</td>
</tr>
<tr class="even">
<td>“sunny”</td>
<td>0.25</td>
<td>0.20</td>
<td>0.05</td>
</tr>
<tr class="odd">
<td>“pleasant”</td>
<td>0.20</td>
<td>0.15</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>其他</td>
<td>0.25</td>
<td>0.30</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>归一化后 <span class="math inline">\(p'\)</span>：<span class="math inline">\(P(\text{sunny}) = P(\text{pleasant}) = 0.5\)</span></p>
<p>假设采样到 “sunny”，最终输出是：<strong>“very nice and sunny”</strong></p>
<p><strong>Step 5: 效率分析</strong></p>
<ul>
<li><strong>无投机解码</strong>：生成 4 个 token 需要 4 次 target 前向，约 <span class="math inline">\(4 \times 70 = 280\)</span> ms</li>
<li><strong>有投机解码</strong>：1 次 draft 前向 + 1 次 target 前向，约 <span class="math inline">\(7 + 70 = 77\)</span> ms</li>
<li><strong>实际接受</strong>：3 个 token + 1 个重采样 = 4 个 token</li>
<li><strong>加速比</strong>：<span class="math inline">\(280 / 77 \approx 3.6\times\)</span></li>
</ul>
<p>实际场景中，由于 draft 和 target 的分布差异，接受率通常在 60–80%，整体加速比约 2–3 倍。</p>
</section>
</section>
<section id="持续批处理continuous-batching" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="持续批处理continuous-batching"><span class="header-section-number">4.3</span> 持续批处理（Continuous Batching）</h3>
<section id="传统批处理的问题" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="传统批处理的问题"><span class="header-section-number">4.3.1</span> 传统批处理的问题</h4>
<p>传统的静态批处理（static batching）要求所有请求同时开始、同时结束：</p>
<pre><code>请求 A: [生成 10 个 token]  ████░░░░░░░░░░░░░░░░░░░░░░░░░░░░
请求 B: [生成 50 个 token]  ████████████████████████████████
请求 C: [生成 20 个 token]  ██████████░░░░░░░░░░░░░░░░░░░░░░
                            ↑                              ↑
                          开始                            结束</code></pre>
<p>请求 A 和 C 早早完成，却要等待请求 B，白白占用 GPU 资源。对于长度分布差异大的场景（这在实际应用中很常见），静态批处理的 GPU 利用率可能不到 30%。</p>
</section>
<section id="orca-的迭代级调度" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="orca-的迭代级调度"><span class="header-section-number">4.3.2</span> Orca 的迭代级调度</h4>
<p>Orca（Yu et al., OSDI 2022）提出了<strong>迭代级调度</strong>（iteration-level scheduling）：在每个 iteration（生成一个 token）的粒度上做调度决策。</p>
<pre><code>请求 A: ████          （10 个 token 后退出，资源释放）
请求 B: ████████████████████████████████
请求 C: ██████████    （20 个 token 后退出）
请求 D:      ████████████████████████    （A 退出后加入）
请求 E:            ████████████████      （C 退出后加入）</code></pre>
<p>这样，短请求不用”陪跑”长请求，系统吞吐量大幅提升。Orca 论文报告了 <strong>36.9 倍</strong>的吞吐提升（与 FasterTransformer 相比，在 GPT-3 175B 上）。</p>
</section>
<section id="pagedattention-与-vllm" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="pagedattention-与-vllm"><span class="header-section-number">4.3.3</span> PagedAttention 与 vLLM</h4>
<p>vLLM（Kwon et al., SOSP 2023）在 Orca 的基础上进一步优化了 KV Cache 管理，提出了 <strong>PagedAttention</strong>。</p>
<p>传统方法为每个请求预分配最大长度的 KV Cache，导致严重浪费：</p>
<pre><code>请求 A (实际 100 token): [████████░░░░░░░░░░░░░░░░░░░░░░░░] 预留 1000 token
请求 B (实际 500 token): [████████████████████░░░░░░░░░░░░] 预留 1000 token
                          ↑ 实际使用        ↑ 浪费</code></pre>
<p>PagedAttention 将 KV Cache 切分成固定大小的页（如 16 tokens 一页），按需分配：</p>
<pre><code>请求 A: [Page 1][Page 2][Page 3][Page 4][Page 5][Page 6][Page 7]  (7 页, 实际 100 token)
请求 B: [Page 8][Page 9]...[Page 39]  (32 页, 实际 500 token)

物理内存: |Page 1|Page 8|Page 2|Page 9|Page 3|... (交错存储，无碎片)</code></pre>
<p>额外好处：当多个请求共享相同的 prompt（如系统指令），它们的 KV Cache 可以共享：</p>
<pre><code>请求 A: [系统提示(共享)][用户问题 A 的 KV]
请求 B: [系统提示(共享)][用户问题 B 的 KV]
                  ↑
            物理内存只存一份</code></pre>
<p>vLLM 结合 PagedAttention 和持续批处理，相比 HuggingFace Transformers 和 FasterTransformer 实现了 <strong>2–4 倍</strong>的吞吐提升。</p>
</section>
</section>
<section id="模型并行推理" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="模型并行推理"><span class="header-section-number">4.4</span> 模型并行推理</h3>
<p>当单卡显存不足以容纳模型时，需要将模型分布到多个 GPU 上。这与第19章讨论的分布式训练类似，但推理场景有其特殊性。</p>
<section id="tensor-parallelismtp" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="tensor-parallelismtp"><span class="header-section-number">4.4.1</span> Tensor Parallelism（TP）</h4>
<p>将单层的矩阵计算切分到多个 GPU：</p>
<pre><code>GPU 0: W[:, :d/2]   →  计算前半部分输出
GPU 1: W[:, d/2:]   →  计算后半部分输出
        ↓ AllReduce ↓
      合并得到完整输出</code></pre>
<p>优点：每层内并行，延迟低 缺点：每层都需要 AllReduce 通信</p>
</section>
<section id="pipeline-parallelismpp" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="pipeline-parallelismpp"><span class="header-section-number">4.4.2</span> Pipeline Parallelism（PP）</h4>
<p>将不同层放到不同 GPU：</p>
<pre><code>GPU 0: Layer 0-9    →  处理前 10 层
GPU 1: Layer 10-19  →  处理中间 10 层
GPU 2: Layer 20-29  →  处理后 10 层</code></pre>
<p>优点：通信量小（只在层边界传递激活） 缺点：存在流水线气泡，增加延迟</p>
</section>
<section id="推理场景的选择" class="level4" data-number="4.4.3">
<h4 data-number="4.4.3" class="anchored" data-anchor-id="推理场景的选择"><span class="header-section-number">4.4.3</span> 推理场景的选择</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>场景</th>
<th>推荐策略</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>低延迟（实时对话）</td>
<td>纯 TP</td>
<td>延迟最低</td>
</tr>
<tr class="even">
<td>高吞吐（批处理）</td>
<td>TP + PP</td>
<td>最大化利用率</td>
</tr>
<tr class="odd">
<td>显存极度受限</td>
<td>PP + 模型卸载</td>
<td>允许更大模型</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="使用-gptq-量化模型" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="使用-gptq-量化模型"><span class="header-section-number">5.1</span> 使用 GPTQ 量化模型</h3>
<div id="d1d12709" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> auto_gptq <span class="im">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载原始模型</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-2-7b-hf"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置量化参数</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>quantize_config <span class="op">=</span> BaseQuantizeConfig(</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    bits<span class="op">=</span><span class="dv">4</span>,                    <span class="co"># 4-bit 量化</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    group_size<span class="op">=</span><span class="dv">128</span>,            <span class="co"># 每 128 个权重共享量化参数</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    desc_act<span class="op">=</span><span class="va">True</span>,             <span class="co"># 激活感知排序（提升精度）</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型并量化</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoGPTQForCausalLM.from_pretrained(</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    quantize_config<span class="op">=</span>quantize_config,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备校准数据</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>calibration_data <span class="op">=</span> [</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text <span class="kw">in</span> [<span class="st">"校准文本1..."</span>, <span class="st">"校准文本2..."</span>, ...]</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 执行量化</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>model.quantize(calibration_data)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存量化后的模型</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>model.save_quantized(<span class="st">"llama-2-7b-gptq-4bit"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="使用-awq-量化模型" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="使用-awq-量化模型"><span class="header-section-number">5.2</span> 使用 AWQ 量化模型</h3>
<div id="ad49dedc" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> awq <span class="im">import</span> AutoAWQForCausalLM</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-2-7b-hf"</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoAWQForCausalLM.from_pretrained(</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># AWQ 量化配置</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>quant_config <span class="op">=</span> {</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"zero_point"</span>: <span class="va">True</span>,        <span class="co"># 使用零点</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"q_group_size"</span>: <span class="dv">128</span>,       <span class="co"># 分组大小</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"w_bit"</span>: <span class="dv">4</span>,                <span class="co"># 4-bit 量化</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"version"</span>: <span class="st">"GEMM"</span>          <span class="co"># 优化版本</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 执行量化</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>model.quantize(tokenizer, quant_config<span class="op">=</span>quant_config)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>model.save_quantized(<span class="st">"llama-2-7b-awq-4bit"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="vllm-部署示例" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="vllm-部署示例"><span class="header-section-number">5.3</span> vLLM 部署示例</h3>
<div id="736b2412" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm <span class="im">import</span> LLM, SamplingParams</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型（自动启用 PagedAttention 和持续批处理）</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"meta-llama/Llama-2-7b-chat-hf"</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    tensor_parallel_size<span class="op">=</span><span class="dv">1</span>,        <span class="co"># 单卡</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    gpu_memory_utilization<span class="op">=</span><span class="fl">0.90</span>,   <span class="co"># GPU 内存利用率</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    max_model_len<span class="op">=</span><span class="dv">4096</span>,            <span class="co"># 最大序列长度</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置采样参数</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>sampling_params <span class="op">=</span> SamplingParams(</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 批量推理</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is the capital of France?"</span>,</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Explain quantum computing in simple terms."</span>,</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Write a haiku about programming."</span>,</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm.generate(prompts, sampling_params)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> output <span class="kw">in</span> outputs:</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>output<span class="sc">.</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Generated: </span><span class="sc">{</span>output<span class="sc">.</span>outputs[<span class="dv">0</span>]<span class="sc">.</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="tgi-部署docker" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="tgi-部署docker"><span class="header-section-number">5.4</span> TGI 部署（Docker）</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 拉取 TGI 镜像</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull ghcr.io/huggingface/text-generation-inference:latest</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 启动服务</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">--gpus</span> all <span class="at">--shm-size</span> 1g <span class="at">-p</span> 8080:80 <span class="dt">\</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">-v</span> <span class="va">$HOME</span>/.cache/huggingface:/data <span class="dt">\</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    ghcr.io/huggingface/text-generation-inference:latest <span class="dt">\</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="dt">\</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quantize</span> gptq <span class="dt">\</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-concurrent-requests</span> 128 <span class="dt">\</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-input-length</span> 2048 <span class="dt">\</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-total-tokens</span> 4096</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 调用 API</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> http://localhost:8080/generate <span class="dt">\</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">-X</span> POST <span class="dt">\</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">-H</span> <span class="st">"Content-Type: application/json"</span> <span class="dt">\</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">-d</span> <span class="st">'{"inputs": "What is deep learning?", "parameters": {"max_new_tokens": 100}}'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<section id="为什么有效理论视角" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</h3>
<section id="量化的信息论基础" class="level4" data-number="6.1.1">
<h4 data-number="6.1.1" class="anchored" data-anchor-id="量化的信息论基础"><span class="header-section-number">6.1.1</span> 量化的信息论基础</h4>
<p>从信息论角度，量化的目标是找到一个编码，使重构误差最小化。对于服从某种分布的权重，最优量化（Lloyd-Max 量化）的误差与分布的方差和量化级数相关。</p>
<p>大模型权重的一个关键特性是其分布高度集中：绝大多数权重的绝对值小于某个阈值，呈近似正态分布。这意味着均匀量化（等间距划分）是次优的——大部分量化区间分配给了稀疏的尾部，而密集的中心区域反而精度不足。</p>
<p>GPTQ 和 AWQ 的优越性部分来自于对这种分布特性的利用：通过非均匀的缩放（GPTQ 的 Hessian 加权，AWQ 的激活感知）更好地适应权重分布。</p>
</section>
<section id="投机解码的概率论基础" class="level4" data-number="6.1.2">
<h4 data-number="6.1.2" class="anchored" data-anchor-id="投机解码的概率论基础"><span class="header-section-number">6.1.2</span> 投机解码的概率论基础</h4>
<p>投机解码的正确性基于<strong>接受-拒绝采样</strong>（acceptance-rejection sampling）的经典结果。这是蒙特卡洛方法的基础技术，数学上可以追溯到 von Neumann（1951）。</p>
<p>核心定理是：如果提议分布 <span class="math inline">\(q\)</span> 满足 <span class="math inline">\(q(x) &gt; 0\)</span> 对所有 <span class="math inline">\(p(x) &gt; 0\)</span> 的 <span class="math inline">\(x\)</span> 成立，那么接受-拒绝采样可以精确采样任何目标分布 <span class="math inline">\(p\)</span>。投机解码的贡献是将这一经典技术巧妙地应用于 LLM 推理加速。</p>
</section>
</section>
<section id="边界条件与失效模式" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="边界条件与失效模式"><span class="header-section-number">6.2</span> 边界条件与失效模式</h3>
<section id="量化的边界条件" class="level4" data-number="6.2.1">
<h4 data-number="6.2.1" class="anchored" data-anchor-id="量化的边界条件"><span class="header-section-number">6.2.1</span> 量化的边界条件</h4>
<p><strong>任务敏感性</strong>：某些任务对精度特别敏感。研究发现，数学推理任务在 4-bit 量化后性能下降明显（5–10%），而常规问答和文本生成任务几乎不受影响。</p>
<p><strong>模型规模</strong>：有趣的是，大模型通常比小模型更容易量化。70B 模型量化后的相对性能损失小于 7B 模型。一种解释是：大模型有更多的冗余，可以更好地”吸收”量化噪声。</p>
<p><strong>校准数据</strong>：GPTQ 和 AWQ 都需要少量校准数据（通常 128–512 样本）。校准数据的领域应该与实际应用匹配——用英文数据校准后处理中文可能效果不佳。</p>
</section>
<section id="投机解码的边界条件" class="level4" data-number="6.2.2">
<h4 data-number="6.2.2" class="anchored" data-anchor-id="投机解码的边界条件"><span class="header-section-number">6.2.2</span> 投机解码的边界条件</h4>
<p><strong>Draft-Target 分布差异</strong>：投机解码的加速比高度依赖于 draft 和 target 分布的相似度。当任务需要创意或探索性输出时（如创意写作），draft 的预测接受率会下降，加速比也随之降低。</p>
<p><strong>Token 长度分布</strong>：对于非常短的输出（&lt; 10 token），投机解码的额外开销可能抵消收益。最适合中等长度的生成任务。</p>
</section>
</section>
<section id="开放研究问题" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.3</span> 开放研究问题</h3>
<ol type="1">
<li><p><strong>超低 bit 量化</strong>：能否可靠地实现 2-bit 甚至 1-bit 量化？目前的研究表明 3-bit 是一个关键阈值，低于此会显著损失性能。突破这一限制需要新的量化技术或训练方法。</p></li>
<li><p><strong>自适应投机解码</strong>：当前的投机解码使用固定的猜测长度 <span class="math inline">\(K\)</span>。能否根据上下文动态调整 <span class="math inline">\(K\)</span>？在 draft 信心高的位置猜更多，信心低的位置猜更少？</p></li>
<li><p><strong>端到端协同优化</strong>：量化、投机解码、批处理目前是独立设计的。它们之间是否存在协同效应或冲突？能否设计一个统一的优化框架？</p></li>
<li><p><strong>边缘设备推理</strong>：手机、边缘设备的算力和内存远小于数据中心 GPU。如何将上述技术适配到极端资源受限的场景？</p></li>
</ol>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="本方法的局限" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</h3>
<p><strong>局限 1：速度-质量权衡不可避免</strong></p>
<p>尽管量化技术不断进步，4-bit 量化仍然会在某些任务上造成可测量的性能损失。对于追求极致质量的场景（如医疗、法律领域的 AI 应用），需要仔细评估量化的影响。</p>
<p><strong>局限 2：技术栈复杂性</strong></p>
<p>完整的推理优化需要叠加多项技术：量化 + 投机解码 + 持续批处理 + KV Cache 优化。每项技术都有自己的超参数和适用条件，组合起来的调优空间巨大。对于资源有限的团队，部署和维护成本不可忽视。</p>
<p><strong>局限 3：硬件依赖性</strong></p>
<p>很多优化依赖特定硬件特性：INT4 需要 GPU 支持（如 A100、H100 的 INT4 Tensor Core），FlashAttention 需要足够的 shared memory。跨平台部署（如 CPU、移动端）时，部分优化可能不可用。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</h3>
<p>推理优化解决了”如何高效地运行大模型”，但没有解决”大模型本身的局限”。即使推理成本降到足够低，模型仍然可能：</p>
<ul>
<li>产生幻觉（hallucination），给出看似合理但实际错误的回答</li>
<li>缺乏实时知识，无法回答最新发生的事件</li>
<li>无法处理超长上下文，在需要综合大量信息的任务上表现不佳</li>
</ul>
<p>这引出了下一章要讨论的<strong>检索增强生成（RAG）</strong>——一种通过外部知识库增强模型能力的范式。推理优化让模型跑得快，RAG 让模型知道更多。两者是互补的技术，共同构成现代 LLM 应用的基础设施。</p>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<ol type="1">
<li><p><strong>问题</strong>：推理成本（显存、延迟、费用）是大模型普及的主要障碍。70B 模型的 FP16 推理需要 140GB 显存，远超单卡容量；自回归生成的 token-by-token 方式让 GPU 利用率仅有 10–30%。</p></li>
<li><p><strong>洞察</strong>：推理优化的本质是在精度、速度、成本三角形中寻找帕累托最优。关键在于识别和利用大模型推理的特殊结构——权重分布集中适合量化，自回归生成可以”猜测-验证”并行化，请求长度差异适合动态批处理。</p></li>
<li><p><strong>方法</strong>：</p>
<ul>
<li><strong>量化</strong>（GPTQ、AWQ）：INT8/INT4 压缩，2–4 倍显存节省，几乎无损</li>
<li><strong>投机解码</strong>：2–3 倍无损加速，数学上可证明正确</li>
<li><strong>持续批处理</strong>（vLLM）：迭代级调度，2–5 倍吞吐提升</li>
<li><strong>PagedAttention</strong>：消除 KV Cache 碎片，支持更大 batch</li>
</ul></li>
<li><p><strong>意义</strong>：这些技术可以叠加使用，将推理成本降低 10 倍以上，让”只有大厂用得起”变成”人人可部署”。</p></li>
</ol>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 42%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>表达式</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>线性量化</td>
<td><span class="math inline">\(w_q = \text{round}((w - z) / s)\)</span></td>
</tr>
<tr class="even">
<td>投机解码接受概率</td>
<td><span class="math inline">\(P_{\text{accept}} = \min(1, p(x) / q(x))\)</span></td>
</tr>
<tr class="odd">
<td>KV Cache 大小</td>
<td><span class="math inline">\(2 \times L \times n \times d \times \text{sizeof(dtype)}\)</span></td>
</tr>
<tr class="even">
<td>内存带宽受限时间</td>
<td><span class="math inline">\(T = \text{参数大小} / \text{带宽}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="方法对比速查" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="方法对比速查"><span class="header-section-number">8.3</span> 方法对比速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>显存影响</th>
<th>延迟影响</th>
<th>吞吐影响</th>
<th>质量影响</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>INT8 量化</td>
<td>-50%</td>
<td>-30%</td>
<td>+50%</td>
<td>~0%</td>
</tr>
<tr class="even">
<td>INT4 量化</td>
<td>-75%</td>
<td>-50%</td>
<td>+100%</td>
<td>1–3%</td>
</tr>
<tr class="odd">
<td>投机解码</td>
<td>+10%（draft）</td>
<td>-50–60%</td>
<td>无</td>
<td>0%（数学无损）</td>
</tr>
<tr class="even">
<td>持续批处理</td>
<td>无</td>
<td>+20%（首 token）</td>
<td>+200–500%</td>
<td>0%</td>
</tr>
<tr class="odd">
<td>PagedAttention</td>
<td>-30–50%</td>
<td>无</td>
<td>+50–100%</td>
<td>0%</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.4</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 解释为什么大模型推理是”内存带宽受限”而非”计算受限”的任务。这对优化策略有什么启示？</p></li>
<li><p><strong>[数学推导]</strong> 证明投机解码的接受-拒绝采样保持目标分布不变。给出完整的概率计算。</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用 vLLM 部署一个 INT4 量化的 LLaMA-7B 模型，测量不同 batch size 下的吞吐量（tokens/s）和首 token 延迟。观察并解释瓶颈点。</p></li>
<li><p><strong>[开放思考]</strong> 量化、投机解码、持续批处理能否同时使用？设计一个完整的推理系统架构，考虑它们之间的协同与冲突。</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<ul>
<li><strong><a href="https://arxiv.org/abs/2210.17323">GPTQ</a></strong>：Frantar et al.&nbsp;(2022)，后训练量化的里程碑
<ul>
<li>重点读：Section 3（方法）、Section 4（实验）</li>
<li>可跳过：Appendix 的详细推导</li>
</ul></li>
<li><strong><a href="https://arxiv.org/abs/2306.00978">AWQ</a></strong>：Lin et al.&nbsp;(2023)，MLSys 2024 最佳论文
<ul>
<li>重点读：Section 3（激活感知量化）、Figure 1-2</li>
</ul></li>
<li><strong><a href="https://arxiv.org/abs/2309.06180">vLLM/PagedAttention</a></strong>：Kwon et al.&nbsp;(2023)，SOSP 2023
<ul>
<li>重点读：Section 3（PagedAttention）、Section 4（系统设计）</li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">9.2</span> 理论基础</h3>
<ul>
<li><strong><a href="https://arxiv.org/abs/2208.07339">LLM.int8()</a></strong>：Dettmers et al.&nbsp;(2022)，发现异常值现象</li>
<li><strong><a href="https://arxiv.org/abs/2211.17192">Speculative Decoding</a></strong>：Leviathan et al.&nbsp;(2023)，投机解码的理论框架</li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.3</span> 后续发展</h3>
<ul>
<li><strong><a href="https://github.com/ggml-org/ggml/blob/master/docs/gguf.md">GGUF Format</a></strong>：llama.cpp 的量化格式，支持 CPU 推理</li>
<li><strong><a href="https://arxiv.org/abs/2305.13245">GQA</a></strong>：Grouped-Query Attention，减少 KV Cache</li>
</ul>
</section>
<section id="代码资源-1" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="代码资源-1"><span class="header-section-number">9.4</span> 代码资源</h3>
<ul>
<li><strong><a href="https://github.com/vllm-project/vllm">vLLM</a></strong>：高性能推理引擎</li>
<li><strong><a href="https://github.com/huggingface/text-generation-inference">TGI</a></strong>：Hugging Face 推理服务</li>
<li><strong><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></strong>：CPU 推理，GGUF 格式</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>推理优化的需求伴随着 GPT-3 的发布而爆发。2020 年，175B 参数的 GPT-3 震惊了 AI 社区，但也让人们意识到：这样的模型几乎无法在任何单一设备上运行。最初，人们认为这只是”有钱人的游戏”——只有 OpenAI、Google 这样的公司才有资源部署如此规模的模型。</p>
<p>转折点出现在 2022 年。Dettmers 的 LLM.int8() 首次展示了大模型可以在”可承受”的硬件上运行。紧随其后，GPTQ 将 175B 模型压缩到单卡可运行的程度。2023 年，vLLM 和 TGI 的出现让高效推理成为”开箱即用”的能力。短短两年间，大模型推理从”天方夜谭”变成了”人人可及”。</p>
<p>这段历史告诉我们：AI 的民主化不仅需要开源模型，更需要开源的基础设施。量化、高效推理引擎这些”不那么性感”的技术，实际上是让 AI 惠及普通人的关键拼图。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第31章：推理优化"</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "From Training to Deployment: The Art of Making Giants Fast and Cheap"</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-29"</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, Inference, Quantization, Speculative Decoding]</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [推理优化, 量化, GPTQ, AWQ, 投机解码, vLLM, PagedAttention, 持续批处理, KV Cache]</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "当大语言模型从训练走向部署，推理成本成为新的瓶颈——一个70B模型的FP16推理需要140GB显存，远超单卡容量；自回归生成的token-by-token方式让GPU利用率仅有10-20%。本章系统梳理推理优化的三条主线：量化技术（GPTQ、AWQ）用精度换内存，将70B模型压缩到单卡运行；投机解码用小模型猜测、大模型验证的方式打破自回归瓶颈，实现2-3倍无损加速；持续批处理和PagedAttention通过动态调度最大化硬件利用率。这些技术可以叠加使用，将推理成本降低10倍以上，让'只有大厂用得起'变成'人人可部署'。"</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-31/original/fig-awq-overview.png"</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：当大语言模型从训练走向部署，如何在成本、延迟、质量三者之间取得最优平衡？</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2022–2024 </span><span class="pp">|</span><span class="at"> LLM.int8() (Dettmers, 2022), GPTQ (Frantar, 2022), SmoothQuant (Xiao, 2022), Speculative Decoding (Leviathan &amp; Chen, 2023), AWQ (Lin, 2023), vLLM (Kwon, 2023) </span><span class="pp">|</span><span class="at"> 从算法到系统的推理优化全栈</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dettmers et al. (2022)** "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale" (<span class="co">[</span><span class="ot">arXiv:2208.07339</span><span class="co">](https://arxiv.org/abs/2208.07339)</span>) — 参考了 outlier 特征的发现和混合精度分解方法</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Frantar et al. (2022)** "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers" (<span class="co">[</span><span class="ot">arXiv:2210.17323</span><span class="co">](https://arxiv.org/abs/2210.17323)</span>) — 参考了基于 Hessian 的逐层量化方法（ICLR 2023）</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Xiao et al. (2022)** "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models" (<span class="co">[</span><span class="ot">arXiv:2211.10438</span><span class="co">](https://arxiv.org/abs/2211.10438)</span>) — 参考了激活平滑技术</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lin et al. (2023)** "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration" (<span class="co">[</span><span class="ot">arXiv:2306.00978</span><span class="co">](https://arxiv.org/abs/2306.00978)</span>) — 参考了激活感知量化方法（MLSys 2024 最佳论文）；从论文提取了1张架构图</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Leviathan et al. (2023)** "Fast Inference from Transformers via Speculative Decoding" (<span class="co">[</span><span class="ot">arXiv:2211.17192</span><span class="co">](https://arxiv.org/abs/2211.17192)</span>) — 参考了投机解码的基本框架和数学证明</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chen et al. (2023)** "Accelerating Large Language Model Decoding with Speculative Sampling" (<span class="co">[</span><span class="ot">arXiv:2302.01318</span><span class="co">](https://arxiv.org/abs/2302.01318)</span>) — 参考了 Google 的投机采样实现</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kwon et al. (2023)** "Efficient Memory Management for Large Language Model Serving with PagedAttention" (<span class="co">[</span><span class="ot">arXiv:2309.06180</span><span class="co">](https://arxiv.org/abs/2309.06180)</span>) — 参考了 PagedAttention 和 vLLM 系统设计（SOSP 2023）</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Yu et al. (2022)** "Orca: A Distributed Serving System for Transformer-Based Generative Models" (OSDI 2022) — 参考了迭代级调度和持续批处理</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ainslie et al. (2023)** "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints" (<span class="co">[</span><span class="ot">arXiv:2305.13245</span><span class="co">](https://arxiv.org/abs/2305.13245)</span>) — 参考了 Grouped-Query Attention 的设计</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS229S** Systems for Machine Learning (2023) — 参考了推理系统优化的框架</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**UC Berkeley CS294** Machine Learning Systems (2024) — 参考了量化技术的教学组织</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">vllm-project/vllm</span><span class="co">](https://github.com/vllm-project/vllm)</span> — vLLM 高性能推理引擎</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">huggingface/text-generation-inference</span><span class="co">](https://github.com/huggingface/text-generation-inference)</span> — TGI 推理服务</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">IST-DASLab/gptq</span><span class="co">](https://github.com/IST-DASLab/gptq)</span> — GPTQ 官方实现</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">mit-han-lab/llm-awq</span><span class="co">](https://github.com/mit-han-lab/llm-awq)</span> — AWQ 官方实现</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>上一章我们见证了参数高效微调（PEFT）的革命——从 Adapter 的层间插入，到 LoRA 的低秩分解，再到 QLoRA 的 4-bit 量化微调。这些技术将大模型微调从"需要一个集群"变成"一张消费级 GPU"。70B 参数的 LLaMA 现在可以在单张 24GB 的 RTX 4090 上完成微调，成本从数万美元降到几百美元。</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>但微调只是模型生命周期的**一次性成本**。一旦模型上线，真正的成本来自**推理**——每一次用户请求都需要消耗计算资源。对于一个有百万日活用户的应用，推理成本可能是微调成本的数百倍。让我们做一个简单的算术：</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>假设你运营一个基于 LLaMA-70B 的 AI 助手：</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**微调成本**（一次性）：QLoRA 在单 GPU 上训练，约 $200（云 GPU 租用）</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**推理成本**（持续）：</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>每次请求平均 500 token（输入 + 输出）</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>每天 100 万次请求</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>70B 模型 FP16 推理需要约 140GB 显存，需要 2 张 A100 80GB</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>A100 租用成本约 $2/小时 × 2 = $4/小时 = **$96/天**</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>每月推理成本：**$2,880**</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>一年下来，推理成本接近 **$35,000**，是微调成本的 175 倍！</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>更糟糕的是，这还是在理想情况下的估算。实际部署中，由于自回归生成的顺序特性，GPU 的利用率通常只有 10–30%。大部分时间，昂贵的 A100 GPU 都在"等待"——等待上一个 token 生成完毕，才能开始下一个 token 的计算。这是一种巨大的浪费。</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>推理优化面临三大核心挑战：</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>**挑战一：显存瓶颈**</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>一个 70B 模型的 FP16 参数占用 140GB，这还没算 KV Cache。在长上下文场景下（比如 128K token），KV Cache 可能占用数百 GB。即使是 8 张 A100 组成的服务器，也可能被显存卡住。</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>**挑战二：延迟瓶颈**</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>自回归生成是天然顺序的——每个 token 都依赖前面所有 token。这意味着生成 100 个 token 需要进行 100 次完整的前向传播。用户看到的是"逐字蹦出"的体验，延迟感明显。</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>**挑战三：吞吐瓶颈**</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>传统的静态批处理要求所有请求同时开始、同时结束。但实际场景中，用户请求长度参差不齐——有的问"今天天气"，有的要求写一篇长文。短请求被迫等待长请求完成，系统吞吐量大打折扣。</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：推理优化的本质是在**精度、速度、成本**三角形中寻找帕累托最优。量化技术通过降低数值精度换取内存和带宽——INT4 量化可以将 70B 模型压缩到 35GB，单卡可运行。投机解码通过"猜测-验证"的并行化打破自回归的顺序瓶颈——小模型一次猜 4 个 token，大模型一次验证全部，实现 2–3 倍无损加速。持续批处理通过迭代级调度最大化硬件利用率——新请求可以随时加入，完成的请求立即退出，不再"陪跑"。这些技术可以叠加使用：一个部署 INT4 量化 + 投机解码 + PagedAttention + 持续批处理的系统，相比 naive 实现可以将成本降低 **10 倍以上**。</span></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a><span class="fu">### 推理的成本结构</span></span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>要优化推理，首先要理解成本从何而来。大模型推理的资源消耗可以分解为三大部分：</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>**模型参数**</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>这是最直观的部分。70B 参数在 FP16 下占用 $70 \times 10^9 \times 2 = 140$ GB。参数需要从显存读取到计算单元（CUDA Core 或 Tensor Core），这是推理的第一个瓶颈。</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>**KV Cache**</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>这是容易被忽视但至关重要的部分。在自回归生成中，为了避免重复计算历史 token 的 Key 和 Value，模型会将它们缓存下来。KV Cache 的大小随序列长度线性增长：</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>\text{KV Cache} = 2 \times L \times n \times d \times \text{sizeof(dtype)}</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>其中 $L$ 是层数，$n$ 是序列长度，$d$ 是隐藏维度，因子 2 来自 K 和 V 两个矩阵。对于 LLaMA-70B（$L=80$，$d=8192$），128K 上下文的 KV Cache 占用：</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>2 \times 80 \times 128000 \times 8192 \times 2 = 335 \text{ GB (FP16)}</span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>这比模型参数本身还大！在长上下文场景，KV Cache 是显存的主要消耗者。</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>**激活值**</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>前向传播过程中的中间结果。相比参数和 KV Cache，激活值占用较小（batch size 为 1 时通常只有几个 GB），但在大 batch 场景下也不可忽视。</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### 自回归解码的瓶颈分析</span></span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>为什么大模型推理这么慢？要理解这一点，我们需要深入分析自回归解码的计算特性。</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>在生成第 $t$ 个 token 时，模型需要：</span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>读取所有模型参数（140GB for 70B model）</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>读取 KV Cache（随 $t$ 线性增长）</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>计算注意力和 FFN</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>输出一个 token 的 logits</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>关键观察是：**生成单个 token 只需要很少的计算（相对于参数量），但需要读取大量数据（所有参数）**。这让推理变成了一个 **内存带宽受限**（memory-bound）而非**计算受限**（compute-bound）的任务。</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>让我们算一下。A100 GPU 的规格是：</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>内存带宽：2 TB/s</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>FP16 算力：312 TFLOPS</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>生成一个 token 需要读取 140GB 参数，按 2 TB/s 带宽计算：</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>\text{读取时间} = \frac{140 \text{ GB}}{2000 \text{ GB/s}} = 70 \text{ ms}</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>而 70B 模型的 FP16 计算量约为 $2 \times 70 \times 10^9 = 140$ GFLOPs（每个参数大约 2 次运算），按 312 TFLOPS 计算：</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>\text{计算时间} = \frac{140 \text{ GFLOPs}}{312 \text{ TFLOPs}} = 0.45 \text{ ms}</span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>计算只需要 0.45ms，但读取参数需要 70ms——**实际利用率不到 1%**！GPU 的强大算力几乎完全浪费在等待数据传输上。</span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>这就是为什么单纯增加 batch size 可以提升吞吐：多个请求共享同一份参数读取，摊薄了内存带宽的开销。但 batch size 受限于显存（更多请求 = 更多 KV Cache），不能无限增大。</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>理解了问题的本质，解决方案的方向就清晰了：</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**减少需要读取的数据量** → **量化**：把 FP16 参数压缩成 INT8 或 INT4，读取量减半或减到四分之一</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**打破顺序依赖** → **投机解码**：一次生成多个 token，减少前向传播次数</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**最大化硬件利用率** → **持续批处理**：动态调度请求，让 GPU 时刻保持忙碌</span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**高效管理 KV Cache** → **PagedAttention**：像操作系统管理内存一样管理 KV Cache</span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>这四类技术相互独立、可以叠加。一个现代推理系统（如 vLLM）会同时使用它们。接下来，我们逐一深入探讨每项技术的原理和实现。</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>在进入技术细节之前，让我们先用直觉建立对各项技术的理解。</span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a><span class="fu">### 量化的直觉：有损压缩</span></span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>想象你要通过一条窄带宽的网络传输一张高清照片。原图是 24-bit RGB，每个像素 3 个字节。如果带宽不够，你会怎么做？最直接的方法是**压缩**——JPEG 就是这样做的，它用 8-bit 甚至更低的精度存储像素，肉眼几乎看不出差别。</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>量化对神经网络做的事情完全类似。预训练的权重是 FP16（16-bit 浮点数），包含了大量"冗余"的精度。研究发现，大模型的权重分布高度集中——绝大多数权重的绝对值都很小，只有极少数"异常值"（outliers）比较大。这种分布意味着我们可以用更少的 bit 来表示大部分权重，而几乎不损失信息。</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>INT8 量化将 16-bit 压缩到 8-bit，参数量减半；INT4 量化更激进，压缩到四分之一。代价是轻微的精度损失——就像 JPEG 压缩会引入一些"噪点"，但通常在可接受范围内。</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### 投机解码的直觉：草稿-审稿流程</span></span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>想象你是一位繁忙的主编，需要审阅大量稿件。你可以亲自一字一句地写每篇文章，但这太慢了。一个更高效的流程是：让一位初级编辑先写草稿，然后你快速审阅——接受好的部分，修改不满意的部分。</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>投机解码（Speculative Decoding）正是这个思路。大模型（target model）太慢，每生成一个 token 都要 70ms。我们引入一个小模型（draft model），它虽然不如大模型准确，但生成速度快得多（比如只需 7ms）。流程变成：</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>小模型快速生成 4 个候选 token：<span class="co">[</span><span class="ot">"The", "cat", "sat", "on"</span><span class="co">]</span></span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>大模型**一次性**验证这 4 个 token 是否可接受</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>如果前 3 个被接受，第 4 个被拒绝，则保留前 3 个，从拒绝位置重新开始</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>关键洞察是：大模型验证 4 个 token 只需要**一次**前向传播（约 70ms），而如果大模型自己生成 4 个 token 需要**四次**前向传播（约 280ms）。只要小模型的预测足够准确（被接受的比例够高），整体速度就能提升。</span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>更妙的是，通过精心设计的接受-拒绝机制，投机解码可以保证最终输出分布与直接使用大模型**完全相同**——这是数学上可证明的无损加速！</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### 持续批处理的直觉：餐厅动态座位安排</span></span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>想象一家繁忙的餐厅。传统的做法是"静态批处理"：服务员等凑齐 8 位客人，一起安排到一张大桌子；所有人必须等最慢的那位吃完，才能一起离席。如果有人只点了一杯咖啡，他也得陪着吃牛排的人坐到最后。</span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>持续批处理（Continuous Batching）更像现代快餐店：客人随到随坐，吃完就走，座位立刻分配给下一位。这样，快速的请求不用等待慢速的请求，系统吞吐量大幅提升。</span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a>在 LLM 推理中，不同请求的输出长度差异巨大——有的回答只需 10 个 token，有的需要 1000 个。静态批处理让短请求"陪跑"长请求，GPU 利用率极低。持续批处理允许在每个 iteration（生成一个 token）的粒度上调度：完成的请求立即退出，新请求可以随时加入。</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a><span class="fu">### PagedAttention的直觉：操作系统的内存分页</span></span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a>传统的 KV Cache 管理是"连续分配"的：为每个请求预留一大块连续显存，足以容纳其最大可能长度。这就像早期操作系统的内存管理——为每个程序分配一整块连续内存，导致严重的碎片化。</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a>PagedAttention（分页注意力）借鉴了现代操作系统的**虚拟内存**技术：将 KV Cache 切分成固定大小的"页"（blocks），按需分配，不要求连续。这样，不同请求可以共享物理显存，碎片化问题迎刃而解。</span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a>一个额外的好处是：当多个请求共享相同的 prompt（比如系统提示），它们的 KV Cache 可以共享同一份物理内存，进一步节省空间。</span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a><span class="fu">### 量化技术</span></span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 量化基础</span></span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a>量化的本质是用离散值近似连续值。给定一个浮点数权重 $w$，量化过程可以表示为：</span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a>w_q = \text{round}\left(\frac{w - z}{s}\right), \quad w_{dq} = w_q \cdot s + z</span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a>其中 $s$ 是**缩放因子**（scale），$z$ 是**零点**（zero point），$w_q$ 是量化后的整数，$w_{dq}$ 是反量化后的近似值。</span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a>量化有几个关键维度：</span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a>**对称 vs 非对称**</span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**对称量化**：假设权重分布关于 0 对称，$z = 0$。只需要存储 $s$。</span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**非对称量化**：允许任意分布，需要同时存储 $s$ 和 $z$。更灵活但开销稍大。</span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>**量化粒度**</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Per-tensor**：整个权重矩阵共享一组 $(s, z)$。最节省空间，但精度最低。</span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Per-channel**：每个输出通道有独立的 $(s, z)$。平衡了精度和开销。</span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Per-group**：每 $g$ 个元素共享 $(s, z)$，比如每 128 个元素一组。GPTQ 和 AWQ 常用。</span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：FP16 → INT4 量化</span></span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a>让我们通过一个具体的例子，手把手走过量化的完整流程。</span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a>**设定**：一个 $4 \times 4$ 的权重矩阵，原始值为 FP16。</span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a>W = \begin{bmatrix}</span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a>-2.1 &amp; 0.3 &amp; 1.5 &amp; -0.8 <span class="sc">\\</span></span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>0.5 &amp; 2.7 &amp; -1.2 &amp; 0.1 <span class="sc">\\</span></span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>1.8 &amp; -0.4 &amp; 0.9 &amp; 2.3 <span class="sc">\\</span></span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a>-1.5 &amp; 1.1 &amp; -0.2 &amp; 0.7</span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a>**Step 1: 计算量化参数**</span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a>首先找到权重的范围：</span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>w_{\min} = -2.1, \quad w_{\max} = 2.7</span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>INT4 可以表示 $2^4 = 16$ 个值，范围是 $<span class="co">[</span><span class="ot">0, 15</span><span class="co">]</span>$（无符号）或 $<span class="co">[</span><span class="ot">-8, 7</span><span class="co">]</span>$（有符号）。我们使用无符号 INT4，范围 $<span class="co">[</span><span class="ot">0, 15</span><span class="co">]</span>$。</span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>计算缩放因子和零点：</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>s = \frac{w_{\max} - w_{\min}}{2^4 - 1} = \frac{2.7 - (-2.1)}{15} = \frac{4.8}{15} = 0.32</span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a>z = \text{round}\left(-\frac{w_{\min}}{s}\right) = \text{round}\left(\frac{2.1}{0.32}\right) = \text{round}(6.56) = 7</span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a>**Step 2: 量化**</span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>对每个权重 $w$，计算量化值：</span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>w_q = \text{clamp}\left(\text{round}\left(\frac{w}{s}\right) + z, \, 0, \, 15\right)</span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>以 $w = 1.5$ 为例：</span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a>w_q = \text{round}\left(\frac{1.5}{0.32}\right) + 7 = \text{round}(4.69) + 7 = 5 + 7 = 12</span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a>完整的量化矩阵：</span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>W_q = \begin{bmatrix}</span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a>0 &amp; 8 &amp; 12 &amp; 4 <span class="sc">\\</span></span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a>9 &amp; 15 &amp; 3 &amp; 7 <span class="sc">\\</span></span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a>13 &amp; 6 &amp; 10 &amp; 14 <span class="sc">\\</span></span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a>2 &amp; 10 &amp; 6 &amp; 9</span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>**Step 3: 反量化与误差分析**</span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>反量化公式：$w_{dq} = (w_q - z) \cdot s$</span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>以 $w_q = 12$（原始 $w = 1.5$）为例：</span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a>w_{dq} = (12 - 7) \cdot 0.32 = 5 \cdot 0.32 = 1.60</span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>量化误差：$|1.5 - 1.60| = 0.10$</span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a>**误差分析**：INT4 量化的最大误差为 $s/2 = 0.16$。平均误差通常为最大误差的一半左右，即 $\sim 0.08$。相比原始权重的典型大小（$\sim 1$），相对误差约 8%。</span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a>**Step 4: 存储空间对比**</span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FP16**：$16 \times 2 = 32$ bytes</span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**INT4**：$16 \times 0.5 + 2 \times 2 = 8 + 4 = 12$ bytes（权重 + scale &amp; zero）</span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**压缩比**：$32 / 12 \approx 2.7\times$</span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>对于更大的矩阵，使用 per-group 量化（如每 128 个元素一组），压缩比可以接近理论值 $4\times$。</span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LLM.int8()：发现异常值</span></span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a>2022 年，Dettmers 等人在尝试直接对 LLM 进行 INT8 量化时，遇到了一个令人惊讶的现象：模型性能**断崖式下降**，远超预期。经过深入分析，他们发现了问题的根源：**异常值特征**（emergent outliers）。</span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>在大模型的某些维度上，激活值会出现极端的异常值——比正常值大 10–100 倍。这些异常值只占全部特征的约 0.1%，但对模型输出至关重要。直接量化会严重损害这些异常值的精度，导致模型崩溃。</span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a>LLM.int8() 的解决方案是**混合精度分解**：</span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>检测异常值维度（绝对值超过阈值的维度）</span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>异常值维度保持 FP16 精度</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>其他维度使用 INT8 量化</span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: LLM.int8() 混合精度矩阵乘法 (Dettmers et al., 2022)</span></span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a>**输入**：</span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>激活矩阵 $X \in \mathbb{R}^{b \times d}$</span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>权重矩阵 $W \in \mathbb{R}^{d \times k}$</span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>异常值阈值 $\alpha$（通常为 6.0）</span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>**输出**：$Y = XW$</span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a><span class="in">1. 检测异常值维度:</span></span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a><span class="in">   O ← {i : max|X[:, i]| &gt; α}  // 异常值维度集合</span></span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a><span class="in">2. 分解输入:</span></span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a><span class="in">   X_outlier ← X[:, O]         // 异常值部分 (FP16)</span></span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a><span class="in">   X_normal ← X[:, ~O]         // 正常部分</span></span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a><span class="in">3. 分解权重:</span></span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a><span class="in">   W_outlier ← W[O, :]         // 对应异常值的权重 (FP16)</span></span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a><span class="in">   W_normal ← W[~O, :]         // 其他权重</span></span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a><span class="in">4. 量化正常部分:</span></span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a><span class="in">   X_q, s_x ← quantize_int8(X_normal)</span></span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a><span class="in">   W_q, s_w ← quantize_int8(W_normal)</span></span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a><span class="in">5. 混合精度计算:</span></span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a><span class="in">   Y_outlier ← matmul_fp16(X_outlier, W_outlier)</span></span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a><span class="in">   Y_normal ← matmul_int8(X_q, W_q) * s_x * s_w</span></span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a><span class="in">6. return Y_outlier + Y_normal</span></span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>**关键设计**：</span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>约 0.1% 的维度是异常值，99.9% 的计算仍在 INT8</span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>内存占用与纯 INT8 相近，推理速度略有下降（约 15–20%）</span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>*Source: Dettmers, T. et al. (2022). "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale". [arXiv:2208.07339](https://arxiv.org/abs/2208.07339)*</span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>LLM.int8() 的重要贡献不仅是方法本身，更是对异常值现象的发现和分析。这一发现启发了后续的 SmoothQuant 和 AWQ 等工作。</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GPTQ：基于 Hessian 的最优量化</span></span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a>GPTQ（Frantar et al., 2022）采用了完全不同的思路：与其简单地四舍五入，不如找到**最优**的量化方式，使量化误差对输出的影响最小化。</span>
<span id="cb16-371"><a href="#cb16-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-372"><a href="#cb16-372" aria-hidden="true" tabindex="-1"></a>核心想法来自经典的**最优脑外科**（Optimal Brain Surgeon）方法：给定一个训练好的网络，如何"手术式"地移除一些参数（或降低精度），同时最小化对输出的影响？</span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a>数学上，我们希望最小化量化引入的重构误差：</span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a>\arg\min_{W_q} <span class="sc">\|</span>WX - W_q X<span class="sc">\|</span>_2^2</span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a>其中 $X$ 是校准数据的激活值。利用二阶 Taylor 展开，这个优化问题可以近似为：</span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>\arg\min_{\delta W} \delta W^\top H \delta W</span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a>其中 $H = 2XX^\top$ 是 Hessian 矩阵（或 Fisher 信息矩阵）。直觉是：Hessian 的特征值反映了参数的"敏感度"——特征值大的方向，参数变化会显著影响输出；特征值小的方向，参数变化影响较小。</span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a>GPTQ 的关键创新是**逐列量化**：不是一次量化整个矩阵，而是一列一列地量化，每量化一列后，用 Hessian 信息更新剩余列，补偿已引入的误差。这种贪心策略在实践中效果极好。</span>
<span id="cb16-389"><a href="#cb16-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-390"><a href="#cb16-390" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: GPTQ 逐列量化 (Frantar et al., 2022)</span></span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a>**输入**：</span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>权重矩阵 $W \in \mathbb{R}^{d_{row} \times d_{col}}$</span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hessian 矩阵 $H = 2XX^\top \in \mathbb{R}^{d_{col} \times d_{col}}$</span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>目标量化位宽 $b$（如 4-bit）</span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>分组大小 $B$（如 128）</span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a>**输出**：量化后的权重 $\hat{W}$</span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a><span class="in">1. Cholesky 分解:</span></span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a><span class="in">   H ← Cholesky(H)^{-1}     // 高效求逆</span></span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a><span class="in">2. 初始化:</span></span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a><span class="in">   E ← 0                     // 累计误差</span></span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a><span class="in">   Q ← empty                 // 量化结果</span></span>
<span id="cb16-408"><a href="#cb16-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-409"><a href="#cb16-409" aria-hidden="true" tabindex="-1"></a><span class="in">3. for i = 0 to d_col - 1 in steps of B:</span></span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a><span class="in">      // 当前块的列索引</span></span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a><span class="in">      cols ← [i, i+1, ..., min(i+B-1, d_col-1)]</span></span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a><span class="in">      // 量化当前块</span></span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a><span class="in">      for j in cols:</span></span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a><span class="in">         q_j ← quantize(W[:, j], b)   // 标准量化</span></span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a><span class="in">         Q[:, j] ← q_j</span></span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a><span class="in">         err ← (W[:, j] - q_j) / H[j, j]</span></span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a><span class="in">         // 更新后续列（误差补偿）</span></span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a><span class="in">         W[:, j+1:] ← W[:, j+1:] - err ⊗ H[j, j+1:]</span></span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a><span class="in">4. return Q</span></span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a>**关键设计**：</span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>逐列量化 + 误差补偿，利用 Hessian 信息指导更新</span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>分块处理，平衡精度和效率</span>
<span id="cb16-428"><a href="#cb16-428" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>时间复杂度 $O(d^2 n)$，约 4 小时可量化 175B 模型</span>
<span id="cb16-429"><a href="#cb16-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a>*Source: Frantar, E. et al. (2022). "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers". ICLR 2023. [arXiv:2210.17323](https://arxiv.org/abs/2210.17323)*</span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-433"><a href="#cb16-433" aria-hidden="true" tabindex="-1"></a>GPTQ 的实验结果令人印象深刻：4-bit 量化后，175B 参数的 OPT-175B 模型的困惑度（perplexity）仅上升 0.5，几乎无损。这使得在单张 A100 上运行 175B 模型成为可能。</span>
<span id="cb16-434"><a href="#cb16-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a><span class="fu">#### AWQ：激活感知的权重重要性</span></span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a>AWQ（Lin et al., 2023）提出了一个简洁而深刻的观察：**不是所有权重都同等重要**。</span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a><span class="al">![AWQ的核心思想：保护重要的权重通道](figures/chapter-31/original/fig-awq-overview.png)</span>{#fig-awq-overview width=90%}</span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a>*Source: Lin et al. (2023) "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration", Figure 1. MLSys 2024 Best Paper.*</span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a>如上图所示，AWQ 的关键洞察是：权重的重要性应该由**激活值**来衡量，而非权重本身的大小。具体来说，如果某个输入通道的激活值很大，那么对应的权重列就很重要——即使这些权重本身数值不大，它们乘以大激活后对输出影响很大。</span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a>AWQ 的策略是：</span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>用校准数据统计每个输入通道的平均激活幅度</span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>对于重要通道（激活大的），放大权重后再量化</span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>放大系数存下来，推理时对输入做相应的缩小（数学等价变换）</span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a>数学上，原始计算是 $Y = XW$。AWQ 引入对角缩放矩阵 $S$：</span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a>Y = XW = (XS^{-1})(SW)</span>
<span id="cb16-457"><a href="#cb16-457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-458"><a href="#cb16-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a>通过选择合适的 $S$，使 $SW$ 更容易量化。具体地，对于重要通道 $i$，设置 $S_{ii} &gt; 1$，这会放大 $W$ 中对应的列，使其在量化时保留更高的精度。</span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a>AWQ 在 MLSys 2024 获得最佳论文奖，其简洁的设计和出色的效果代表了量化技术的前沿水平。</span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 量化技术对比</span></span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 位宽 <span class="pp">|</span> 核心思想 <span class="pp">|</span> 校准数据 <span class="pp">|</span> 速度 <span class="pp">|</span> 精度损失 <span class="pp">|</span></span>
<span id="cb16-466"><a href="#cb16-466" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|---------|---------|------|---------|</span></span>
<span id="cb16-467"><a href="#cb16-467" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LLM.int8() <span class="pp">|</span> W8A8 <span class="pp">|</span> 混合精度分解 <span class="pp">|</span> 无 <span class="pp">|</span> +50% <span class="pp">|</span> ~0 <span class="pp">|</span></span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SmoothQuant <span class="pp">|</span> W8A8 <span class="pp">|</span> 平滑激活分布 <span class="pp">|</span> ~1000 样本 <span class="pp">|</span> +80% <span class="pp">|</span> ~0 <span class="pp">|</span></span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPTQ <span class="pp">|</span> W4A16 <span class="pp">|</span> Hessian 最优量化 <span class="pp">|</span> ~128 样本 <span class="pp">|</span> +200% <span class="pp">|</span> 0.1-0.5 <span class="pp">|</span></span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> AWQ <span class="pp">|</span> W4A16 <span class="pp">|</span> 激活感知重要性 <span class="pp">|</span> ~128 样本 <span class="pp">|</span> +200% <span class="pp">|</span> 0.1-0.3 <span class="pp">|</span></span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-472"><a href="#cb16-472" aria-hidden="true" tabindex="-1"></a><span class="fu">### 投机解码（Speculative Decoding）</span></span>
<span id="cb16-473"><a href="#cb16-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 基本原理</span></span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a>投机解码的核心思想惊人地简单：用一个快速但不那么准确的小模型（draft model）生成候选 token 序列，然后用目标大模型（target model）**并行验证**这些候选。</span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a>传统自回归生成：</span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a><span class="in">Token 1 → Token 2 → Token 3 → Token 4</span></span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a><span class="in">   70ms      70ms      70ms      70ms</span></span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a><span class="in">Total: 280ms</span></span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a>投机解码：</span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a><span class="in">Draft: Token 1,2,3,4 (并行生成)  →  Target: 验证全部 (一次前向)</span></span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a><span class="in">           28ms                          70ms</span></span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a><span class="in">Total: ~100ms (假设全部接受)</span></span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a>关键问题是：如果 draft 的预测被拒绝怎么办？投机解码使用一种精心设计的**接受-拒绝采样**机制，保证最终输出分布与直接使用 target model 完全相同。</span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数学原理：为什么能保证无损？</span></span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a>设 $p(x)$ 是 target model 在某位置的概率分布，$q(x)$ 是 draft model 的分布。投机解码的接受规则是：</span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>\text{Accept } x \text{ with probability } \min\left(1, \frac{p(x)}{q(x)}\right)</span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a>如果 $x$ 被接受，输出就是 $x$。如果被拒绝，从修正分布 $p'(x)$ 中重新采样：</span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a>p'(x) = \text{normalize}\left(\max(0, p(x) - q(x))\right)</span>
<span id="cb16-506"><a href="#cb16-506" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-507"><a href="#cb16-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a>**定理**：在上述接受-拒绝规则下，最终输出的分布恰好是 $p(x)$。</span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a>**证明思路**：</span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a>输出 $x$ 的总概率 = (由 draft 生成 $x$ 的概率) × (接受 $x$ 的概率) + (从修正分布采样到 $x$ 的概率)</span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a>P(\text{output}=x) = q(x) \cdot \min\left(1, \frac{p(x)}{q(x)}\right) + P(\text{reject}) \cdot p'(x)</span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a>经过仔细计算，可以验证这确实等于 $p(x)$。关键洞察是：当 $q(x) &gt; p(x)$ 时，我们"过度采样"了 $x$，需要通过拒绝来修正；当 $q(x) &lt; p(x)$ 时，我们"欠采样"了 $x$，通过修正分布来补充。</span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a>这个结果非常优雅：它意味着投机解码在数学上是**无损**的，不会改变输出质量，只会提高速度。</span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：Draft-Verify 流程</span></span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a>让我们通过一个具体例子，完整走过投机解码的一轮验证。</span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Target model：70B 参数</span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Draft model：7B 参数（同系列的小模型）</span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>猜测长度 $K = 4$</span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>当前已生成的前缀："The weather today is"</span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a>**Step 1: Draft 生成**</span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a>Draft model 并行生成 4 个候选 token 及其概率：</span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-535"><a href="#cb16-535" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 位置 <span class="pp">|</span> Token <span class="pp">|</span> Draft 概率 $q(x)$ <span class="pp">|</span></span>
<span id="cb16-536"><a href="#cb16-536" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------|-------------------|</span></span>
<span id="cb16-537"><a href="#cb16-537" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1 <span class="pp">|</span> "very" <span class="pp">|</span> 0.60 <span class="pp">|</span></span>
<span id="cb16-538"><a href="#cb16-538" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2 <span class="pp">|</span> "nice" <span class="pp">|</span> 0.55 <span class="pp">|</span></span>
<span id="cb16-539"><a href="#cb16-539" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 3 <span class="pp">|</span> "and" <span class="pp">|</span> 0.40 <span class="pp">|</span></span>
<span id="cb16-540"><a href="#cb16-540" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 4 <span class="pp">|</span> "warm" <span class="pp">|</span> 0.35 <span class="pp">|</span></span>
<span id="cb16-541"><a href="#cb16-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-542"><a href="#cb16-542" aria-hidden="true" tabindex="-1"></a>**Step 2: Target 验证（一次前向传播）**</span>
<span id="cb16-543"><a href="#cb16-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-544"><a href="#cb16-544" aria-hidden="true" tabindex="-1"></a>Target model 对序列 "The weather today is very nice and warm" 计算条件概率。这是**单次**前向传播，但输出每个位置的概率分布：</span>
<span id="cb16-545"><a href="#cb16-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-546"><a href="#cb16-546" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 位置 <span class="pp">|</span> Token <span class="pp">|</span> Target 概率 $p(x)$ <span class="pp">|</span> Draft 概率 $q(x)$ <span class="pp">|</span></span>
<span id="cb16-547"><a href="#cb16-547" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------|-------------------|-------------------|</span></span>
<span id="cb16-548"><a href="#cb16-548" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1 <span class="pp">|</span> "very" <span class="pp">|</span> 0.70 <span class="pp">|</span> 0.60 <span class="pp">|</span></span>
<span id="cb16-549"><a href="#cb16-549" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2 <span class="pp">|</span> "nice" <span class="pp">|</span> 0.45 <span class="pp">|</span> 0.55 <span class="pp">|</span></span>
<span id="cb16-550"><a href="#cb16-550" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 3 <span class="pp">|</span> "and" <span class="pp">|</span> 0.50 <span class="pp">|</span> 0.40 <span class="pp">|</span></span>
<span id="cb16-551"><a href="#cb16-551" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 4 <span class="pp">|</span> "warm" <span class="pp">|</span> 0.30 <span class="pp">|</span> 0.35 <span class="pp">|</span></span>
<span id="cb16-552"><a href="#cb16-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-553"><a href="#cb16-553" aria-hidden="true" tabindex="-1"></a>**Step 3: 逐位置接受/拒绝**</span>
<span id="cb16-554"><a href="#cb16-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-555"><a href="#cb16-555" aria-hidden="true" tabindex="-1"></a>对每个位置，计算接受概率 $\min(1, p/q)$ 并掷骰子：</span>
<span id="cb16-556"><a href="#cb16-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-557"><a href="#cb16-557" aria-hidden="true" tabindex="-1"></a>**位置 1 ("very")**：</span>
<span id="cb16-558"><a href="#cb16-558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-559"><a href="#cb16-559" aria-hidden="true" tabindex="-1"></a>P(\text{accept}) = \min\left(1, \frac{0.70}{0.60}\right) = \min(1, 1.17) = 1.0</span>
<span id="cb16-560"><a href="#cb16-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-561"><a href="#cb16-561" aria-hidden="true" tabindex="-1"></a>因为 $p &gt; q$，target 比 draft 更喜欢这个 token，一定接受。✅</span>
<span id="cb16-562"><a href="#cb16-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-563"><a href="#cb16-563" aria-hidden="true" tabindex="-1"></a>**位置 2 ("nice")**：</span>
<span id="cb16-564"><a href="#cb16-564" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-565"><a href="#cb16-565" aria-hidden="true" tabindex="-1"></a>P(\text{accept}) = \min\left(1, \frac{0.45}{0.55}\right) = 0.82</span>
<span id="cb16-566"><a href="#cb16-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-567"><a href="#cb16-567" aria-hidden="true" tabindex="-1"></a>掷骰子得到 0.65 &lt; 0.82，接受。✅</span>
<span id="cb16-568"><a href="#cb16-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-569"><a href="#cb16-569" aria-hidden="true" tabindex="-1"></a>**位置 3 ("and")**：</span>
<span id="cb16-570"><a href="#cb16-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-571"><a href="#cb16-571" aria-hidden="true" tabindex="-1"></a>P(\text{accept}) = \min\left(1, \frac{0.50}{0.40}\right) = 1.0</span>
<span id="cb16-572"><a href="#cb16-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-573"><a href="#cb16-573" aria-hidden="true" tabindex="-1"></a>一定接受。✅</span>
<span id="cb16-574"><a href="#cb16-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-575"><a href="#cb16-575" aria-hidden="true" tabindex="-1"></a>**位置 4 ("warm")**：</span>
<span id="cb16-576"><a href="#cb16-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-577"><a href="#cb16-577" aria-hidden="true" tabindex="-1"></a>P(\text{accept}) = \min\left(1, \frac{0.30}{0.35}\right) = 0.86</span>
<span id="cb16-578"><a href="#cb16-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-579"><a href="#cb16-579" aria-hidden="true" tabindex="-1"></a>掷骰子得到 0.91 &gt; 0.86，拒绝。❌</span>
<span id="cb16-580"><a href="#cb16-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-581"><a href="#cb16-581" aria-hidden="true" tabindex="-1"></a>**Step 4: 处理拒绝**</span>
<span id="cb16-582"><a href="#cb16-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-583"><a href="#cb16-583" aria-hidden="true" tabindex="-1"></a>位置 4 被拒绝，需要从修正分布 $p'$ 中重新采样。假设 target 在位置 4 的完整分布是：</span>
<span id="cb16-584"><a href="#cb16-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-585"><a href="#cb16-585" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Token <span class="pp">|</span> $p(x)$ <span class="pp">|</span> $q(x)$ <span class="pp">|</span> $\max(0, p-q)$ <span class="pp">|</span></span>
<span id="cb16-586"><a href="#cb16-586" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------|--------|--------|----------------|</span></span>
<span id="cb16-587"><a href="#cb16-587" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "warm" <span class="pp">|</span> 0.30 <span class="pp">|</span> 0.35 <span class="pp">|</span> 0.00 <span class="pp">|</span></span>
<span id="cb16-588"><a href="#cb16-588" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "sunny" <span class="pp">|</span> 0.25 <span class="pp">|</span> 0.20 <span class="pp">|</span> 0.05 <span class="pp">|</span></span>
<span id="cb16-589"><a href="#cb16-589" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "pleasant" <span class="pp">|</span> 0.20 <span class="pp">|</span> 0.15 <span class="pp">|</span> 0.05 <span class="pp">|</span></span>
<span id="cb16-590"><a href="#cb16-590" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 其他 <span class="pp">|</span> 0.25 <span class="pp">|</span> 0.30 <span class="pp">|</span> 0.00 <span class="pp">|</span></span>
<span id="cb16-591"><a href="#cb16-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-592"><a href="#cb16-592" aria-hidden="true" tabindex="-1"></a>归一化后 $p'$：$P(\text{sunny}) = P(\text{pleasant}) = 0.5$</span>
<span id="cb16-593"><a href="#cb16-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-594"><a href="#cb16-594" aria-hidden="true" tabindex="-1"></a>假设采样到 "sunny"，最终输出是：**"very nice and sunny"**</span>
<span id="cb16-595"><a href="#cb16-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-596"><a href="#cb16-596" aria-hidden="true" tabindex="-1"></a>**Step 5: 效率分析**</span>
<span id="cb16-597"><a href="#cb16-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-598"><a href="#cb16-598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**无投机解码**：生成 4 个 token 需要 4 次 target 前向，约 $4 \times 70 = 280$ ms</span>
<span id="cb16-599"><a href="#cb16-599" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**有投机解码**：1 次 draft 前向 + 1 次 target 前向，约 $7 + 70 = 77$ ms</span>
<span id="cb16-600"><a href="#cb16-600" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**实际接受**：3 个 token + 1 个重采样 = 4 个 token</span>
<span id="cb16-601"><a href="#cb16-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**加速比**：$280 / 77 \approx 3.6\times$</span>
<span id="cb16-602"><a href="#cb16-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-603"><a href="#cb16-603" aria-hidden="true" tabindex="-1"></a>实际场景中，由于 draft 和 target 的分布差异，接受率通常在 60–80%，整体加速比约 2–3 倍。</span>
<span id="cb16-604"><a href="#cb16-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-605"><a href="#cb16-605" aria-hidden="true" tabindex="-1"></a><span class="fu">### 持续批处理（Continuous Batching）</span></span>
<span id="cb16-606"><a href="#cb16-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-607"><a href="#cb16-607" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 传统批处理的问题</span></span>
<span id="cb16-608"><a href="#cb16-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-609"><a href="#cb16-609" aria-hidden="true" tabindex="-1"></a>传统的静态批处理（static batching）要求所有请求同时开始、同时结束：</span>
<span id="cb16-610"><a href="#cb16-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-611"><a href="#cb16-611" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-612"><a href="#cb16-612" aria-hidden="true" tabindex="-1"></a><span class="in">请求 A: [生成 10 个 token]  ████░░░░░░░░░░░░░░░░░░░░░░░░░░░░</span></span>
<span id="cb16-613"><a href="#cb16-613" aria-hidden="true" tabindex="-1"></a><span class="in">请求 B: [生成 50 个 token]  ████████████████████████████████</span></span>
<span id="cb16-614"><a href="#cb16-614" aria-hidden="true" tabindex="-1"></a><span class="in">请求 C: [生成 20 个 token]  ██████████░░░░░░░░░░░░░░░░░░░░░░</span></span>
<span id="cb16-615"><a href="#cb16-615" aria-hidden="true" tabindex="-1"></a><span class="in">                            ↑                              ↑</span></span>
<span id="cb16-616"><a href="#cb16-616" aria-hidden="true" tabindex="-1"></a><span class="in">                          开始                            结束</span></span>
<span id="cb16-617"><a href="#cb16-617" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-618"><a href="#cb16-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-619"><a href="#cb16-619" aria-hidden="true" tabindex="-1"></a>请求 A 和 C 早早完成，却要等待请求 B，白白占用 GPU 资源。对于长度分布差异大的场景（这在实际应用中很常见），静态批处理的 GPU 利用率可能不到 30%。</span>
<span id="cb16-620"><a href="#cb16-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-621"><a href="#cb16-621" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Orca 的迭代级调度</span></span>
<span id="cb16-622"><a href="#cb16-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-623"><a href="#cb16-623" aria-hidden="true" tabindex="-1"></a>Orca（Yu et al., OSDI 2022）提出了**迭代级调度**（iteration-level scheduling）：在每个 iteration（生成一个 token）的粒度上做调度决策。</span>
<span id="cb16-624"><a href="#cb16-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-625"><a href="#cb16-625" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-626"><a href="#cb16-626" aria-hidden="true" tabindex="-1"></a><span class="in">请求 A: ████          （10 个 token 后退出，资源释放）</span></span>
<span id="cb16-627"><a href="#cb16-627" aria-hidden="true" tabindex="-1"></a><span class="in">请求 B: ████████████████████████████████</span></span>
<span id="cb16-628"><a href="#cb16-628" aria-hidden="true" tabindex="-1"></a><span class="in">请求 C: ██████████    （20 个 token 后退出）</span></span>
<span id="cb16-629"><a href="#cb16-629" aria-hidden="true" tabindex="-1"></a><span class="in">请求 D:      ████████████████████████    （A 退出后加入）</span></span>
<span id="cb16-630"><a href="#cb16-630" aria-hidden="true" tabindex="-1"></a><span class="in">请求 E:            ████████████████      （C 退出后加入）</span></span>
<span id="cb16-631"><a href="#cb16-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-632"><a href="#cb16-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-633"><a href="#cb16-633" aria-hidden="true" tabindex="-1"></a>这样，短请求不用"陪跑"长请求，系统吞吐量大幅提升。Orca 论文报告了 **36.9 倍**的吞吐提升（与 FasterTransformer 相比，在 GPT-3 175B 上）。</span>
<span id="cb16-634"><a href="#cb16-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-635"><a href="#cb16-635" aria-hidden="true" tabindex="-1"></a><span class="fu">#### PagedAttention 与 vLLM</span></span>
<span id="cb16-636"><a href="#cb16-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-637"><a href="#cb16-637" aria-hidden="true" tabindex="-1"></a>vLLM（Kwon et al., SOSP 2023）在 Orca 的基础上进一步优化了 KV Cache 管理，提出了 **PagedAttention**。</span>
<span id="cb16-638"><a href="#cb16-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-639"><a href="#cb16-639" aria-hidden="true" tabindex="-1"></a>传统方法为每个请求预分配最大长度的 KV Cache，导致严重浪费：</span>
<span id="cb16-640"><a href="#cb16-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-641"><a href="#cb16-641" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-642"><a href="#cb16-642" aria-hidden="true" tabindex="-1"></a><span class="in">请求 A (实际 100 token): [████████░░░░░░░░░░░░░░░░░░░░░░░░] 预留 1000 token</span></span>
<span id="cb16-643"><a href="#cb16-643" aria-hidden="true" tabindex="-1"></a><span class="in">请求 B (实际 500 token): [████████████████████░░░░░░░░░░░░] 预留 1000 token</span></span>
<span id="cb16-644"><a href="#cb16-644" aria-hidden="true" tabindex="-1"></a><span class="in">                          ↑ 实际使用        ↑ 浪费</span></span>
<span id="cb16-645"><a href="#cb16-645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-646"><a href="#cb16-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-647"><a href="#cb16-647" aria-hidden="true" tabindex="-1"></a>PagedAttention 将 KV Cache 切分成固定大小的页（如 16 tokens 一页），按需分配：</span>
<span id="cb16-648"><a href="#cb16-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-649"><a href="#cb16-649" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-650"><a href="#cb16-650" aria-hidden="true" tabindex="-1"></a><span class="in">请求 A: [Page 1][Page 2][Page 3][Page 4][Page 5][Page 6][Page 7]  (7 页, 实际 100 token)</span></span>
<span id="cb16-651"><a href="#cb16-651" aria-hidden="true" tabindex="-1"></a><span class="in">请求 B: [Page 8][Page 9]...[Page 39]  (32 页, 实际 500 token)</span></span>
<span id="cb16-652"><a href="#cb16-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-653"><a href="#cb16-653" aria-hidden="true" tabindex="-1"></a><span class="in">物理内存: |Page 1|Page 8|Page 2|Page 9|Page 3|... (交错存储，无碎片)</span></span>
<span id="cb16-654"><a href="#cb16-654" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-655"><a href="#cb16-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-656"><a href="#cb16-656" aria-hidden="true" tabindex="-1"></a>额外好处：当多个请求共享相同的 prompt（如系统指令），它们的 KV Cache 可以共享：</span>
<span id="cb16-657"><a href="#cb16-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-658"><a href="#cb16-658" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-659"><a href="#cb16-659" aria-hidden="true" tabindex="-1"></a><span class="in">请求 A: [系统提示(共享)][用户问题 A 的 KV]</span></span>
<span id="cb16-660"><a href="#cb16-660" aria-hidden="true" tabindex="-1"></a><span class="in">请求 B: [系统提示(共享)][用户问题 B 的 KV]</span></span>
<span id="cb16-661"><a href="#cb16-661" aria-hidden="true" tabindex="-1"></a><span class="in">                  ↑</span></span>
<span id="cb16-662"><a href="#cb16-662" aria-hidden="true" tabindex="-1"></a><span class="in">            物理内存只存一份</span></span>
<span id="cb16-663"><a href="#cb16-663" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-664"><a href="#cb16-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-665"><a href="#cb16-665" aria-hidden="true" tabindex="-1"></a>vLLM 结合 PagedAttention 和持续批处理，相比 HuggingFace Transformers 和 FasterTransformer 实现了 **2–4 倍**的吞吐提升。</span>
<span id="cb16-666"><a href="#cb16-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-667"><a href="#cb16-667" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型并行推理</span></span>
<span id="cb16-668"><a href="#cb16-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-669"><a href="#cb16-669" aria-hidden="true" tabindex="-1"></a>当单卡显存不足以容纳模型时，需要将模型分布到多个 GPU 上。这与第19章讨论的分布式训练类似，但推理场景有其特殊性。</span>
<span id="cb16-670"><a href="#cb16-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-671"><a href="#cb16-671" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Tensor Parallelism（TP）</span></span>
<span id="cb16-672"><a href="#cb16-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-673"><a href="#cb16-673" aria-hidden="true" tabindex="-1"></a>将单层的矩阵计算切分到多个 GPU：</span>
<span id="cb16-674"><a href="#cb16-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-675"><a href="#cb16-675" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-676"><a href="#cb16-676" aria-hidden="true" tabindex="-1"></a><span class="in">GPU 0: W[:, :d/2]   →  计算前半部分输出</span></span>
<span id="cb16-677"><a href="#cb16-677" aria-hidden="true" tabindex="-1"></a><span class="in">GPU 1: W[:, d/2:]   →  计算后半部分输出</span></span>
<span id="cb16-678"><a href="#cb16-678" aria-hidden="true" tabindex="-1"></a><span class="in">        ↓ AllReduce ↓</span></span>
<span id="cb16-679"><a href="#cb16-679" aria-hidden="true" tabindex="-1"></a><span class="in">      合并得到完整输出</span></span>
<span id="cb16-680"><a href="#cb16-680" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-681"><a href="#cb16-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-682"><a href="#cb16-682" aria-hidden="true" tabindex="-1"></a>优点：每层内并行，延迟低</span>
<span id="cb16-683"><a href="#cb16-683" aria-hidden="true" tabindex="-1"></a>缺点：每层都需要 AllReduce 通信</span>
<span id="cb16-684"><a href="#cb16-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-685"><a href="#cb16-685" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pipeline Parallelism（PP）</span></span>
<span id="cb16-686"><a href="#cb16-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-687"><a href="#cb16-687" aria-hidden="true" tabindex="-1"></a>将不同层放到不同 GPU：</span>
<span id="cb16-688"><a href="#cb16-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-689"><a href="#cb16-689" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-690"><a href="#cb16-690" aria-hidden="true" tabindex="-1"></a><span class="in">GPU 0: Layer 0-9    →  处理前 10 层</span></span>
<span id="cb16-691"><a href="#cb16-691" aria-hidden="true" tabindex="-1"></a><span class="in">GPU 1: Layer 10-19  →  处理中间 10 层</span></span>
<span id="cb16-692"><a href="#cb16-692" aria-hidden="true" tabindex="-1"></a><span class="in">GPU 2: Layer 20-29  →  处理后 10 层</span></span>
<span id="cb16-693"><a href="#cb16-693" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-694"><a href="#cb16-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-695"><a href="#cb16-695" aria-hidden="true" tabindex="-1"></a>优点：通信量小（只在层边界传递激活）</span>
<span id="cb16-696"><a href="#cb16-696" aria-hidden="true" tabindex="-1"></a>缺点：存在流水线气泡，增加延迟</span>
<span id="cb16-697"><a href="#cb16-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-698"><a href="#cb16-698" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 推理场景的选择</span></span>
<span id="cb16-699"><a href="#cb16-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-700"><a href="#cb16-700" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 场景 <span class="pp">|</span> 推荐策略 <span class="pp">|</span> 原因 <span class="pp">|</span></span>
<span id="cb16-701"><a href="#cb16-701" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|------|</span></span>
<span id="cb16-702"><a href="#cb16-702" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 低延迟（实时对话） <span class="pp">|</span> 纯 TP <span class="pp">|</span> 延迟最低 <span class="pp">|</span></span>
<span id="cb16-703"><a href="#cb16-703" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 高吞吐（批处理） <span class="pp">|</span> TP + PP <span class="pp">|</span> 最大化利用率 <span class="pp">|</span></span>
<span id="cb16-704"><a href="#cb16-704" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 显存极度受限 <span class="pp">|</span> PP + 模型卸载 <span class="pp">|</span> 允许更大模型 <span class="pp">|</span></span>
<span id="cb16-705"><a href="#cb16-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-706"><a href="#cb16-706" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-707"><a href="#cb16-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-708"><a href="#cb16-708" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb16-709"><a href="#cb16-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-710"><a href="#cb16-710" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用 GPTQ 量化模型</span></span>
<span id="cb16-711"><a href="#cb16-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-714"><a href="#cb16-714" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-715"><a href="#cb16-715" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb16-716"><a href="#cb16-716" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb16-717"><a href="#cb16-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-718"><a href="#cb16-718" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb16-719"><a href="#cb16-719" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> auto_gptq <span class="im">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig</span>
<span id="cb16-720"><a href="#cb16-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-721"><a href="#cb16-721" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载原始模型</span></span>
<span id="cb16-722"><a href="#cb16-722" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-2-7b-hf"</span></span>
<span id="cb16-723"><a href="#cb16-723" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb16-724"><a href="#cb16-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-725"><a href="#cb16-725" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置量化参数</span></span>
<span id="cb16-726"><a href="#cb16-726" aria-hidden="true" tabindex="-1"></a>quantize_config <span class="op">=</span> BaseQuantizeConfig(</span>
<span id="cb16-727"><a href="#cb16-727" aria-hidden="true" tabindex="-1"></a>    bits<span class="op">=</span><span class="dv">4</span>,                    <span class="co"># 4-bit 量化</span></span>
<span id="cb16-728"><a href="#cb16-728" aria-hidden="true" tabindex="-1"></a>    group_size<span class="op">=</span><span class="dv">128</span>,            <span class="co"># 每 128 个权重共享量化参数</span></span>
<span id="cb16-729"><a href="#cb16-729" aria-hidden="true" tabindex="-1"></a>    desc_act<span class="op">=</span><span class="va">True</span>,             <span class="co"># 激活感知排序（提升精度）</span></span>
<span id="cb16-730"><a href="#cb16-730" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-731"><a href="#cb16-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-732"><a href="#cb16-732" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型并量化</span></span>
<span id="cb16-733"><a href="#cb16-733" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoGPTQForCausalLM.from_pretrained(</span>
<span id="cb16-734"><a href="#cb16-734" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb16-735"><a href="#cb16-735" aria-hidden="true" tabindex="-1"></a>    quantize_config<span class="op">=</span>quantize_config,</span>
<span id="cb16-736"><a href="#cb16-736" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb16-737"><a href="#cb16-737" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-738"><a href="#cb16-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-739"><a href="#cb16-739" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备校准数据</span></span>
<span id="cb16-740"><a href="#cb16-740" aria-hidden="true" tabindex="-1"></a>calibration_data <span class="op">=</span> [</span>
<span id="cb16-741"><a href="#cb16-741" aria-hidden="true" tabindex="-1"></a>    tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb16-742"><a href="#cb16-742" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text <span class="kw">in</span> [<span class="st">"校准文本1..."</span>, <span class="st">"校准文本2..."</span>, ...]</span>
<span id="cb16-743"><a href="#cb16-743" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-744"><a href="#cb16-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-745"><a href="#cb16-745" aria-hidden="true" tabindex="-1"></a><span class="co"># 执行量化</span></span>
<span id="cb16-746"><a href="#cb16-746" aria-hidden="true" tabindex="-1"></a>model.quantize(calibration_data)</span>
<span id="cb16-747"><a href="#cb16-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-748"><a href="#cb16-748" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存量化后的模型</span></span>
<span id="cb16-749"><a href="#cb16-749" aria-hidden="true" tabindex="-1"></a>model.save_quantized(<span class="st">"llama-2-7b-gptq-4bit"</span>)</span>
<span id="cb16-750"><a href="#cb16-750" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-751"><a href="#cb16-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-752"><a href="#cb16-752" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用 AWQ 量化模型</span></span>
<span id="cb16-753"><a href="#cb16-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-756"><a href="#cb16-756" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-757"><a href="#cb16-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb16-758"><a href="#cb16-758" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb16-759"><a href="#cb16-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-760"><a href="#cb16-760" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> awq <span class="im">import</span> AutoAWQForCausalLM</span>
<span id="cb16-761"><a href="#cb16-761" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb16-762"><a href="#cb16-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-763"><a href="#cb16-763" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb16-764"><a href="#cb16-764" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-2-7b-hf"</span></span>
<span id="cb16-765"><a href="#cb16-765" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoAWQForCausalLM.from_pretrained(</span>
<span id="cb16-766"><a href="#cb16-766" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb16-767"><a href="#cb16-767" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb16-768"><a href="#cb16-768" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-769"><a href="#cb16-769" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb16-770"><a href="#cb16-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-771"><a href="#cb16-771" aria-hidden="true" tabindex="-1"></a><span class="co"># AWQ 量化配置</span></span>
<span id="cb16-772"><a href="#cb16-772" aria-hidden="true" tabindex="-1"></a>quant_config <span class="op">=</span> {</span>
<span id="cb16-773"><a href="#cb16-773" aria-hidden="true" tabindex="-1"></a>    <span class="st">"zero_point"</span>: <span class="va">True</span>,        <span class="co"># 使用零点</span></span>
<span id="cb16-774"><a href="#cb16-774" aria-hidden="true" tabindex="-1"></a>    <span class="st">"q_group_size"</span>: <span class="dv">128</span>,       <span class="co"># 分组大小</span></span>
<span id="cb16-775"><a href="#cb16-775" aria-hidden="true" tabindex="-1"></a>    <span class="st">"w_bit"</span>: <span class="dv">4</span>,                <span class="co"># 4-bit 量化</span></span>
<span id="cb16-776"><a href="#cb16-776" aria-hidden="true" tabindex="-1"></a>    <span class="st">"version"</span>: <span class="st">"GEMM"</span>          <span class="co"># 优化版本</span></span>
<span id="cb16-777"><a href="#cb16-777" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-778"><a href="#cb16-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-779"><a href="#cb16-779" aria-hidden="true" tabindex="-1"></a><span class="co"># 执行量化</span></span>
<span id="cb16-780"><a href="#cb16-780" aria-hidden="true" tabindex="-1"></a>model.quantize(tokenizer, quant_config<span class="op">=</span>quant_config)</span>
<span id="cb16-781"><a href="#cb16-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-782"><a href="#cb16-782" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存</span></span>
<span id="cb16-783"><a href="#cb16-783" aria-hidden="true" tabindex="-1"></a>model.save_quantized(<span class="st">"llama-2-7b-awq-4bit"</span>)</span>
<span id="cb16-784"><a href="#cb16-784" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-785"><a href="#cb16-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-786"><a href="#cb16-786" aria-hidden="true" tabindex="-1"></a><span class="fu">### vLLM 部署示例</span></span>
<span id="cb16-787"><a href="#cb16-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-790"><a href="#cb16-790" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-791"><a href="#cb16-791" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb16-792"><a href="#cb16-792" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb16-793"><a href="#cb16-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-794"><a href="#cb16-794" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm <span class="im">import</span> LLM, SamplingParams</span>
<span id="cb16-795"><a href="#cb16-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-796"><a href="#cb16-796" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型（自动启用 PagedAttention 和持续批处理）</span></span>
<span id="cb16-797"><a href="#cb16-797" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(</span>
<span id="cb16-798"><a href="#cb16-798" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"meta-llama/Llama-2-7b-chat-hf"</span>,</span>
<span id="cb16-799"><a href="#cb16-799" aria-hidden="true" tabindex="-1"></a>    tensor_parallel_size<span class="op">=</span><span class="dv">1</span>,        <span class="co"># 单卡</span></span>
<span id="cb16-800"><a href="#cb16-800" aria-hidden="true" tabindex="-1"></a>    gpu_memory_utilization<span class="op">=</span><span class="fl">0.90</span>,   <span class="co"># GPU 内存利用率</span></span>
<span id="cb16-801"><a href="#cb16-801" aria-hidden="true" tabindex="-1"></a>    max_model_len<span class="op">=</span><span class="dv">4096</span>,            <span class="co"># 最大序列长度</span></span>
<span id="cb16-802"><a href="#cb16-802" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-803"><a href="#cb16-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-804"><a href="#cb16-804" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置采样参数</span></span>
<span id="cb16-805"><a href="#cb16-805" aria-hidden="true" tabindex="-1"></a>sampling_params <span class="op">=</span> SamplingParams(</span>
<span id="cb16-806"><a href="#cb16-806" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb16-807"><a href="#cb16-807" aria-hidden="true" tabindex="-1"></a>    top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb16-808"><a href="#cb16-808" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb16-809"><a href="#cb16-809" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-810"><a href="#cb16-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-811"><a href="#cb16-811" aria-hidden="true" tabindex="-1"></a><span class="co"># 批量推理</span></span>
<span id="cb16-812"><a href="#cb16-812" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [</span>
<span id="cb16-813"><a href="#cb16-813" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is the capital of France?"</span>,</span>
<span id="cb16-814"><a href="#cb16-814" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Explain quantum computing in simple terms."</span>,</span>
<span id="cb16-815"><a href="#cb16-815" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Write a haiku about programming."</span>,</span>
<span id="cb16-816"><a href="#cb16-816" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-817"><a href="#cb16-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-818"><a href="#cb16-818" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm.generate(prompts, sampling_params)</span>
<span id="cb16-819"><a href="#cb16-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-820"><a href="#cb16-820" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> output <span class="kw">in</span> outputs:</span>
<span id="cb16-821"><a href="#cb16-821" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>output<span class="sc">.</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-822"><a href="#cb16-822" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Generated: </span><span class="sc">{</span>output<span class="sc">.</span>outputs[<span class="dv">0</span>]<span class="sc">.</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-823"><a href="#cb16-823" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb16-824"><a href="#cb16-824" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-825"><a href="#cb16-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-826"><a href="#cb16-826" aria-hidden="true" tabindex="-1"></a><span class="fu">### TGI 部署（Docker）</span></span>
<span id="cb16-827"><a href="#cb16-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-828"><a href="#cb16-828" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb16-829"><a href="#cb16-829" aria-hidden="true" tabindex="-1"></a><span class="co"># 拉取 TGI 镜像</span></span>
<span id="cb16-830"><a href="#cb16-830" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull ghcr.io/huggingface/text-generation-inference:latest</span>
<span id="cb16-831"><a href="#cb16-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-832"><a href="#cb16-832" aria-hidden="true" tabindex="-1"></a><span class="co"># 启动服务</span></span>
<span id="cb16-833"><a href="#cb16-833" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">--gpus</span> all <span class="at">--shm-size</span> 1g <span class="at">-p</span> 8080:80 <span class="dt">\</span></span>
<span id="cb16-834"><a href="#cb16-834" aria-hidden="true" tabindex="-1"></a>    <span class="at">-v</span> <span class="va">$HOME</span>/.cache/huggingface:/data <span class="dt">\</span></span>
<span id="cb16-835"><a href="#cb16-835" aria-hidden="true" tabindex="-1"></a>    ghcr.io/huggingface/text-generation-inference:latest <span class="dt">\</span></span>
<span id="cb16-836"><a href="#cb16-836" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="dt">\</span></span>
<span id="cb16-837"><a href="#cb16-837" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quantize</span> gptq <span class="dt">\</span></span>
<span id="cb16-838"><a href="#cb16-838" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-concurrent-requests</span> 128 <span class="dt">\</span></span>
<span id="cb16-839"><a href="#cb16-839" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-input-length</span> 2048 <span class="dt">\</span></span>
<span id="cb16-840"><a href="#cb16-840" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-total-tokens</span> 4096</span>
<span id="cb16-841"><a href="#cb16-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-842"><a href="#cb16-842" aria-hidden="true" tabindex="-1"></a><span class="co"># 调用 API</span></span>
<span id="cb16-843"><a href="#cb16-843" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> http://localhost:8080/generate <span class="dt">\</span></span>
<span id="cb16-844"><a href="#cb16-844" aria-hidden="true" tabindex="-1"></a>    <span class="at">-X</span> POST <span class="dt">\</span></span>
<span id="cb16-845"><a href="#cb16-845" aria-hidden="true" tabindex="-1"></a>    <span class="at">-H</span> <span class="st">"Content-Type: application/json"</span> <span class="dt">\</span></span>
<span id="cb16-846"><a href="#cb16-846" aria-hidden="true" tabindex="-1"></a>    <span class="at">-d</span> <span class="st">'{"inputs": "What is deep learning?", "parameters": {"max_new_tokens": 100}}'</span></span>
<span id="cb16-847"><a href="#cb16-847" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-848"><a href="#cb16-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-849"><a href="#cb16-849" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-850"><a href="#cb16-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-851"><a href="#cb16-851" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb16-852"><a href="#cb16-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-853"><a href="#cb16-853" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb16-854"><a href="#cb16-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-855"><a href="#cb16-855" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 量化的信息论基础</span></span>
<span id="cb16-856"><a href="#cb16-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-857"><a href="#cb16-857" aria-hidden="true" tabindex="-1"></a>从信息论角度，量化的目标是找到一个编码，使重构误差最小化。对于服从某种分布的权重，最优量化（Lloyd-Max 量化）的误差与分布的方差和量化级数相关。</span>
<span id="cb16-858"><a href="#cb16-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-859"><a href="#cb16-859" aria-hidden="true" tabindex="-1"></a>大模型权重的一个关键特性是其分布高度集中：绝大多数权重的绝对值小于某个阈值，呈近似正态分布。这意味着均匀量化（等间距划分）是次优的——大部分量化区间分配给了稀疏的尾部，而密集的中心区域反而精度不足。</span>
<span id="cb16-860"><a href="#cb16-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-861"><a href="#cb16-861" aria-hidden="true" tabindex="-1"></a>GPTQ 和 AWQ 的优越性部分来自于对这种分布特性的利用：通过非均匀的缩放（GPTQ 的 Hessian 加权，AWQ 的激活感知）更好地适应权重分布。</span>
<span id="cb16-862"><a href="#cb16-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-863"><a href="#cb16-863" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 投机解码的概率论基础</span></span>
<span id="cb16-864"><a href="#cb16-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-865"><a href="#cb16-865" aria-hidden="true" tabindex="-1"></a>投机解码的正确性基于**接受-拒绝采样**（acceptance-rejection sampling）的经典结果。这是蒙特卡洛方法的基础技术，数学上可以追溯到 von Neumann（1951）。</span>
<span id="cb16-866"><a href="#cb16-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-867"><a href="#cb16-867" aria-hidden="true" tabindex="-1"></a>核心定理是：如果提议分布 $q$ 满足 $q(x) &gt; 0$ 对所有 $p(x) &gt; 0$ 的 $x$ 成立，那么接受-拒绝采样可以精确采样任何目标分布 $p$。投机解码的贡献是将这一经典技术巧妙地应用于 LLM 推理加速。</span>
<span id="cb16-868"><a href="#cb16-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-869"><a href="#cb16-869" aria-hidden="true" tabindex="-1"></a><span class="fu">### 边界条件与失效模式</span></span>
<span id="cb16-870"><a href="#cb16-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-871"><a href="#cb16-871" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 量化的边界条件</span></span>
<span id="cb16-872"><a href="#cb16-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-873"><a href="#cb16-873" aria-hidden="true" tabindex="-1"></a>**任务敏感性**：某些任务对精度特别敏感。研究发现，数学推理任务在 4-bit 量化后性能下降明显（5–10%），而常规问答和文本生成任务几乎不受影响。</span>
<span id="cb16-874"><a href="#cb16-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-875"><a href="#cb16-875" aria-hidden="true" tabindex="-1"></a>**模型规模**：有趣的是，大模型通常比小模型更容易量化。70B 模型量化后的相对性能损失小于 7B 模型。一种解释是：大模型有更多的冗余，可以更好地"吸收"量化噪声。</span>
<span id="cb16-876"><a href="#cb16-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-877"><a href="#cb16-877" aria-hidden="true" tabindex="-1"></a>**校准数据**：GPTQ 和 AWQ 都需要少量校准数据（通常 128–512 样本）。校准数据的领域应该与实际应用匹配——用英文数据校准后处理中文可能效果不佳。</span>
<span id="cb16-878"><a href="#cb16-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-879"><a href="#cb16-879" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 投机解码的边界条件</span></span>
<span id="cb16-880"><a href="#cb16-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-881"><a href="#cb16-881" aria-hidden="true" tabindex="-1"></a>**Draft-Target 分布差异**：投机解码的加速比高度依赖于 draft 和 target 分布的相似度。当任务需要创意或探索性输出时（如创意写作），draft 的预测接受率会下降，加速比也随之降低。</span>
<span id="cb16-882"><a href="#cb16-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-883"><a href="#cb16-883" aria-hidden="true" tabindex="-1"></a>**Token 长度分布**：对于非常短的输出（&lt; 10 token），投机解码的额外开销可能抵消收益。最适合中等长度的生成任务。</span>
<span id="cb16-884"><a href="#cb16-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-885"><a href="#cb16-885" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb16-886"><a href="#cb16-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-887"><a href="#cb16-887" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**超低 bit 量化**：能否可靠地实现 2-bit 甚至 1-bit 量化？目前的研究表明 3-bit 是一个关键阈值，低于此会显著损失性能。突破这一限制需要新的量化技术或训练方法。</span>
<span id="cb16-888"><a href="#cb16-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-889"><a href="#cb16-889" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**自适应投机解码**：当前的投机解码使用固定的猜测长度 $K$。能否根据上下文动态调整 $K$？在 draft 信心高的位置猜更多，信心低的位置猜更少？</span>
<span id="cb16-890"><a href="#cb16-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-891"><a href="#cb16-891" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**端到端协同优化**：量化、投机解码、批处理目前是独立设计的。它们之间是否存在协同效应或冲突？能否设计一个统一的优化框架？</span>
<span id="cb16-892"><a href="#cb16-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-893"><a href="#cb16-893" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**边缘设备推理**：手机、边缘设备的算力和内存远小于数据中心 GPU。如何将上述技术适配到极端资源受限的场景？</span>
<span id="cb16-894"><a href="#cb16-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-895"><a href="#cb16-895" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-896"><a href="#cb16-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-897"><a href="#cb16-897" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb16-898"><a href="#cb16-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-899"><a href="#cb16-899" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本方法的局限</span></span>
<span id="cb16-900"><a href="#cb16-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-901"><a href="#cb16-901" aria-hidden="true" tabindex="-1"></a>**局限 1：速度-质量权衡不可避免**</span>
<span id="cb16-902"><a href="#cb16-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-903"><a href="#cb16-903" aria-hidden="true" tabindex="-1"></a>尽管量化技术不断进步，4-bit 量化仍然会在某些任务上造成可测量的性能损失。对于追求极致质量的场景（如医疗、法律领域的 AI 应用），需要仔细评估量化的影响。</span>
<span id="cb16-904"><a href="#cb16-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-905"><a href="#cb16-905" aria-hidden="true" tabindex="-1"></a>**局限 2：技术栈复杂性**</span>
<span id="cb16-906"><a href="#cb16-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-907"><a href="#cb16-907" aria-hidden="true" tabindex="-1"></a>完整的推理优化需要叠加多项技术：量化 + 投机解码 + 持续批处理 + KV Cache 优化。每项技术都有自己的超参数和适用条件，组合起来的调优空间巨大。对于资源有限的团队，部署和维护成本不可忽视。</span>
<span id="cb16-908"><a href="#cb16-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-909"><a href="#cb16-909" aria-hidden="true" tabindex="-1"></a>**局限 3：硬件依赖性**</span>
<span id="cb16-910"><a href="#cb16-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-911"><a href="#cb16-911" aria-hidden="true" tabindex="-1"></a>很多优化依赖特定硬件特性：INT4 需要 GPU 支持（如 A100、H100 的 INT4 Tensor Core），FlashAttention 需要足够的 shared memory。跨平台部署（如 CPU、移动端）时，部分优化可能不可用。</span>
<span id="cb16-912"><a href="#cb16-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-913"><a href="#cb16-913" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb16-914"><a href="#cb16-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-915"><a href="#cb16-915" aria-hidden="true" tabindex="-1"></a>推理优化解决了"如何高效地运行大模型"，但没有解决"大模型本身的局限"。即使推理成本降到足够低，模型仍然可能：</span>
<span id="cb16-916"><a href="#cb16-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-917"><a href="#cb16-917" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>产生幻觉（hallucination），给出看似合理但实际错误的回答</span>
<span id="cb16-918"><a href="#cb16-918" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>缺乏实时知识，无法回答最新发生的事件</span>
<span id="cb16-919"><a href="#cb16-919" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>无法处理超长上下文，在需要综合大量信息的任务上表现不佳</span>
<span id="cb16-920"><a href="#cb16-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-921"><a href="#cb16-921" aria-hidden="true" tabindex="-1"></a>这引出了下一章要讨论的**检索增强生成（RAG）**——一种通过外部知识库增强模型能力的范式。推理优化让模型跑得快，RAG 让模型知道更多。两者是互补的技术，共同构成现代 LLM 应用的基础设施。</span>
<span id="cb16-922"><a href="#cb16-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-923"><a href="#cb16-923" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-924"><a href="#cb16-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-925"><a href="#cb16-925" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb16-926"><a href="#cb16-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-927"><a href="#cb16-927" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb16-928"><a href="#cb16-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-929"><a href="#cb16-929" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：推理成本（显存、延迟、费用）是大模型普及的主要障碍。70B 模型的 FP16 推理需要 140GB 显存，远超单卡容量；自回归生成的 token-by-token 方式让 GPU 利用率仅有 10–30%。</span>
<span id="cb16-930"><a href="#cb16-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-931"><a href="#cb16-931" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**洞察**：推理优化的本质是在精度、速度、成本三角形中寻找帕累托最优。关键在于识别和利用大模型推理的特殊结构——权重分布集中适合量化，自回归生成可以"猜测-验证"并行化，请求长度差异适合动态批处理。</span>
<span id="cb16-932"><a href="#cb16-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-933"><a href="#cb16-933" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**方法**：</span>
<span id="cb16-934"><a href="#cb16-934" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**量化**（GPTQ、AWQ）：INT8/INT4 压缩，2–4 倍显存节省，几乎无损</span>
<span id="cb16-935"><a href="#cb16-935" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**投机解码**：2–3 倍无损加速，数学上可证明正确</span>
<span id="cb16-936"><a href="#cb16-936" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**持续批处理**（vLLM）：迭代级调度，2–5 倍吞吐提升</span>
<span id="cb16-937"><a href="#cb16-937" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**PagedAttention**：消除 KV Cache 碎片，支持更大 batch</span>
<span id="cb16-938"><a href="#cb16-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-939"><a href="#cb16-939" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**意义**：这些技术可以叠加使用，将推理成本降低 10 倍以上，让"只有大厂用得起"变成"人人可部署"。</span>
<span id="cb16-940"><a href="#cb16-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-941"><a href="#cb16-941" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb16-942"><a href="#cb16-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-943"><a href="#cb16-943" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 表达式 <span class="pp">|</span></span>
<span id="cb16-944"><a href="#cb16-944" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|</span></span>
<span id="cb16-945"><a href="#cb16-945" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 线性量化 <span class="pp">|</span> $w_q = \text{round}((w - z) / s)$ <span class="pp">|</span></span>
<span id="cb16-946"><a href="#cb16-946" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 投机解码接受概率 <span class="pp">|</span> $P_{\text{accept}} = \min(1, p(x) / q(x))$ <span class="pp">|</span></span>
<span id="cb16-947"><a href="#cb16-947" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> KV Cache 大小 <span class="pp">|</span> $2 \times L \times n \times d \times \text{sizeof(dtype)}$ <span class="pp">|</span></span>
<span id="cb16-948"><a href="#cb16-948" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 内存带宽受限时间 <span class="pp">|</span> $T = \text{参数大小} / \text{带宽}$ <span class="pp">|</span></span>
<span id="cb16-949"><a href="#cb16-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-950"><a href="#cb16-950" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法对比速查</span></span>
<span id="cb16-951"><a href="#cb16-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-952"><a href="#cb16-952" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 显存影响 <span class="pp">|</span> 延迟影响 <span class="pp">|</span> 吞吐影响 <span class="pp">|</span> 质量影响 <span class="pp">|</span></span>
<span id="cb16-953"><a href="#cb16-953" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|---------|---------|---------|</span></span>
<span id="cb16-954"><a href="#cb16-954" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> INT8 量化 <span class="pp">|</span> -50% <span class="pp">|</span> -30% <span class="pp">|</span> +50% <span class="pp">|</span> ~0% <span class="pp">|</span></span>
<span id="cb16-955"><a href="#cb16-955" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> INT4 量化 <span class="pp">|</span> -75% <span class="pp">|</span> -50% <span class="pp">|</span> +100% <span class="pp">|</span> 1–3% <span class="pp">|</span></span>
<span id="cb16-956"><a href="#cb16-956" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 投机解码 <span class="pp">|</span> +10%（draft） <span class="pp">|</span> -50–60% <span class="pp">|</span> 无 <span class="pp">|</span> 0%（数学无损） <span class="pp">|</span></span>
<span id="cb16-957"><a href="#cb16-957" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 持续批处理 <span class="pp">|</span> 无 <span class="pp">|</span> +20%（首 token） <span class="pp">|</span> +200–500% <span class="pp">|</span> 0% <span class="pp">|</span></span>
<span id="cb16-958"><a href="#cb16-958" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PagedAttention <span class="pp">|</span> -30–50% <span class="pp">|</span> 无 <span class="pp">|</span> +50–100% <span class="pp">|</span> 0% <span class="pp">|</span></span>
<span id="cb16-959"><a href="#cb16-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-960"><a href="#cb16-960" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb16-961"><a href="#cb16-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-962"><a href="#cb16-962" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 解释为什么大模型推理是"内存带宽受限"而非"计算受限"的任务。这对优化策略有什么启示？</span>
<span id="cb16-963"><a href="#cb16-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-964"><a href="#cb16-964" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明投机解码的接受-拒绝采样保持目标分布不变。给出完整的概率计算。</span>
<span id="cb16-965"><a href="#cb16-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-966"><a href="#cb16-966" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用 vLLM 部署一个 INT4 量化的 LLaMA-7B 模型，测量不同 batch size 下的吞吐量（tokens/s）和首 token 延迟。观察并解释瓶颈点。</span>
<span id="cb16-967"><a href="#cb16-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-968"><a href="#cb16-968" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** 量化、投机解码、持续批处理能否同时使用？设计一个完整的推理系统架构，考虑它们之间的协同与冲突。</span>
<span id="cb16-969"><a href="#cb16-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-970"><a href="#cb16-970" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-971"><a href="#cb16-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-972"><a href="#cb16-972" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb16-973"><a href="#cb16-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-974"><a href="#cb16-974" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb16-975"><a href="#cb16-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-976"><a href="#cb16-976" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[GPTQ](https://arxiv.org/abs/2210.17323)**：Frantar et al. (2022)，后训练量化的里程碑</span>
<span id="cb16-977"><a href="#cb16-977" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（方法）、Section 4（实验）</span>
<span id="cb16-978"><a href="#cb16-978" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>可跳过：Appendix 的详细推导</span>
<span id="cb16-979"><a href="#cb16-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-980"><a href="#cb16-980" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[AWQ](https://arxiv.org/abs/2306.00978)**：Lin et al. (2023)，MLSys 2024 最佳论文</span>
<span id="cb16-981"><a href="#cb16-981" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（激活感知量化）、Figure 1-2</span>
<span id="cb16-982"><a href="#cb16-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-983"><a href="#cb16-983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[vLLM/PagedAttention](https://arxiv.org/abs/2309.06180)**：Kwon et al. (2023)，SOSP 2023</span>
<span id="cb16-984"><a href="#cb16-984" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（PagedAttention）、Section 4（系统设计）</span>
<span id="cb16-985"><a href="#cb16-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-986"><a href="#cb16-986" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb16-987"><a href="#cb16-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-988"><a href="#cb16-988" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[LLM.int8()](https://arxiv.org/abs/2208.07339)**：Dettmers et al. (2022)，发现异常值现象</span>
<span id="cb16-989"><a href="#cb16-989" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Speculative Decoding](https://arxiv.org/abs/2211.17192)**：Leviathan et al. (2023)，投机解码的理论框架</span>
<span id="cb16-990"><a href="#cb16-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-991"><a href="#cb16-991" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb16-992"><a href="#cb16-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-993"><a href="#cb16-993" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[GGUF Format](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)**：llama.cpp 的量化格式，支持 CPU 推理</span>
<span id="cb16-994"><a href="#cb16-994" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[GQA](https://arxiv.org/abs/2305.13245)**：Grouped-Query Attention，减少 KV Cache</span>
<span id="cb16-995"><a href="#cb16-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-996"><a href="#cb16-996" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb16-997"><a href="#cb16-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-998"><a href="#cb16-998" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[vLLM](https://github.com/vllm-project/vllm)**：高性能推理引擎</span>
<span id="cb16-999"><a href="#cb16-999" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[TGI](https://github.com/huggingface/text-generation-inference)**：Hugging Face 推理服务</span>
<span id="cb16-1000"><a href="#cb16-1000" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[llama.cpp](https://github.com/ggerganov/llama.cpp)**：CPU 推理，GGUF 格式</span>
<span id="cb16-1001"><a href="#cb16-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1002"><a href="#cb16-1002" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb16-1003"><a href="#cb16-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1004"><a href="#cb16-1004" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb16-1005"><a href="#cb16-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1006"><a href="#cb16-1006" aria-hidden="true" tabindex="-1"></a>推理优化的需求伴随着 GPT-3 的发布而爆发。2020 年，175B 参数的 GPT-3 震惊了 AI 社区，但也让人们意识到：这样的模型几乎无法在任何单一设备上运行。最初，人们认为这只是"有钱人的游戏"——只有 OpenAI、Google 这样的公司才有资源部署如此规模的模型。</span>
<span id="cb16-1007"><a href="#cb16-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1008"><a href="#cb16-1008" aria-hidden="true" tabindex="-1"></a>转折点出现在 2022 年。Dettmers 的 LLM.int8() 首次展示了大模型可以在"可承受"的硬件上运行。紧随其后，GPTQ 将 175B 模型压缩到单卡可运行的程度。2023 年，vLLM 和 TGI 的出现让高效推理成为"开箱即用"的能力。短短两年间，大模型推理从"天方夜谭"变成了"人人可及"。</span>
<span id="cb16-1009"><a href="#cb16-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1010"><a href="#cb16-1010" aria-hidden="true" tabindex="-1"></a>这段历史告诉我们：AI 的民主化不仅需要开源模型，更需要开源的基础设施。量化、高效推理引擎这些"不那么性感"的技术，实际上是让 AI 惠及普通人的关键拼图。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>