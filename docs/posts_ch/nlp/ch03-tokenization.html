<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-25">
<meta name="description" content="Tokenizer不是预处理工具，它是模型架构的隐藏维度：从词级别到子词方法的演进，以及分词策略对模型能力的深远影响。">

<title>第3章：Tokenization与数据基础 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#什么是一个词" id="toc-什么是一个词" class="nav-link" data-scroll-target="#什么是一个词"><span class="header-section-number">2.1</span> 什么是一个”词”？</a></li>
  <li><a href="#问题的精确定义" id="toc-问题的精确定义" class="nav-link" data-scroll-target="#问题的精确定义"><span class="header-section-number">2.2</span> 问题的精确定义</a></li>
  <li><a href="#为什么这个问题重要" id="toc-为什么这个问题重要" class="nav-link" data-scroll-target="#为什么这个问题重要"><span class="header-section-number">2.3</span> 为什么这个问题重要？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#三种基本策略" id="toc-三种基本策略" class="nav-link" data-scroll-target="#三种基本策略"><span class="header-section-number">3.1</span> 三种基本策略</a></li>
  <li><a href="#子词分词的直觉数据压缩" id="toc-子词分词的直觉数据压缩" class="nav-link" data-scroll-target="#子词分词的直觉数据压缩"><span class="header-section-number">3.2</span> 子词分词的直觉：数据压缩</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#bpe算法从数据压缩到nlp" id="toc-bpe算法从数据压缩到nlp" class="nav-link" data-scroll-target="#bpe算法从数据压缩到nlp"><span class="header-section-number">4.1</span> BPE算法：从数据压缩到NLP</a></li>
  <li><a href="#wordpiecebert的选择" id="toc-wordpiecebert的选择" class="nav-link" data-scroll-target="#wordpiecebert的选择"><span class="header-section-number">4.2</span> WordPiece：BERT的选择</a></li>
  <li><a href="#unigram-language-model概率视角" id="toc-unigram-language-model概率视角" class="nav-link" data-scroll-target="#unigram-language-model概率视角"><span class="header-section-number">4.3</span> Unigram Language Model：概率视角</a></li>
  <li><a href="#sentencepiece语言无关的统一方案" id="toc-sentencepiece语言无关的统一方案" class="nav-link" data-scroll-target="#sentencepiece语言无关的统一方案"><span class="header-section-number">4.4</span> SentencePiece：语言无关的统一方案</a></li>
  <li><a href="#byte-level-bpegpt-2的创新" id="toc-byte-level-bpegpt-2的创新" class="nav-link" data-scroll-target="#byte-level-bpegpt-2的创新"><span class="header-section-number">4.5</span> Byte-level BPE：GPT-2的创新</a></li>
  </ul></li>
  <li><a href="#tokenizer是模型的一部分" id="toc-tokenizer是模型的一部分" class="nav-link" data-scroll-target="#tokenizer是模型的一部分"><span class="header-section-number">5</span> Tokenizer是模型的一部分</a>
  <ul class="collapse">
  <li><a href="#计算效率序列长度的隐藏成本" id="toc-计算效率序列长度的隐藏成本" class="nav-link" data-scroll-target="#计算效率序列长度的隐藏成本"><span class="header-section-number">5.1</span> 计算效率：序列长度的隐藏成本</a></li>
  <li><a href="#多语言公平性隐藏的不平等" id="toc-多语言公平性隐藏的不平等" class="nav-link" data-scroll-target="#多语言公平性隐藏的不平等"><span class="header-section-number">5.2</span> 多语言公平性：隐藏的不平等</a></li>
  <li><a href="#数学和代码能力数字的切分" id="toc-数学和代码能力数字的切分" class="nav-link" data-scroll-target="#数学和代码能力数字的切分"><span class="header-section-number">5.3</span> 数学和代码能力：数字的切分</a></li>
  <li><a href="#安全性tokenization边界的攻击" id="toc-安全性tokenization边界的攻击" class="nav-link" data-scroll-target="#安全性tokenization边界的攻击"><span class="header-section-number">5.4</span> 安全性：Tokenization边界的攻击</a></li>
  </ul></li>
  <li><a href="#数据质量清洗去重与污染" id="toc-数据质量清洗去重与污染" class="nav-link" data-scroll-target="#数据质量清洗去重与污染"><span class="header-section-number">6</span> 数据质量：清洗、去重与污染</a>
  <ul class="collapse">
  <li><a href="#数据清洗从噪声中提取信号" id="toc-数据清洗从噪声中提取信号" class="nav-link" data-scroll-target="#数据清洗从噪声中提取信号"><span class="header-section-number">6.1</span> 数据清洗：从噪声中提取信号</a></li>
  <li><a href="#数据去重重复的危害" id="toc-数据去重重复的危害" class="nav-link" data-scroll-target="#数据去重重复的危害"><span class="header-section-number">6.2</span> 数据去重：重复的危害</a></li>
  <li><a href="#数据污染训练-测试泄漏" id="toc-数据污染训练-测试泄漏" class="nav-link" data-scroll-target="#数据污染训练-测试泄漏"><span class="header-section-number">6.3</span> 数据污染：训练-测试泄漏</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">7</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#理论视角最优分词的条件" id="toc-理论视角最优分词的条件" class="nav-link" data-scroll-target="#理论视角最优分词的条件"><span class="header-section-number">7.1</span> 理论视角：最优分词的条件</a></li>
  <li><a href="#边界条件子词分词的假设" id="toc-边界条件子词分词的假设" class="nav-link" data-scroll-target="#边界条件子词分词的假设"><span class="header-section-number">7.2</span> 边界条件：子词分词的假设</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">7.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#工程实践hugging-face-tokenizers库使用" id="toc-工程实践hugging-face-tokenizers库使用" class="nav-link" data-scroll-target="#工程实践hugging-face-tokenizers库使用"><span class="header-section-number">8</span> 工程实践：Hugging Face Tokenizers库使用</a>
  <ul class="collapse">
  <li><a href="#使用预训练tokenizer" id="toc-使用预训练tokenizer" class="nav-link" data-scroll-target="#使用预训练tokenizer"><span class="header-section-number">8.1</span> 使用预训练Tokenizer</a></li>
  <li><a href="#比较不同tokenizer" id="toc-比较不同tokenizer" class="nav-link" data-scroll-target="#比较不同tokenizer"><span class="header-section-number">8.2</span> 比较不同Tokenizer</a></li>
  <li><a href="#训练自定义tokenizer" id="toc-训练自定义tokenizer" class="nav-link" data-scroll-target="#训练自定义tokenizer"><span class="header-section-number">8.3</span> 训练自定义Tokenizer</a></li>
  <li><a href="#分析tokenization效率" id="toc-分析tokenization效率" class="nav-link" data-scroll-target="#分析tokenization效率"><span class="header-section-number">8.4</span> 分析Tokenization效率</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#tokenization的固有局限" id="toc-tokenization的固有局限" class="nav-link" data-scroll-target="#tokenization的固有局限"><span class="header-section-number">9.1</span> Tokenization的固有局限</a></li>
  <li><a href="#下一章的铺垫" id="toc-下一章的铺垫" class="nav-link" data-scroll-target="#下一章的铺垫"><span class="header-section-number">9.2</span> 下一章的铺垫</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">10</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</a></li>
  <li><a href="#关键概念速查" id="toc-关键概念速查" class="nav-link" data-scroll-target="#关键概念速查"><span class="header-section-number">10.2</span> 关键概念速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">10.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">11</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论分析" id="toc-理论分析" class="nav-link" data-scroll-target="#理论分析"><span class="header-section-number">11.2</span> 理论分析</a></li>
  <li><a href="#实证研究" id="toc-实证研究" class="nav-link" data-scroll-target="#实证研究"><span class="header-section-number">11.3</span> 实证研究</a></li>
  <li><a href="#工具和资源" id="toc-工具和资源" class="nav-link" data-scroll-target="#工具和资源"><span class="header-section-number">11.4</span> 工具和资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">12</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第3章：Tokenization与数据基础</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">被低估的基础设施：从文本到模型输入的艺术</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Tokenization</div>
    <div class="quarto-category">BPE</div>
    <div class="quarto-category">WordPiece</div>
    <div class="quarto-category">SentencePiece</div>
    <div class="quarto-category">Data</div>
  </div>
  </div>

<div>
  <div class="description">
    Tokenizer不是预处理工具，它是模型架构的隐藏维度：从词级别到子词方法的演进，以及分词策略对模型能力的深远影响。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心论点</strong>：Tokenizer不是预处理工具，它是模型架构的隐藏维度。</p>
<p><strong>历史坐标</strong>：2016 | Sennrich et al.&nbsp;(BPE for NMT) | 从词级别到子词的范式转变</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们学习了词向量——如何用稠密向量表示词的语义。Word2Vec、GloVe、FastText让我们第一次能够在数学上捕获”cat”和”dog”的相似性。这是表示学习的巨大进步。</p>
<p>但我们回避了一个看似简单却极其重要的问题：<strong>这些”词”是从哪里来的？</strong></p>
<p>回顾FastText的出现动机：Word2Vec无法处理训练时没见过的词（OOV问题）。FastText通过将词分解为子词（character n-grams）来解决这个问题——即使”unfriendliness”没出现过，只要它的子词（“un”、“friend”、“ness”）出现过，就能组合出一个合理的向量。</p>
<p>FastText的成功暗示了一个深刻的洞察：<strong>“词”并不是文本处理的最佳单位</strong>。</p>
<p>这引出了本章的核心问题：在把文本送入模型之前，我们应该如何切分它？按词切？按字符切？还是有更聪明的方法？这个看似是”预处理”的问题，实际上深刻地影响着模型的能力、效率和公平性。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：Tokenizer决定了模型”看到”什么。它不是可以随意选择的预处理步骤，而是模型架构的隐藏维度——影响计算效率、多语言公平性、数学推理能力，甚至安全性。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="什么是一个词" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="什么是一个词"><span class="header-section-number">2.1</span> 什么是一个”词”？</h3>
<p>这个问题比看起来复杂得多。</p>
<p>对于英语，我们可能直觉地认为”词就是空格分隔的单位”。但仔细想想：</p>
<ul>
<li>“don’t”是一个词还是两个？</li>
<li>“New York”是一个词还是两个？</li>
<li>“ice-cream”呢？</li>
<li>“it’s”和”its”在tokenization层面应该相同还是不同？</li>
</ul>
<p>中文更麻烦。中文没有空格分隔词，需要专门的<strong>分词</strong>（word segmentation）算法。考虑这个句子：</p>
<blockquote class="blockquote">
<p>“研究生命的起源”</p>
</blockquote>
<p>这应该切分为”研究/生命/的/起源”还是”研究生/命/的/起源”？没有上下文，无法确定。中文分词本身就是一个复杂的NLP任务，需要模型来完成——但我们现在讨论的是如何把文本送入模型，这形成了一个先有鸡还是先有蛋的悖论。</p>
<p>再看形态丰富的语言，如德语、芬兰语、土耳其语。德语允许复合词任意组合，“Donaudampfschifffahrtsgesellschaftskapitän”（多瑙河轮船公司船长）是一个合法的德语词。土耳其语是黏着语，一个词可以通过添加后缀表达复杂的语法关系。如果坚持用词作为基本单位，词汇表会爆炸式增长，而且每个词的训练样本会稀少到无法学习。</p>
</section>
<section id="问题的精确定义" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="问题的精确定义"><span class="header-section-number">2.2</span> 问题的精确定义</h3>
<p>让我们精确地定义Tokenization要解决的问题。</p>
<p>给定一个字符串<span class="math inline">\(s\)</span>，我们需要找到一个函数<span class="math inline">\(\text{tokenize}: \Sigma^* \rightarrow \mathcal{V}^*\)</span>，把任意字符串映射为一个token序列，其中<span class="math inline">\(\mathcal{V}\)</span>是预定义的词汇表（vocabulary）。</p>
<p>这个函数需要满足几个关键性质：<strong>完备性</strong>——任何输入字符串都能被tokenize，不能有”无法处理”的情况；<strong>确定性</strong>——相同的输入总是产生相同的输出；以及理想情况下的<strong>可逆性</strong>——从token序列能够恢复原始字符串。</p>
<p>除了这些基本约束，我们还希望函数在多个维度上表现良好。<strong>词汇表大小</strong>不能太大（否则embedding矩阵爆炸），也不能太小（否则序列太长）。产生的<strong>序列长度</strong>要合理（否则计算成本高，超出上下文窗口）。理想情况下，每个token应该有明确的<strong>语义完整性</strong>。此外，不同语言的相同内容，token数量应该大致相当——即<strong>跨语言公平性</strong>。</p>
<p>这些目标之间存在张力。词汇表越小，序列越长；词汇表越大，稀有token越多。找到正确的平衡是tokenization的核心挑战。</p>
</section>
<section id="为什么这个问题重要" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="为什么这个问题重要"><span class="header-section-number">2.3</span> 为什么这个问题重要？</h3>
<p>你可能会问：这真的那么重要吗？不就是切分文本吗？</p>
<p>让我用几个具体的例子说明tokenizer选择对模型的深远影响。</p>
<p><strong>计算效率</strong>。同样的文本，不同tokenizer产生的序列长度可能相差2-3倍。如果你的模型有4096的上下文窗口，一个高效的tokenizer可以让你处理的实际文本量翻倍。Transformer的计算复杂度是<span class="math inline">\(O(n^2)\)</span>（<span class="math inline">\(n\)</span>是序列长度），序列长度减半意味着计算量减少75%。</p>
<p><strong>多语言公平性</strong>。同样的语义内容，英文可能只需要1个token，而中文可能需要2-3个token，日文可能需要更多。这意味着在相同的上下文窗口内，模型能处理的中文内容比英文少。这不是技术实现的细节，而是系统性的不公平。</p>
<p><strong>数学和代码能力</strong>。数字”380”可能被切分为单个token”380”，也可能被切分为”3”、“8”、“0”三个token。研究表明，后者会严重损害模型的算术推理能力——模型很难理解”380”和”381”只差1。类似地，代码中的缩进、变量名、运算符的切分方式直接影响模型理解代码的能力。</p>
<p><strong>安全性</strong>。Tokenizer的切分边界可能被攻击者利用。某些prompt injection攻击正是利用了特定token序列的特殊行为。理解tokenizer是理解模型安全边界的前提。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="三种基本策略" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="三种基本策略"><span class="header-section-number">3.1</span> 三种基本策略</h3>
<p>面对”如何切分文本”这个问题，有三种基本策略，它们代表了不同的权衡。</p>
<p><strong>策略一：词级别（Word-level）</strong></p>
<p>最直观的方法是按词切分。建立一个词汇表，包含训练语料中出现的所有词（或最常见的N个词），然后把每个词映射到一个token。</p>
<p>这种方法的优点是每个token都有明确的语义——“cat”就是猫，“running”就是正在跑。但缺点也很明显：词汇表庞大（英语常用词就有几十万），OOV问题严重（任何不在词汇表中的词都无法处理），形态变化浪费空间（“run”、“runs”、“running”、“ran”是四个独立的token）。</p>
<p><strong>策略二：字符级别（Character-level）</strong></p>
<p>走向另一个极端：按字符切分。词汇表只需要包含所有可能的字符（英语只需要不到100个），OOV问题彻底消失。</p>
<p>但代价是序列变得极长。“tokenization”这个词变成了13个token，计算成本大增。更糟糕的是，字符本身几乎没有语义——“t”、“o”、“k”…模型需要从这些原子构建词的含义，这是一个巨大的学习负担。</p>
<p><strong>策略三：子词级别（Subword-level）</strong></p>
<p>能不能找到一个中间地带？这正是现代NLP的选择：<strong>子词分词</strong>（subword tokenization）。</p>
<p>核心思想是：常见的词保持完整，罕见的词分解为更小的、有意义的单元。比如：</p>
<ul>
<li>“running” → “running”（常见，保持完整）</li>
<li>“tokenization” → “token” + “ization”（罕见，分解为常见子词）</li>
<li>“unfriendliness” → “un” + “friend” + “li” + “ness”</li>
</ul>
<p>这样，词汇表大小可控（通常32K-64K），序列长度合理，而且任何词都能被切分（最坏情况退化到字符级别）。子词还经常具有语义或语法含义——“un-”表示否定，“-ing”表示进行时，“-ness”表示名词化。</p>
<div id="fig-tokenization-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tokenization-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-3/fig-tokenization-strategies.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tokenization-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 三种Tokenization策略对比：以”tokenization”为例，词级别产生1个token，字符级别产生12个token，而子词（BPE）只需要2个token——“token”和”ization”。子词方法在词汇表大小和序列长度之间找到了最佳平衡。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Python生成，概念参考 Sennrich et al.&nbsp;(2016) “Neural Machine Translation of Rare Words with Subword Units”. <a href="https://arxiv.org/abs/1508.07909">arXiv:1508.07909</a></em></p>
</div>
</section>
<section id="子词分词的直觉数据压缩" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="子词分词的直觉数据压缩"><span class="header-section-number">3.2</span> 子词分词的直觉：数据压缩</h3>
<p>理解子词分词的最佳方式是把它类比为<strong>数据压缩</strong>。</p>
<p>想象你要设计一个编码方案，用最少的符号表示大量文本。直觉上，你会：</p>
<ol type="1">
<li>给最常见的模式分配短码</li>
<li>让不常见的模式用常见模式的组合表示</li>
</ol>
<p>这正是BPE（Byte Pair Encoding）的核心思想，而BPE最初就是一种数据压缩算法。</p>
<p>考虑一个简单的例子。假设你的文本中经常出现”th”、“the”、“ing”这些模式。如果每次都用两个或三个字符表示它们，很浪费。不如给”th”一个专门的符号（比如”θ”），给”the”另一个符号（比如”Θ”）。这样，原本需要3个符号的”the”只需要1个符号。</p>
<p>子词分词做的就是这件事：自动发现文本中的高频模式，给它们分配token。最终的词汇表是数据驱动的，反映了训练语料的统计规律。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="bpe算法从数据压缩到nlp" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="bpe算法从数据压缩到nlp"><span class="header-section-number">4.1</span> BPE算法：从数据压缩到NLP</h3>
<p>BPE（Byte Pair Encoding）是目前最广泛使用的子词分词算法之一，被GPT系列、RoBERTa等模型采用。</p>
<p><strong>算法思想</strong>：从字符开始，迭代地合并最频繁的相邻token对，直到达到目标词汇表大小。</p>
<p>让我们用一个具体的例子来理解这个过程。</p>
<section id="完整数值示例bpe词汇表构建" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="完整数值示例bpe词汇表构建"><span class="header-section-number">4.1.1</span> 完整数值示例：BPE词汇表构建</h4>
<p><strong>设定</strong>：训练语料包含以下词及其频率：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>词</th>
<th>频率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>low</td>
<td>5</td>
</tr>
<tr class="even">
<td>lower</td>
<td>2</td>
</tr>
<tr class="odd">
<td>newest</td>
<td>6</td>
</tr>
<tr class="even">
<td>widest</td>
<td>3</td>
</tr>
</tbody>
</table>
<p><strong>Step 0：初始化</strong></p>
<p>首先，我们把每个词表示为字符序列，并添加一个特殊的词尾标记”_“（表示词边界）：</p>
<pre><code>l o w _     : 5
l o w e r _ : 2
n e w e s t _ : 6
w i d e s t _ : 3</code></pre>
<p>初始词汇表是所有字符：<code>{l, o, w, e, r, n, s, t, i, d, _}</code>，共11个token。</p>
<p><strong>Step 1：统计相邻token对频率</strong></p>
<p>遍历所有词，统计相邻token对出现的总次数：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Token对</th>
<th>频率计算</th>
<th>总频率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(l, o)</td>
<td>5 + 2 = 7</td>
<td>7</td>
</tr>
<tr class="even">
<td>(o, w)</td>
<td>5 + 2 = 7</td>
<td>7</td>
</tr>
<tr class="odd">
<td>(w, _)</td>
<td>5</td>
<td>5</td>
</tr>
<tr class="even">
<td>(w, e)</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="odd">
<td>(e, r)</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="even">
<td>(r, _)</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="odd">
<td>(n, e)</td>
<td>6</td>
<td>6</td>
</tr>
<tr class="even">
<td>(e, w)</td>
<td>6</td>
<td>6</td>
</tr>
<tr class="odd">
<td>(e, s)</td>
<td>6 + 3 = 9</td>
<td><strong>9</strong></td>
</tr>
<tr class="even">
<td>(s, t)</td>
<td>6 + 3 = 9</td>
<td><strong>9</strong></td>
</tr>
<tr class="odd">
<td>(t, _)</td>
<td>6 + 3 = 9</td>
<td><strong>9</strong></td>
</tr>
<tr class="even">
<td>(w, i)</td>
<td>3</td>
<td>3</td>
</tr>
<tr class="odd">
<td>(i, d)</td>
<td>3</td>
<td>3</td>
</tr>
<tr class="even">
<td>(d, e)</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>最高频的对是 (e, s)、(s, t)、(t, _)，都是9次。我们选择第一个遇到的：<strong>(e, s)</strong>。</p>
<p><strong>Step 2：合并最频繁的对</strong></p>
<p>创建新token “es”，替换所有出现的 (e, s)：</p>
<pre><code>l o w _     : 5
l o w e r _ : 2
n e w es t _ : 6
w i d es t _ : 3</code></pre>
<p>词汇表变为：<code>{l, o, w, e, r, n, s, t, i, d, _, es}</code>，12个token。</p>
<p><strong>Step 3：重复统计和合并</strong></p>
<p>继续统计相邻对频率：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Token对</th>
<th>总频率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(es, t)</td>
<td>6 + 3 = <strong>9</strong></td>
</tr>
<tr class="even">
<td>(l, o)</td>
<td>7</td>
</tr>
<tr class="odd">
<td>(o, w)</td>
<td>7</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>最高频：<strong>(es, t) = 9</strong>，合并为 “est”：</p>
<pre><code>l o w _     : 5
l o w e r _ : 2
n e w est _ : 6
w i d est _ : 3</code></pre>
<p>词汇表：<code>{l, o, w, e, r, n, s, t, i, d, _, es, est}</code>，13个token。</p>
<p><strong>Step 4：继续合并</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Token对</th>
<th>总频率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(est, _)</td>
<td>6 + 3 = <strong>9</strong></td>
</tr>
<tr class="even">
<td>(l, o)</td>
<td>7</td>
</tr>
<tr class="odd">
<td>(o, w)</td>
<td>7</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>合并 **(est, _)** 为 “est_”：</p>
<pre><code>l o w _     : 5
l o w e r _ : 2
n e w est_ : 6
w i d est_ : 3</code></pre>
<p><strong>Step 5-7：继续迭代</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>迭代</th>
<th>合并的对</th>
<th>新token</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5</td>
<td>(l, o)</td>
<td>lo</td>
</tr>
<tr class="even">
<td>6</td>
<td>(lo, w)</td>
<td>low</td>
</tr>
<tr class="odd">
<td>7</td>
<td>(low, _)</td>
<td>low_</td>
</tr>
</tbody>
</table>
<p>经过7次合并后：</p>
<pre><code>low_     : 5
low e r _ : 2
n e w est_ : 6
w i d est_ : 3</code></pre>
<p><strong>最终词汇表</strong>（假设目标大小是18）：</p>
<pre><code>{l, o, w, e, r, n, s, t, i, d, _, es, est, est_, lo, low, low_, ...}</code></pre>
<p><strong>关键观察</strong>：</p>
<ol type="1">
<li>高频词”low”被学习为完整token</li>
<li>常见后缀”est_“被学习为一个token</li>
<li>罕见词”lower”仍然需要多个token：low + e + r + _</li>
<li>词汇表从纯字符逐步构建，包含字符、常见子词、常见词</li>
</ol>
<div id="fig-bpe-merge-process" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bpe-merge-process-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-3/fig-bpe-merge-process.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bpe-merge-process-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: BPE合并过程可视化：以”lowest”为例，从字符序列出发，依次合并最高频的相邻对（e,s）→“es”、（es,t）→“est”、（l,o）→“lo”，词汇表逐步增长。每一步合并都由语料统计驱动。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Python生成，数值示例基于 Sennrich et al.&nbsp;(2016) “Neural Machine Translation of Rare Words with Subword Units”, Section 3. <a href="https://arxiv.org/abs/1508.07909">arXiv:1508.07909</a></em></p>
</div>
</section>
<section id="bpe的形式化算法" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="bpe的形式化算法"><span class="header-section-number">4.1.2</span> BPE的形式化算法</h4>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: Learn BPE Operations (Sennrich et al., 2016)
</div>
</div>
<div class="callout-body-container callout-body">
<p>以下是论文原文中的Python实现。这段代码展示了BPE算法的核心逻辑：统计相邻符号对的频率，合并最频繁的对，重复直到达到目标词汇表大小。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re, collections</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_stats(vocab):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""统计所有相邻符号对的频率"""</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    pairs <span class="op">=</span> collections.defaultdict(<span class="bu">int</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, freq <span class="kw">in</span> vocab.items():</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        symbols <span class="op">=</span> word.split()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(symbols)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            pairs[symbols[i],symbols[i<span class="op">+</span><span class="dv">1</span>]] <span class="op">+=</span> freq</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pairs</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_vocab(pair, v_in):</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""将vocab中所有出现的pair合并为新符号"""</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    v_out <span class="op">=</span> {}</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    bigram <span class="op">=</span> re.escape(<span class="st">' '</span>.join(pair))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> re.<span class="bu">compile</span>(<span class="vs">r'</span><span class="kw">(</span><span class="fu">?&lt;</span><span class="vs">!</span><span class="dv">\S</span><span class="kw">)</span><span class="vs">'</span> <span class="op">+</span> bigram <span class="op">+</span> <span class="vs">r'</span><span class="ex">(</span><span class="fu">?!</span><span class="dv">\S</span><span class="ex">)</span><span class="vs">'</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> v_in:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        w_out <span class="op">=</span> p.sub(<span class="st">''</span>.join(pair), word)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        v_out[w_out] <span class="op">=</span> v_in[word]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v_out</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例词汇表（词用空格分隔的字符序列表示）</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {<span class="st">'l o w &lt;/w&gt;'</span> : <span class="dv">5</span>, <span class="st">'l o w e r &lt;/w&gt;'</span> : <span class="dv">2</span>,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>         <span class="st">'n e w e s t &lt;/w&gt;'</span>:<span class="dv">6</span>, <span class="st">'w i d e s t &lt;/w&gt;'</span>:<span class="dv">3</span>}</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>num_merges <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_merges):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    pairs <span class="op">=</span> get_stats(vocab)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    best <span class="op">=</span> <span class="bu">max</span>(pairs, key<span class="op">=</span>pairs.get)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> merge_vocab(best, vocab)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(best)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>输出</strong>（前4次合并）：</p>
<pre><code>('e', 's')      →  es
('es', 't')     →  est
('est', '&lt;/w&gt;') →  est&lt;/w&gt;
('l', 'o')      →  lo
...</code></pre>
<p><em>Source: <a href="https://arxiv.org/abs/1508.07909">Sennrich et al.&nbsp;(2016)</a>, Algorithm 1</em></p>
</div>
</div>
<p>论文中使用<code>&lt;/w&gt;</code>作为词尾标记（end-of-word），这与我们之前示例中的<code>_</code>作用相同。下面是一个更详细的实现版本，带有完整注释：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> learn_bpe(corpus, num_merges):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">    学习BPE词汇表</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus: 词频字典 {word: frequency}</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">        num_merges: 合并次数（决定最终词汇表大小）</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">        merges: 合并规则列表</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab: 最终词汇表</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 初始化——将词拆分为字符，添加词尾标记</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> {}</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, freq <span class="kw">in</span> corpus.items():</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># "low" -&gt; ['l', 'o', 'w', '_']</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        chars <span class="op">=</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'_'</span>]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        vocab[<span class="bu">tuple</span>(chars)] <span class="op">=</span> freq</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    merges <span class="op">=</span> []</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_merges):</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 统计所有相邻token对的频率</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        pairs <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, freq <span class="kw">in</span> vocab.items():</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(word) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>                pairs[(word[j], word[j<span class="op">+</span><span class="dv">1</span>])] <span class="op">+=</span> freq</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> pairs:</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 找到最频繁的对</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        best_pair <span class="op">=</span> <span class="bu">max</span>(pairs, key<span class="op">=</span>pairs.get)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        merges.append(best_pair)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 合并这个对，更新vocab</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        new_vocab <span class="op">=</span> {}</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, freq <span class="kw">in</span> vocab.items():</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>            new_word <span class="op">=</span> merge_pair(word, best_pair)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>            new_vocab[new_word] <span class="op">=</span> freq</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        vocab <span class="op">=</span> new_vocab</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> merges, vocab</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_pair(word, pair):</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""将word中所有出现的pair合并"""</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    new_word <span class="op">=</span> []</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(word):</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(word) <span class="op">-</span> <span class="dv">1</span> <span class="kw">and</span> (word[i], word[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">==</span> pair:</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            new_word.append(word[i] <span class="op">+</span> word[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>            i <span class="op">+=</span> <span class="dv">2</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>            new_word.append(word[i])</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>            i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">tuple</span>(new_word)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="使用bpe进行tokenization" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="使用bpe进行tokenization"><span class="header-section-number">4.1.3</span> 使用BPE进行Tokenization</h4>
<p>训练好BPE词汇表后，如何tokenize一个新词？</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_bpe(word, merges):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    使用学习到的BPE规则tokenize一个词</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    核心思想：按照学习时的合并顺序，依次应用合并规则</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化为字符序列</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    word <span class="op">=</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'_'</span>]</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 按顺序应用每个合并规则</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pair <span class="kw">in</span> merges:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(word) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (word[i], word[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">==</span> pair:</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>                word <span class="op">=</span> word[:i] <span class="op">+</span> [word[i] <span class="op">+</span> word[i<span class="op">+</span><span class="dv">1</span>]] <span class="op">+</span> word[i<span class="op">+</span><span class="dv">2</span>:]</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> word</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>例子</strong>：tokenize “lowest”</p>
<ul>
<li>初始：[‘l’, ‘o’, ‘w’, ‘e’, ‘s’, ‘t’, ’_’]</li>
<li>应用 (e, s) → ‘es’：[‘l’, ‘o’, ‘w’, ‘es’, ‘t’, ’_’]</li>
<li>应用 (es, t) → ‘est’：[‘l’, ‘o’, ‘w’, ‘est’, ’_’]</li>
<li>应用 (est, _) → ‘est_’：[‘l’, ‘o’, ‘w’, ‘est_’]</li>
<li>应用 (l, o) → ‘lo’：[‘lo’, ‘w’, ‘est_’]</li>
<li>应用 (lo, w) → ‘low’：[‘low’, ‘est_’]</li>
</ul>
<p>最终结果：<strong>[‘low’, ‘est_’]</strong></p>
<p>即使”lowest”没有出现在训练语料中，BPE也能将它合理地分解为”low”和”est_“——这两个都是学习到的有意义的子词。</p>
</section>
<section id="bpe与其他分词方法的实证对比" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="bpe与其他分词方法的实证对比"><span class="header-section-number">4.1.4</span> BPE与其他分词方法的实证对比</h4>
<p>Sennrich et al.&nbsp;(2016) 在德语语料上对比了不同分词方法的效果。下表展示了各方法的token数、词汇表大小和未知词数量：</p>
<div id="tbl-segmentation-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-segmentation-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: 不同分词技术在德语训练语料上的统计对比。<em>Source: <a href="https://arxiv.org/abs/1508.07909">Sennrich et al.&nbsp;(2016)</a>, Table 1</em>
</figcaption>
<div aria-describedby="tbl-segmentation-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>分词方法</th>
<th>Token数量</th>
<th>词汇表大小</th>
<th>测试集未知词</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>不分词（词级别）</td>
<td>100M</td>
<td>1,750,000</td>
<td>1,079</td>
</tr>
<tr class="even">
<td>字符级别</td>
<td>550M</td>
<td>3,000</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Character bigrams</td>
<td>306M</td>
<td>20,000</td>
<td>34</td>
</tr>
<tr class="even">
<td>Character trigrams</td>
<td>214M</td>
<td>120,000</td>
<td>59</td>
</tr>
<tr class="odd">
<td>复合词拆分</td>
<td>102M</td>
<td>1,100,000</td>
<td>643</td>
</tr>
<tr class="even">
<td>Morfessor</td>
<td>109M</td>
<td>544,000</td>
<td>237</td>
</tr>
<tr class="odd">
<td>连字符分割</td>
<td>186M</td>
<td>404,000</td>
<td>230</td>
</tr>
<tr class="even">
<td><strong>BPE</strong></td>
<td>112M</td>
<td>63,000</td>
<td><strong>0</strong></td>
</tr>
<tr class="odd">
<td><strong>BPE (joint)</strong></td>
<td>111M</td>
<td>82,000</td>
<td>32</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>关键观察</strong>：</p>
<ul>
<li><strong>字符级别</strong>：完全消除未知词，但token数量爆炸（5.5倍），计算成本极高</li>
<li><strong>传统形态学方法</strong>（复合词拆分、Morfessor）：仍有大量未知词，词汇表依然庞大</li>
<li><strong>BPE</strong>：在保持合理token数量（仅比词级别多12%）的同时，实现了<strong>零未知词</strong>，词汇表紧凑（63K）</li>
</ul>
<p>这正是BPE成为现代NLP标准的原因：它在<strong>词汇表大小</strong>、<strong>序列长度</strong>和<strong>OOV处理</strong>之间找到了最佳平衡。</p>
</section>
</section>
<section id="wordpiecebert的选择" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="wordpiecebert的选择"><span class="header-section-number">4.2</span> WordPiece：BERT的选择</h3>
<p>WordPiece是Google提出的子词算法，被BERT、DistilBERT等模型使用。它与BPE的思想相似，但在选择合并哪个pair时使用了不同的标准。</p>
<p><strong>BPE vs WordPiece的关键区别</strong>：</p>
<ul>
<li>BPE选择<strong>出现频率最高</strong>的pair</li>
<li>WordPiece选择<strong>合并后使语言模型困惑度下降最多</strong>的pair</li>
</ul>
<p>WordPiece的选择标准可以写成：</p>
<p><span class="math display">\[
\text{score}(x, y) = \frac{\text{freq}(xy)}{\text{freq}(x) \times \text{freq}(y)}
\]</span></p>
<p>这个分数本质上是在问：<span class="math inline">\(xy\)</span>一起出现的频率，相对于<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>各自出现频率的乘积，高多少？这类似于点互信息（PMI），衡量的是<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>的共现是否超出了随机期望。</p>
<p><strong>直觉理解</strong>：考虑”th”和”qu”。在英语中，“th”几乎总是一起出现（“the”、“this”、“that”），而”qu”后面几乎总是跟着”u”（“question”、“queen”）。WordPiece的标准会优先合并这种”绑定程度”高的pair，而不仅仅是出现次数多的pair。</p>
<p><strong>WordPiece的特殊标记</strong>：</p>
<p>WordPiece使用”##“前缀标记非词首的子词。比如”tokenization”可能被切分为：</p>
<pre><code>["token", "##ization"]</code></pre>
<p>“##”告诉我们”ization”不是一个独立的词，而是接在前一个token后面的后缀。这个设计使得从token序列恢复原始文本更容易。</p>
</section>
<section id="unigram-language-model概率视角" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="unigram-language-model概率视角"><span class="header-section-number">4.3</span> Unigram Language Model：概率视角</h3>
<p>Unigram LM是另一种子词分词方法，由SentencePiece库实现，被T5、ALBERT等模型使用。它采用了一个根本不同的视角：不是从小到大构建词汇表，而是从大到小剪枝。</p>
<p><strong>核心思想</strong>：假设每个子词独立出现，用一个unigram语言模型给每种切分方式打分。</p>
<p>给定词汇表<span class="math inline">\(\mathcal{V}\)</span>和每个子词<span class="math inline">\(x\)</span>的概率<span class="math inline">\(p(x)\)</span>，一个句子<span class="math inline">\(S\)</span>的某种切分<span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)</span>的概率是：</p>
<p><span class="math display">\[
P(\mathbf{x}) = \prod_{i=1}^{n} p(x_i)
\]</span></p>
<p>最优切分是使这个概率最大化的切分：</p>
<p><span class="math display">\[
\mathbf{x}^* = \arg\max_{\mathbf{x}} P(\mathbf{x}) = \arg\max_{\mathbf{x}} \sum_{i=1}^{n} \log p(x_i)
\]</span></p>
<p><strong>训练过程</strong>（与BPE相反）：</p>
<ol type="1">
<li>从一个很大的初始词汇表开始（包含所有字符、常见子串等）</li>
<li>用EM算法估计每个子词的概率</li>
<li>计算每个子词的”贡献”——移除它会使整体likelihood下降多少</li>
<li>移除贡献最小的子词（保留一定比例）</li>
<li>重复2-4，直到词汇表达到目标大小</li>
</ol>
<p>Unigram相比BPE有几个独特的优势。首先，它理论上更优雅——每种切分都有明确的概率解释，而不是像BPE那样依赖贪心的频率统计。更重要的是，Unigram可以为同一个词给出多种切分，这使得subword regularization成为可能：训练时按概率采样不同的切分方式作为数据增强，从而增强模型的鲁棒性。</p>
<p><strong>例子</strong>：对于”unigram”，可能的切分包括：</p>
<ul>
<li>[“uni”, “gram”] 概率<span class="math inline">\(p_1\)</span></li>
<li>[“un”, “i”, “gram”] 概率<span class="math inline">\(p_2\)</span></li>
<li>[“u”, “n”, “i”, “g”, “r”, “a”, “m”] 概率<span class="math inline">\(p_3\)</span></li>
</ul>
<p>推理时选择概率最高的切分，训练时可以按概率采样不同切分。</p>
</section>
<section id="sentencepiece语言无关的统一方案" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="sentencepiece语言无关的统一方案"><span class="header-section-number">4.4</span> SentencePiece：语言无关的统一方案</h3>
<p>SentencePiece是Google开发的一个tokenization库，它的关键创新是<strong>直接在原始文本上操作</strong>，不需要预先的词分割。</p>
<p><strong>传统流程的问题</strong>：</p>
<pre><code>原始文本 → 词分割（按空格或分词器）→ 子词分割（BPE/WordPiece）</code></pre>
<p>这个流程对英语工作良好（空格是自然的词边界），但对中文、日文等没有空格的语言不友好——需要先用一个分词器，引入了额外的复杂性和错误来源。</p>
<p><strong>SentencePiece的方案</strong>：</p>
<pre><code>原始文本（包括空格）→ 子词分割</code></pre>
<p>SentencePiece把空格也当作一个普通字符处理，用特殊符号”▁“（U+2581）表示词边界。比如”Hello world”变成”▁Hello▁world”，然后直接在这个字符序列上做BPE或Unigram。</p>
<p>这样，不需要任何语言特定的预处理，同一个算法可以处理任何语言。空格信息被保留在token中（以”▁“的形式），不会丢失。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SentencePiece的tokenization示例</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sentencepiece <span class="im">as</span> spm</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练模型</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>sp <span class="op">=</span> spm.SentencePieceProcessor()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>sp.Load(<span class="st">"model.model"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello world"</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> sp.EncodeAsPieces(text)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出: ['▁Hello', '▁world']</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>text_zh <span class="op">=</span> <span class="st">"你好世界"</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>tokens_zh <span class="op">=</span> sp.EncodeAsPieces(text_zh)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出: ['▁', '你', '好', '世', '界']  # 中文字符通常各自成为token</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="byte-level-bpegpt-2的创新" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="byte-level-bpegpt-2的创新"><span class="header-section-number">4.5</span> Byte-level BPE：GPT-2的创新</h3>
<p>GPT-2引入了一个重要的创新：<strong>Byte-level BPE</strong>。</p>
<p><strong>传统方法的问题</strong>：即使是SentencePiece，也需要定义一个字符集。如果遇到训练时没见过的字符（比如某种稀有语言的Unicode字符），仍然会变成未知token。</p>
<p><strong>Byte-level的解决方案</strong>：不在字符级别操作，而是在<strong>字节级别</strong>操作。任何文本都可以用UTF-8编码为字节序列（0-255），所以基础词汇表只需要256个token。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 任何字符串都可以变成字节序列</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello 你好 🎉"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>bytes_seq <span class="op">=</span> text.encode(<span class="st">'utf-8'</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># b'Hello \xe4\xbd\xa0\xe5\xa5\xbd \xf0\x9f\x8e\x89'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后在这个字节序列上做BPE，学习常见的字节组合。</p>
<p>Byte-level方法的关键优势在于它彻底解决了OOV问题——任何Unicode字符都能编码，无论是emoji、稀有语言、甚至乱码。同时它完全语言无关，不需要任何语言特定的处理，而且词汇表大小可控，从256个基础字节出发，通过合并操作构建到目标大小。</p>
<p>不过这种方法也有潜在代价：多字节字符可能被切开。UTF-8编码中，中文字符通常占3个字节，一个中文字符可能被切分为多个token，导致中文的token效率比英文低。</p>
<p><strong>为什么现代LLM都用Byte-level？</strong></p>
<p>因为它是最”安全”的选择。对于要部署到全世界、处理任意输入的大型语言模型，不能有任何”无法处理”的情况。Byte-level BPE保证了完备性——任何输入都能被tokenize，最坏情况只是效率低一点（字节序列很长）。</p>
<hr>
</section>
</section>
<section id="tokenizer是模型的一部分" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="tokenizer是模型的一部分"><span class="header-section-number">5</span> Tokenizer是模型的一部分</h2>
<p>这是本章最重要的观点：<strong>Tokenizer不是可以随意选择的预处理步骤，它是模型架构的隐藏维度</strong>。</p>
<section id="计算效率序列长度的隐藏成本" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="计算效率序列长度的隐藏成本"><span class="header-section-number">5.1</span> 计算效率：序列长度的隐藏成本</h3>
<p>同样的文本，不同tokenizer产生的序列长度可能相差巨大。</p>
<p>考虑这个英文句子：</p>
<blockquote class="blockquote">
<p>“The transformer architecture revolutionized natural language processing.”</p>
</blockquote>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Tokenizer</th>
<th>Token数量</th>
<th>Token序列</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>词级别</td>
<td>7</td>
<td>[The, transformer, architecture, revolutionized, natural, language, processing]</td>
</tr>
<tr class="even">
<td>GPT-2 (BPE)</td>
<td>7</td>
<td>[The, Ġtransformer, Ġarchitecture, Ġrevolution, ized, Ġnatural, Ġlanguage, Ġprocessing]</td>
</tr>
<tr class="odd">
<td>字符级别</td>
<td>62</td>
<td>[T, h, e, ␣, t, r, a, n, s, …]</td>
</tr>
</tbody>
</table>
<p>对于这个简单句子，差异不大。但看看代码：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_attention(query, key, value):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">@</span> value</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tokenizer</th>
<th>Token数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-2</td>
<td>~40</td>
</tr>
<tr class="even">
<td>字符级别</td>
<td>~150</td>
</tr>
</tbody>
</table>
<p>差异接近4倍！在Transformer中，attention的计算复杂度是<span class="math inline">\(O(n^2)\)</span>，这意味着字符级别的计算量是GPT-2的16倍。</p>
</section>
<section id="多语言公平性隐藏的不平等" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="多语言公平性隐藏的不平等"><span class="header-section-number">5.2</span> 多语言公平性：隐藏的不平等</h3>
<p>这是一个经常被忽视但极其重要的问题。</p>
<p>考虑”人工智能”这个概念：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>语言</th>
<th>文本</th>
<th>GPT-2 Token数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>英语</td>
<td>artificial intelligence</td>
<td>2</td>
</tr>
<tr class="even">
<td>中文</td>
<td>人工智能</td>
<td>4-6（取决于具体tokenizer）</td>
</tr>
<tr class="odd">
<td>日语</td>
<td>人工知能</td>
<td>4-6</td>
</tr>
<tr class="even">
<td>阿拉伯语</td>
<td>الذكاء الاصطناعي</td>
<td>8-12</td>
</tr>
</tbody>
</table>
<div id="fig-multilingual-efficiency" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multilingual-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-3/fig-multilingual-efficiency.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multilingual-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 不同语言表达”Artificial Intelligence”所需的token数量差异（GPT-2 Tokenizer）。英文仅需2个token，而阿拉伯语需要9个，中文和日文也远多于英文。这种不对称源于tokenizer训练语料中英文的主导地位。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Python生成，数据参考 Petrov et al.&nbsp;(2023) “Language Model Tokenizers Introduce Unfairness Between Languages”. <a href="https://arxiv.org/abs/2305.15425">arXiv:2305.15425</a></em></p>
</div>
<p><strong>这意味着什么？</strong> 在相同的上下文窗口（比如4096 tokens）内，模型能处理的中文内容比英文少得多。由于API按token计费，中文用户的调用成本也更高。更深层地说，模型学习中文需要消耗更多的”token带宽”，这可能导致中文能力系统性地弱于英文。</p>
<p>这不是技术实现的细节，而是系统性的不公平。解决这个问题需要在设计tokenizer时就考虑多语言平衡，或者使用专门为某种语言优化的tokenizer。</p>
</section>
<section id="数学和代码能力数字的切分" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="数学和代码能力数字的切分"><span class="header-section-number">5.3</span> 数学和代码能力：数字的切分</h3>
<p>数字的tokenization对模型的算术能力有深远影响。</p>
<p>考虑数字”12345”的几种切分方式：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>切分方式</th>
<th>Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>整体</td>
<td>[“12345”]</td>
</tr>
<tr class="even">
<td>按位</td>
<td>[“1”, “2”, “3”, “4”, “5”]</td>
</tr>
<tr class="odd">
<td>混合</td>
<td>[“123”, “45”]</td>
</tr>
</tbody>
</table>
<p>研究表明，当数字被切分为多个token时，模型很难进行算术运算。</p>
<p><strong>为什么？</strong></p>
<p>考虑加法”12345 + 67890”。如果模型看到的是：</p>
<pre><code>["123", "45", "+", "678", "90"]</code></pre>
<p>它需要： 1. 理解”123”和”45”组合成12345 2. 理解”678”和”90”组合成67890 3. 进行加法 4. 把结果拆分回token</p>
<p>这比直接处理完整数字复杂得多。而且，不同的数字可能有不同的切分方式，模型需要学习无数种组合模式。</p>
<p><strong>现代方法的改进</strong>：</p>
<ul>
<li>一些模型专门把数字处理为单独的token</li>
<li>或者把数字按位切分，统一处理方式</li>
<li>或者在训练数据中加入大量算术例子，让模型学习切分模式</li>
</ul>
</section>
<section id="安全性tokenization边界的攻击" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="安全性tokenization边界的攻击"><span class="header-section-number">5.4</span> 安全性：Tokenization边界的攻击</h3>
<p>Tokenizer的切分边界可能被攻击者利用。</p>
<p><strong>例子：Token边界绕过</strong></p>
<p>假设某个安全过滤器阻止了”dangerous”这个词。但如果tokenizer把它切分为”danger” + “ous”，攻击者可能通过改变输入方式（比如添加空格或特殊字符）来改变切分，绕过过滤。</p>
<p><strong>例子：特殊Token利用</strong></p>
<p>某些tokenizer有特殊token（如<code>&lt;|endoftext|&gt;</code>），如果用户输入能够产生这些token，可能导致意外行为。这是一些prompt injection攻击的基础。</p>
<p><strong>安全启示</strong>：</p>
<ul>
<li>安全过滤应该同时在原始文本和token级别进行</li>
<li>理解tokenizer的行为是理解模型安全边界的前提</li>
<li>特殊token需要特别处理，防止用户输入产生它们</li>
</ul>
<hr>
</section>
</section>
<section id="数据质量清洗去重与污染" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="数据质量清洗去重与污染"><span class="header-section-number">6</span> 数据质量：清洗、去重与污染</h2>
<p>Tokenization是数据进入模型的入口，但数据本身的质量同样关键。“Garbage in, garbage out”在大语言模型时代更加明显。</p>
<section id="数据清洗从噪声中提取信号" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="数据清洗从噪声中提取信号"><span class="header-section-number">6.1</span> 数据清洗：从噪声中提取信号</h3>
<p>互联网上的原始文本充满噪声——HTML标签和JavaScript代码混杂其中，导航菜单、广告、版权声明占据大量篇幅，重复的模板文本（如页脚）反复出现，编码错误导致的乱码以及色情、暴力、仇恨言论也难以避免。</p>
<p>清洗流程通常从<strong>语言过滤</strong>开始，识别文本语言并保留目标语言；然后进行<strong>质量过滤</strong>，基于困惑度、文本长度、特殊字符比例等指标去除低质量文本；接着是<strong>内容过滤</strong>，移除有害内容、广告和模板文本；最后是<strong>格式清理</strong>，统一编码，移除HTML标签，规范化空白字符。</p>
<p>研究表明，在同等数据量下，高质量数据训练的模型显著优于在原始数据上训练的模型。这也是为什么数据清洗是大模型训练的核心竞争力之一。</p>
</section>
<section id="数据去重重复的危害" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="数据去重重复的危害"><span class="header-section-number">6.2</span> 数据去重：重复的危害</h3>
<p>大规模语料中存在大量重复。有些是完全相同的文档被爬取多次，有些是近似重复（如同一新闻的不同来源）。</p>
<p>重复数据的危害是多方面的。最直接的影响是<strong>浪费计算</strong>——模型在相同内容上反复训练。更隐蔽的是，某些被大量复制的内容会获得不成比例的影响力，<strong>偏向特定来源</strong>。模型也可能倾向于<strong>记忆</strong>这些重复内容，而不是学习通用模式。此外，如果测试数据在训练集中出现过，评估结果将不可信——这就是<strong>评估失真</strong>问题。</p>
<p>DeepMind的Chinchilla研究进一步凸显了数据质量的重要性：更大的、更干净的数据集可以让更小的模型达到更大模型的效果。这直接促使了对数据去重的重视。</p>
<p>在实践中，去重方法可以按精度分级：<strong>精确去重</strong>基于hash（如MD5、SHA）检测完全相同的文档；<strong>近似去重</strong>使用MinHash、SimHash等技术识别相似但不完全相同的文档；<strong>N-gram去重</strong>则更为激进，移除包含训练集中大段连续文本的样本。</p>
</section>
<section id="数据污染训练-测试泄漏" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="数据污染训练-测试泄漏"><span class="header-section-number">6.3</span> 数据污染：训练-测试泄漏</h3>
<p>一个更微妙的问题是<strong>数据污染</strong>（data contamination）：测试集的内容出现在训练集中。</p>
<p><strong>为什么这是问题？</strong></p>
<p>如果模型在训练时”见过”测试题和答案，它可能只是在检索记忆，而不是真正的泛化。这会导致benchmark分数虚高，无法反映模型的真实能力。</p>
<p><strong>污染来源</strong>：</p>
<ul>
<li>互联网上有大量benchmark数据集的讨论和解答</li>
<li>某些评测集本身就是从互联网收集的</li>
<li>不同数据集可能有重叠的来源</li>
</ul>
<p><strong>检测方法</strong>：</p>
<ul>
<li>检查测试样本是否在训练集中有高度重叠</li>
<li>使用模型的困惑度——如果模型对测试样本的困惑度异常低，可能是记忆</li>
<li>对测试样本做微小修改（如改变人名），观察模型表现变化</li>
</ul>
<p><strong>案例</strong>：GPT-4的技术报告专门讨论了数据污染问题，并对可能被污染的benchmark结果做了标注。这种透明度是负责任的做法。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">7</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨tokenization的理论基础、边界条件和开放问题</p>
</blockquote>
<section id="理论视角最优分词的条件" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="理论视角最优分词的条件"><span class="header-section-number">7.1</span> 理论视角：最优分词的条件</h3>
<p>从信息论角度，最优的tokenization应该最小化描述数据的总比特数。这等价于最大化：</p>
<p><span class="math display">\[
\mathcal{L} = \sum_{s \in \mathcal{D}} \log P(s)
\]</span></p>
<p>其中<span class="math inline">\(P(s)\)</span>是分词方案下句子<span class="math inline">\(s\)</span>的概率。</p>
<p>对于Unigram模型，这有闭式解。但对于BPE这种贪心算法，只能保证局部最优。</p>
<p><strong>有趣的问题</strong>：BPE的贪心策略与全局最优差多少？实证研究表明，在大多数情况下差异不大，但理论上的保证仍然缺失。</p>
</section>
<section id="边界条件子词分词的假设" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="边界条件子词分词的假设"><span class="header-section-number">7.2</span> 边界条件：子词分词的假设</h3>
<p>子词分词隐含了几个假设：</p>
<p><strong>假设1：词可以有意义地分解</strong></p>
<p>BPE假设常见的子词组合是有意义的。但这并不总是成立——“ing”作为后缀有语法意义，但”qu”只是拼写惯例，没有独立含义。</p>
<p><strong>假设2：统计规律反映语言结构</strong></p>
<p>BPE用频率来决定合并顺序，假设高频模式是语言中重要的单元。但频率受训练语料影响——如果语料偏向某个领域，分词结果也会偏向该领域。</p>
<p><strong>假设3：分词策略语言无关</strong></p>
<p>SentencePiece等工具用相同算法处理所有语言。但不同语言有不同的结构——中文是孤立语（morphology少），土耳其语是黏着语（morphology丰富）。同一算法可能对它们的效果不同。</p>
</section>
<section id="开放研究问题" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">7.3</span> 开放研究问题</h3>
<p>如果你要在tokenization方向做研究，可以考虑这些问题。</p>
<p><strong>问题1：最优词汇表大小是多少？</strong></p>
<p>词汇表大小是一个关键超参数。太小导致序列太长，太大导致稀有token训练不足。最优大小取决于数据量、模型大小、目标语言等因素。是否存在一个理论指导？</p>
<p><strong>问题2：如何实现真正的多语言公平？</strong></p>
<p>现有tokenizer对英文有系统性偏好。如何设计一个对所有语言公平的tokenizer？这可能需要权衡——让英文效率降低来提升其他语言，是否可接受？</p>
<p><strong>问题3：Tokenization与模型能力的关系</strong></p>
<p>数字切分影响算术能力，这已被证实。还有哪些模型能力受tokenization影响？推理、常识、代码理解…？能否设计专门的tokenization策略来增强特定能力？</p>
<p><strong>问题4：端到端学习tokenization</strong></p>
<p>目前tokenization与模型训练是分离的。能否让模型自己学习如何分词？这涉及到离散结构的可微分学习，是一个开放问题。</p>
<hr>
</section>
</section>
<section id="工程实践hugging-face-tokenizers库使用" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="工程实践hugging-face-tokenizers库使用"><span class="header-section-number">8</span> 工程实践：Hugging Face Tokenizers库使用</h2>
<section id="使用预训练tokenizer" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="使用预训练tokenizer"><span class="header-section-number">8.1</span> 使用预训练Tokenizer</h3>
<div id="be89ed54" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载GPT-2的tokenizer（Byte-level BPE）</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 基本tokenization</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, how are you doing today?"</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer.tokenize(text)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', 'Ġdoing', 'Ġtoday', '?']</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：Ġ表示词前的空格（Byte-level BPE的特点）</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 转换为ID</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input IDs: </span><span class="sc">{</span>input_ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 解码回文本</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> tokenizer.decode(input_ids)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoded: </span><span class="sc">{</span>decoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 批量处理，带padding和truncation</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">"Hello world"</span>, <span class="st">"This is a longer sentence for testing"</span>]</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> tokenizer(texts, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">20</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input IDs shape: </span><span class="sc">{</span>encoded[<span class="st">'input_ids'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention mask: </span><span class="sc">{</span>encoded[<span class="st">'attention_mask'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="比较不同tokenizer" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="比较不同tokenizer"><span class="header-section-number">8.2</span> 比较不同Tokenizer</h3>
<div id="8d930de1" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载不同模型的tokenizer</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>tokenizers <span class="op">=</span> {</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"GPT-2"</span>: AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>),</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"BERT"</span>: AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>),</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"T5"</span>: AutoTokenizer.from_pretrained(<span class="st">"t5-base"</span>),</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LLaMA"</span>: AutoTokenizer.from_pretrained(<span class="st">"meta-llama/Llama-2-7b-hf"</span>),</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试文本</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>test_texts <span class="op">=</span> [</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The quick brown fox jumps over the lazy dog."</span>,</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"人工智能正在改变世界。"</span>,</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"def fibonacci(n): return n if n &lt;= 1 else fibonacci(n-1) + fibonacci(n-2)"</span>,</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"12345 + 67890 = 80235"</span>,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Token数量比较：</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> test_texts:</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text: </span><span class="sc">{</span>text[:<span class="dv">50</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, tok <span class="kw">in</span> tokenizers.items():</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> tok.tokenize(text)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="训练自定义tokenizer" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="训练自定义tokenizer"><span class="header-section-number">8.3</span> 训练自定义Tokenizer</h3>
<div id="9b263711" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer, models, trainers, pre_tokenizers, decoders</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建BPE tokenizer</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(models.BPE())</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置预分词器（按空格和标点分割）</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>tokenizer.pre_tokenizer <span class="op">=</span> pre_tokenizers.ByteLevel(add_prefix_space<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置解码器</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>tokenizer.decoder <span class="op">=</span> decoders.ByteLevel()</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备训练器</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> trainers.BpeTrainer(</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    min_frequency<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    special_tokens<span class="op">=</span>[<span class="st">"&lt;pad&gt;"</span>, <span class="st">"&lt;unk&gt;"</span>, <span class="st">"&lt;s&gt;"</span>, <span class="st">"&lt;/s&gt;"</span>, <span class="st">"&lt;mask&gt;"</span>]</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练（从文件或迭代器）</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>files <span class="op">=</span> [<span class="st">"train_data.txt"</span>]</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>tokenizer.train(files, trainer)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>tokenizer.save(<span class="st">"my_tokenizer.json"</span>)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载并使用</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>loaded_tokenizer <span class="op">=</span> Tokenizer.from_file(<span class="st">"my_tokenizer.json"</span>)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> loaded_tokenizer.encode(<span class="st">"Hello, world!"</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>output<span class="sc">.</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"IDs: </span><span class="sc">{</span>output<span class="sc">.</span>ids<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="分析tokenization效率" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="分析tokenization效率"><span class="header-section-number">8.4</span> 分析Tokenization效率</h3>
<div id="8db2e67c" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_tokenizer(tokenizer, texts):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""分析tokenizer在给定文本上的效率"""</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    total_chars <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(t) <span class="cf">for</span> t <span class="kw">in</span> texts)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(tokenizer.tokenize(t)) <span class="cf">for</span> t <span class="kw">in</span> texts)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 字符/token比率（越高越高效）</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    ratio <span class="op">=</span> total_chars <span class="op">/</span> total_tokens</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Token长度分布</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    all_tokens <span class="op">=</span> []</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> texts:</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        all_tokens.extend(tokenizer.tokenize(t))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    token_lengths <span class="op">=</span> [<span class="bu">len</span>(t.replace(<span class="st">'Ġ'</span>, <span class="st">''</span>).replace(<span class="st">'▁'</span>, <span class="st">''</span>)) <span class="cf">for</span> t <span class="kw">in</span> all_tokens]</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total characters: </span><span class="sc">{</span>total_chars<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total tokens: </span><span class="sc">{</span>total_tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Characters per token: </span><span class="sc">{</span>ratio<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token length distribution:"</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Mean: </span><span class="sc">{</span>np<span class="sc">.</span>mean(token_lengths)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Median: </span><span class="sc">{</span>np<span class="sc">.</span>median(token_lengths)<span class="sc">:.0f}</span><span class="ss">"</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Min: </span><span class="sc">{</span><span class="bu">min</span>(token_lengths)<span class="sc">}</span><span class="ss">, Max: </span><span class="sc">{</span><span class="bu">max</span>(token_lengths)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 最常见的token</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    token_counts <span class="op">=</span> Counter(all_tokens)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top 10 most common tokens:"</span>)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token, count <span class="kw">in</span> token_counts.most_common(<span class="dv">10</span>):</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">repr</span>(token)<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>sample_texts <span class="op">=</span> [</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Machine learning is a subset of artificial intelligence."</span>,</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep neural networks have revolutionized natural language processing."</span>,</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Transformers use self-attention mechanisms to process sequential data."</span>,</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>analyze_tokenizer(tokenizer, sample_texts)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</h2>
<section id="tokenization的固有局限" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="tokenization的固有局限"><span class="header-section-number">9.1</span> Tokenization的固有局限</h3>
<p>即使是最先进的子词tokenizer，也有一些固有的局限性。</p>
<p><strong>语言偏见无法完全消除</strong>。Tokenizer是在特定语料上训练的，必然反映该语料的语言分布。如果训练语料以英文为主，tokenizer就会对英文优化。这不是算法的问题，而是数据的问题——真正的多语言公平需要在数据收集阶段就考虑。</p>
<p><strong>分词边界可能破坏语义</strong>。无论BPE还是Unigram，都是基于统计的方法，不理解语言的语义结构。“unhappiness”可能被切分为”un”+“happiness”（语义上合理），也可能被切分为”unha”+“ppiness”（纯粹基于频率）。后者对模型学习不利。</p>
<p><strong>Tokenization与模型训练的割裂</strong>。目前的做法是先确定tokenizer，然后固定它训练模型。但最优的tokenization可能依赖于下游任务和模型架构。这种割裂是否限制了模型的上限？</p>
</section>
<section id="下一章的铺垫" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="下一章的铺垫"><span class="header-section-number">9.2</span> 下一章的铺垫</h3>
<p>我们已经知道如何把文本转换为token序列，也知道如何用词向量表示每个token。但一个句子不只是token的无序集合——<strong>词序蕴含着关键的语义信息</strong>。</p>
<p>“狗咬人”和”人咬狗”有完全不同的含义，但如果我们只是把词向量加起来或取平均，就会丢失这种区分。我们需要一种方法来建模<strong>序列结构</strong>，捕获词与词之间的顺序关系和长距离依赖。</p>
<p>这正是下一章要讨论的问题。循环神经网络（RNN）通过”记忆”之前看到的内容，让模型能够处理变长序列。LSTM和GRU通过门控机制解决了梯度消失问题。但它们也有自己的局限——无法并行、长距离依赖仍然困难——这些局限最终导向了Attention机制和Transformer的诞生。</p>
<blockquote class="blockquote">
<p>下一章预告：第4章将介绍循环神经网络，理解它如何通过”时间上的权重共享”来处理序列，以及门控机制如何让信息在长序列中存活。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">10</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</h3>
<p>这一章我们深入探讨了一个常被忽视但极其重要的问题：<strong>如何将文本切分为模型可处理的单元</strong>。</p>
<p>核心洞察是：<strong>Tokenizer不是预处理工具，而是模型架构的隐藏维度</strong>。它影响：</p>
<ul>
<li><strong>计算效率</strong>：序列长度直接决定计算成本</li>
<li><strong>多语言公平性</strong>：不同语言的token效率差异导致系统性不平等</li>
<li><strong>模型能力</strong>：数字、代码的切分方式影响相应的推理能力</li>
<li><strong>安全性</strong>：tokenization边界可能被攻击者利用</li>
</ul>
<p>我们学习了三种基本的tokenization策略：</p>
<ul>
<li><strong>词级别</strong>：语义清晰但OOV严重、词汇表爆炸</li>
<li><strong>字符级别</strong>：没有OOV但序列太长、语义碎片化</li>
<li><strong>子词级别</strong>：在两者之间找到平衡，是现代NLP的标准选择</li>
</ul>
<p>子词分词的核心算法包括：</p>
<ul>
<li><strong>BPE</strong>：贪心地合并最频繁的相邻对，从字符构建词汇表</li>
<li><strong>WordPiece</strong>：类似BPE但用likelihood改进选择合并对</li>
<li><strong>Unigram</strong>：概率视角，从大词汇表剪枝到目标大小</li>
<li><strong>Byte-level BPE</strong>：在字节级别操作，彻底解决未知字符问题</li>
</ul>
<p>最后，我们讨论了数据质量的重要性——清洗、去重、避免污染是大模型训练的关键环节。</p>
</section>
<section id="关键概念速查" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="关键概念速查"><span class="header-section-number">10.2</span> 关键概念速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>概念</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BPE</td>
<td>Byte Pair Encoding，迭代合并最频繁token对</td>
</tr>
<tr class="even">
<td>WordPiece</td>
<td>Google的子词算法，用于BERT</td>
</tr>
<tr class="odd">
<td>Unigram LM</td>
<td>概率视角的子词分词，支持多种切分</td>
</tr>
<tr class="even">
<td>SentencePiece</td>
<td>语言无关的tokenization库</td>
</tr>
<tr class="odd">
<td>Byte-level</td>
<td>在字节级别操作，彻底解决OOV</td>
</tr>
<tr class="even">
<td>OOV</td>
<td>Out-of-Vocabulary，无法处理的未知词</td>
</tr>
<tr class="odd">
<td>数据污染</td>
<td>测试集内容出现在训练集中</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">10.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么说”Tokenizer是模型架构的隐藏维度”？试举三个具体例子说明tokenizer选择如何影响模型行为。</p></li>
<li><p><strong>[算法实践]</strong> 手动执行BPE算法。给定语料 {“ab”: 5, “abc”: 3, “abcd”: 2}，执行3次合并，写出每一步的词汇表和合并规则。</p></li>
<li><p><strong>[工程实践]</strong> 使用Hugging Face的tokenizers库，比较GPT-2、BERT、T5的tokenizer在中文文本上的效率。计算每个tokenizer的”字符/token比率”，并分析差异的原因。</p></li>
<li><p><strong>[研究思考]</strong> 设计一个实验来量化tokenization对模型算术能力的影响。你会如何控制变量？需要什么样的数据集？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">11</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</h3>
<ul>
<li><strong><a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a> (Sennrich et al., 2016)</strong>：BPE在NLP中的应用
<ul>
<li><strong>arXiv</strong>: 1508.07909</li>
<li><strong>重点读</strong>：
<ul>
<li>Section 3.2：BPE算法描述和Algorithm 1伪代码</li>
<li>Table 1：不同分词方法的统计对比</li>
<li>Section 5.2：翻译示例分析</li>
</ul></li>
<li><strong>可跳过</strong>：Section 4的实验细节（除非你要做机器翻译）</li>
<li><strong>核心贡献</strong>：将1994年的数据压缩算法引入NLP，解决OOV问题</li>
<li><strong>官方代码</strong>：<a href="https://github.com/rsennrich/subword-nmt">github.com/rsennrich/subword-nmt</a></li>
</ul></li>
<li><strong><a href="https://arxiv.org/abs/1808.06226">SentencePiece: A simple and language independent subword tokenizer</a> (Kudo &amp; Richardson, 2018)</strong>
<ul>
<li><strong>arXiv</strong>: 1808.06226</li>
<li><strong>重点读</strong>：语言无关设计的动机——为什么不需要预分词</li>
</ul></li>
</ul>
</section>
<section id="理论分析" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="理论分析"><span class="header-section-number">11.2</span> 理论分析</h3>
<ul>
<li><strong><a href="https://arxiv.org/abs/1804.10959">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a> (Kudo, 2018)</strong>：Unigram LM的原始论文
<ul>
<li><strong>arXiv</strong>: 1804.10959</li>
<li>概率视角的子词分词，支持训练时采样不同切分</li>
<li><strong>重点读</strong>：Section 3的Unigram Language Model formulation</li>
</ul></li>
</ul>
</section>
<section id="实证研究" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="实证研究"><span class="header-section-number">11.3</span> 实证研究</h3>
<ul>
<li><strong>How Good is Your Tokenizer? (Rust et al., 2021)</strong>：多语言tokenizer的系统性评估
<ul>
<li>揭示了英语偏见的严重程度</li>
</ul></li>
<li><strong>Tokenizer Choice Matters: Downstream Tasks Benefit from Task-Specific Tokenization (2023)</strong>
<ul>
<li>不同任务可能需要不同的tokenization策略</li>
</ul></li>
</ul>
</section>
<section id="工具和资源" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="工具和资源"><span class="header-section-number">11.4</span> 工具和资源</h3>
<ul>
<li><strong>Hugging Face Tokenizers库</strong>：高效的tokenizer训练和使用</li>
<li><strong>SentencePiece官方库</strong>：Google的tokenization工具</li>
<li><strong>tiktoken</strong>：OpenAI的BPE实现，用于GPT系列</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">12</span> 历史注脚</h2>
<p>BPE（Byte Pair Encoding）的历史颇为有趣。它最初是Philip Gage在1994年提出的<strong>数据压缩算法</strong>，与NLP毫无关系。算法思想很简单：找到数据中最频繁的字节对，用一个新字节替换它们，重复这个过程直到无法进一步压缩。</p>
<p>2016年，Edinburgh大学的Rico Sennrich等人在研究机器翻译时遇到了一个问题：神经翻译模型无法处理罕见词。他们想到了BPE——既然BPE能够压缩数据，它应该也能产生一种”压缩”的文本表示，其中常见模式被合并为单一单元。</p>
<p>这个跨领域的知识迁移非常成功。BPE不仅解决了OOV问题，还显著提升了翻译质量，特别是在形态丰富的语言上。论文发表后迅速被广泛采用，成为现代NLP的标准组件。</p>
<p>一个有趣的细节是，Sennrich等人的论文最初投稿时被审稿人质疑”太简单”。但正如Word2Vec的故事，最有影响力的工作往往不是最复杂的，而是找到了正确的简化。BPE的成功说明，有时候领域外的老技术，在新问题上可能有意想不到的效果。</p>
<p>今天，几乎所有大型语言模型都使用BPE或其变体。从GPT到LLaMA，从Claude到Gemini，BPE（或其byte-level版本）是它们的共同基础。一个1994年的数据压缩算法，成为了2020年代AI革命的基石——这本身就是技术史上的一个美丽故事。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第3章：Tokenization与数据基础"</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "被低估的基础设施：从文本到模型输入的艺术"</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-25"</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Tokenization, BPE, WordPiece, SentencePiece, Data]</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [分词, 子词, Byte-level BPE, Unigram LM, 数据清洗, 多语言]</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Tokenizer不是预处理工具，它是模型架构的隐藏维度：从词级别到子词方法的演进，以及分词策略对模型能力的深远影响。"</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-3/fig-bpe-merge-process.png"</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心论点**：Tokenizer不是预处理工具，它是模型架构的隐藏维度。</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2016 </span><span class="pp">|</span><span class="at"> Sennrich et al. (BPE for NMT) </span><span class="pp">|</span><span class="at"> 从词级别到子词的范式转变</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>上一章我们学习了词向量——如何用稠密向量表示词的语义。Word2Vec、GloVe、FastText让我们第一次能够在数学上捕获"cat"和"dog"的相似性。这是表示学习的巨大进步。</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>但我们回避了一个看似简单却极其重要的问题：**这些"词"是从哪里来的？**</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>回顾FastText的出现动机：Word2Vec无法处理训练时没见过的词（OOV问题）。FastText通过将词分解为子词（character n-grams）来解决这个问题——即使"unfriendliness"没出现过，只要它的子词（"un"、"friend"、"ness"）出现过，就能组合出一个合理的向量。</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>FastText的成功暗示了一个深刻的洞察：**"词"并不是文本处理的最佳单位**。</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>这引出了本章的核心问题：在把文本送入模型之前，我们应该如何切分它？按词切？按字符切？还是有更聪明的方法？这个看似是"预处理"的问题，实际上深刻地影响着模型的能力、效率和公平性。</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：Tokenizer决定了模型"看到"什么。它不是可以随意选择的预处理步骤，而是模型架构的隐藏维度——影响计算效率、多语言公平性、数学推理能力，甚至安全性。</span></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### 什么是一个"词"？</span></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>这个问题比看起来复杂得多。</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>对于英语，我们可能直觉地认为"词就是空格分隔的单位"。但仔细想想：</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"don't"是一个词还是两个？</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"New York"是一个词还是两个？</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"ice-cream"呢？</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"it's"和"its"在tokenization层面应该相同还是不同？</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>中文更麻烦。中文没有空格分隔词，需要专门的**分词**（word segmentation）算法。考虑这个句子：</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "研究生命的起源"</span></span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>这应该切分为"研究/生命/的/起源"还是"研究生/命/的/起源"？没有上下文，无法确定。中文分词本身就是一个复杂的NLP任务，需要模型来完成——但我们现在讨论的是如何把文本送入模型，这形成了一个先有鸡还是先有蛋的悖论。</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>再看形态丰富的语言，如德语、芬兰语、土耳其语。德语允许复合词任意组合，"Donaudampfschifffahrtsgesellschaftskapitän"（多瑙河轮船公司船长）是一个合法的德语词。土耳其语是黏着语，一个词可以通过添加后缀表达复杂的语法关系。如果坚持用词作为基本单位，词汇表会爆炸式增长，而且每个词的训练样本会稀少到无法学习。</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的精确定义</span></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>让我们精确地定义Tokenization要解决的问题。</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>给定一个字符串$s$，我们需要找到一个函数$\text{tokenize}: \Sigma^* \rightarrow \mathcal{V}^*$，把任意字符串映射为一个token序列，其中$\mathcal{V}$是预定义的词汇表（vocabulary）。</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>这个函数需要满足几个关键性质：**完备性**——任何输入字符串都能被tokenize，不能有"无法处理"的情况；**确定性**——相同的输入总是产生相同的输出；以及理想情况下的**可逆性**——从token序列能够恢复原始字符串。</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a>除了这些基本约束，我们还希望函数在多个维度上表现良好。**词汇表大小**不能太大（否则embedding矩阵爆炸），也不能太小（否则序列太长）。产生的**序列长度**要合理（否则计算成本高，超出上下文窗口）。理想情况下，每个token应该有明确的**语义完整性**。此外，不同语言的相同内容，token数量应该大致相当——即**跨语言公平性**。</span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>这些目标之间存在张力。词汇表越小，序列越长；词汇表越大，稀有token越多。找到正确的平衡是tokenization的核心挑战。</span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么这个问题重要？</span></span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>你可能会问：这真的那么重要吗？不就是切分文本吗？</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a>让我用几个具体的例子说明tokenizer选择对模型的深远影响。</span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a>**计算效率**。同样的文本，不同tokenizer产生的序列长度可能相差2-3倍。如果你的模型有4096的上下文窗口，一个高效的tokenizer可以让你处理的实际文本量翻倍。Transformer的计算复杂度是$O(n^2)$（$n$是序列长度），序列长度减半意味着计算量减少75%。</span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>**多语言公平性**。同样的语义内容，英文可能只需要1个token，而中文可能需要2-3个token，日文可能需要更多。这意味着在相同的上下文窗口内，模型能处理的中文内容比英文少。这不是技术实现的细节，而是系统性的不公平。</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a>**数学和代码能力**。数字"380"可能被切分为单个token"380"，也可能被切分为"3"、"8"、"0"三个token。研究表明，后者会严重损害模型的算术推理能力——模型很难理解"380"和"381"只差1。类似地，代码中的缩进、变量名、运算符的切分方式直接影响模型理解代码的能力。</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a>**安全性**。Tokenizer的切分边界可能被攻击者利用。某些prompt injection攻击正是利用了特定token序列的特殊行为。理解tokenizer是理解模型安全边界的前提。</span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### 三种基本策略</span></span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a>面对"如何切分文本"这个问题，有三种基本策略，它们代表了不同的权衡。</span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-98"><a href="#cb22-98" aria-hidden="true" tabindex="-1"></a>**策略一：词级别（Word-level）**</span>
<span id="cb22-99"><a href="#cb22-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-100"><a href="#cb22-100" aria-hidden="true" tabindex="-1"></a>最直观的方法是按词切分。建立一个词汇表，包含训练语料中出现的所有词（或最常见的N个词），然后把每个词映射到一个token。</span>
<span id="cb22-101"><a href="#cb22-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-102"><a href="#cb22-102" aria-hidden="true" tabindex="-1"></a>这种方法的优点是每个token都有明确的语义——"cat"就是猫，"running"就是正在跑。但缺点也很明显：词汇表庞大（英语常用词就有几十万），OOV问题严重（任何不在词汇表中的词都无法处理），形态变化浪费空间（"run"、"runs"、"running"、"ran"是四个独立的token）。</span>
<span id="cb22-103"><a href="#cb22-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-104"><a href="#cb22-104" aria-hidden="true" tabindex="-1"></a>**策略二：字符级别（Character-level）**</span>
<span id="cb22-105"><a href="#cb22-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-106"><a href="#cb22-106" aria-hidden="true" tabindex="-1"></a>走向另一个极端：按字符切分。词汇表只需要包含所有可能的字符（英语只需要不到100个），OOV问题彻底消失。</span>
<span id="cb22-107"><a href="#cb22-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-108"><a href="#cb22-108" aria-hidden="true" tabindex="-1"></a>但代价是序列变得极长。"tokenization"这个词变成了13个token，计算成本大增。更糟糕的是，字符本身几乎没有语义——"t"、"o"、"k"...模型需要从这些原子构建词的含义，这是一个巨大的学习负担。</span>
<span id="cb22-109"><a href="#cb22-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-110"><a href="#cb22-110" aria-hidden="true" tabindex="-1"></a>**策略三：子词级别（Subword-level）**</span>
<span id="cb22-111"><a href="#cb22-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-112"><a href="#cb22-112" aria-hidden="true" tabindex="-1"></a>能不能找到一个中间地带？这正是现代NLP的选择：**子词分词**（subword tokenization）。</span>
<span id="cb22-113"><a href="#cb22-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-114"><a href="#cb22-114" aria-hidden="true" tabindex="-1"></a>核心思想是：常见的词保持完整，罕见的词分解为更小的、有意义的单元。比如：</span>
<span id="cb22-115"><a href="#cb22-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-116"><a href="#cb22-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"running" → "running"（常见，保持完整）</span>
<span id="cb22-117"><a href="#cb22-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"tokenization" → "token" + "ization"（罕见，分解为常见子词）</span>
<span id="cb22-118"><a href="#cb22-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"unfriendliness" → "un" + "friend" + "li" + "ness"</span>
<span id="cb22-119"><a href="#cb22-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-120"><a href="#cb22-120" aria-hidden="true" tabindex="-1"></a>这样，词汇表大小可控（通常32K-64K），序列长度合理，而且任何词都能被切分（最坏情况退化到字符级别）。子词还经常具有语义或语法含义——"un-"表示否定，"-ing"表示进行时，"-ness"表示名词化。</span>
<span id="cb22-121"><a href="#cb22-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-122"><a href="#cb22-122" aria-hidden="true" tabindex="-1"></a><span class="al">![三种Tokenization策略对比：以"tokenization"为例，词级别产生1个token，字符级别产生12个token，而子词（BPE）只需要2个token——"token"和"ization"。子词方法在词汇表大小和序列长度之间找到了最佳平衡。](figures/chapter-3/fig-tokenization-strategies.png)</span>{#fig-tokenization-strategies}</span>
<span id="cb22-123"><a href="#cb22-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-124"><a href="#cb22-124" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb22-125"><a href="#cb22-125" aria-hidden="true" tabindex="-1"></a>*Python生成，概念参考 Sennrich et al. (2016) "Neural Machine Translation of Rare Words with Subword Units". [arXiv:1508.07909](https://arxiv.org/abs/1508.07909)*</span>
<span id="cb22-126"><a href="#cb22-126" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-127"><a href="#cb22-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-128"><a href="#cb22-128" aria-hidden="true" tabindex="-1"></a><span class="fu">### 子词分词的直觉：数据压缩</span></span>
<span id="cb22-129"><a href="#cb22-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-130"><a href="#cb22-130" aria-hidden="true" tabindex="-1"></a>理解子词分词的最佳方式是把它类比为**数据压缩**。</span>
<span id="cb22-131"><a href="#cb22-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-132"><a href="#cb22-132" aria-hidden="true" tabindex="-1"></a>想象你要设计一个编码方案，用最少的符号表示大量文本。直觉上，你会：</span>
<span id="cb22-133"><a href="#cb22-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-134"><a href="#cb22-134" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>给最常见的模式分配短码</span>
<span id="cb22-135"><a href="#cb22-135" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>让不常见的模式用常见模式的组合表示</span>
<span id="cb22-136"><a href="#cb22-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-137"><a href="#cb22-137" aria-hidden="true" tabindex="-1"></a>这正是BPE（Byte Pair Encoding）的核心思想，而BPE最初就是一种数据压缩算法。</span>
<span id="cb22-138"><a href="#cb22-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-139"><a href="#cb22-139" aria-hidden="true" tabindex="-1"></a>考虑一个简单的例子。假设你的文本中经常出现"th"、"the"、"ing"这些模式。如果每次都用两个或三个字符表示它们，很浪费。不如给"th"一个专门的符号（比如"θ"），给"the"另一个符号（比如"Θ"）。这样，原本需要3个符号的"the"只需要1个符号。</span>
<span id="cb22-140"><a href="#cb22-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-141"><a href="#cb22-141" aria-hidden="true" tabindex="-1"></a>子词分词做的就是这件事：自动发现文本中的高频模式，给它们分配token。最终的词汇表是数据驱动的，反映了训练语料的统计规律。</span>
<span id="cb22-142"><a href="#cb22-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-143"><a href="#cb22-143" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-144"><a href="#cb22-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-145"><a href="#cb22-145" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb22-146"><a href="#cb22-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-147"><a href="#cb22-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### BPE算法：从数据压缩到NLP</span></span>
<span id="cb22-148"><a href="#cb22-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-149"><a href="#cb22-149" aria-hidden="true" tabindex="-1"></a>BPE（Byte Pair Encoding）是目前最广泛使用的子词分词算法之一，被GPT系列、RoBERTa等模型采用。</span>
<span id="cb22-150"><a href="#cb22-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-151"><a href="#cb22-151" aria-hidden="true" tabindex="-1"></a>**算法思想**：从字符开始，迭代地合并最频繁的相邻token对，直到达到目标词汇表大小。</span>
<span id="cb22-152"><a href="#cb22-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-153"><a href="#cb22-153" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的例子来理解这个过程。</span>
<span id="cb22-154"><a href="#cb22-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-155"><a href="#cb22-155" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：BPE词汇表构建</span></span>
<span id="cb22-156"><a href="#cb22-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-157"><a href="#cb22-157" aria-hidden="true" tabindex="-1"></a>**设定**：训练语料包含以下词及其频率：</span>
<span id="cb22-158"><a href="#cb22-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-159"><a href="#cb22-159" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词 <span class="pp">|</span> 频率 <span class="pp">|</span></span>
<span id="cb22-160"><a href="#cb22-160" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|</span></span>
<span id="cb22-161"><a href="#cb22-161" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> low <span class="pp">|</span> 5 <span class="pp">|</span></span>
<span id="cb22-162"><a href="#cb22-162" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> lower <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb22-163"><a href="#cb22-163" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> newest <span class="pp">|</span> 6 <span class="pp">|</span></span>
<span id="cb22-164"><a href="#cb22-164" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> widest <span class="pp">|</span> 3 <span class="pp">|</span></span>
<span id="cb22-165"><a href="#cb22-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-166"><a href="#cb22-166" aria-hidden="true" tabindex="-1"></a>**Step 0：初始化**</span>
<span id="cb22-167"><a href="#cb22-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-168"><a href="#cb22-168" aria-hidden="true" tabindex="-1"></a>首先，我们把每个词表示为字符序列，并添加一个特殊的词尾标记"_"（表示词边界）：</span>
<span id="cb22-169"><a href="#cb22-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-170"><a href="#cb22-170" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-171"><a href="#cb22-171" aria-hidden="true" tabindex="-1"></a><span class="in">l o w _     : 5</span></span>
<span id="cb22-172"><a href="#cb22-172" aria-hidden="true" tabindex="-1"></a><span class="in">l o w e r _ : 2</span></span>
<span id="cb22-173"><a href="#cb22-173" aria-hidden="true" tabindex="-1"></a><span class="in">n e w e s t _ : 6</span></span>
<span id="cb22-174"><a href="#cb22-174" aria-hidden="true" tabindex="-1"></a><span class="in">w i d e s t _ : 3</span></span>
<span id="cb22-175"><a href="#cb22-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-176"><a href="#cb22-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-177"><a href="#cb22-177" aria-hidden="true" tabindex="-1"></a>初始词汇表是所有字符：<span class="in">`{l, o, w, e, r, n, s, t, i, d, _}`</span>，共11个token。</span>
<span id="cb22-178"><a href="#cb22-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-179"><a href="#cb22-179" aria-hidden="true" tabindex="-1"></a>**Step 1：统计相邻token对频率**</span>
<span id="cb22-180"><a href="#cb22-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-181"><a href="#cb22-181" aria-hidden="true" tabindex="-1"></a>遍历所有词，统计相邻token对出现的总次数：</span>
<span id="cb22-182"><a href="#cb22-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-183"><a href="#cb22-183" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Token对 <span class="pp">|</span> 频率计算 <span class="pp">|</span> 总频率 <span class="pp">|</span></span>
<span id="cb22-184"><a href="#cb22-184" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|</span></span>
<span id="cb22-185"><a href="#cb22-185" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (l, o) <span class="pp">|</span> 5 + 2 = 7 <span class="pp">|</span> 7 <span class="pp">|</span></span>
<span id="cb22-186"><a href="#cb22-186" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (o, w) <span class="pp">|</span> 5 + 2 = 7 <span class="pp">|</span> 7 <span class="pp">|</span></span>
<span id="cb22-187"><a href="#cb22-187" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (w, _) <span class="pp">|</span> 5 <span class="pp">|</span> 5 <span class="pp">|</span></span>
<span id="cb22-188"><a href="#cb22-188" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (w, e) <span class="pp">|</span> 2 <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb22-189"><a href="#cb22-189" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (e, r) <span class="pp">|</span> 2 <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb22-190"><a href="#cb22-190" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (r, _) <span class="pp">|</span> 2 <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb22-191"><a href="#cb22-191" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (n, e) <span class="pp">|</span> 6 <span class="pp">|</span> 6 <span class="pp">|</span></span>
<span id="cb22-192"><a href="#cb22-192" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (e, w) <span class="pp">|</span> 6 <span class="pp">|</span> 6 <span class="pp">|</span></span>
<span id="cb22-193"><a href="#cb22-193" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (e, s) <span class="pp">|</span> 6 + 3 = 9 <span class="pp">|</span> **9** <span class="pp">|</span></span>
<span id="cb22-194"><a href="#cb22-194" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (s, t) <span class="pp">|</span> 6 + 3 = 9 <span class="pp">|</span> **9** <span class="pp">|</span></span>
<span id="cb22-195"><a href="#cb22-195" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (t, _) <span class="pp">|</span> 6 + 3 = 9 <span class="pp">|</span> **9** <span class="pp">|</span></span>
<span id="cb22-196"><a href="#cb22-196" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (w, i) <span class="pp">|</span> 3 <span class="pp">|</span> 3 <span class="pp">|</span></span>
<span id="cb22-197"><a href="#cb22-197" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (i, d) <span class="pp">|</span> 3 <span class="pp">|</span> 3 <span class="pp">|</span></span>
<span id="cb22-198"><a href="#cb22-198" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (d, e) <span class="pp">|</span> 3 <span class="pp">|</span> 3 <span class="pp">|</span></span>
<span id="cb22-199"><a href="#cb22-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-200"><a href="#cb22-200" aria-hidden="true" tabindex="-1"></a>最高频的对是 (e, s)、(s, t)、(t, _)，都是9次。我们选择第一个遇到的：**(e, s)**。</span>
<span id="cb22-201"><a href="#cb22-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-202"><a href="#cb22-202" aria-hidden="true" tabindex="-1"></a>**Step 2：合并最频繁的对**</span>
<span id="cb22-203"><a href="#cb22-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-204"><a href="#cb22-204" aria-hidden="true" tabindex="-1"></a>创建新token "es"，替换所有出现的 (e, s)：</span>
<span id="cb22-205"><a href="#cb22-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-206"><a href="#cb22-206" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-207"><a href="#cb22-207" aria-hidden="true" tabindex="-1"></a><span class="in">l o w _     : 5</span></span>
<span id="cb22-208"><a href="#cb22-208" aria-hidden="true" tabindex="-1"></a><span class="in">l o w e r _ : 2</span></span>
<span id="cb22-209"><a href="#cb22-209" aria-hidden="true" tabindex="-1"></a><span class="in">n e w es t _ : 6</span></span>
<span id="cb22-210"><a href="#cb22-210" aria-hidden="true" tabindex="-1"></a><span class="in">w i d es t _ : 3</span></span>
<span id="cb22-211"><a href="#cb22-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-212"><a href="#cb22-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-213"><a href="#cb22-213" aria-hidden="true" tabindex="-1"></a>词汇表变为：<span class="in">`{l, o, w, e, r, n, s, t, i, d, _, es}`</span>，12个token。</span>
<span id="cb22-214"><a href="#cb22-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-215"><a href="#cb22-215" aria-hidden="true" tabindex="-1"></a>**Step 3：重复统计和合并**</span>
<span id="cb22-216"><a href="#cb22-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-217"><a href="#cb22-217" aria-hidden="true" tabindex="-1"></a>继续统计相邻对频率：</span>
<span id="cb22-218"><a href="#cb22-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-219"><a href="#cb22-219" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Token对 <span class="pp">|</span> 总频率 <span class="pp">|</span></span>
<span id="cb22-220"><a href="#cb22-220" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|</span></span>
<span id="cb22-221"><a href="#cb22-221" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (es, t) <span class="pp">|</span> 6 + 3 = **9** <span class="pp">|</span></span>
<span id="cb22-222"><a href="#cb22-222" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (l, o) <span class="pp">|</span> 7 <span class="pp">|</span></span>
<span id="cb22-223"><a href="#cb22-223" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (o, w) <span class="pp">|</span> 7 <span class="pp">|</span></span>
<span id="cb22-224"><a href="#cb22-224" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ... <span class="pp">|</span> ... <span class="pp">|</span></span>
<span id="cb22-225"><a href="#cb22-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-226"><a href="#cb22-226" aria-hidden="true" tabindex="-1"></a>最高频：**(es, t) = 9**，合并为 "est"：</span>
<span id="cb22-227"><a href="#cb22-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-228"><a href="#cb22-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-229"><a href="#cb22-229" aria-hidden="true" tabindex="-1"></a><span class="in">l o w _     : 5</span></span>
<span id="cb22-230"><a href="#cb22-230" aria-hidden="true" tabindex="-1"></a><span class="in">l o w e r _ : 2</span></span>
<span id="cb22-231"><a href="#cb22-231" aria-hidden="true" tabindex="-1"></a><span class="in">n e w est _ : 6</span></span>
<span id="cb22-232"><a href="#cb22-232" aria-hidden="true" tabindex="-1"></a><span class="in">w i d est _ : 3</span></span>
<span id="cb22-233"><a href="#cb22-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-234"><a href="#cb22-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-235"><a href="#cb22-235" aria-hidden="true" tabindex="-1"></a>词汇表：<span class="in">`{l, o, w, e, r, n, s, t, i, d, _, es, est}`</span>，13个token。</span>
<span id="cb22-236"><a href="#cb22-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-237"><a href="#cb22-237" aria-hidden="true" tabindex="-1"></a>**Step 4：继续合并**</span>
<span id="cb22-238"><a href="#cb22-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-239"><a href="#cb22-239" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Token对 <span class="pp">|</span> 总频率 <span class="pp">|</span></span>
<span id="cb22-240"><a href="#cb22-240" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|</span></span>
<span id="cb22-241"><a href="#cb22-241" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (est, _) <span class="pp">|</span> 6 + 3 = **9** <span class="pp">|</span></span>
<span id="cb22-242"><a href="#cb22-242" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (l, o) <span class="pp">|</span> 7 <span class="pp">|</span></span>
<span id="cb22-243"><a href="#cb22-243" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> (o, w) <span class="pp">|</span> 7 <span class="pp">|</span></span>
<span id="cb22-244"><a href="#cb22-244" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ... <span class="pp">|</span> ... <span class="pp">|</span></span>
<span id="cb22-245"><a href="#cb22-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-246"><a href="#cb22-246" aria-hidden="true" tabindex="-1"></a>合并 **(est, _)** 为 "est_"：</span>
<span id="cb22-247"><a href="#cb22-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-248"><a href="#cb22-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-249"><a href="#cb22-249" aria-hidden="true" tabindex="-1"></a><span class="in">l o w _     : 5</span></span>
<span id="cb22-250"><a href="#cb22-250" aria-hidden="true" tabindex="-1"></a><span class="in">l o w e r _ : 2</span></span>
<span id="cb22-251"><a href="#cb22-251" aria-hidden="true" tabindex="-1"></a><span class="in">n e w est_ : 6</span></span>
<span id="cb22-252"><a href="#cb22-252" aria-hidden="true" tabindex="-1"></a><span class="in">w i d est_ : 3</span></span>
<span id="cb22-253"><a href="#cb22-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-254"><a href="#cb22-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-255"><a href="#cb22-255" aria-hidden="true" tabindex="-1"></a>**Step 5-7：继续迭代**</span>
<span id="cb22-256"><a href="#cb22-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-257"><a href="#cb22-257" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 迭代 <span class="pp">|</span> 合并的对 <span class="pp">|</span> 新token <span class="pp">|</span></span>
<span id="cb22-258"><a href="#cb22-258" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|</span></span>
<span id="cb22-259"><a href="#cb22-259" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 5 <span class="pp">|</span> (l, o) <span class="pp">|</span> lo <span class="pp">|</span></span>
<span id="cb22-260"><a href="#cb22-260" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 6 <span class="pp">|</span> (lo, w) <span class="pp">|</span> low <span class="pp">|</span></span>
<span id="cb22-261"><a href="#cb22-261" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 7 <span class="pp">|</span> (low, _) | low_ <span class="pp">|</span></span>
<span id="cb22-262"><a href="#cb22-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-263"><a href="#cb22-263" aria-hidden="true" tabindex="-1"></a>经过7次合并后：</span>
<span id="cb22-264"><a href="#cb22-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-265"><a href="#cb22-265" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-266"><a href="#cb22-266" aria-hidden="true" tabindex="-1"></a><span class="in">low_     : 5</span></span>
<span id="cb22-267"><a href="#cb22-267" aria-hidden="true" tabindex="-1"></a><span class="in">low e r _ : 2</span></span>
<span id="cb22-268"><a href="#cb22-268" aria-hidden="true" tabindex="-1"></a><span class="in">n e w est_ : 6</span></span>
<span id="cb22-269"><a href="#cb22-269" aria-hidden="true" tabindex="-1"></a><span class="in">w i d est_ : 3</span></span>
<span id="cb22-270"><a href="#cb22-270" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-271"><a href="#cb22-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-272"><a href="#cb22-272" aria-hidden="true" tabindex="-1"></a>**最终词汇表**（假设目标大小是18）：</span>
<span id="cb22-273"><a href="#cb22-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-274"><a href="#cb22-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-275"><a href="#cb22-275" aria-hidden="true" tabindex="-1"></a><span class="in">{l, o, w, e, r, n, s, t, i, d, _, es, est, est_, lo, low, low_, ...}</span></span>
<span id="cb22-276"><a href="#cb22-276" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-277"><a href="#cb22-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-278"><a href="#cb22-278" aria-hidden="true" tabindex="-1"></a>**关键观察**：</span>
<span id="cb22-279"><a href="#cb22-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-280"><a href="#cb22-280" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>高频词"low"被学习为完整token</span>
<span id="cb22-281"><a href="#cb22-281" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>常见后缀"est_"被学习为一个token</span>
<span id="cb22-282"><a href="#cb22-282" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>罕见词"lower"仍然需要多个token：low + e + r + _</span>
<span id="cb22-283"><a href="#cb22-283" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>词汇表从纯字符逐步构建，包含字符、常见子词、常见词</span>
<span id="cb22-284"><a href="#cb22-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-285"><a href="#cb22-285" aria-hidden="true" tabindex="-1"></a><span class="al">![BPE合并过程可视化：以"lowest"为例，从字符序列出发，依次合并最高频的相邻对（e,s）→"es"、（es,t）→"est"、（l,o）→"lo"，词汇表逐步增长。每一步合并都由语料统计驱动。](figures/chapter-3/fig-bpe-merge-process.png)</span>{#fig-bpe-merge-process}</span>
<span id="cb22-286"><a href="#cb22-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-287"><a href="#cb22-287" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb22-288"><a href="#cb22-288" aria-hidden="true" tabindex="-1"></a>*Python生成，数值示例基于 Sennrich et al. (2016) "Neural Machine Translation of Rare Words with Subword Units", Section 3. [arXiv:1508.07909](https://arxiv.org/abs/1508.07909)*</span>
<span id="cb22-289"><a href="#cb22-289" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-290"><a href="#cb22-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-291"><a href="#cb22-291" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BPE的形式化算法</span></span>
<span id="cb22-292"><a href="#cb22-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-293"><a href="#cb22-293" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb22-294"><a href="#cb22-294" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: Learn BPE Operations (Sennrich et al., 2016)</span></span>
<span id="cb22-295"><a href="#cb22-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-296"><a href="#cb22-296" aria-hidden="true" tabindex="-1"></a>以下是论文原文中的Python实现。这段代码展示了BPE算法的核心逻辑：统计相邻符号对的频率，合并最频繁的对，重复直到达到目标词汇表大小。</span>
<span id="cb22-297"><a href="#cb22-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-298"><a href="#cb22-298" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb22-299"><a href="#cb22-299" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re, collections</span>
<span id="cb22-300"><a href="#cb22-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-301"><a href="#cb22-301" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_stats(vocab):</span>
<span id="cb22-302"><a href="#cb22-302" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""统计所有相邻符号对的频率"""</span></span>
<span id="cb22-303"><a href="#cb22-303" aria-hidden="true" tabindex="-1"></a>    pairs <span class="op">=</span> collections.defaultdict(<span class="bu">int</span>)</span>
<span id="cb22-304"><a href="#cb22-304" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, freq <span class="kw">in</span> vocab.items():</span>
<span id="cb22-305"><a href="#cb22-305" aria-hidden="true" tabindex="-1"></a>        symbols <span class="op">=</span> word.split()</span>
<span id="cb22-306"><a href="#cb22-306" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(symbols)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb22-307"><a href="#cb22-307" aria-hidden="true" tabindex="-1"></a>            pairs[symbols[i],symbols[i<span class="op">+</span><span class="dv">1</span>]] <span class="op">+=</span> freq</span>
<span id="cb22-308"><a href="#cb22-308" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pairs</span>
<span id="cb22-309"><a href="#cb22-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-310"><a href="#cb22-310" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_vocab(pair, v_in):</span>
<span id="cb22-311"><a href="#cb22-311" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""将vocab中所有出现的pair合并为新符号"""</span></span>
<span id="cb22-312"><a href="#cb22-312" aria-hidden="true" tabindex="-1"></a>    v_out <span class="op">=</span> {}</span>
<span id="cb22-313"><a href="#cb22-313" aria-hidden="true" tabindex="-1"></a>    bigram <span class="op">=</span> re.escape(<span class="st">' '</span>.join(pair))</span>
<span id="cb22-314"><a href="#cb22-314" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> re.<span class="bu">compile</span>(<span class="vs">r'</span><span class="kw">(</span><span class="fu">?&lt;</span><span class="vs">!</span><span class="dv">\S</span><span class="kw">)</span><span class="vs">'</span> <span class="op">+</span> bigram <span class="op">+</span> <span class="vs">r'</span><span class="ex">(</span><span class="fu">?!</span><span class="dv">\S</span><span class="ex">)</span><span class="vs">'</span>)</span>
<span id="cb22-315"><a href="#cb22-315" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> v_in:</span>
<span id="cb22-316"><a href="#cb22-316" aria-hidden="true" tabindex="-1"></a>        w_out <span class="op">=</span> p.sub(<span class="st">''</span>.join(pair), word)</span>
<span id="cb22-317"><a href="#cb22-317" aria-hidden="true" tabindex="-1"></a>        v_out[w_out] <span class="op">=</span> v_in[word]</span>
<span id="cb22-318"><a href="#cb22-318" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v_out</span>
<span id="cb22-319"><a href="#cb22-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-320"><a href="#cb22-320" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例词汇表（词用空格分隔的字符序列表示）</span></span>
<span id="cb22-321"><a href="#cb22-321" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {<span class="st">'l o w &lt;/w&gt;'</span> : <span class="dv">5</span>, <span class="st">'l o w e r &lt;/w&gt;'</span> : <span class="dv">2</span>,</span>
<span id="cb22-322"><a href="#cb22-322" aria-hidden="true" tabindex="-1"></a>         <span class="st">'n e w e s t &lt;/w&gt;'</span>:<span class="dv">6</span>, <span class="st">'w i d e s t &lt;/w&gt;'</span>:<span class="dv">3</span>}</span>
<span id="cb22-323"><a href="#cb22-323" aria-hidden="true" tabindex="-1"></a>num_merges <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb22-324"><a href="#cb22-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-325"><a href="#cb22-325" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_merges):</span>
<span id="cb22-326"><a href="#cb22-326" aria-hidden="true" tabindex="-1"></a>    pairs <span class="op">=</span> get_stats(vocab)</span>
<span id="cb22-327"><a href="#cb22-327" aria-hidden="true" tabindex="-1"></a>    best <span class="op">=</span> <span class="bu">max</span>(pairs, key<span class="op">=</span>pairs.get)</span>
<span id="cb22-328"><a href="#cb22-328" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> merge_vocab(best, vocab)</span>
<span id="cb22-329"><a href="#cb22-329" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(best)</span>
<span id="cb22-330"><a href="#cb22-330" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-331"><a href="#cb22-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-332"><a href="#cb22-332" aria-hidden="true" tabindex="-1"></a>**输出**（前4次合并）：</span>
<span id="cb22-333"><a href="#cb22-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-334"><a href="#cb22-334" aria-hidden="true" tabindex="-1"></a><span class="in">('e', 's')      →  es</span></span>
<span id="cb22-335"><a href="#cb22-335" aria-hidden="true" tabindex="-1"></a><span class="in">('es', 't')     →  est</span></span>
<span id="cb22-336"><a href="#cb22-336" aria-hidden="true" tabindex="-1"></a><span class="in">('est', '&lt;/w&gt;') →  est&lt;/w&gt;</span></span>
<span id="cb22-337"><a href="#cb22-337" aria-hidden="true" tabindex="-1"></a><span class="in">('l', 'o')      →  lo</span></span>
<span id="cb22-338"><a href="#cb22-338" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb22-339"><a href="#cb22-339" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-340"><a href="#cb22-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-341"><a href="#cb22-341" aria-hidden="true" tabindex="-1"></a>*Source: [Sennrich et al. (2016)](https://arxiv.org/abs/1508.07909), Algorithm 1*</span>
<span id="cb22-342"><a href="#cb22-342" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-343"><a href="#cb22-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-344"><a href="#cb22-344" aria-hidden="true" tabindex="-1"></a>论文中使用<span class="in">`&lt;/w&gt;`</span>作为词尾标记（end-of-word），这与我们之前示例中的<span class="in">`_`</span>作用相同。下面是一个更详细的实现版本，带有完整注释：</span>
<span id="cb22-345"><a href="#cb22-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-346"><a href="#cb22-346" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb22-347"><a href="#cb22-347" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> learn_bpe(corpus, num_merges):</span>
<span id="cb22-348"><a href="#cb22-348" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-349"><a href="#cb22-349" aria-hidden="true" tabindex="-1"></a><span class="co">    学习BPE词汇表</span></span>
<span id="cb22-350"><a href="#cb22-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-351"><a href="#cb22-351" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb22-352"><a href="#cb22-352" aria-hidden="true" tabindex="-1"></a><span class="co">        corpus: 词频字典 {word: frequency}</span></span>
<span id="cb22-353"><a href="#cb22-353" aria-hidden="true" tabindex="-1"></a><span class="co">        num_merges: 合并次数（决定最终词汇表大小）</span></span>
<span id="cb22-354"><a href="#cb22-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-355"><a href="#cb22-355" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb22-356"><a href="#cb22-356" aria-hidden="true" tabindex="-1"></a><span class="co">        merges: 合并规则列表</span></span>
<span id="cb22-357"><a href="#cb22-357" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab: 最终词汇表</span></span>
<span id="cb22-358"><a href="#cb22-358" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-359"><a href="#cb22-359" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 初始化——将词拆分为字符，添加词尾标记</span></span>
<span id="cb22-360"><a href="#cb22-360" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> {}</span>
<span id="cb22-361"><a href="#cb22-361" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, freq <span class="kw">in</span> corpus.items():</span>
<span id="cb22-362"><a href="#cb22-362" aria-hidden="true" tabindex="-1"></a>        <span class="co"># "low" -&gt; ['l', 'o', 'w', '_']</span></span>
<span id="cb22-363"><a href="#cb22-363" aria-hidden="true" tabindex="-1"></a>        chars <span class="op">=</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'_'</span>]</span>
<span id="cb22-364"><a href="#cb22-364" aria-hidden="true" tabindex="-1"></a>        vocab[<span class="bu">tuple</span>(chars)] <span class="op">=</span> freq</span>
<span id="cb22-365"><a href="#cb22-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-366"><a href="#cb22-366" aria-hidden="true" tabindex="-1"></a>    merges <span class="op">=</span> []</span>
<span id="cb22-367"><a href="#cb22-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-368"><a href="#cb22-368" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_merges):</span>
<span id="cb22-369"><a href="#cb22-369" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 统计所有相邻token对的频率</span></span>
<span id="cb22-370"><a href="#cb22-370" aria-hidden="true" tabindex="-1"></a>        pairs <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb22-371"><a href="#cb22-371" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, freq <span class="kw">in</span> vocab.items():</span>
<span id="cb22-372"><a href="#cb22-372" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(word) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb22-373"><a href="#cb22-373" aria-hidden="true" tabindex="-1"></a>                pairs[(word[j], word[j<span class="op">+</span><span class="dv">1</span>])] <span class="op">+=</span> freq</span>
<span id="cb22-374"><a href="#cb22-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-375"><a href="#cb22-375" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> pairs:</span>
<span id="cb22-376"><a href="#cb22-376" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb22-377"><a href="#cb22-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-378"><a href="#cb22-378" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 找到最频繁的对</span></span>
<span id="cb22-379"><a href="#cb22-379" aria-hidden="true" tabindex="-1"></a>        best_pair <span class="op">=</span> <span class="bu">max</span>(pairs, key<span class="op">=</span>pairs.get)</span>
<span id="cb22-380"><a href="#cb22-380" aria-hidden="true" tabindex="-1"></a>        merges.append(best_pair)</span>
<span id="cb22-381"><a href="#cb22-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-382"><a href="#cb22-382" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 合并这个对，更新vocab</span></span>
<span id="cb22-383"><a href="#cb22-383" aria-hidden="true" tabindex="-1"></a>        new_vocab <span class="op">=</span> {}</span>
<span id="cb22-384"><a href="#cb22-384" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, freq <span class="kw">in</span> vocab.items():</span>
<span id="cb22-385"><a href="#cb22-385" aria-hidden="true" tabindex="-1"></a>            new_word <span class="op">=</span> merge_pair(word, best_pair)</span>
<span id="cb22-386"><a href="#cb22-386" aria-hidden="true" tabindex="-1"></a>            new_vocab[new_word] <span class="op">=</span> freq</span>
<span id="cb22-387"><a href="#cb22-387" aria-hidden="true" tabindex="-1"></a>        vocab <span class="op">=</span> new_vocab</span>
<span id="cb22-388"><a href="#cb22-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-389"><a href="#cb22-389" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> merges, vocab</span>
<span id="cb22-390"><a href="#cb22-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-391"><a href="#cb22-391" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_pair(word, pair):</span>
<span id="cb22-392"><a href="#cb22-392" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""将word中所有出现的pair合并"""</span></span>
<span id="cb22-393"><a href="#cb22-393" aria-hidden="true" tabindex="-1"></a>    new_word <span class="op">=</span> []</span>
<span id="cb22-394"><a href="#cb22-394" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-395"><a href="#cb22-395" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(word):</span>
<span id="cb22-396"><a href="#cb22-396" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(word) <span class="op">-</span> <span class="dv">1</span> <span class="kw">and</span> (word[i], word[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">==</span> pair:</span>
<span id="cb22-397"><a href="#cb22-397" aria-hidden="true" tabindex="-1"></a>            new_word.append(word[i] <span class="op">+</span> word[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb22-398"><a href="#cb22-398" aria-hidden="true" tabindex="-1"></a>            i <span class="op">+=</span> <span class="dv">2</span></span>
<span id="cb22-399"><a href="#cb22-399" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-400"><a href="#cb22-400" aria-hidden="true" tabindex="-1"></a>            new_word.append(word[i])</span>
<span id="cb22-401"><a href="#cb22-401" aria-hidden="true" tabindex="-1"></a>            i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-402"><a href="#cb22-402" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">tuple</span>(new_word)</span>
<span id="cb22-403"><a href="#cb22-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-404"><a href="#cb22-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-405"><a href="#cb22-405" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 使用BPE进行Tokenization</span></span>
<span id="cb22-406"><a href="#cb22-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-407"><a href="#cb22-407" aria-hidden="true" tabindex="-1"></a>训练好BPE词汇表后，如何tokenize一个新词？</span>
<span id="cb22-408"><a href="#cb22-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-409"><a href="#cb22-409" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb22-410"><a href="#cb22-410" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_bpe(word, merges):</span>
<span id="cb22-411"><a href="#cb22-411" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-412"><a href="#cb22-412" aria-hidden="true" tabindex="-1"></a><span class="co">    使用学习到的BPE规则tokenize一个词</span></span>
<span id="cb22-413"><a href="#cb22-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-414"><a href="#cb22-414" aria-hidden="true" tabindex="-1"></a><span class="co">    核心思想：按照学习时的合并顺序，依次应用合并规则</span></span>
<span id="cb22-415"><a href="#cb22-415" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-416"><a href="#cb22-416" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化为字符序列</span></span>
<span id="cb22-417"><a href="#cb22-417" aria-hidden="true" tabindex="-1"></a>    word <span class="op">=</span> <span class="bu">list</span>(word) <span class="op">+</span> [<span class="st">'_'</span>]</span>
<span id="cb22-418"><a href="#cb22-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-419"><a href="#cb22-419" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 按顺序应用每个合并规则</span></span>
<span id="cb22-420"><a href="#cb22-420" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pair <span class="kw">in</span> merges:</span>
<span id="cb22-421"><a href="#cb22-421" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-422"><a href="#cb22-422" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(word) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb22-423"><a href="#cb22-423" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (word[i], word[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">==</span> pair:</span>
<span id="cb22-424"><a href="#cb22-424" aria-hidden="true" tabindex="-1"></a>                word <span class="op">=</span> word[:i] <span class="op">+</span> [word[i] <span class="op">+</span> word[i<span class="op">+</span><span class="dv">1</span>]] <span class="op">+</span> word[i<span class="op">+</span><span class="dv">2</span>:]</span>
<span id="cb22-425"><a href="#cb22-425" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb22-426"><a href="#cb22-426" aria-hidden="true" tabindex="-1"></a>                i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-427"><a href="#cb22-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-428"><a href="#cb22-428" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> word</span>
<span id="cb22-429"><a href="#cb22-429" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-430"><a href="#cb22-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-431"><a href="#cb22-431" aria-hidden="true" tabindex="-1"></a>**例子**：tokenize "lowest"</span>
<span id="cb22-432"><a href="#cb22-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-433"><a href="#cb22-433" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>初始：<span class="co">[</span><span class="ot">'l', 'o', 'w', 'e', 's', 't', '_'</span><span class="co">]</span></span>
<span id="cb22-434"><a href="#cb22-434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>应用 (e, s) → 'es'：<span class="co">[</span><span class="ot">'l', 'o', 'w', 'es', 't', '_'</span><span class="co">]</span></span>
<span id="cb22-435"><a href="#cb22-435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>应用 (es, t) → 'est'：<span class="co">[</span><span class="ot">'l', 'o', 'w', 'est', '_'</span><span class="co">]</span></span>
<span id="cb22-436"><a href="#cb22-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>应用 (est, _) → 'est_'：['l', 'o', 'w', 'est_']</span>
<span id="cb22-437"><a href="#cb22-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>应用 (l, o) → 'lo'：<span class="co">[</span><span class="ot">'lo', 'w', 'est_'</span><span class="co">]</span></span>
<span id="cb22-438"><a href="#cb22-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>应用 (lo, w) → 'low'：<span class="co">[</span><span class="ot">'low', 'est_'</span><span class="co">]</span></span>
<span id="cb22-439"><a href="#cb22-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-440"><a href="#cb22-440" aria-hidden="true" tabindex="-1"></a>最终结果：**['low', 'est_']**</span>
<span id="cb22-441"><a href="#cb22-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-442"><a href="#cb22-442" aria-hidden="true" tabindex="-1"></a>即使"lowest"没有出现在训练语料中，BPE也能将它合理地分解为"low"和"est_"——这两个都是学习到的有意义的子词。</span>
<span id="cb22-443"><a href="#cb22-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-444"><a href="#cb22-444" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BPE与其他分词方法的实证对比</span></span>
<span id="cb22-445"><a href="#cb22-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-446"><a href="#cb22-446" aria-hidden="true" tabindex="-1"></a>Sennrich et al. (2016) 在德语语料上对比了不同分词方法的效果。下表展示了各方法的token数、词汇表大小和未知词数量：</span>
<span id="cb22-447"><a href="#cb22-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-448"><a href="#cb22-448" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 分词方法 <span class="pp">|</span> Token数量 <span class="pp">|</span> 词汇表大小 <span class="pp">|</span> 测试集未知词 <span class="pp">|</span></span>
<span id="cb22-449"><a href="#cb22-449" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|-----------|------------|--------------|</span></span>
<span id="cb22-450"><a href="#cb22-450" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 不分词（词级别） <span class="pp">|</span> 100M <span class="pp">|</span> 1,750,000 <span class="pp">|</span> 1,079 <span class="pp">|</span></span>
<span id="cb22-451"><a href="#cb22-451" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 字符级别 <span class="pp">|</span> 550M <span class="pp">|</span> 3,000 <span class="pp">|</span> 0 <span class="pp">|</span></span>
<span id="cb22-452"><a href="#cb22-452" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Character bigrams <span class="pp">|</span> 306M <span class="pp">|</span> 20,000 <span class="pp">|</span> 34 <span class="pp">|</span></span>
<span id="cb22-453"><a href="#cb22-453" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Character trigrams <span class="pp">|</span> 214M <span class="pp">|</span> 120,000 <span class="pp">|</span> 59 <span class="pp">|</span></span>
<span id="cb22-454"><a href="#cb22-454" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 复合词拆分 <span class="pp">|</span> 102M <span class="pp">|</span> 1,100,000 <span class="pp">|</span> 643 <span class="pp">|</span></span>
<span id="cb22-455"><a href="#cb22-455" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Morfessor <span class="pp">|</span> 109M <span class="pp">|</span> 544,000 <span class="pp">|</span> 237 <span class="pp">|</span></span>
<span id="cb22-456"><a href="#cb22-456" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 连字符分割 <span class="pp">|</span> 186M <span class="pp">|</span> 404,000 <span class="pp">|</span> 230 <span class="pp">|</span></span>
<span id="cb22-457"><a href="#cb22-457" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **BPE** | 112M | 63,000 | **0** <span class="pp">|</span></span>
<span id="cb22-458"><a href="#cb22-458" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **BPE (joint)** <span class="pp">|</span> 111M <span class="pp">|</span> 82,000 <span class="pp">|</span> 32 <span class="pp">|</span></span>
<span id="cb22-459"><a href="#cb22-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-460"><a href="#cb22-460" aria-hidden="true" tabindex="-1"></a>: 不同分词技术在德语训练语料上的统计对比。*Source: [Sennrich et al. (2016)](https://arxiv.org/abs/1508.07909), Table 1* {#tbl-segmentation-comparison}</span>
<span id="cb22-461"><a href="#cb22-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-462"><a href="#cb22-462" aria-hidden="true" tabindex="-1"></a>**关键观察**：</span>
<span id="cb22-463"><a href="#cb22-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-464"><a href="#cb22-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**字符级别**：完全消除未知词，但token数量爆炸（5.5倍），计算成本极高</span>
<span id="cb22-465"><a href="#cb22-465" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**传统形态学方法**（复合词拆分、Morfessor）：仍有大量未知词，词汇表依然庞大</span>
<span id="cb22-466"><a href="#cb22-466" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BPE**：在保持合理token数量（仅比词级别多12%）的同时，实现了**零未知词**，词汇表紧凑（63K）</span>
<span id="cb22-467"><a href="#cb22-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-468"><a href="#cb22-468" aria-hidden="true" tabindex="-1"></a>这正是BPE成为现代NLP标准的原因：它在**词汇表大小**、**序列长度**和**OOV处理**之间找到了最佳平衡。</span>
<span id="cb22-469"><a href="#cb22-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-470"><a href="#cb22-470" aria-hidden="true" tabindex="-1"></a><span class="fu">### WordPiece：BERT的选择</span></span>
<span id="cb22-471"><a href="#cb22-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-472"><a href="#cb22-472" aria-hidden="true" tabindex="-1"></a>WordPiece是Google提出的子词算法，被BERT、DistilBERT等模型使用。它与BPE的思想相似，但在选择合并哪个pair时使用了不同的标准。</span>
<span id="cb22-473"><a href="#cb22-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-474"><a href="#cb22-474" aria-hidden="true" tabindex="-1"></a>**BPE vs WordPiece的关键区别**：</span>
<span id="cb22-475"><a href="#cb22-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-476"><a href="#cb22-476" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>BPE选择**出现频率最高**的pair</span>
<span id="cb22-477"><a href="#cb22-477" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>WordPiece选择**合并后使语言模型困惑度下降最多**的pair</span>
<span id="cb22-478"><a href="#cb22-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-479"><a href="#cb22-479" aria-hidden="true" tabindex="-1"></a>WordPiece的选择标准可以写成：</span>
<span id="cb22-480"><a href="#cb22-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-481"><a href="#cb22-481" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-482"><a href="#cb22-482" aria-hidden="true" tabindex="-1"></a>\text{score}(x, y) = \frac{\text{freq}(xy)}{\text{freq}(x) \times \text{freq}(y)}</span>
<span id="cb22-483"><a href="#cb22-483" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-484"><a href="#cb22-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-485"><a href="#cb22-485" aria-hidden="true" tabindex="-1"></a>这个分数本质上是在问：$xy$一起出现的频率，相对于$x$和$y$各自出现频率的乘积，高多少？这类似于点互信息（PMI），衡量的是$x$和$y$的共现是否超出了随机期望。</span>
<span id="cb22-486"><a href="#cb22-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-487"><a href="#cb22-487" aria-hidden="true" tabindex="-1"></a>**直觉理解**：考虑"th"和"qu"。在英语中，"th"几乎总是一起出现（"the"、"this"、"that"），而"qu"后面几乎总是跟着"u"（"question"、"queen"）。WordPiece的标准会优先合并这种"绑定程度"高的pair，而不仅仅是出现次数多的pair。</span>
<span id="cb22-488"><a href="#cb22-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-489"><a href="#cb22-489" aria-hidden="true" tabindex="-1"></a>**WordPiece的特殊标记**：</span>
<span id="cb22-490"><a href="#cb22-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-491"><a href="#cb22-491" aria-hidden="true" tabindex="-1"></a>WordPiece使用"##"前缀标记非词首的子词。比如"tokenization"可能被切分为：</span>
<span id="cb22-492"><a href="#cb22-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-493"><a href="#cb22-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-494"><a href="#cb22-494" aria-hidden="true" tabindex="-1"></a><span class="in">["token", "##ization"]</span></span>
<span id="cb22-495"><a href="#cb22-495" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-496"><a href="#cb22-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-497"><a href="#cb22-497" aria-hidden="true" tabindex="-1"></a>"##"告诉我们"ization"不是一个独立的词，而是接在前一个token后面的后缀。这个设计使得从token序列恢复原始文本更容易。</span>
<span id="cb22-498"><a href="#cb22-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-499"><a href="#cb22-499" aria-hidden="true" tabindex="-1"></a><span class="fu">### Unigram Language Model：概率视角</span></span>
<span id="cb22-500"><a href="#cb22-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-501"><a href="#cb22-501" aria-hidden="true" tabindex="-1"></a>Unigram LM是另一种子词分词方法，由SentencePiece库实现，被T5、ALBERT等模型使用。它采用了一个根本不同的视角：不是从小到大构建词汇表，而是从大到小剪枝。</span>
<span id="cb22-502"><a href="#cb22-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-503"><a href="#cb22-503" aria-hidden="true" tabindex="-1"></a>**核心思想**：假设每个子词独立出现，用一个unigram语言模型给每种切分方式打分。</span>
<span id="cb22-504"><a href="#cb22-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-505"><a href="#cb22-505" aria-hidden="true" tabindex="-1"></a>给定词汇表$\mathcal{V}$和每个子词$x$的概率$p(x)$，一个句子$S$的某种切分$\mathbf{x} = (x_1, x_2, \ldots, x_n)$的概率是：</span>
<span id="cb22-506"><a href="#cb22-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-507"><a href="#cb22-507" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-508"><a href="#cb22-508" aria-hidden="true" tabindex="-1"></a>P(\mathbf{x}) = \prod_{i=1}^{n} p(x_i)</span>
<span id="cb22-509"><a href="#cb22-509" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-510"><a href="#cb22-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-511"><a href="#cb22-511" aria-hidden="true" tabindex="-1"></a>最优切分是使这个概率最大化的切分：</span>
<span id="cb22-512"><a href="#cb22-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-513"><a href="#cb22-513" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-514"><a href="#cb22-514" aria-hidden="true" tabindex="-1"></a>\mathbf{x}^* = \arg\max_{\mathbf{x}} P(\mathbf{x}) = \arg\max_{\mathbf{x}} \sum_{i=1}^{n} \log p(x_i)</span>
<span id="cb22-515"><a href="#cb22-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-516"><a href="#cb22-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-517"><a href="#cb22-517" aria-hidden="true" tabindex="-1"></a>**训练过程**（与BPE相反）：</span>
<span id="cb22-518"><a href="#cb22-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-519"><a href="#cb22-519" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>从一个很大的初始词汇表开始（包含所有字符、常见子串等）</span>
<span id="cb22-520"><a href="#cb22-520" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>用EM算法估计每个子词的概率</span>
<span id="cb22-521"><a href="#cb22-521" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>计算每个子词的"贡献"——移除它会使整体likelihood下降多少</span>
<span id="cb22-522"><a href="#cb22-522" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>移除贡献最小的子词（保留一定比例）</span>
<span id="cb22-523"><a href="#cb22-523" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>重复2-4，直到词汇表达到目标大小</span>
<span id="cb22-524"><a href="#cb22-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-525"><a href="#cb22-525" aria-hidden="true" tabindex="-1"></a>Unigram相比BPE有几个独特的优势。首先，它理论上更优雅——每种切分都有明确的概率解释，而不是像BPE那样依赖贪心的频率统计。更重要的是，Unigram可以为同一个词给出多种切分，这使得subword regularization成为可能：训练时按概率采样不同的切分方式作为数据增强，从而增强模型的鲁棒性。</span>
<span id="cb22-526"><a href="#cb22-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-527"><a href="#cb22-527" aria-hidden="true" tabindex="-1"></a>**例子**：对于"unigram"，可能的切分包括：</span>
<span id="cb22-528"><a href="#cb22-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-529"><a href="#cb22-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">"uni", "gram"</span><span class="co">]</span> 概率$p_1$</span>
<span id="cb22-530"><a href="#cb22-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">"un", "i", "gram"</span><span class="co">]</span> 概率$p_2$</span>
<span id="cb22-531"><a href="#cb22-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">"u", "n", "i", "g", "r", "a", "m"</span><span class="co">]</span> 概率$p_3$</span>
<span id="cb22-532"><a href="#cb22-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-533"><a href="#cb22-533" aria-hidden="true" tabindex="-1"></a>推理时选择概率最高的切分，训练时可以按概率采样不同切分。</span>
<span id="cb22-534"><a href="#cb22-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-535"><a href="#cb22-535" aria-hidden="true" tabindex="-1"></a><span class="fu">### SentencePiece：语言无关的统一方案</span></span>
<span id="cb22-536"><a href="#cb22-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-537"><a href="#cb22-537" aria-hidden="true" tabindex="-1"></a>SentencePiece是Google开发的一个tokenization库，它的关键创新是**直接在原始文本上操作**，不需要预先的词分割。</span>
<span id="cb22-538"><a href="#cb22-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-539"><a href="#cb22-539" aria-hidden="true" tabindex="-1"></a>**传统流程的问题**：</span>
<span id="cb22-540"><a href="#cb22-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-541"><a href="#cb22-541" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-542"><a href="#cb22-542" aria-hidden="true" tabindex="-1"></a><span class="in">原始文本 → 词分割（按空格或分词器）→ 子词分割（BPE/WordPiece）</span></span>
<span id="cb22-543"><a href="#cb22-543" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-544"><a href="#cb22-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-545"><a href="#cb22-545" aria-hidden="true" tabindex="-1"></a>这个流程对英语工作良好（空格是自然的词边界），但对中文、日文等没有空格的语言不友好——需要先用一个分词器，引入了额外的复杂性和错误来源。</span>
<span id="cb22-546"><a href="#cb22-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-547"><a href="#cb22-547" aria-hidden="true" tabindex="-1"></a>**SentencePiece的方案**：</span>
<span id="cb22-548"><a href="#cb22-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-549"><a href="#cb22-549" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-550"><a href="#cb22-550" aria-hidden="true" tabindex="-1"></a><span class="in">原始文本（包括空格）→ 子词分割</span></span>
<span id="cb22-551"><a href="#cb22-551" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-552"><a href="#cb22-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-553"><a href="#cb22-553" aria-hidden="true" tabindex="-1"></a>SentencePiece把空格也当作一个普通字符处理，用特殊符号"▁"（U+2581）表示词边界。比如"Hello world"变成"▁Hello▁world"，然后直接在这个字符序列上做BPE或Unigram。</span>
<span id="cb22-554"><a href="#cb22-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-555"><a href="#cb22-555" aria-hidden="true" tabindex="-1"></a>这样，不需要任何语言特定的预处理，同一个算法可以处理任何语言。空格信息被保留在token中（以"▁"的形式），不会丢失。</span>
<span id="cb22-556"><a href="#cb22-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-557"><a href="#cb22-557" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb22-558"><a href="#cb22-558" aria-hidden="true" tabindex="-1"></a><span class="co"># SentencePiece的tokenization示例</span></span>
<span id="cb22-559"><a href="#cb22-559" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sentencepiece <span class="im">as</span> spm</span>
<span id="cb22-560"><a href="#cb22-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-561"><a href="#cb22-561" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练模型</span></span>
<span id="cb22-562"><a href="#cb22-562" aria-hidden="true" tabindex="-1"></a>sp <span class="op">=</span> spm.SentencePieceProcessor()</span>
<span id="cb22-563"><a href="#cb22-563" aria-hidden="true" tabindex="-1"></a>sp.Load(<span class="st">"model.model"</span>)</span>
<span id="cb22-564"><a href="#cb22-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-565"><a href="#cb22-565" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize</span></span>
<span id="cb22-566"><a href="#cb22-566" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello world"</span></span>
<span id="cb22-567"><a href="#cb22-567" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> sp.EncodeAsPieces(text)</span>
<span id="cb22-568"><a href="#cb22-568" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出: ['▁Hello', '▁world']</span></span>
<span id="cb22-569"><a href="#cb22-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-570"><a href="#cb22-570" aria-hidden="true" tabindex="-1"></a>text_zh <span class="op">=</span> <span class="st">"你好世界"</span></span>
<span id="cb22-571"><a href="#cb22-571" aria-hidden="true" tabindex="-1"></a>tokens_zh <span class="op">=</span> sp.EncodeAsPieces(text_zh)</span>
<span id="cb22-572"><a href="#cb22-572" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出: ['▁', '你', '好', '世', '界']  # 中文字符通常各自成为token</span></span>
<span id="cb22-573"><a href="#cb22-573" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-574"><a href="#cb22-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-575"><a href="#cb22-575" aria-hidden="true" tabindex="-1"></a><span class="fu">### Byte-level BPE：GPT-2的创新</span></span>
<span id="cb22-576"><a href="#cb22-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-577"><a href="#cb22-577" aria-hidden="true" tabindex="-1"></a>GPT-2引入了一个重要的创新：**Byte-level BPE**。</span>
<span id="cb22-578"><a href="#cb22-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-579"><a href="#cb22-579" aria-hidden="true" tabindex="-1"></a>**传统方法的问题**：即使是SentencePiece，也需要定义一个字符集。如果遇到训练时没见过的字符（比如某种稀有语言的Unicode字符），仍然会变成未知token。</span>
<span id="cb22-580"><a href="#cb22-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-581"><a href="#cb22-581" aria-hidden="true" tabindex="-1"></a>**Byte-level的解决方案**：不在字符级别操作，而是在**字节级别**操作。任何文本都可以用UTF-8编码为字节序列（0-255），所以基础词汇表只需要256个token。</span>
<span id="cb22-582"><a href="#cb22-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-583"><a href="#cb22-583" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb22-584"><a href="#cb22-584" aria-hidden="true" tabindex="-1"></a><span class="co"># 任何字符串都可以变成字节序列</span></span>
<span id="cb22-585"><a href="#cb22-585" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello 你好 🎉"</span></span>
<span id="cb22-586"><a href="#cb22-586" aria-hidden="true" tabindex="-1"></a>bytes_seq <span class="op">=</span> text.encode(<span class="st">'utf-8'</span>)</span>
<span id="cb22-587"><a href="#cb22-587" aria-hidden="true" tabindex="-1"></a><span class="co"># b'Hello \xe4\xbd\xa0\xe5\xa5\xbd \xf0\x9f\x8e\x89'</span></span>
<span id="cb22-588"><a href="#cb22-588" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-589"><a href="#cb22-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-590"><a href="#cb22-590" aria-hidden="true" tabindex="-1"></a>然后在这个字节序列上做BPE，学习常见的字节组合。</span>
<span id="cb22-591"><a href="#cb22-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-592"><a href="#cb22-592" aria-hidden="true" tabindex="-1"></a>Byte-level方法的关键优势在于它彻底解决了OOV问题——任何Unicode字符都能编码，无论是emoji、稀有语言、甚至乱码。同时它完全语言无关，不需要任何语言特定的处理，而且词汇表大小可控，从256个基础字节出发，通过合并操作构建到目标大小。</span>
<span id="cb22-593"><a href="#cb22-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-594"><a href="#cb22-594" aria-hidden="true" tabindex="-1"></a>不过这种方法也有潜在代价：多字节字符可能被切开。UTF-8编码中，中文字符通常占3个字节，一个中文字符可能被切分为多个token，导致中文的token效率比英文低。</span>
<span id="cb22-595"><a href="#cb22-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-596"><a href="#cb22-596" aria-hidden="true" tabindex="-1"></a>**为什么现代LLM都用Byte-level？**</span>
<span id="cb22-597"><a href="#cb22-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-598"><a href="#cb22-598" aria-hidden="true" tabindex="-1"></a>因为它是最"安全"的选择。对于要部署到全世界、处理任意输入的大型语言模型，不能有任何"无法处理"的情况。Byte-level BPE保证了完备性——任何输入都能被tokenize，最坏情况只是效率低一点（字节序列很长）。</span>
<span id="cb22-599"><a href="#cb22-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-600"><a href="#cb22-600" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-601"><a href="#cb22-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-602"><a href="#cb22-602" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tokenizer是模型的一部分</span></span>
<span id="cb22-603"><a href="#cb22-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-604"><a href="#cb22-604" aria-hidden="true" tabindex="-1"></a>这是本章最重要的观点：**Tokenizer不是可以随意选择的预处理步骤，它是模型架构的隐藏维度**。</span>
<span id="cb22-605"><a href="#cb22-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-606"><a href="#cb22-606" aria-hidden="true" tabindex="-1"></a><span class="fu">### 计算效率：序列长度的隐藏成本</span></span>
<span id="cb22-607"><a href="#cb22-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-608"><a href="#cb22-608" aria-hidden="true" tabindex="-1"></a>同样的文本，不同tokenizer产生的序列长度可能相差巨大。</span>
<span id="cb22-609"><a href="#cb22-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-610"><a href="#cb22-610" aria-hidden="true" tabindex="-1"></a>考虑这个英文句子：</span>
<span id="cb22-611"><a href="#cb22-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-612"><a href="#cb22-612" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "The transformer architecture revolutionized natural language processing."</span></span>
<span id="cb22-613"><a href="#cb22-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-614"><a href="#cb22-614" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Tokenizer <span class="pp">|</span> Token数量 <span class="pp">|</span> Token序列 <span class="pp">|</span></span>
<span id="cb22-615"><a href="#cb22-615" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|</span></span>
<span id="cb22-616"><a href="#cb22-616" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词级别 <span class="pp">|</span> 7 <span class="pp">|</span> <span class="co">[</span><span class="ot">The, transformer, architecture, revolutionized, natural, language, processing</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb22-617"><a href="#cb22-617" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-2 (BPE) <span class="pp">|</span> 7 <span class="pp">|</span> <span class="co">[</span><span class="ot">The, Ġtransformer, Ġarchitecture, Ġrevolution, ized, Ġnatural, Ġlanguage, Ġprocessing</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb22-618"><a href="#cb22-618" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 字符级别 <span class="pp">|</span> 62 <span class="pp">|</span> <span class="co">[</span><span class="ot">T, h, e, ␣, t, r, a, n, s, ...</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb22-619"><a href="#cb22-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-620"><a href="#cb22-620" aria-hidden="true" tabindex="-1"></a>对于这个简单句子，差异不大。但看看代码：</span>
<span id="cb22-621"><a href="#cb22-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-622"><a href="#cb22-622" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb22-623"><a href="#cb22-623" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_attention(query, key, value):</span>
<span id="cb22-624"><a href="#cb22-624" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb22-625"><a href="#cb22-625" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">@</span> value</span>
<span id="cb22-626"><a href="#cb22-626" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-627"><a href="#cb22-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-628"><a href="#cb22-628" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Tokenizer <span class="pp">|</span> Token数量 <span class="pp">|</span></span>
<span id="cb22-629"><a href="#cb22-629" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|</span></span>
<span id="cb22-630"><a href="#cb22-630" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-2 <span class="pp">|</span> ~40 <span class="pp">|</span></span>
<span id="cb22-631"><a href="#cb22-631" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 字符级别 <span class="pp">|</span> ~150 <span class="pp">|</span></span>
<span id="cb22-632"><a href="#cb22-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-633"><a href="#cb22-633" aria-hidden="true" tabindex="-1"></a>差异接近4倍！在Transformer中，attention的计算复杂度是$O(n^2)$，这意味着字符级别的计算量是GPT-2的16倍。</span>
<span id="cb22-634"><a href="#cb22-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-635"><a href="#cb22-635" aria-hidden="true" tabindex="-1"></a><span class="fu">### 多语言公平性：隐藏的不平等</span></span>
<span id="cb22-636"><a href="#cb22-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-637"><a href="#cb22-637" aria-hidden="true" tabindex="-1"></a>这是一个经常被忽视但极其重要的问题。</span>
<span id="cb22-638"><a href="#cb22-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-639"><a href="#cb22-639" aria-hidden="true" tabindex="-1"></a>考虑"人工智能"这个概念：</span>
<span id="cb22-640"><a href="#cb22-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-641"><a href="#cb22-641" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 语言 <span class="pp">|</span> 文本 <span class="pp">|</span> GPT-2 Token数 <span class="pp">|</span></span>
<span id="cb22-642"><a href="#cb22-642" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|</span></span>
<span id="cb22-643"><a href="#cb22-643" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 英语 <span class="pp">|</span> artificial intelligence <span class="pp">|</span> 2 <span class="pp">|</span></span>
<span id="cb22-644"><a href="#cb22-644" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 中文 <span class="pp">|</span> 人工智能 <span class="pp">|</span> 4-6（取决于具体tokenizer）<span class="pp">|</span></span>
<span id="cb22-645"><a href="#cb22-645" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 日语 <span class="pp">|</span> 人工知能 <span class="pp">|</span> 4-6 <span class="pp">|</span></span>
<span id="cb22-646"><a href="#cb22-646" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 阿拉伯语 <span class="pp">|</span> الذكاء الاصطناعي <span class="pp">|</span> 8-12 <span class="pp">|</span></span>
<span id="cb22-647"><a href="#cb22-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-648"><a href="#cb22-648" aria-hidden="true" tabindex="-1"></a><span class="al">![不同语言表达"Artificial Intelligence"所需的token数量差异（GPT-2 Tokenizer）。英文仅需2个token，而阿拉伯语需要9个，中文和日文也远多于英文。这种不对称源于tokenizer训练语料中英文的主导地位。](figures/chapter-3/fig-multilingual-efficiency.png)</span>{#fig-multilingual-efficiency}</span>
<span id="cb22-649"><a href="#cb22-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-650"><a href="#cb22-650" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb22-651"><a href="#cb22-651" aria-hidden="true" tabindex="-1"></a>*Python生成，数据参考 Petrov et al. (2023) "Language Model Tokenizers Introduce Unfairness Between Languages". [arXiv:2305.15425](https://arxiv.org/abs/2305.15425)*</span>
<span id="cb22-652"><a href="#cb22-652" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb22-653"><a href="#cb22-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-654"><a href="#cb22-654" aria-hidden="true" tabindex="-1"></a>**这意味着什么？** 在相同的上下文窗口（比如4096 tokens）内，模型能处理的中文内容比英文少得多。由于API按token计费，中文用户的调用成本也更高。更深层地说，模型学习中文需要消耗更多的"token带宽"，这可能导致中文能力系统性地弱于英文。</span>
<span id="cb22-655"><a href="#cb22-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-656"><a href="#cb22-656" aria-hidden="true" tabindex="-1"></a>这不是技术实现的细节，而是系统性的不公平。解决这个问题需要在设计tokenizer时就考虑多语言平衡，或者使用专门为某种语言优化的tokenizer。</span>
<span id="cb22-657"><a href="#cb22-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-658"><a href="#cb22-658" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数学和代码能力：数字的切分</span></span>
<span id="cb22-659"><a href="#cb22-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-660"><a href="#cb22-660" aria-hidden="true" tabindex="-1"></a>数字的tokenization对模型的算术能力有深远影响。</span>
<span id="cb22-661"><a href="#cb22-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-662"><a href="#cb22-662" aria-hidden="true" tabindex="-1"></a>考虑数字"12345"的几种切分方式：</span>
<span id="cb22-663"><a href="#cb22-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-664"><a href="#cb22-664" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 切分方式 <span class="pp">|</span> Tokens <span class="pp">|</span></span>
<span id="cb22-665"><a href="#cb22-665" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|</span></span>
<span id="cb22-666"><a href="#cb22-666" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 整体 <span class="pp">|</span> <span class="co">[</span><span class="ot">"12345"</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb22-667"><a href="#cb22-667" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 按位 <span class="pp">|</span> <span class="co">[</span><span class="ot">"1", "2", "3", "4", "5"</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb22-668"><a href="#cb22-668" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 混合 <span class="pp">|</span> <span class="co">[</span><span class="ot">"123", "45"</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb22-669"><a href="#cb22-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-670"><a href="#cb22-670" aria-hidden="true" tabindex="-1"></a>研究表明，当数字被切分为多个token时，模型很难进行算术运算。</span>
<span id="cb22-671"><a href="#cb22-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-672"><a href="#cb22-672" aria-hidden="true" tabindex="-1"></a>**为什么？**</span>
<span id="cb22-673"><a href="#cb22-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-674"><a href="#cb22-674" aria-hidden="true" tabindex="-1"></a>考虑加法"12345 + 67890"。如果模型看到的是：</span>
<span id="cb22-675"><a href="#cb22-675" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-676"><a href="#cb22-676" aria-hidden="true" tabindex="-1"></a><span class="in">["123", "45", "+", "678", "90"]</span></span>
<span id="cb22-677"><a href="#cb22-677" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-678"><a href="#cb22-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-679"><a href="#cb22-679" aria-hidden="true" tabindex="-1"></a>它需要：</span>
<span id="cb22-680"><a href="#cb22-680" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>理解"123"和"45"组合成12345</span>
<span id="cb22-681"><a href="#cb22-681" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>理解"678"和"90"组合成67890</span>
<span id="cb22-682"><a href="#cb22-682" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>进行加法</span>
<span id="cb22-683"><a href="#cb22-683" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>把结果拆分回token</span>
<span id="cb22-684"><a href="#cb22-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-685"><a href="#cb22-685" aria-hidden="true" tabindex="-1"></a>这比直接处理完整数字复杂得多。而且，不同的数字可能有不同的切分方式，模型需要学习无数种组合模式。</span>
<span id="cb22-686"><a href="#cb22-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-687"><a href="#cb22-687" aria-hidden="true" tabindex="-1"></a>**现代方法的改进**：</span>
<span id="cb22-688"><a href="#cb22-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-689"><a href="#cb22-689" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>一些模型专门把数字处理为单独的token</span>
<span id="cb22-690"><a href="#cb22-690" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>或者把数字按位切分，统一处理方式</span>
<span id="cb22-691"><a href="#cb22-691" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>或者在训练数据中加入大量算术例子，让模型学习切分模式</span>
<span id="cb22-692"><a href="#cb22-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-693"><a href="#cb22-693" aria-hidden="true" tabindex="-1"></a><span class="fu">### 安全性：Tokenization边界的攻击</span></span>
<span id="cb22-694"><a href="#cb22-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-695"><a href="#cb22-695" aria-hidden="true" tabindex="-1"></a>Tokenizer的切分边界可能被攻击者利用。</span>
<span id="cb22-696"><a href="#cb22-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-697"><a href="#cb22-697" aria-hidden="true" tabindex="-1"></a>**例子：Token边界绕过**</span>
<span id="cb22-698"><a href="#cb22-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-699"><a href="#cb22-699" aria-hidden="true" tabindex="-1"></a>假设某个安全过滤器阻止了"dangerous"这个词。但如果tokenizer把它切分为"danger" + "ous"，攻击者可能通过改变输入方式（比如添加空格或特殊字符）来改变切分，绕过过滤。</span>
<span id="cb22-700"><a href="#cb22-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-701"><a href="#cb22-701" aria-hidden="true" tabindex="-1"></a>**例子：特殊Token利用**</span>
<span id="cb22-702"><a href="#cb22-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-703"><a href="#cb22-703" aria-hidden="true" tabindex="-1"></a>某些tokenizer有特殊token（如<span class="in">`&lt;|endoftext|&gt;`</span>），如果用户输入能够产生这些token，可能导致意外行为。这是一些prompt injection攻击的基础。</span>
<span id="cb22-704"><a href="#cb22-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-705"><a href="#cb22-705" aria-hidden="true" tabindex="-1"></a>**安全启示**：</span>
<span id="cb22-706"><a href="#cb22-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-707"><a href="#cb22-707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>安全过滤应该同时在原始文本和token级别进行</span>
<span id="cb22-708"><a href="#cb22-708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>理解tokenizer的行为是理解模型安全边界的前提</span>
<span id="cb22-709"><a href="#cb22-709" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>特殊token需要特别处理，防止用户输入产生它们</span>
<span id="cb22-710"><a href="#cb22-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-711"><a href="#cb22-711" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-712"><a href="#cb22-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-713"><a href="#cb22-713" aria-hidden="true" tabindex="-1"></a><span class="fu">## 数据质量：清洗、去重与污染</span></span>
<span id="cb22-714"><a href="#cb22-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-715"><a href="#cb22-715" aria-hidden="true" tabindex="-1"></a>Tokenization是数据进入模型的入口，但数据本身的质量同样关键。"Garbage in, garbage out"在大语言模型时代更加明显。</span>
<span id="cb22-716"><a href="#cb22-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-717"><a href="#cb22-717" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据清洗：从噪声中提取信号</span></span>
<span id="cb22-718"><a href="#cb22-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-719"><a href="#cb22-719" aria-hidden="true" tabindex="-1"></a>互联网上的原始文本充满噪声——HTML标签和JavaScript代码混杂其中，导航菜单、广告、版权声明占据大量篇幅，重复的模板文本（如页脚）反复出现，编码错误导致的乱码以及色情、暴力、仇恨言论也难以避免。</span>
<span id="cb22-720"><a href="#cb22-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-721"><a href="#cb22-721" aria-hidden="true" tabindex="-1"></a>清洗流程通常从**语言过滤**开始，识别文本语言并保留目标语言；然后进行**质量过滤**，基于困惑度、文本长度、特殊字符比例等指标去除低质量文本；接着是**内容过滤**，移除有害内容、广告和模板文本；最后是**格式清理**，统一编码，移除HTML标签，规范化空白字符。</span>
<span id="cb22-722"><a href="#cb22-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-723"><a href="#cb22-723" aria-hidden="true" tabindex="-1"></a>研究表明，在同等数据量下，高质量数据训练的模型显著优于在原始数据上训练的模型。这也是为什么数据清洗是大模型训练的核心竞争力之一。</span>
<span id="cb22-724"><a href="#cb22-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-725"><a href="#cb22-725" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据去重：重复的危害</span></span>
<span id="cb22-726"><a href="#cb22-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-727"><a href="#cb22-727" aria-hidden="true" tabindex="-1"></a>大规模语料中存在大量重复。有些是完全相同的文档被爬取多次，有些是近似重复（如同一新闻的不同来源）。</span>
<span id="cb22-728"><a href="#cb22-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-729"><a href="#cb22-729" aria-hidden="true" tabindex="-1"></a>重复数据的危害是多方面的。最直接的影响是**浪费计算**——模型在相同内容上反复训练。更隐蔽的是，某些被大量复制的内容会获得不成比例的影响力，**偏向特定来源**。模型也可能倾向于**记忆**这些重复内容，而不是学习通用模式。此外，如果测试数据在训练集中出现过，评估结果将不可信——这就是**评估失真**问题。</span>
<span id="cb22-730"><a href="#cb22-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-731"><a href="#cb22-731" aria-hidden="true" tabindex="-1"></a>DeepMind的Chinchilla研究进一步凸显了数据质量的重要性：更大的、更干净的数据集可以让更小的模型达到更大模型的效果。这直接促使了对数据去重的重视。</span>
<span id="cb22-732"><a href="#cb22-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-733"><a href="#cb22-733" aria-hidden="true" tabindex="-1"></a>在实践中，去重方法可以按精度分级：**精确去重**基于hash（如MD5、SHA）检测完全相同的文档；**近似去重**使用MinHash、SimHash等技术识别相似但不完全相同的文档；**N-gram去重**则更为激进，移除包含训练集中大段连续文本的样本。</span>
<span id="cb22-734"><a href="#cb22-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-735"><a href="#cb22-735" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据污染：训练-测试泄漏</span></span>
<span id="cb22-736"><a href="#cb22-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-737"><a href="#cb22-737" aria-hidden="true" tabindex="-1"></a>一个更微妙的问题是**数据污染**（data contamination）：测试集的内容出现在训练集中。</span>
<span id="cb22-738"><a href="#cb22-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-739"><a href="#cb22-739" aria-hidden="true" tabindex="-1"></a>**为什么这是问题？**</span>
<span id="cb22-740"><a href="#cb22-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-741"><a href="#cb22-741" aria-hidden="true" tabindex="-1"></a>如果模型在训练时"见过"测试题和答案，它可能只是在检索记忆，而不是真正的泛化。这会导致benchmark分数虚高，无法反映模型的真实能力。</span>
<span id="cb22-742"><a href="#cb22-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-743"><a href="#cb22-743" aria-hidden="true" tabindex="-1"></a>**污染来源**：</span>
<span id="cb22-744"><a href="#cb22-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-745"><a href="#cb22-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>互联网上有大量benchmark数据集的讨论和解答</span>
<span id="cb22-746"><a href="#cb22-746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>某些评测集本身就是从互联网收集的</span>
<span id="cb22-747"><a href="#cb22-747" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>不同数据集可能有重叠的来源</span>
<span id="cb22-748"><a href="#cb22-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-749"><a href="#cb22-749" aria-hidden="true" tabindex="-1"></a>**检测方法**：</span>
<span id="cb22-750"><a href="#cb22-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-751"><a href="#cb22-751" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>检查测试样本是否在训练集中有高度重叠</span>
<span id="cb22-752"><a href="#cb22-752" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>使用模型的困惑度——如果模型对测试样本的困惑度异常低，可能是记忆</span>
<span id="cb22-753"><a href="#cb22-753" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>对测试样本做微小修改（如改变人名），观察模型表现变化</span>
<span id="cb22-754"><a href="#cb22-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-755"><a href="#cb22-755" aria-hidden="true" tabindex="-1"></a>**案例**：GPT-4的技术报告专门讨论了数据污染问题，并对可能被污染的benchmark结果做了标注。这种透明度是负责任的做法。</span>
<span id="cb22-756"><a href="#cb22-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-757"><a href="#cb22-757" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-758"><a href="#cb22-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-759"><a href="#cb22-759" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb22-760"><a href="#cb22-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-761"><a href="#cb22-761" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨tokenization的理论基础、边界条件和开放问题</span></span>
<span id="cb22-762"><a href="#cb22-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-763"><a href="#cb22-763" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论视角：最优分词的条件</span></span>
<span id="cb22-764"><a href="#cb22-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-765"><a href="#cb22-765" aria-hidden="true" tabindex="-1"></a>从信息论角度，最优的tokenization应该最小化描述数据的总比特数。这等价于最大化：</span>
<span id="cb22-766"><a href="#cb22-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-767"><a href="#cb22-767" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-768"><a href="#cb22-768" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \sum_{s \in \mathcal{D}} \log P(s)</span>
<span id="cb22-769"><a href="#cb22-769" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb22-770"><a href="#cb22-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-771"><a href="#cb22-771" aria-hidden="true" tabindex="-1"></a>其中$P(s)$是分词方案下句子$s$的概率。</span>
<span id="cb22-772"><a href="#cb22-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-773"><a href="#cb22-773" aria-hidden="true" tabindex="-1"></a>对于Unigram模型，这有闭式解。但对于BPE这种贪心算法，只能保证局部最优。</span>
<span id="cb22-774"><a href="#cb22-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-775"><a href="#cb22-775" aria-hidden="true" tabindex="-1"></a>**有趣的问题**：BPE的贪心策略与全局最优差多少？实证研究表明，在大多数情况下差异不大，但理论上的保证仍然缺失。</span>
<span id="cb22-776"><a href="#cb22-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-777"><a href="#cb22-777" aria-hidden="true" tabindex="-1"></a><span class="fu">### 边界条件：子词分词的假设</span></span>
<span id="cb22-778"><a href="#cb22-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-779"><a href="#cb22-779" aria-hidden="true" tabindex="-1"></a>子词分词隐含了几个假设：</span>
<span id="cb22-780"><a href="#cb22-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-781"><a href="#cb22-781" aria-hidden="true" tabindex="-1"></a>**假设1：词可以有意义地分解**</span>
<span id="cb22-782"><a href="#cb22-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-783"><a href="#cb22-783" aria-hidden="true" tabindex="-1"></a>BPE假设常见的子词组合是有意义的。但这并不总是成立——"ing"作为后缀有语法意义，但"qu"只是拼写惯例，没有独立含义。</span>
<span id="cb22-784"><a href="#cb22-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-785"><a href="#cb22-785" aria-hidden="true" tabindex="-1"></a>**假设2：统计规律反映语言结构**</span>
<span id="cb22-786"><a href="#cb22-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-787"><a href="#cb22-787" aria-hidden="true" tabindex="-1"></a>BPE用频率来决定合并顺序，假设高频模式是语言中重要的单元。但频率受训练语料影响——如果语料偏向某个领域，分词结果也会偏向该领域。</span>
<span id="cb22-788"><a href="#cb22-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-789"><a href="#cb22-789" aria-hidden="true" tabindex="-1"></a>**假设3：分词策略语言无关**</span>
<span id="cb22-790"><a href="#cb22-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-791"><a href="#cb22-791" aria-hidden="true" tabindex="-1"></a>SentencePiece等工具用相同算法处理所有语言。但不同语言有不同的结构——中文是孤立语（morphology少），土耳其语是黏着语（morphology丰富）。同一算法可能对它们的效果不同。</span>
<span id="cb22-792"><a href="#cb22-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-793"><a href="#cb22-793" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb22-794"><a href="#cb22-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-795"><a href="#cb22-795" aria-hidden="true" tabindex="-1"></a>如果你要在tokenization方向做研究，可以考虑这些问题。</span>
<span id="cb22-796"><a href="#cb22-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-797"><a href="#cb22-797" aria-hidden="true" tabindex="-1"></a>**问题1：最优词汇表大小是多少？**</span>
<span id="cb22-798"><a href="#cb22-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-799"><a href="#cb22-799" aria-hidden="true" tabindex="-1"></a>词汇表大小是一个关键超参数。太小导致序列太长，太大导致稀有token训练不足。最优大小取决于数据量、模型大小、目标语言等因素。是否存在一个理论指导？</span>
<span id="cb22-800"><a href="#cb22-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-801"><a href="#cb22-801" aria-hidden="true" tabindex="-1"></a>**问题2：如何实现真正的多语言公平？**</span>
<span id="cb22-802"><a href="#cb22-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-803"><a href="#cb22-803" aria-hidden="true" tabindex="-1"></a>现有tokenizer对英文有系统性偏好。如何设计一个对所有语言公平的tokenizer？这可能需要权衡——让英文效率降低来提升其他语言，是否可接受？</span>
<span id="cb22-804"><a href="#cb22-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-805"><a href="#cb22-805" aria-hidden="true" tabindex="-1"></a>**问题3：Tokenization与模型能力的关系**</span>
<span id="cb22-806"><a href="#cb22-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-807"><a href="#cb22-807" aria-hidden="true" tabindex="-1"></a>数字切分影响算术能力，这已被证实。还有哪些模型能力受tokenization影响？推理、常识、代码理解...？能否设计专门的tokenization策略来增强特定能力？</span>
<span id="cb22-808"><a href="#cb22-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-809"><a href="#cb22-809" aria-hidden="true" tabindex="-1"></a>**问题4：端到端学习tokenization**</span>
<span id="cb22-810"><a href="#cb22-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-811"><a href="#cb22-811" aria-hidden="true" tabindex="-1"></a>目前tokenization与模型训练是分离的。能否让模型自己学习如何分词？这涉及到离散结构的可微分学习，是一个开放问题。</span>
<span id="cb22-812"><a href="#cb22-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-813"><a href="#cb22-813" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-814"><a href="#cb22-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-815"><a href="#cb22-815" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：Hugging Face Tokenizers库使用</span></span>
<span id="cb22-816"><a href="#cb22-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-817"><a href="#cb22-817" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用预训练Tokenizer</span></span>
<span id="cb22-818"><a href="#cb22-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-821"><a href="#cb22-821" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb22-822"><a href="#cb22-822" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb22-823"><a href="#cb22-823" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb22-824"><a href="#cb22-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-825"><a href="#cb22-825" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb22-826"><a href="#cb22-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-827"><a href="#cb22-827" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载GPT-2的tokenizer（Byte-level BPE）</span></span>
<span id="cb22-828"><a href="#cb22-828" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb22-829"><a href="#cb22-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-830"><a href="#cb22-830" aria-hidden="true" tabindex="-1"></a><span class="co"># 基本tokenization</span></span>
<span id="cb22-831"><a href="#cb22-831" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, how are you doing today?"</span></span>
<span id="cb22-832"><a href="#cb22-832" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer.tokenize(text)</span>
<span id="cb22-833"><a href="#cb22-833" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-834"><a href="#cb22-834" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', 'Ġdoing', 'Ġtoday', '?']</span></span>
<span id="cb22-835"><a href="#cb22-835" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：Ġ表示词前的空格（Byte-level BPE的特点）</span></span>
<span id="cb22-836"><a href="#cb22-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-837"><a href="#cb22-837" aria-hidden="true" tabindex="-1"></a><span class="co"># 转换为ID</span></span>
<span id="cb22-838"><a href="#cb22-838" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb22-839"><a href="#cb22-839" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input IDs: </span><span class="sc">{</span>input_ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-840"><a href="#cb22-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-841"><a href="#cb22-841" aria-hidden="true" tabindex="-1"></a><span class="co"># 解码回文本</span></span>
<span id="cb22-842"><a href="#cb22-842" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> tokenizer.decode(input_ids)</span>
<span id="cb22-843"><a href="#cb22-843" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoded: </span><span class="sc">{</span>decoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-844"><a href="#cb22-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-845"><a href="#cb22-845" aria-hidden="true" tabindex="-1"></a><span class="co"># 批量处理，带padding和truncation</span></span>
<span id="cb22-846"><a href="#cb22-846" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">"Hello world"</span>, <span class="st">"This is a longer sentence for testing"</span>]</span>
<span id="cb22-847"><a href="#cb22-847" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> tokenizer(texts, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">20</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb22-848"><a href="#cb22-848" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input IDs shape: </span><span class="sc">{</span>encoded[<span class="st">'input_ids'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-849"><a href="#cb22-849" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention mask: </span><span class="sc">{</span>encoded[<span class="st">'attention_mask'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-850"><a href="#cb22-850" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-851"><a href="#cb22-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-852"><a href="#cb22-852" aria-hidden="true" tabindex="-1"></a><span class="fu">### 比较不同Tokenizer</span></span>
<span id="cb22-853"><a href="#cb22-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-856"><a href="#cb22-856" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb22-857"><a href="#cb22-857" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb22-858"><a href="#cb22-858" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb22-859"><a href="#cb22-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-860"><a href="#cb22-860" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb22-861"><a href="#cb22-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-862"><a href="#cb22-862" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载不同模型的tokenizer</span></span>
<span id="cb22-863"><a href="#cb22-863" aria-hidden="true" tabindex="-1"></a>tokenizers <span class="op">=</span> {</span>
<span id="cb22-864"><a href="#cb22-864" aria-hidden="true" tabindex="-1"></a>    <span class="st">"GPT-2"</span>: AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>),</span>
<span id="cb22-865"><a href="#cb22-865" aria-hidden="true" tabindex="-1"></a>    <span class="st">"BERT"</span>: AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>),</span>
<span id="cb22-866"><a href="#cb22-866" aria-hidden="true" tabindex="-1"></a>    <span class="st">"T5"</span>: AutoTokenizer.from_pretrained(<span class="st">"t5-base"</span>),</span>
<span id="cb22-867"><a href="#cb22-867" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LLaMA"</span>: AutoTokenizer.from_pretrained(<span class="st">"meta-llama/Llama-2-7b-hf"</span>),</span>
<span id="cb22-868"><a href="#cb22-868" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-869"><a href="#cb22-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-870"><a href="#cb22-870" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试文本</span></span>
<span id="cb22-871"><a href="#cb22-871" aria-hidden="true" tabindex="-1"></a>test_texts <span class="op">=</span> [</span>
<span id="cb22-872"><a href="#cb22-872" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The quick brown fox jumps over the lazy dog."</span>,</span>
<span id="cb22-873"><a href="#cb22-873" aria-hidden="true" tabindex="-1"></a>    <span class="st">"人工智能正在改变世界。"</span>,</span>
<span id="cb22-874"><a href="#cb22-874" aria-hidden="true" tabindex="-1"></a>    <span class="st">"def fibonacci(n): return n if n &lt;= 1 else fibonacci(n-1) + fibonacci(n-2)"</span>,</span>
<span id="cb22-875"><a href="#cb22-875" aria-hidden="true" tabindex="-1"></a>    <span class="st">"12345 + 67890 = 80235"</span>,</span>
<span id="cb22-876"><a href="#cb22-876" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb22-877"><a href="#cb22-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-878"><a href="#cb22-878" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Token数量比较：</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb22-879"><a href="#cb22-879" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> test_texts:</span>
<span id="cb22-880"><a href="#cb22-880" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text: </span><span class="sc">{</span>text[:<span class="dv">50</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb22-881"><a href="#cb22-881" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, tok <span class="kw">in</span> tokenizers.items():</span>
<span id="cb22-882"><a href="#cb22-882" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> tok.tokenize(text)</span>
<span id="cb22-883"><a href="#cb22-883" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb22-884"><a href="#cb22-884" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb22-885"><a href="#cb22-885" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-886"><a href="#cb22-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-887"><a href="#cb22-887" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练自定义Tokenizer</span></span>
<span id="cb22-888"><a href="#cb22-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-891"><a href="#cb22-891" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb22-892"><a href="#cb22-892" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb22-893"><a href="#cb22-893" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb22-894"><a href="#cb22-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-895"><a href="#cb22-895" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer, models, trainers, pre_tokenizers, decoders</span>
<span id="cb22-896"><a href="#cb22-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-897"><a href="#cb22-897" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建BPE tokenizer</span></span>
<span id="cb22-898"><a href="#cb22-898" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(models.BPE())</span>
<span id="cb22-899"><a href="#cb22-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-900"><a href="#cb22-900" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置预分词器（按空格和标点分割）</span></span>
<span id="cb22-901"><a href="#cb22-901" aria-hidden="true" tabindex="-1"></a>tokenizer.pre_tokenizer <span class="op">=</span> pre_tokenizers.ByteLevel(add_prefix_space<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-902"><a href="#cb22-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-903"><a href="#cb22-903" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置解码器</span></span>
<span id="cb22-904"><a href="#cb22-904" aria-hidden="true" tabindex="-1"></a>tokenizer.decoder <span class="op">=</span> decoders.ByteLevel()</span>
<span id="cb22-905"><a href="#cb22-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-906"><a href="#cb22-906" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备训练器</span></span>
<span id="cb22-907"><a href="#cb22-907" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> trainers.BpeTrainer(</span>
<span id="cb22-908"><a href="#cb22-908" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>,</span>
<span id="cb22-909"><a href="#cb22-909" aria-hidden="true" tabindex="-1"></a>    min_frequency<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb22-910"><a href="#cb22-910" aria-hidden="true" tabindex="-1"></a>    special_tokens<span class="op">=</span>[<span class="st">"&lt;pad&gt;"</span>, <span class="st">"&lt;unk&gt;"</span>, <span class="st">"&lt;s&gt;"</span>, <span class="st">"&lt;/s&gt;"</span>, <span class="st">"&lt;mask&gt;"</span>]</span>
<span id="cb22-911"><a href="#cb22-911" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-912"><a href="#cb22-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-913"><a href="#cb22-913" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练（从文件或迭代器）</span></span>
<span id="cb22-914"><a href="#cb22-914" aria-hidden="true" tabindex="-1"></a>files <span class="op">=</span> [<span class="st">"train_data.txt"</span>]</span>
<span id="cb22-915"><a href="#cb22-915" aria-hidden="true" tabindex="-1"></a>tokenizer.train(files, trainer)</span>
<span id="cb22-916"><a href="#cb22-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-917"><a href="#cb22-917" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存</span></span>
<span id="cb22-918"><a href="#cb22-918" aria-hidden="true" tabindex="-1"></a>tokenizer.save(<span class="st">"my_tokenizer.json"</span>)</span>
<span id="cb22-919"><a href="#cb22-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-920"><a href="#cb22-920" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载并使用</span></span>
<span id="cb22-921"><a href="#cb22-921" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb22-922"><a href="#cb22-922" aria-hidden="true" tabindex="-1"></a>loaded_tokenizer <span class="op">=</span> Tokenizer.from_file(<span class="st">"my_tokenizer.json"</span>)</span>
<span id="cb22-923"><a href="#cb22-923" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> loaded_tokenizer.encode(<span class="st">"Hello, world!"</span>)</span>
<span id="cb22-924"><a href="#cb22-924" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>output<span class="sc">.</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-925"><a href="#cb22-925" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"IDs: </span><span class="sc">{</span>output<span class="sc">.</span>ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-926"><a href="#cb22-926" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-927"><a href="#cb22-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-928"><a href="#cb22-928" aria-hidden="true" tabindex="-1"></a><span class="fu">### 分析Tokenization效率</span></span>
<span id="cb22-929"><a href="#cb22-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-932"><a href="#cb22-932" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb22-933"><a href="#cb22-933" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb22-934"><a href="#cb22-934" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb22-935"><a href="#cb22-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-936"><a href="#cb22-936" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-937"><a href="#cb22-937" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb22-938"><a href="#cb22-938" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb22-939"><a href="#cb22-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-940"><a href="#cb22-940" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_tokenizer(tokenizer, texts):</span>
<span id="cb22-941"><a href="#cb22-941" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""分析tokenizer在给定文本上的效率"""</span></span>
<span id="cb22-942"><a href="#cb22-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-943"><a href="#cb22-943" aria-hidden="true" tabindex="-1"></a>    total_chars <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(t) <span class="cf">for</span> t <span class="kw">in</span> texts)</span>
<span id="cb22-944"><a href="#cb22-944" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(tokenizer.tokenize(t)) <span class="cf">for</span> t <span class="kw">in</span> texts)</span>
<span id="cb22-945"><a href="#cb22-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-946"><a href="#cb22-946" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 字符/token比率（越高越高效）</span></span>
<span id="cb22-947"><a href="#cb22-947" aria-hidden="true" tabindex="-1"></a>    ratio <span class="op">=</span> total_chars <span class="op">/</span> total_tokens</span>
<span id="cb22-948"><a href="#cb22-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-949"><a href="#cb22-949" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Token长度分布</span></span>
<span id="cb22-950"><a href="#cb22-950" aria-hidden="true" tabindex="-1"></a>    all_tokens <span class="op">=</span> []</span>
<span id="cb22-951"><a href="#cb22-951" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> texts:</span>
<span id="cb22-952"><a href="#cb22-952" aria-hidden="true" tabindex="-1"></a>        all_tokens.extend(tokenizer.tokenize(t))</span>
<span id="cb22-953"><a href="#cb22-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-954"><a href="#cb22-954" aria-hidden="true" tabindex="-1"></a>    token_lengths <span class="op">=</span> [<span class="bu">len</span>(t.replace(<span class="st">'Ġ'</span>, <span class="st">''</span>).replace(<span class="st">'▁'</span>, <span class="st">''</span>)) <span class="cf">for</span> t <span class="kw">in</span> all_tokens]</span>
<span id="cb22-955"><a href="#cb22-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-956"><a href="#cb22-956" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total characters: </span><span class="sc">{</span>total_chars<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-957"><a href="#cb22-957" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total tokens: </span><span class="sc">{</span>total_tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-958"><a href="#cb22-958" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Characters per token: </span><span class="sc">{</span>ratio<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb22-959"><a href="#cb22-959" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token length distribution:"</span>)</span>
<span id="cb22-960"><a href="#cb22-960" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Mean: </span><span class="sc">{</span>np<span class="sc">.</span>mean(token_lengths)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb22-961"><a href="#cb22-961" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Median: </span><span class="sc">{</span>np<span class="sc">.</span>median(token_lengths)<span class="sc">:.0f}</span><span class="ss">"</span>)</span>
<span id="cb22-962"><a href="#cb22-962" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Min: </span><span class="sc">{</span><span class="bu">min</span>(token_lengths)<span class="sc">}</span><span class="ss">, Max: </span><span class="sc">{</span><span class="bu">max</span>(token_lengths)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-963"><a href="#cb22-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-964"><a href="#cb22-964" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 最常见的token</span></span>
<span id="cb22-965"><a href="#cb22-965" aria-hidden="true" tabindex="-1"></a>    token_counts <span class="op">=</span> Counter(all_tokens)</span>
<span id="cb22-966"><a href="#cb22-966" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top 10 most common tokens:"</span>)</span>
<span id="cb22-967"><a href="#cb22-967" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token, count <span class="kw">in</span> token_counts.most_common(<span class="dv">10</span>):</span>
<span id="cb22-968"><a href="#cb22-968" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">repr</span>(token)<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-969"><a href="#cb22-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-970"><a href="#cb22-970" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb22-971"><a href="#cb22-971" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb22-972"><a href="#cb22-972" aria-hidden="true" tabindex="-1"></a>sample_texts <span class="op">=</span> [</span>
<span id="cb22-973"><a href="#cb22-973" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Machine learning is a subset of artificial intelligence."</span>,</span>
<span id="cb22-974"><a href="#cb22-974" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep neural networks have revolutionized natural language processing."</span>,</span>
<span id="cb22-975"><a href="#cb22-975" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Transformers use self-attention mechanisms to process sequential data."</span>,</span>
<span id="cb22-976"><a href="#cb22-976" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb22-977"><a href="#cb22-977" aria-hidden="true" tabindex="-1"></a>analyze_tokenizer(tokenizer, sample_texts)</span>
<span id="cb22-978"><a href="#cb22-978" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb22-979"><a href="#cb22-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-980"><a href="#cb22-980" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-981"><a href="#cb22-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-982"><a href="#cb22-982" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb22-983"><a href="#cb22-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-984"><a href="#cb22-984" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tokenization的固有局限</span></span>
<span id="cb22-985"><a href="#cb22-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-986"><a href="#cb22-986" aria-hidden="true" tabindex="-1"></a>即使是最先进的子词tokenizer，也有一些固有的局限性。</span>
<span id="cb22-987"><a href="#cb22-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-988"><a href="#cb22-988" aria-hidden="true" tabindex="-1"></a>**语言偏见无法完全消除**。Tokenizer是在特定语料上训练的，必然反映该语料的语言分布。如果训练语料以英文为主，tokenizer就会对英文优化。这不是算法的问题，而是数据的问题——真正的多语言公平需要在数据收集阶段就考虑。</span>
<span id="cb22-989"><a href="#cb22-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-990"><a href="#cb22-990" aria-hidden="true" tabindex="-1"></a>**分词边界可能破坏语义**。无论BPE还是Unigram，都是基于统计的方法，不理解语言的语义结构。"unhappiness"可能被切分为"un"+"happiness"（语义上合理），也可能被切分为"unha"+"ppiness"（纯粹基于频率）。后者对模型学习不利。</span>
<span id="cb22-991"><a href="#cb22-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-992"><a href="#cb22-992" aria-hidden="true" tabindex="-1"></a>**Tokenization与模型训练的割裂**。目前的做法是先确定tokenizer，然后固定它训练模型。但最优的tokenization可能依赖于下游任务和模型架构。这种割裂是否限制了模型的上限？</span>
<span id="cb22-993"><a href="#cb22-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-994"><a href="#cb22-994" aria-hidden="true" tabindex="-1"></a><span class="fu">### 下一章的铺垫</span></span>
<span id="cb22-995"><a href="#cb22-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-996"><a href="#cb22-996" aria-hidden="true" tabindex="-1"></a>我们已经知道如何把文本转换为token序列，也知道如何用词向量表示每个token。但一个句子不只是token的无序集合——**词序蕴含着关键的语义信息**。</span>
<span id="cb22-997"><a href="#cb22-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-998"><a href="#cb22-998" aria-hidden="true" tabindex="-1"></a>"狗咬人"和"人咬狗"有完全不同的含义，但如果我们只是把词向量加起来或取平均，就会丢失这种区分。我们需要一种方法来建模**序列结构**，捕获词与词之间的顺序关系和长距离依赖。</span>
<span id="cb22-999"><a href="#cb22-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1000"><a href="#cb22-1000" aria-hidden="true" tabindex="-1"></a>这正是下一章要讨论的问题。循环神经网络（RNN）通过"记忆"之前看到的内容，让模型能够处理变长序列。LSTM和GRU通过门控机制解决了梯度消失问题。但它们也有自己的局限——无法并行、长距离依赖仍然困难——这些局限最终导向了Attention机制和Transformer的诞生。</span>
<span id="cb22-1001"><a href="#cb22-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1002"><a href="#cb22-1002" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第4章将介绍循环神经网络，理解它如何通过"时间上的权重共享"来处理序列，以及门控机制如何让信息在长序列中存活。</span></span>
<span id="cb22-1003"><a href="#cb22-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1004"><a href="#cb22-1004" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-1005"><a href="#cb22-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1006"><a href="#cb22-1006" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb22-1007"><a href="#cb22-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1008"><a href="#cb22-1008" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb22-1009"><a href="#cb22-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1010"><a href="#cb22-1010" aria-hidden="true" tabindex="-1"></a>这一章我们深入探讨了一个常被忽视但极其重要的问题：**如何将文本切分为模型可处理的单元**。</span>
<span id="cb22-1011"><a href="#cb22-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1012"><a href="#cb22-1012" aria-hidden="true" tabindex="-1"></a>核心洞察是：**Tokenizer不是预处理工具，而是模型架构的隐藏维度**。它影响：</span>
<span id="cb22-1013"><a href="#cb22-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1014"><a href="#cb22-1014" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**计算效率**：序列长度直接决定计算成本</span>
<span id="cb22-1015"><a href="#cb22-1015" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**多语言公平性**：不同语言的token效率差异导致系统性不平等</span>
<span id="cb22-1016"><a href="#cb22-1016" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**模型能力**：数字、代码的切分方式影响相应的推理能力</span>
<span id="cb22-1017"><a href="#cb22-1017" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**安全性**：tokenization边界可能被攻击者利用</span>
<span id="cb22-1018"><a href="#cb22-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1019"><a href="#cb22-1019" aria-hidden="true" tabindex="-1"></a>我们学习了三种基本的tokenization策略：</span>
<span id="cb22-1020"><a href="#cb22-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1021"><a href="#cb22-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**词级别**：语义清晰但OOV严重、词汇表爆炸</span>
<span id="cb22-1022"><a href="#cb22-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**字符级别**：没有OOV但序列太长、语义碎片化</span>
<span id="cb22-1023"><a href="#cb22-1023" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**子词级别**：在两者之间找到平衡，是现代NLP的标准选择</span>
<span id="cb22-1024"><a href="#cb22-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1025"><a href="#cb22-1025" aria-hidden="true" tabindex="-1"></a>子词分词的核心算法包括：</span>
<span id="cb22-1026"><a href="#cb22-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1027"><a href="#cb22-1027" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BPE**：贪心地合并最频繁的相邻对，从字符构建词汇表</span>
<span id="cb22-1028"><a href="#cb22-1028" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**WordPiece**：类似BPE但用likelihood改进选择合并对</span>
<span id="cb22-1029"><a href="#cb22-1029" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Unigram**：概率视角，从大词汇表剪枝到目标大小</span>
<span id="cb22-1030"><a href="#cb22-1030" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Byte-level BPE**：在字节级别操作，彻底解决未知字符问题</span>
<span id="cb22-1031"><a href="#cb22-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1032"><a href="#cb22-1032" aria-hidden="true" tabindex="-1"></a>最后，我们讨论了数据质量的重要性——清洗、去重、避免污染是大模型训练的关键环节。</span>
<span id="cb22-1033"><a href="#cb22-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1034"><a href="#cb22-1034" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键概念速查</span></span>
<span id="cb22-1035"><a href="#cb22-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1036"><a href="#cb22-1036" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 概念 <span class="pp">|</span> 含义 <span class="pp">|</span></span>
<span id="cb22-1037"><a href="#cb22-1037" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb22-1038"><a href="#cb22-1038" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BPE <span class="pp">|</span> Byte Pair Encoding，迭代合并最频繁token对 <span class="pp">|</span></span>
<span id="cb22-1039"><a href="#cb22-1039" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> WordPiece <span class="pp">|</span> Google的子词算法，用于BERT <span class="pp">|</span></span>
<span id="cb22-1040"><a href="#cb22-1040" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Unigram LM <span class="pp">|</span> 概率视角的子词分词，支持多种切分 <span class="pp">|</span></span>
<span id="cb22-1041"><a href="#cb22-1041" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SentencePiece <span class="pp">|</span> 语言无关的tokenization库 <span class="pp">|</span></span>
<span id="cb22-1042"><a href="#cb22-1042" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Byte-level <span class="pp">|</span> 在字节级别操作，彻底解决OOV <span class="pp">|</span></span>
<span id="cb22-1043"><a href="#cb22-1043" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> OOV <span class="pp">|</span> Out-of-Vocabulary，无法处理的未知词 <span class="pp">|</span></span>
<span id="cb22-1044"><a href="#cb22-1044" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 数据污染 <span class="pp">|</span> 测试集内容出现在训练集中 <span class="pp">|</span></span>
<span id="cb22-1045"><a href="#cb22-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1046"><a href="#cb22-1046" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb22-1047"><a href="#cb22-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1048"><a href="#cb22-1048" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么说"Tokenizer是模型架构的隐藏维度"？试举三个具体例子说明tokenizer选择如何影响模型行为。</span>
<span id="cb22-1049"><a href="#cb22-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1050"><a href="#cb22-1050" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[算法实践]** 手动执行BPE算法。给定语料 {"ab": 5, "abc": 3, "abcd": 2}，执行3次合并，写出每一步的词汇表和合并规则。</span>
<span id="cb22-1051"><a href="#cb22-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1052"><a href="#cb22-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用Hugging Face的tokenizers库，比较GPT-2、BERT、T5的tokenizer在中文文本上的效率。计算每个tokenizer的"字符/token比率"，并分析差异的原因。</span>
<span id="cb22-1053"><a href="#cb22-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1054"><a href="#cb22-1054" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** 设计一个实验来量化tokenization对模型算术能力的影响。你会如何控制变量？需要什么样的数据集？</span>
<span id="cb22-1055"><a href="#cb22-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1056"><a href="#cb22-1056" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-1057"><a href="#cb22-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1058"><a href="#cb22-1058" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb22-1059"><a href="#cb22-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1060"><a href="#cb22-1060" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb22-1061"><a href="#cb22-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1062"><a href="#cb22-1062" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) (Sennrich et al., 2016)**：BPE在NLP中的应用</span>
<span id="cb22-1063"><a href="#cb22-1063" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**arXiv**: 1508.07909</span>
<span id="cb22-1064"><a href="#cb22-1064" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**重点读**：</span>
<span id="cb22-1065"><a href="#cb22-1065" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Section 3.2：BPE算法描述和Algorithm 1伪代码</span>
<span id="cb22-1066"><a href="#cb22-1066" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Table 1：不同分词方法的统计对比</span>
<span id="cb22-1067"><a href="#cb22-1067" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Section 5.2：翻译示例分析</span>
<span id="cb22-1068"><a href="#cb22-1068" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**可跳过**：Section 4的实验细节（除非你要做机器翻译）</span>
<span id="cb22-1069"><a href="#cb22-1069" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**核心贡献**：将1994年的数据压缩算法引入NLP，解决OOV问题</span>
<span id="cb22-1070"><a href="#cb22-1070" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**官方代码**：<span class="co">[</span><span class="ot">github.com/rsennrich/subword-nmt</span><span class="co">](https://github.com/rsennrich/subword-nmt)</span></span>
<span id="cb22-1071"><a href="#cb22-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1072"><a href="#cb22-1072" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[SentencePiece: A simple and language independent subword tokenizer](https://arxiv.org/abs/1808.06226) (Kudo &amp; Richardson, 2018)**</span>
<span id="cb22-1073"><a href="#cb22-1073" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**arXiv**: 1808.06226</span>
<span id="cb22-1074"><a href="#cb22-1074" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**重点读**：语言无关设计的动机——为什么不需要预分词</span>
<span id="cb22-1075"><a href="#cb22-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1076"><a href="#cb22-1076" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论分析</span></span>
<span id="cb22-1077"><a href="#cb22-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1078"><a href="#cb22-1078" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959) (Kudo, 2018)**：Unigram LM的原始论文</span>
<span id="cb22-1079"><a href="#cb22-1079" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**arXiv**: 1804.10959</span>
<span id="cb22-1080"><a href="#cb22-1080" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>概率视角的子词分词，支持训练时采样不同切分</span>
<span id="cb22-1081"><a href="#cb22-1081" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**重点读**：Section 3的Unigram Language Model formulation</span>
<span id="cb22-1082"><a href="#cb22-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1083"><a href="#cb22-1083" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实证研究</span></span>
<span id="cb22-1084"><a href="#cb22-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1085"><a href="#cb22-1085" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**How Good is Your Tokenizer? (Rust et al., 2021)**：多语言tokenizer的系统性评估</span>
<span id="cb22-1086"><a href="#cb22-1086" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>揭示了英语偏见的严重程度</span>
<span id="cb22-1087"><a href="#cb22-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1088"><a href="#cb22-1088" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tokenizer Choice Matters: Downstream Tasks Benefit from Task-Specific Tokenization (2023)**</span>
<span id="cb22-1089"><a href="#cb22-1089" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>不同任务可能需要不同的tokenization策略</span>
<span id="cb22-1090"><a href="#cb22-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1091"><a href="#cb22-1091" aria-hidden="true" tabindex="-1"></a><span class="fu">### 工具和资源</span></span>
<span id="cb22-1092"><a href="#cb22-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1093"><a href="#cb22-1093" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Tokenizers库**：高效的tokenizer训练和使用</span>
<span id="cb22-1094"><a href="#cb22-1094" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SentencePiece官方库**：Google的tokenization工具</span>
<span id="cb22-1095"><a href="#cb22-1095" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**tiktoken**：OpenAI的BPE实现，用于GPT系列</span>
<span id="cb22-1096"><a href="#cb22-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1097"><a href="#cb22-1097" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb22-1098"><a href="#cb22-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1099"><a href="#cb22-1099" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb22-1100"><a href="#cb22-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1101"><a href="#cb22-1101" aria-hidden="true" tabindex="-1"></a>BPE（Byte Pair Encoding）的历史颇为有趣。它最初是Philip Gage在1994年提出的**数据压缩算法**，与NLP毫无关系。算法思想很简单：找到数据中最频繁的字节对，用一个新字节替换它们，重复这个过程直到无法进一步压缩。</span>
<span id="cb22-1102"><a href="#cb22-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1103"><a href="#cb22-1103" aria-hidden="true" tabindex="-1"></a>2016年，Edinburgh大学的Rico Sennrich等人在研究机器翻译时遇到了一个问题：神经翻译模型无法处理罕见词。他们想到了BPE——既然BPE能够压缩数据，它应该也能产生一种"压缩"的文本表示，其中常见模式被合并为单一单元。</span>
<span id="cb22-1104"><a href="#cb22-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1105"><a href="#cb22-1105" aria-hidden="true" tabindex="-1"></a>这个跨领域的知识迁移非常成功。BPE不仅解决了OOV问题，还显著提升了翻译质量，特别是在形态丰富的语言上。论文发表后迅速被广泛采用，成为现代NLP的标准组件。</span>
<span id="cb22-1106"><a href="#cb22-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1107"><a href="#cb22-1107" aria-hidden="true" tabindex="-1"></a>一个有趣的细节是，Sennrich等人的论文最初投稿时被审稿人质疑"太简单"。但正如Word2Vec的故事，最有影响力的工作往往不是最复杂的，而是找到了正确的简化。BPE的成功说明，有时候领域外的老技术，在新问题上可能有意想不到的效果。</span>
<span id="cb22-1108"><a href="#cb22-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-1109"><a href="#cb22-1109" aria-hidden="true" tabindex="-1"></a>今天，几乎所有大型语言模型都使用BPE或其变体。从GPT到LLaMA，从Claude到Gemini，BPE（或其byte-level版本）是它们的共同基础。一个1994年的数据压缩算法，成为了2020年代AI革命的基石——这本身就是技术史上的一个美丽故事。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>