<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="Transformer的自注意力机制需要O(n²)的计算复杂度，这在处理长序列时成为瓶颈。但O(n²)是序列建模的唯一选择吗？状态空间模型（SSM）提供了一条全新的路径：借鉴控制论中的线性时不变系统，通过结构化的状态转移矩阵实现O(n)或O(n log n)的序列建模。从HiPPO的记忆理论，到S4的结构化参数化，再到Mamba的选择性机制——SSM正在成为Transformer的有力替代者。本章将带你深入这条’从循环到选择’的技术演进之路。">

<title>第28章：状态空间模型——序列建模的另一条路 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#attention的二次瓶颈" id="toc-attention的二次瓶颈" class="nav-link" data-scroll-target="#attention的二次瓶颈"><span class="header-section-number">2.1</span> Attention的二次瓶颈</a></li>
  <li><a href="#重新审视rnn" id="toc-重新审视rnn" class="nav-link" data-scroll-target="#重新审视rnn"><span class="header-section-number">2.2</span> 重新审视RNN</a></li>
  <li><a href="#我们需要什么样的序列模型" id="toc-我们需要什么样的序列模型" class="nav-link" data-scroll-target="#我们需要什么样的序列模型"><span class="header-section-number">2.3</span> 我们需要什么样的序列模型？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#从连续到离散状态空间的基本形式" id="toc-从连续到离散状态空间的基本形式" class="nav-link" data-scroll-target="#从连续到离散状态空间的基本形式"><span class="header-section-number">3.1</span> 从连续到离散：状态空间的基本形式</a></li>
  <li><a href="#离散化从连续到可计算" id="toc-离散化从连续到可计算" class="nav-link" data-scroll-target="#离散化从连续到可计算"><span class="header-section-number">3.2</span> 离散化：从连续到可计算</a></li>
  <li><a href="#递归视角-vs-卷积视角" id="toc-递归视角-vs-卷积视角" class="nav-link" data-scroll-target="#递归视角-vs-卷积视角"><span class="header-section-number">3.3</span> 递归视角 vs 卷积视角</a></li>
  <li><a href="#hippo长程记忆的数学基础" id="toc-hippo长程记忆的数学基础" class="nav-link" data-scroll-target="#hippo长程记忆的数学基础"><span class="header-section-number">3.4</span> HiPPO：长程记忆的数学基础</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#s4结构化状态空间的突破" id="toc-s4结构化状态空间的突破" class="nav-link" data-scroll-target="#s4结构化状态空间的突破"><span class="header-section-number">4.1</span> S4：结构化状态空间的突破</a></li>
  <li><a href="#mamba选择性状态空间" id="toc-mamba选择性状态空间" class="nav-link" data-scroll-target="#mamba选择性状态空间"><span class="header-section-number">4.2</span> Mamba：选择性状态空间</a></li>
  <li><a href="#为什么选择性机制是关键" id="toc-为什么选择性机制是关键" class="nav-link" data-scroll-target="#为什么选择性机制是关键"><span class="header-section-number">4.3</span> 为什么选择性机制是关键？</a></li>
  <li><a href="#硬件感知的选择性扫描算法" id="toc-硬件感知的选择性扫描算法" class="nav-link" data-scroll-target="#硬件感知的选择性扫描算法"><span class="header-section-number">4.4</span> 硬件感知的选择性扫描算法</a></li>
  <li><a href="#mamba架构" id="toc-mamba架构" class="nav-link" data-scroll-target="#mamba架构"><span class="header-section-number">4.5</span> Mamba架构</a></li>
  <li><a href="#scaling-laws与性能" id="toc-scaling-laws与性能" class="nav-link" data-scroll-target="#scaling-laws与性能"><span class="header-section-number">4.6</span> Scaling Laws与性能</a></li>
  <li><a href="#数值示例选择性ssm的状态更新" id="toc-数值示例选择性ssm的状态更新" class="nav-link" data-scroll-target="#数值示例选择性ssm的状态更新"><span class="header-section-number">4.7</span> 数值示例：选择性SSM的状态更新</a></li>
  </ul></li>
  <li><a href="#mamba-2与状态空间对偶性" id="toc-mamba-2与状态空间对偶性" class="nav-link" data-scroll-target="#mamba-2与状态空间对偶性"><span class="header-section-number">5</span> Mamba-2与状态空间对偶性</a>
  <ul class="collapse">
  <li><a href="#ssd结构化状态空间对偶" id="toc-ssd结构化状态空间对偶" class="nav-link" data-scroll-target="#ssd结构化状态空间对偶"><span class="header-section-number">5.1</span> SSD：结构化状态空间对偶</a></li>
  <li><a href="#两种计算视角的统一" id="toc-两种计算视角的统一" class="nav-link" data-scroll-target="#两种计算视角的统一"><span class="header-section-number">5.2</span> 两种计算视角的统一</a></li>
  <li><a href="#mamba-2的改进" id="toc-mamba-2的改进" class="nav-link" data-scroll-target="#mamba-2的改进"><span class="header-section-number">5.3</span> Mamba-2的改进</a></li>
  </ul></li>
  <li><a href="#混合架构结合两种范式的优势" id="toc-混合架构结合两种范式的优势" class="nav-link" data-scroll-target="#混合架构结合两种范式的优势"><span class="header-section-number">6</span> 混合架构：结合两种范式的优势</a>
  <ul class="collapse">
  <li><a href="#为什么需要混合" id="toc-为什么需要混合" class="nav-link" data-scroll-target="#为什么需要混合"><span class="header-section-number">6.1</span> 为什么需要混合？</a></li>
  <li><a href="#jamba混合transformer-mamba架构" id="toc-jamba混合transformer-mamba架构" class="nav-link" data-scroll-target="#jamba混合transformer-mamba架构"><span class="header-section-number">6.2</span> Jamba：混合Transformer-Mamba架构</a></li>
  <li><a href="#混合架构的设计原则" id="toc-混合架构的设计原则" class="nav-link" data-scroll-target="#混合架构的设计原则"><span class="header-section-number">6.3</span> 混合架构的设计原则</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">7</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#使用hugging-face加载mamba模型" id="toc-使用hugging-face加载mamba模型" class="nav-link" data-scroll-target="#使用hugging-face加载mamba模型"><span class="header-section-number">7.1</span> 使用Hugging Face加载Mamba模型</a></li>
  <li><a href="#从零实现简化版ssm" id="toc-从零实现简化版ssm" class="nav-link" data-scroll-target="#从零实现简化版ssm"><span class="header-section-number">7.2</span> 从零实现简化版SSM</a></li>
  <li><a href="#比较mamba与transformer的推理速度" id="toc-比较mamba与transformer的推理速度" class="nav-link" data-scroll-target="#比较mamba与transformer的推理速度"><span class="header-section-number">7.3</span> 比较Mamba与Transformer的推理速度</a></li>
  <li><a href="#可视化ssm的状态演化" id="toc-可视化ssm的状态演化" class="nav-link" data-scroll-target="#可视化ssm的状态演化"><span class="header-section-number">7.4</span> 可视化SSM的状态演化</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">8</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">8.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#为什么有效实证视角" id="toc-为什么有效实证视角" class="nav-link" data-scroll-target="#为什么有效实证视角"><span class="header-section-number">8.2</span> 为什么有效？——实证视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">8.3</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">8.4</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#本方法的局限" id="toc-本方法的局限" class="nav-link" data-scroll-target="#本方法的局限"><span class="header-section-number">9.1</span> 本方法的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">9.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">10</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">10.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">10.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">11</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">11.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">11.3</span> 后续发展</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">11.4</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">11.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">12</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第28章：状态空间模型——序列建模的另一条路</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Beyond Attention: Linear-Time Sequence Modeling with Structured State Spaces</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">SSM</div>
    <div class="quarto-category">Mamba</div>
    <div class="quarto-category">序列建模</div>
    <div class="quarto-category">状态空间</div>
  </div>
  </div>

<div>
  <div class="description">
    Transformer的自注意力机制需要O(n²)的计算复杂度，这在处理长序列时成为瓶颈。但O(n²)是序列建模的唯一选择吗？状态空间模型（SSM）提供了一条全新的路径：借鉴控制论中的线性时不变系统，通过结构化的状态转移矩阵实现O(n)或O(n log n)的序列建模。从HiPPO的记忆理论，到S4的结构化参数化，再到Mamba的选择性机制——SSM正在成为Transformer的有力替代者。本章将带你深入这条’从循环到选择’的技术演进之路。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：Transformer的<span class="math inline">\(O(n^2)\)</span>注意力复杂度是序列建模的唯一选择吗？能否设计一种既能捕获长距离依赖、又具有线性复杂度的序列模型？</p>
<p><strong>历史坐标</strong>：2020–2024 | HiPPO (2020) → S4 (2021) → Mamba (2023) → Mamba-2 (2024) → Jamba (2024) | 从控制论到深度学习的跨界融合</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Gu et al.&nbsp;(2020)</strong> “HiPPO: Recurrent Memory with Optimal Polynomial Projections” (<a href="https://arxiv.org/abs/2008.07669">arXiv:2008.07669</a>) — 参考了 Section 2-4 (HiPPO框架、Legendre多项式投影)；NeurIPS 2020，SSM的理论基础</li>
<li><strong>Gu et al.&nbsp;(2021)</strong> “Efficiently Modeling Long Sequences with Structured State Spaces” (<a href="https://arxiv.org/abs/2111.00396">arXiv:2111.00396</a>) — 参考了 Section 2-4 (S4架构、HiPPO初始化、Cauchy核计算)、Figure 1-3；ICLR 2022 Outstanding Paper</li>
<li><strong>Gu &amp; Dao (2023)</strong> “Mamba: Linear-Time Sequence Modeling with Selective State Spaces” (<a href="https://arxiv.org/abs/2312.00752">arXiv:2312.00752</a>) — 参考了 Section 2-4 (选择性机制、硬件感知算法)、Figure 1-3, 6；从论文提取了4张原图</li>
<li><strong>Dao &amp; Gu (2024)</strong> “Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality” (<a href="https://arxiv.org/abs/2405.21060">arXiv:2405.21060</a>) — 参考了 Section 2-4 (状态空间对偶性、SSM与Attention的联系)；ICML 2024</li>
<li><strong>Lieber et al.&nbsp;(2024)</strong> “Jamba: A Hybrid Transformer-Mamba Language Model” (<a href="https://arxiv.org/abs/2403.19887">arXiv:2403.19887</a>) — 参考了 Section 2-3 (混合架构设计、Attention:Mamba比例)</li>
</ul>
</section>
<section id="教材与博客" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材与博客"><span class="header-section-number">0.2</span> 教材与博客</h3>
<ul>
<li><strong>Goomba Lab</strong> “State Space Duality (Mamba-2) Part I &amp; II” — 参考了Mamba-2的详细解释和代码实现</li>
<li><strong>Princeton PLI</strong> “Mamba-2: Algorithms and Systems” — 参考了SSD算法的解释</li>
</ul>
</section>
<section id="综述" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="综述"><span class="header-section-number">0.3</span> 综述</h3>
<ul>
<li><strong>Wang et al.&nbsp;(2024)</strong> “State Space Model for New-Generation Network Alternative to Transformers: A Survey” — 参考了SSM演进脉络的梳理</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们探讨了Mixture of Experts（MoE）——通过稀疏激活实现参数量与计算量的解耦。MoE让模型可以拥有数千亿甚至万亿参数，而每次推理只激活其中一小部分。这是对Dense Transformer第一个根本挑战（“所有参数对每个token都激活”）的优雅回应。</p>
<p>然而，MoE解决的是FFN层的计算效率问题，并没有触及Transformer的另一个核心瓶颈：<strong>自注意力的<span class="math inline">\(O(n^2)\)</span>复杂度</strong>。</p>
<p>让我们用具体数字来感受这个问题的严重性。假设序列长度<span class="math inline">\(n = 4096\)</span>（这在2024年已经算是”短序列”了）：</p>
<ul>
<li>注意力矩阵大小：<span class="math inline">\(4096 \times 4096 = 16.7M\)</span>个元素</li>
<li>如果序列长度翻倍到<span class="math inline">\(n = 8192\)</span>：矩阵大小变为<span class="math inline">\(67M\)</span>，增加了4倍</li>
<li>如果是GPT-4级别的<span class="math inline">\(n = 128K\)</span>：矩阵大小达到<span class="math inline">\(16.4B\)</span>个元素</li>
</ul>
<p>这种二次增长意味着：<strong>处理长文档、长对话、代码仓库、甚至整本书时，注意力计算会成为不可逾越的瓶颈</strong>。</p>
<p>上一章结尾我们提到，除了让模型”更大”（MoE），还有一条路径是让模型”更高效地利用参数”。这正是状态空间模型（State Space Model, SSM）要探索的方向。</p>
<p>SSM的核心问题是：<strong>序列建模一定需要让每个位置都与所有其他位置交互吗？</strong></p>
<p>回想一下RNN——它用一个隐状态<span class="math inline">\(h_t\)</span>来压缩所有历史信息，只需要<span class="math inline">\(O(n)\)</span>的计算。RNN的问题不是复杂度，而是：</p>
<ol type="1">
<li>顺序计算，无法并行训练</li>
<li>梯度消失/爆炸，难以建模长距离依赖</li>
</ol>
<p>如果我们能保留RNN的线性复杂度优势，同时解决它的并行性和长程依赖问题呢？</p>
<p>这正是状态空间模型的出发点。SSM借鉴了控制论中研究了几十年的线性时不变系统（Linear Time-Invariant, LTI），通过精心设计的状态转移矩阵，实现了：</p>
<ul>
<li><strong>可并行训练</strong>：通过卷积视角</li>
<li><strong>高效推理</strong>：通过递归视角</li>
<li><strong>长程依赖</strong>：通过HiPPO初始化</li>
</ul>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：序列建模不一定需要attention。通过将连续时间的状态空间方程离散化，并用结构化的方式参数化状态矩阵，可以得到一类既能并行训练、又有线性推理复杂度、还能有效捕获长距离依赖的序列模型。Mamba更进一步，通过引入”选择性机制”让模型参数依赖于输入内容，弥补了LTI模型无法进行内容感知推理的根本缺陷。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="attention的二次瓶颈" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="attention的二次瓶颈"><span class="header-section-number">2.1</span> Attention的二次瓶颈</h3>
<p>Transformer的自注意力计算如下：</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]</span></p>
<p>这个公式中，<span class="math inline">\(QK^\top\)</span>是一个<span class="math inline">\(n \times n\)</span>的矩阵，其中<span class="math inline">\(n\)</span>是序列长度。这意味着：</p>
<ul>
<li><strong>计算复杂度</strong>：<span class="math inline">\(O(n^2 d)\)</span></li>
<li><strong>内存复杂度</strong>：<span class="math inline">\(O(n^2)\)</span>（需要存储完整的注意力矩阵）</li>
</ul>
<p>当<span class="math inline">\(n\)</span>增大时，这种二次增长很快变得不可接受。例如：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>序列长度</th>
<th>注意力矩阵大小</th>
<th>FP16内存</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2,048</td>
<td>4.2M</td>
<td>8 MB</td>
</tr>
<tr class="even">
<td>8,192</td>
<td>67M</td>
<td>134 MB</td>
</tr>
<tr class="odd">
<td>32,768</td>
<td>1.07B</td>
<td>2.1 GB</td>
</tr>
<tr class="even">
<td>131,072</td>
<td>17.2B</td>
<td>34 GB</td>
</tr>
</tbody>
</table>
<p>即使有FlashAttention这样的IO优化，<span class="math inline">\(O(n^2)\)</span>的本质没有改变——它只是把常数因子降低了，但增长速度仍然是二次的。</p>
</section>
<section id="重新审视rnn" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="重新审视rnn"><span class="header-section-number">2.2</span> 重新审视RNN</h3>
<p>在Attention成为主流之前，RNN是序列建模的标准方法。RNN的核心是一个简单的递归方程：</p>
<p><span class="math display">\[
h_t = \sigma(W_h h_{t-1} + W_x x_t + b)
\]</span></p>
<p>其中<span class="math inline">\(h_t\)</span>是时刻<span class="math inline">\(t\)</span>的隐状态，<span class="math inline">\(x_t\)</span>是输入。</p>
<p>RNN的计算复杂度是<span class="math inline">\(O(n)\)</span>——这是线性的！每个时间步只需要常数时间的计算。那为什么RNN被Transformer取代了呢？</p>
<p><strong>问题一：顺序依赖，无法并行</strong></p>
<p>计算<span class="math inline">\(h_t\)</span>必须先知道<span class="math inline">\(h_{t-1}\)</span>，而<span class="math inline">\(h_{t-1}\)</span>必须先知道<span class="math inline">\(h_{t-2}\)</span>…这意味着训练时必须按顺序计算，无法利用GPU的并行能力。一个长度为1000的序列，RNN需要串行执行1000步；而Transformer可以一次性并行计算所有位置。</p>
<p><strong>问题二：梯度消失/爆炸</strong></p>
<p>当反向传播经过很多时间步时，梯度要么指数级衰减（消失），要么指数级增长（爆炸）。LSTM和GRU通过门控机制缓解了这个问题，但并没有根本解决。</p>
<p><strong>问题三：信息压缩瓶颈</strong></p>
<p>所有历史信息都被压缩到一个固定维度的向量<span class="math inline">\(h_t\)</span>中。当序列很长时，早期信息必然会被”覆盖”或”遗忘”。</p>
</section>
<section id="我们需要什么样的序列模型" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的序列模型"><span class="header-section-number">2.3</span> 我们需要什么样的序列模型？</h3>
<p>理想的序列模型应该同时具备：</p>
<ol type="1">
<li><strong>线性复杂度</strong>：<span class="math inline">\(O(n)\)</span>或<span class="math inline">\(O(n \log n)\)</span>，而非<span class="math inline">\(O(n^2)\)</span></li>
<li><strong>可并行训练</strong>：能够充分利用GPU的并行计算能力</li>
<li><strong>长程依赖建模</strong>：能够有效记住和利用远距离的信息</li>
<li><strong>高效推理</strong>：生成任务中能够像RNN一样常数时间更新状态</li>
</ol>
<p>状态空间模型（SSM）正是朝着这个目标前进的一次重要尝试。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="从连续到离散状态空间的基本形式" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="从连续到离散状态空间的基本形式"><span class="header-section-number">3.1</span> 从连续到离散：状态空间的基本形式</h3>
<p>状态空间模型起源于控制论，描述的是一类连续时间的动态系统。最简单的形式是线性时不变（LTI）系统：</p>
<p><span class="math display">\[
\frac{dx(t)}{dt} = Ax(t) + Bu(t)
\]</span> <span class="math display">\[
y(t) = Cx(t) + Du(t)
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(u(t) \in \mathbb{R}\)</span>：输入信号（控制输入）</li>
<li><span class="math inline">\(x(t) \in \mathbb{R}^N\)</span>：状态向量（系统的”记忆”）</li>
<li><span class="math inline">\(y(t) \in \mathbb{R}\)</span>：输出信号</li>
<li><span class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span>：状态转移矩阵（决定状态如何演化）</li>
<li><span class="math inline">\(B \in \mathbb{R}^{N \times 1}\)</span>：输入矩阵</li>
<li><span class="math inline">\(C \in \mathbb{R}^{1 \times N}\)</span>：输出矩阵</li>
<li><span class="math inline">\(D \in \mathbb{R}\)</span>：直接传递项（通常设为0或忽略）</li>
</ul>
<p>这个系统的直觉是：</p>
<ul>
<li><strong>状态<span class="math inline">\(x(t)\)</span>是系统的”记忆”</strong>：它编码了所有历史输入的信息</li>
<li><strong>矩阵<span class="math inline">\(A\)</span>决定了”遗忘模式”</strong>：它控制状态如何随时间演化</li>
<li><strong>矩阵<span class="math inline">\(B\)</span>决定了”输入方式”</strong>：新信息如何进入状态</li>
<li><strong>矩阵<span class="math inline">\(C\)</span>决定了”读取方式”</strong>：如何从状态中提取输出</li>
</ul>
<p>用一个简单的类比：想象一个水箱系统。水箱中的水位就是”状态”，进水管道的流量是”输入”，出水管道的流量是”输出”。<span class="math inline">\(A\)</span>决定了水箱的”漏水率”（状态的自然衰减），<span class="math inline">\(B\)</span>决定了进水管道的效率，<span class="math inline">\(C\)</span>决定了出水管道的效率。</p>
</section>
<section id="离散化从连续到可计算" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="离散化从连续到可计算"><span class="header-section-number">3.2</span> 离散化：从连续到可计算</h3>
<p>神经网络处理的是离散的序列（token序列），而不是连续的信号。因此需要将连续的状态空间方程<strong>离散化</strong>。</p>
<p>最常用的方法是<strong>零阶保持（Zero-Order Hold, ZOH）</strong>离散化。假设输入信号在每个采样间隔<span class="math inline">\(\Delta\)</span>内保持常数，可以得到离散形式：</p>
<p><span class="math display">\[
x_k = \bar{A} x_{k-1} + \bar{B} u_k
\]</span> <span class="math display">\[
y_k = C x_k
\]</span></p>
<p>其中离散化参数为：</p>
<p><span class="math display">\[
\bar{A} = \exp(\Delta A)
\]</span> <span class="math display">\[
\bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B
\]</span></p>
<p>这个形式和RNN非常相似！关键区别在于：</p>
<ul>
<li><strong>矩阵<span class="math inline">\(\bar{A}\)</span>不是任意学习的，而是通过连续矩阵<span class="math inline">\(A\)</span>和步长<span class="math inline">\(\Delta\)</span>推导出来的</strong></li>
<li><strong>通过特殊的<span class="math inline">\(A\)</span>矩阵初始化（如HiPPO），可以获得长程记忆能力</strong></li>
</ul>
</section>
<section id="递归视角-vs-卷积视角" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="递归视角-vs-卷积视角"><span class="header-section-number">3.3</span> 递归视角 vs 卷积视角</h3>
<p>SSM的一个关键优势是它有两种等价的计算方式：</p>
<p><strong>递归视角（Recurrent View）</strong>：</p>
<p>逐步计算状态更新：</p>
<p><span class="math display">\[
x_k = \bar{A} x_{k-1} + \bar{B} u_k, \quad y_k = C x_k
\]</span></p>
<p>这种方式适合<strong>推理</strong>——每生成一个新token，只需要<span class="math inline">\(O(N)\)</span>的计算来更新状态。</p>
<p><strong>卷积视角（Convolutional View）</strong>：</p>
<p>将整个输入序列和一个卷积核进行卷积：</p>
<p><span class="math display">\[
\bar{K} = (C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B}, \ldots, C\bar{A}^{L-1}\bar{B})
\]</span> <span class="math display">\[
y = \bar{K} * u
\]</span></p>
<p>这种方式适合<strong>训练</strong>——卷积可以用FFT高效计算，复杂度为<span class="math inline">\(O(n \log n)\)</span>，而且可以高度并行。</p>
<p>这种”训练时用卷积，推理时用递归”的双重性质，是SSM相对于传统RNN的关键优势。</p>
</section>
<section id="hippo长程记忆的数学基础" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="hippo长程记忆的数学基础"><span class="header-section-number">3.4</span> HiPPO：长程记忆的数学基础</h3>
<p>普通的RNN为什么无法建模长距离依赖？根本原因是：当矩阵<span class="math inline">\(A\)</span>（或<span class="math inline">\(\bar{A}\)</span>）被随机初始化时，状态会快速”遗忘”早期信息。</p>
<p>HiPPO（High-order Polynomial Projection Operators）提供了一个精妙的解决方案。核心思想是：<strong>用正交多项式来近似历史信号</strong>。</p>
<p>具体来说，HiPPO将历史输入信号<span class="math inline">\(u(\tau)\)</span>（<span class="math inline">\(\tau \leq t\)</span>）投影到Legendre多项式的系数上。这些系数就是状态向量<span class="math inline">\(x(t)\)</span>的各个分量。</p>
<p>为什么这样做有效？因为：</p>
<ol type="1">
<li><strong>正交多项式系数可以增量更新</strong>：当新输入到达时，不需要重新计算所有系数</li>
<li><strong>低阶系数捕获”全局趋势”</strong>：即使序列很长，也能保留关于整体形状的信息</li>
<li><strong>高阶系数捕获”局部细节”</strong>：近期的细节信息也不会丢失</li>
</ol>
<p>HiPPO-LegS（Legendre Scaled）版本的<span class="math inline">\(A\)</span>矩阵有如下形式：</p>
<p><span class="math display">\[
A_{nk} = -\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2} &amp; \text{if } n &gt; k \\
n+1 &amp; \text{if } n = k \\
0 &amp; \text{if } n &lt; k
\end{cases}
\]</span></p>
<p>这是一个下三角矩阵，具有特殊的数学性质，能够保证状态向量以最优方式压缩历史信息。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="s4结构化状态空间的突破" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="s4结构化状态空间的突破"><span class="header-section-number">4.1</span> S4：结构化状态空间的突破</h3>
<p>S4（Structured State Space for Sequences）是将SSM引入深度学习的里程碑工作。它解决了一个关键的计算瓶颈：<strong>如何高效计算卷积核<span class="math inline">\(\bar{K}\)</span>？</strong></p>
<div id="fig-s4-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-s4-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-28/original/fig-s4-architecture.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-s4-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: S4模型架构概览。左侧展示了状态空间模型的基本形式（连续→离散化→递归/卷积双视角）；中间展示了S4如何通过HiPPO初始化和结构化参数化实现高效计算；右侧是完整的S4层设计。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Gu et al.&nbsp;(2021) “Efficiently Modeling Long Sequences with Structured State Spaces”, Figure 1</em></p>
</div>
<p>朴素方法需要计算<span class="math inline">\(A^k\)</span>对于<span class="math inline">\(k = 0, 1, \ldots, L-1\)</span>，复杂度是<span class="math inline">\(O(N^2 L)\)</span>或<span class="math inline">\(O(N^3 L)\)</span>（取决于矩阵乘法方式）——这在长序列上是不可接受的。</p>
<p>S4的关键洞察是：<strong>通过低秩修正将HiPPO矩阵对角化</strong>。</p>
<p>具体来说，S4假设<span class="math inline">\(A\)</span>矩阵可以分解为：</p>
<p><span class="math display">\[
A = V \Lambda V^{-1} - PQ^\top
\]</span></p>
<p>其中<span class="math inline">\(\Lambda\)</span>是对角矩阵，<span class="math inline">\(P, Q\)</span>是低秩矩阵。这种结构化使得：</p>
<ol type="1">
<li><strong>卷积核可以用Cauchy核高效计算</strong>：复杂度降到<span class="math inline">\(O(N + L)\)</span></li>
<li><strong>训练时整体复杂度为<span class="math inline">\(O(N + L)\log(N + L)\)</span></strong>：接近线性</li>
</ol>
<p>S4在长程竞技场（Long Range Arena）基准测试上取得了突破性成果，首次解决了Path-X任务——这是一个需要关注长度16384序列中首尾信息的分类任务，此前所有模型（包括Transformer）都失败了。</p>
</section>
<section id="mamba选择性状态空间" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="mamba选择性状态空间"><span class="header-section-number">4.2</span> Mamba：选择性状态空间</h3>
<p>S4虽然解决了计算效率问题，但它有一个根本性的限制：<strong>参数是时不变的（Time-Invariant）</strong>。</p>
<p>这意味着矩阵<span class="math inline">\(A\)</span>、<span class="math inline">\(B\)</span>、<span class="math inline">\(C\)</span>对所有输入都是相同的——模型无法根据输入内容调整自己的行为。这与Transformer形成鲜明对比：Attention的权重<span class="math inline">\(\text{softmax}(QK^\top)\)</span>完全依赖于输入内容。</p>
<p>Mamba的核心创新是引入<strong>选择性机制（Selection Mechanism）</strong>：让参数<span class="math inline">\(B\)</span>、<span class="math inline">\(C\)</span>和步长<span class="math inline">\(\Delta\)</span>成为输入的函数。</p>
<div id="fig-mamba-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mamba-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-28/original/fig1-mamba-overview.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mamba-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Mamba的核心创新：选择性SSM。结构化SSM独立地将每个输入通道映射到输出，通过一个潜在状态实现。选择性机制通过让SSM参数（<span class="math inline">\(\Delta\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>）依赖于输入，为模型添加了输入依赖的动态特性。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Gu &amp; Dao (2023) “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, Figure 1</em></p>
</div>
<p>具体来说：</p>
<p><span class="math display">\[
B_t = \text{Linear}_B(x_t), \quad C_t = \text{Linear}_C(x_t), \quad \Delta_t = \text{softplus}(\text{Linear}_\Delta(x_t))
\]</span></p>
<p>其中<span class="math inline">\(x_t\)</span>是时刻<span class="math inline">\(t\)</span>的输入。这意味着：</p>
<ul>
<li><strong><span class="math inline">\(B_t\)</span>控制”什么信息进入状态”</strong>：模型可以根据内容决定是否”记住”某个输入</li>
<li><strong><span class="math inline">\(C_t\)</span>控制”什么信息被读取”</strong>：模型可以根据内容决定关注状态的哪些方面</li>
<li><strong><span class="math inline">\(\Delta_t\)</span>控制”步长大小”</strong>：可以理解为”时间流逝速度”——步长大意味着更快遗忘旧信息</li>
</ul>
</section>
<section id="为什么选择性机制是关键" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="为什么选择性机制是关键"><span class="header-section-number">4.3</span> 为什么选择性机制是关键？</h3>
<p>让我们用两个具体任务来理解为什么LTI模型不够用：</p>
<div id="fig-selective-tasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-selective-tasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-28/original/fig2-selective-tasks.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-selective-tasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 选择性机制的必要性。上图：选择性复制任务——模型需要记住用特殊标记标记的token（彩色），忽略其他。下图：归纳头任务——模型需要进行关联召回。LTI模型在这两个任务上都会失败。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Gu &amp; Dao (2023) “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, Figure 2</em></p>
</div>
<p><strong>选择性复制（Selective Copying）</strong>：</p>
<p>给定序列”A X B X C X”，要求输出”A B C”。中间的”X”是噪声，应该被忽略。</p>
<p>LTI模型无法完成这个任务，因为它对每个输入的处理方式是相同的——无法区分”有意义的A”和”无意义的X”。</p>
<p>选择性SSM可以学会：当输入是”X”时，让<span class="math inline">\(\Delta\)</span>很小（不更新状态）；当输入是有意义的字母时，让<span class="math inline">\(\Delta\)</span>正常大小（写入状态）。</p>
<p><strong>归纳头（Induction Heads）</strong>：</p>
<p>给定”…A B … A ?“，要求预测”B”。这需要：(1) 记住”A后面跟着B”的模式；(2) 在看到第二个”A”时召回这个模式。</p>
<p>LTI模型的状态更新与输入内容无关，无法实现这种”根据内容检索”的能力。</p>
<p>选择性SSM可以学会：用<span class="math inline">\(B_t\)</span>编码”这是什么内容”，用<span class="math inline">\(C_t\)</span>编码”我需要找什么内容”，从而实现关联召回。</p>
</section>
<section id="硬件感知的选择性扫描算法" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="硬件感知的选择性扫描算法"><span class="header-section-number">4.4</span> 硬件感知的选择性扫描算法</h3>
<p>选择性机制带来了一个计算挑战：参数现在依赖于输入，无法再用FFT-based的卷积高效计算。</p>
<p>Mamba论文提出了一个<strong>硬件感知的并行扫描算法</strong>。核心思想是：</p>
<ol type="1">
<li><strong>利用并行扫描（Parallel Scan）</strong>：扫描操作可以用<span class="math inline">\(O(\log L)\)</span>的并行步骤完成<span class="math inline">\(L\)</span>次递归</li>
<li><strong>内存优化</strong>：不具体化完整的状态序列，而是在SRAM中逐块计算</li>
<li><strong>内核融合</strong>：将离散化、状态更新、输出计算融合到单个GPU内核中</li>
</ol>
<p>这个算法使得Mamba在训练时比S4快40倍以上，同时保持线性的内存复杂度。</p>
</section>
<section id="mamba架构" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="mamba架构"><span class="header-section-number">4.5</span> Mamba架构</h3>
<p>Mamba不仅仅是一个SSM层，还定义了一个完整的神经网络架构：</p>
<div id="fig-mamba-block" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mamba-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-28/original/fig3-mamba-block.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mamba-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Mamba Block架构。与H3架构相比，Mamba简化了设计：用激活函数替代乘性门，并融合了多个投影步骤。结果是一个更简单、更高效的块结构。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Gu &amp; Dao (2023) “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, Figure 3</em></p>
</div>
<p>Mamba Block的设计借鉴了门控MLP的思想：</p>
<ol type="1">
<li><strong>输入投影</strong>：将输入扩展到更高维度（通常2倍）</li>
<li><strong>卷积层</strong>：一个短的深度可分离卷积，捕获局部模式</li>
<li><strong>选择性SSM</strong>：核心的序列建模组件</li>
<li><strong>门控乘法</strong>：将SSM输出与另一条分支相乘</li>
<li><strong>输出投影</strong>：投影回原始维度</li>
</ol>
<p>关键设计决策：</p>
<ul>
<li><strong>没有Attention</strong>：整个模型完全基于SSM</li>
<li><strong>没有MLP块</strong>：SSM本身就包含了足够的非线性</li>
<li><strong>状态维度<span class="math inline">\(N\)</span></strong>：通常设为16，远小于隐藏维度</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: Selective SSM Forward Pass (Mamba)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> selective_ssm(u, A, B, C, delta):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    选择性状态空间模型的前向传播</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        u: [batch, seq_len, d_model] 输入序列</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        A: [d_inner, N] 状态转移矩阵参数</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        B: [batch, seq_len, N] 输入依赖的B矩阵</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        C: [batch, seq_len, N] 输入依赖的C矩阵</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        delta: [batch, seq_len, d_inner] 输入依赖的步长</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">        y: [batch, seq_len, d_model] 输出序列</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 离散化 (ZOH)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A_bar = exp(delta * A)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    A_bar <span class="op">=</span> torch.exp(delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> A)  <span class="co"># [batch, seq_len, d_inner, N]</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    B_bar <span class="op">=</span> delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> B.unsqueeze(<span class="dv">2</span>)  <span class="co"># [batch, seq_len, d_inner, N]</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 并行扫描计算状态序列</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_k = A_bar_k * x_{k-1} + B_bar_k * u_k</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> parallel_scan(A_bar, B_bar <span class="op">*</span> u.unsqueeze(<span class="op">-</span><span class="dv">1</span>))  <span class="co"># [batch, seq_len, d_inner, N]</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 计算输出</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_k = C_k * x_k</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.einsum(<span class="st">'bldn,bln-&gt;bld'</span>, x, C)  <span class="co"># [batch, seq_len, d_inner]</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parallel_scan(A_bar, BU):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">    并行扫描算法：O(log L) 并行步骤完成 L 次递归</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co">    关键洞察：递归 x_k = A_k * x_{k-1} + b_k 可以重写为</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co">    结合律的二元运算 (A_k, b_k) * (A_{k-1}, b_{k-1}) = (A_k*A_{k-1}, A_k*b_{k-1}+b_k)</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co">    然后用并行前缀和算法高效计算</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 具体实现涉及GPU内核优化，此处省略</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Adapted from Gu &amp; Dao (2023) “Mamba”, Section 3.3</em></p>
</div>
</div>
</section>
<section id="scaling-laws与性能" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="scaling-laws与性能"><span class="header-section-number">4.6</span> Scaling Laws与性能</h3>
<p>Mamba在语言建模任务上表现出色：</p>
<div id="fig-scaling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-28/original/fig6-scaling-laws.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Mamba的Scaling Law。在预训练困惑度上，Mamba优于其他subquadratic模型，并与Transformer++（一个强优化的Transformer基线）相当。从125M到1.3B参数规模都保持这种优势。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Gu &amp; Dao (2023) “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, Figure 6</em></p>
</div>
<p>关键发现：</p>
<ul>
<li>Mamba-3B在预训练困惑度上与Transformer-3B相当</li>
<li>在下游评估中，Mamba-3B与Transformer-6B相当（2倍参数效率）</li>
<li>推理吞吐量是Transformer的5倍（归功于线性复杂度）</li>
</ul>
</section>
<section id="数值示例选择性ssm的状态更新" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="数值示例选择性ssm的状态更新"><span class="header-section-number">4.7</span> 数值示例：选择性SSM的状态更新</h3>
<p>让我们用一个简化的数值例子来理解选择性SSM的工作方式。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>状态维度 <span class="math inline">\(N = 2\)</span></li>
<li>输入维度 <span class="math inline">\(d = 1\)</span>（简化为标量）</li>
<li>3个时间步</li>
</ul>
<p><strong>输入序列</strong>：<span class="math inline">\(u = [0.5, 0.8, 0.2]\)</span>（假设这是一个”重要-重要-不重要”的模式）</p>
<p><strong>固定参数</strong>：</p>
<p><span class="math display">\[
A = \begin{bmatrix} -0.5 &amp; 0 \\ 0 &amp; -1.0 \end{bmatrix} \quad \text{(负值，保证稳定性)}
\]</span></p>
<p><strong>输入依赖的参数</strong>（经过网络计算后）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>时间步 <span class="math inline">\(t\)</span></th>
<th>输入 <span class="math inline">\(u_t\)</span></th>
<th><span class="math inline">\(\Delta_t\)</span></th>
<th><span class="math inline">\(B_t\)</span></th>
<th><span class="math inline">\(C_t\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.5</td>
<td>0.8</td>
<td>[1.0, 0.5]</td>
<td>[0.6, 0.4]</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.8</td>
<td>0.9</td>
<td>[1.2, 0.6]</td>
<td>[0.5, 0.5]</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.2</td>
<td>0.1</td>
<td>[0.3, 0.1]</td>
<td>[0.7, 0.3]</td>
</tr>
</tbody>
</table>
<p>注意：<span class="math inline">\(\Delta_3 = 0.1\)</span>很小，因为输入<span class="math inline">\(u_3 = 0.2\)</span>被判断为”不重要”。</p>
<p><strong>Step 1：离散化（<span class="math inline">\(t=1\)</span>）</strong></p>
<p><span class="math display">\[
\bar{A}_1 = \exp(\Delta_1 \cdot A) = \exp\left(0.8 \times \begin{bmatrix} -0.5 &amp; 0 \\ 0 &amp; -1.0 \end{bmatrix}\right) = \begin{bmatrix} e^{-0.4} &amp; 0 \\ 0 &amp; e^{-0.8} \end{bmatrix} \approx \begin{bmatrix} 0.67 &amp; 0 \\ 0 &amp; 0.45 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\bar{B}_1 \approx \Delta_1 \cdot B_1 = 0.8 \times [1.0, 0.5] = [0.8, 0.4]
\]</span></p>
<p><strong>Step 2：状态更新</strong></p>
<p>初始状态 <span class="math inline">\(x_0 = [0, 0]\)</span></p>
<p><span class="math display">\[
x_1 = \bar{A}_1 x_0 + \bar{B}_1 u_1 = [0, 0] + [0.8, 0.4] \times 0.5 = [0.4, 0.2]
\]</span></p>
<p><span class="math display">\[
y_1 = C_1 \cdot x_1 = [0.6, 0.4] \cdot [0.4, 0.2] = 0.24 + 0.08 = 0.32
\]</span></p>
<p>类似地计算 <span class="math inline">\(t=2\)</span>：</p>
<p><span class="math display">\[
x_2 = \bar{A}_2 x_1 + \bar{B}_2 u_2 = [0.64 \times 0.4, 0.41 \times 0.2] + [1.08, 0.54] \times 0.8
\]</span> <span class="math display">\[
= [0.256, 0.082] + [0.864, 0.432] = [1.12, 0.51]
\]</span></p>
<p><span class="math display">\[
y_2 = C_2 \cdot x_2 = [0.5, 0.5] \cdot [1.12, 0.51] = 0.82
\]</span></p>
<p>对于 <span class="math inline">\(t=3\)</span>，由于 <span class="math inline">\(\Delta_3 = 0.1\)</span> 很小：</p>
<p><span class="math display">\[
\bar{A}_3 = \begin{bmatrix} e^{-0.05} &amp; 0 \\ 0 &amp; e^{-0.1} \end{bmatrix} \approx \begin{bmatrix} 0.95 &amp; 0 \\ 0 &amp; 0.90 \end{bmatrix}
\]</span></p>
<p>状态几乎不变：</p>
<p><span class="math display">\[
x_3 \approx [0.95 \times 1.12, 0.90 \times 0.51] + [0.03, 0.01] \times 0.2 = [1.07, 0.46]
\]</span></p>
<p><strong>关键观察</strong>：</p>
<ul>
<li>当输入”重要”（<span class="math inline">\(u_1, u_2\)</span>较大）时，<span class="math inline">\(\Delta\)</span>较大，状态更新明显</li>
<li>当输入”不重要”（<span class="math inline">\(u_3\)</span>较小）时，<span class="math inline">\(\Delta\)</span>较小，状态几乎保持不变</li>
<li>这就是”选择性”的含义：模型学会了何时记忆、何时忽略</li>
</ul>
<hr>
</section>
</section>
<section id="mamba-2与状态空间对偶性" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="mamba-2与状态空间对偶性"><span class="header-section-number">5</span> Mamba-2与状态空间对偶性</h2>
<section id="ssd结构化状态空间对偶" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="ssd结构化状态空间对偶"><span class="header-section-number">5.1</span> SSD：结构化状态空间对偶</h3>
<p>Mamba-2论文揭示了一个深刻的理论联系：<strong>特定形式的SSM与特定形式的Attention在数学上是等价的</strong>。</p>
<p>这种联系被称为<strong>状态空间对偶性（Structured State Space Duality, SSD）</strong>。</p>
<p>具体来说，当SSM的状态矩阵<span class="math inline">\(A\)</span>是”标量乘以单位矩阵”的形式时：</p>
<p><span class="math display">\[
A = -a \cdot I, \quad a &gt; 0
\]</span></p>
<p>SSM的输入输出关系可以写成：</p>
<p><span class="math display">\[
y = M \cdot (L \odot V)
\]</span></p>
<p>其中<span class="math inline">\(L\)</span>是一个<strong>1-半可分（1-semiseparable）因果掩码矩阵</strong>，<span class="math inline">\(V\)</span>是值向量，<span class="math inline">\(M\)</span>是一个累乘矩阵。</p>
<p>这与masked self-attention有惊人的相似性——attention也是对值向量做加权求和，权重由查询-键相似度决定。</p>
</section>
<section id="两种计算视角的统一" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="两种计算视角的统一"><span class="header-section-number">5.2</span> 两种计算视角的统一</h3>
<p>SSD框架的核心洞察是：<strong>同一个序列变换可以用两种算法实现</strong>：</p>
<ol type="1">
<li><strong>SSM视角</strong>：<span class="math inline">\(O(n)\)</span>的递归计算</li>
<li><strong>Attention视角</strong>：<span class="math inline">\(O(n^2)\)</span>的矩阵乘法计算</li>
</ol>
<p>在实践中，我们可以根据硬件特性选择最优实现：</p>
<ul>
<li><strong>短序列</strong>：使用矩阵乘法（更好地利用GPU的tensor core）</li>
<li><strong>长序列</strong>：使用递归（避免<span class="math inline">\(O(n^2)\)</span>的内存开销）</li>
<li><strong>混合策略</strong>：将长序列分成块，块内用矩阵乘法，块间用递归</li>
</ul>
<p>Mamba-2利用这种灵活性，实现了比Mamba快2-8倍的训练速度。</p>
</section>
<section id="mamba-2的改进" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="mamba-2的改进"><span class="header-section-number">5.3</span> Mamba-2的改进</h3>
<p>除了SSD框架，Mamba-2还引入了几个实用改进：</p>
<ol type="1">
<li><strong>多头结构</strong>：类似Transformer的multi-head attention，Mamba-2将状态空间分成多个”头”，每个头独立处理</li>
<li><strong>归一化</strong>：在SSM后添加RMSNorm，提高训练稳定性</li>
<li><strong>更大的状态维度</strong>：从Mamba的<span class="math inline">\(N=16\)</span>增加到<span class="math inline">\(N=64\)</span>或更大</li>
</ol>
<hr>
</section>
</section>
<section id="混合架构结合两种范式的优势" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="混合架构结合两种范式的优势"><span class="header-section-number">6</span> 混合架构：结合两种范式的优势</h2>
<section id="为什么需要混合" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么需要混合"><span class="header-section-number">6.1</span> 为什么需要混合？</h3>
<p>尽管Mamba展示了impressive的性能，但它并非在所有任务上都优于Transformer。研究发现：</p>
<ul>
<li><strong>Attention擅长</strong>：精确的信息检索、复杂的推理、需要全局视角的任务</li>
<li><strong>SSM擅长</strong>：长序列建模、高效推理、流式处理</li>
</ul>
<p>一个自然的想法是：<strong>何不把两者结合起来？</strong></p>
</section>
<section id="jamba混合transformer-mamba架构" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="jamba混合transformer-mamba架构"><span class="header-section-number">6.2</span> Jamba：混合Transformer-Mamba架构</h3>
<p>Jamba是AI21 Labs在2024年3月发布的混合架构模型，也是第一个大规模的Transformer-Mamba混合模型。</p>
<p><strong>核心设计</strong>：</p>
<ul>
<li><strong>交替层</strong>：Attention层和Mamba层以1:7的比例交替</li>
<li><strong>MoE集成</strong>：每隔一层使用MoE，提供更大的模型容量</li>
<li><strong>总参数</strong>：52B（12B激活参数）</li>
<li><strong>上下文长度</strong>：256K tokens</li>
</ul>
<p><strong>架构细节</strong>：</p>
<pre><code>Layer 0: Mamba
Layer 1: Mamba + MoE
Layer 2: Mamba
Layer 3: Attention + MoE
Layer 4: Mamba
Layer 5: Mamba + MoE
Layer 6: Mamba
Layer 7: Attention + MoE
...（重复）</code></pre>
<p><strong>关键发现</strong>：</p>
<ol type="1">
<li><strong>Mamba层不需要位置编码</strong>：与Transformer不同，Mamba天然具有位置感知能力（通过递归状态）</li>
<li><strong>少量Attention就够了</strong>：1:7的比例（约12.5%的Attention层）已经足够保留Transformer的优势</li>
<li><strong>内存效率</strong>：由于大部分层是Mamba（无KV cache），整体内存需求远低于纯Transformer</li>
</ol>
<p><strong>性能表现</strong>：</p>
<ul>
<li>在256K上下文窗口内：接近Claude-2的质量</li>
<li>推理速度：显著快于同规模的纯Transformer</li>
<li>内存占用：可以在单个80GB GPU上运行完整的52B模型</li>
</ul>
</section>
<section id="混合架构的设计原则" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="混合架构的设计原则"><span class="header-section-number">6.3</span> 混合架构的设计原则</h3>
<p>从Jamba和后续工作中，我们可以总结出一些混合架构的设计原则：</p>
<ol type="1">
<li><strong>Attention层放在关键位置</strong>：早期层和晚期层可能更需要全局交互</li>
<li><strong>Mamba处理”重复性”计算</strong>：中间层的大量计算可以交给更高效的Mamba</li>
<li><strong>可以结合MoE</strong>：三种技术（Attention、SSM、MoE）可以正交地组合</li>
</ol>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">7</span> 工程实践</h2>
<section id="使用hugging-face加载mamba模型" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="使用hugging-face加载mamba模型"><span class="header-section-number">7.1</span> 使用Hugging Face加载Mamba模型</h3>
<p>Mamba模型已经集成到Hugging Face的<code>transformers</code>库中，可以方便地加载和使用：</p>
<div id="5833d674" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装依赖</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers torch mamba-ssm causal-conv1d</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> MambaConfig, MambaForCausalLM, AutoTokenizer</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的Mamba模型</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"state-spaces/mamba-130m-hf"</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MambaForCausalLM.from_pretrained(model_name)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置pad token（Mamba使用EOS作为pad）</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 文本生成示例</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"The capital of France is"</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成文本</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model.generate(</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        inputs.input_ids,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        max_new_tokens<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generated: </span><span class="sc">{</span>generated_text<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="从零实现简化版ssm" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="从零实现简化版ssm"><span class="header-section-number">7.2</span> 从零实现简化版SSM</h3>
<p>为了深入理解SSM的工作原理，让我们实现一个简化版本：</p>
<div id="07ab47bc" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleSSM(nn.Module):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    简化版状态空间模型（用于教学目的）</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    关键组件：</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - A: 状态转移矩阵（对角化，用于稳定性）</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - B, C: 输入/输出投影</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - delta: 离散化步长</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_state<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_state <span class="op">=</span> d_state</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A矩阵：使用负值保证稳定性（对角化简化）</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 初始化为HiPPO-like的值</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> torch.arange(<span class="dv">1</span>, d_state <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A_log <span class="op">=</span> nn.Parameter(torch.log(A))  <span class="co"># 学习log(A)保证正值</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B, C投影</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.B_proj <span class="op">=</span> nn.Linear(d_model, d_state, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C_proj <span class="op">=</span> nn.Linear(d_model, d_state, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Delta投影（控制离散化步长）</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.delta_proj <span class="op">=</span> nn.Linear(d_model, d_model, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出投影</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="co">            x: [batch, seq_len, d_model]</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co">            y: [batch, seq_len, d_model]</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        batch, seq_len, _ <span class="op">=</span> x.shape</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算输入依赖的参数（选择性机制的核心）</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        B <span class="op">=</span> <span class="va">self</span>.B_proj(x)  <span class="co"># [batch, seq_len, d_state]</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> <span class="va">self</span>.C_proj(x)  <span class="co"># [batch, seq_len, d_state]</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算delta（步长），使用softplus保证正值</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> F.softplus(<span class="va">self</span>.delta_proj(x))  <span class="co"># [batch, seq_len, d_model]</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 获取A（对角矩阵，用向量表示）</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> <span class="op">-</span>torch.exp(<span class="va">self</span>.A_log)  <span class="co"># [d_state]，负值保证稳定</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 离散化：使用简化的ZOH近似</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A_bar = exp(delta * A) ≈ 1 + delta * A（一阶近似）</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 这里我们对每个通道独立处理</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        delta_A <span class="op">=</span> delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> A  <span class="co"># [batch, seq_len, d_model, d_state]</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        A_bar <span class="op">=</span> torch.exp(delta_A)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B_bar = delta * B</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        B_bar <span class="op">=</span> delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> B.unsqueeze(<span class="dv">2</span>)  <span class="co"># [batch, seq_len, d_model, d_state]</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 递归计算状态（简化版，实际Mamba使用并行扫描）</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.zeros(batch, <span class="va">self</span>.d_model, <span class="va">self</span>.d_state, device<span class="op">=</span>x.device)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> []</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 状态更新: h_t = A_bar * h_{t-1} + B_bar * x_t</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> A_bar[:, t] <span class="op">*</span> h <span class="op">+</span> B_bar[:, t] <span class="op">*</span> x[:, t:t<span class="op">+</span><span class="dv">1</span>, :].transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 输出: y_t = C_t * h_t</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>            y_t <span class="op">=</span> torch.einsum(<span class="st">'bdn,bn-&gt;bd'</span>, h, C[:, t])</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>            outputs.append(y_t)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.stack(outputs, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch, seq_len, d_model]</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out_proj(y)</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试</span></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, d_model <span class="op">=</span> <span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">64</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    ssm <span class="op">=</span> SimpleSSM(d_model<span class="op">=</span>d_model, d_state<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(batch_size, seq_len, d_model)</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> ssm(x)</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> ssm.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="比较mamba与transformer的推理速度" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="比较mamba与transformer的推理速度"><span class="header-section-number">7.3</span> 比较Mamba与Transformer的推理速度</h3>
<p>以下代码展示如何比较两种架构的推理效率：</p>
<div id="73f2f6c5" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    MambaForCausalLM,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    GPT2LMHeadModel,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark_generation(model, tokenizer, prompt, max_new_tokens<span class="op">=</span><span class="dv">100</span>, num_runs<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""测量生成速度"""</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> {k: v.cuda() <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()}</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warmup</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> model.generate(inputs[<span class="st">"input_ids"</span>], max_new_tokens<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Benchmark</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> []</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        torch.cuda.synchronize() <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> time.perf_counter()</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model.generate(</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>                inputs[<span class="st">"input_ids"</span>],</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                max_new_tokens<span class="op">=</span>max_new_tokens,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>                do_sample<span class="op">=</span><span class="va">False</span>,  <span class="co"># Greedy for reproducibility</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        torch.cuda.synchronize() <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        times.append(time.perf_counter() <span class="op">-</span> start)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    avg_time <span class="op">=</span> <span class="bu">sum</span>(times) <span class="op">/</span> <span class="bu">len</span>(times)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    tokens_per_sec <span class="op">=</span> max_new_tokens <span class="op">/</span> avg_time</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_time, tokens_per_sec, outputs</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 比较Mamba和GPT2（相近规模）</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"In the field of artificial intelligence, large language models have"</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading Mamba-130M..."</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>mamba_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"state-spaces/mamba-130m-hf"</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>mamba_model <span class="op">=</span> MambaForCausalLM.from_pretrained(<span class="st">"state-spaces/mamba-130m-hf"</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>mamba_tokenizer.pad_token <span class="op">=</span> mamba_tokenizer.eos_token</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>mamba_time, mamba_tps, mamba_out <span class="op">=</span> benchmark_generation(</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    mamba_model, mamba_tokenizer, prompt</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mamba-130M: </span><span class="sc">{</span>mamba_time<span class="sc">:.3f}</span><span class="ss">s, </span><span class="sc">{</span>mamba_tps<span class="sc">:.1f}</span><span class="ss"> tokens/s"</span>)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Loading GPT2 (124M)..."</span>)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>gpt2_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>gpt2_model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>gpt2_tokenizer.pad_token <span class="op">=</span> gpt2_tokenizer.eos_token</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>gpt2_time, gpt2_tps, gpt2_out <span class="op">=</span> benchmark_generation(</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>    gpt2_model, gpt2_tokenizer, prompt</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT2-124M: </span><span class="sc">{</span>gpt2_time<span class="sc">:.3f}</span><span class="ss">s, </span><span class="sc">{</span>gpt2_tps<span class="sc">:.1f}</span><span class="ss"> tokens/s"</span>)</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>gpt2_time <span class="op">/</span> mamba_time<span class="sc">:.2f}</span><span class="ss">x"</span>)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>运行提示
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>GPU推荐</strong>：虽然CPU也能运行，但GPU上性能差异更明显</li>
<li><strong>长序列测试</strong>：Mamba的优势在长序列上更显著，可以尝试更长的<code>max_new_tokens</code></li>
<li><strong>内存监控</strong>：观察KV cache增长（GPT2）vs 固定状态（Mamba）</li>
</ol>
</div>
</div>
</section>
<section id="可视化ssm的状态演化" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="可视化ssm的状态演化"><span class="header-section-number">7.4</span> 可视化SSM的状态演化</h3>
<p>理解SSM如何”记忆”信息的一个好方法是可视化状态向量的演化：</p>
<div id="8fba9886" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_ssm_state(A_diag, B, C, inputs, title<span class="op">=</span><span class="st">"SSM State Evolution"</span>):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">    可视化SSM状态随时间的演化</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">        A_diag: 对角A矩阵的值 [d_state]</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">        B: 输入矩阵 [d_state]</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">        C: 输出矩阵 [d_state]</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: 输入序列 [seq_len]</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    d_state <span class="op">=</span> <span class="bu">len</span>(A_diag)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> np.zeros((seq_len <span class="op">+</span> <span class="dv">1</span>, d_state))</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.zeros(seq_len)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 模拟SSM</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 固定步长</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    A_bar <span class="op">=</span> np.exp(delta <span class="op">*</span> A_diag)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    B_bar <span class="op">=</span> delta <span class="op">*</span> B</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 状态更新</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        states[t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> A_bar <span class="op">*</span> states[t] <span class="op">+</span> B_bar <span class="op">*</span> inputs[t]</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        outputs[t] <span class="op">=</span> np.dot(C, states[t <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 绘图</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输入序列</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].stem(<span class="bu">range</span>(seq_len), inputs, basefmt<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_title(<span class="st">"Input Sequence"</span>)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_xlabel(<span class="st">"Time step"</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Input value"</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 状态演化（热力图）</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    im <span class="op">=</span> axes[<span class="dv">1</span>].imshow(states[<span class="dv">1</span>:].T, aspect<span class="op">=</span><span class="st">'auto'</span>, cmap<span class="op">=</span><span class="st">'RdBu_r'</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_title(<span class="st">"State Evolution (each row = one state dimension)"</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_xlabel(<span class="st">"Time step"</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_ylabel(<span class="st">"State dimension"</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    plt.colorbar(im, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输出序列</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].plot(outputs, <span class="st">'g-o'</span>, markersize<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_title(<span class="st">"Output Sequence"</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_xlabel(<span class="st">"Time step"</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_ylabel(<span class="st">"Output value"</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(title, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="st">"ssm_state_evolution.png"</span>, dpi<span class="op">=</span><span class="dv">150</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例：观察SSM如何记住脉冲输入</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>d_state <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="co"># HiPPO-like初始化：不同的衰减率</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>A_diag <span class="op">=</span> <span class="op">-</span>np.arange(<span class="dv">1</span>, d_state <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.5</span>  <span class="co"># 负值，越高维度衰减越快</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.ones(d_state) <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.random.randn(d_state) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a><span class="co"># 输入：一个脉冲 + 一些噪声</span></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.zeros(<span class="dv">50</span>)</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>inputs[<span class="dv">5</span>] <span class="op">=</span> <span class="fl">1.0</span>   <span class="co"># 脉冲1</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>inputs[<span class="dv">25</span>] <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># 脉冲2</span></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>visualize_ssm_state(A_diag, B, C, inputs, <span class="st">"SSM Response to Impulse Inputs"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>这个可视化展示了几个关键点：</p>
<ol type="1">
<li><strong>不同状态维度的衰减速率不同</strong>：低维度（小的<span class="math inline">\(|A|\)</span>）衰减慢，保留长期记忆</li>
<li><strong>脉冲响应</strong>：输入的脉冲会在状态中留下”痕迹”，逐渐衰减</li>
<li><strong>输出是状态的线性组合</strong>：<span class="math inline">\(C\)</span>矩阵决定了如何从状态中”读取”信息</li>
</ol>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">8</span> 深入理解</h2>
<section id="为什么有效理论视角" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">8.1</span> 为什么有效？——理论视角</h3>
<section id="ssm的表达能力" class="level4" data-number="8.1.1">
<h4 data-number="8.1.1" class="anchored" data-anchor-id="ssm的表达能力"><span class="header-section-number">8.1.1</span> SSM的表达能力</h4>
<p>从理论上，SSM可以被理解为一类特殊的<strong>线性递归神经网络（Linear RNN）</strong>。线性RNN的表达能力受限于其线性特性——它只能表示线性函数。</p>
<p>然而，SSM通过以下机制获得了更强的表达能力：</p>
<ol type="1">
<li><strong>非线性激活</strong>：SSM层后通常跟着非线性激活</li>
<li><strong>多层堆叠</strong>：深层SSM可以逼近复杂的非线性映射</li>
<li><strong>选择性机制</strong>：Mamba的输入依赖参数引入了数据依赖的非线性</li>
</ol>
</section>
<section id="与attention的理论联系" class="level4" data-number="8.1.2">
<h4 data-number="8.1.2" class="anchored" data-anchor-id="与attention的理论联系"><span class="header-section-number">8.1.2</span> 与Attention的理论联系</h4>
<p>Mamba-2的SSD框架揭示了SSM和Attention之间的深层联系。从某种意义上说，SSM可以被看作是一种”结构化的Attention”——它使用特定形式的掩码矩阵（1-semiseparable），而不是任意学习的attention权重。</p>
<p>这种结构化带来的限制是：SSM无法实现任意的长距离信息交互。它必须通过状态向量”传递”信息，而状态向量的容量是有限的。</p>
</section>
</section>
<section id="为什么有效实证视角" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="为什么有效实证视角"><span class="header-section-number">8.2</span> 为什么有效？——实证视角</h3>
<section id="长程建模的证据" class="level4" data-number="8.2.1">
<h4 data-number="8.2.1" class="anchored" data-anchor-id="长程建模的证据"><span class="header-section-number">8.2.1</span> 长程建模的证据</h4>
<p>S4在Long Range Arena基准测试上的表现证明了SSM可以有效建模长距离依赖：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>任务</th>
<th>序列长度</th>
<th>S4</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ListOps</td>
<td>2,048</td>
<td>59.6%</td>
<td>36.4%</td>
</tr>
<tr class="even">
<td>Text</td>
<td>4,096</td>
<td>86.8%</td>
<td>64.3%</td>
</tr>
<tr class="odd">
<td>Retrieval</td>
<td>4,000</td>
<td>90.9%</td>
<td>53.4%</td>
</tr>
<tr class="even">
<td>Image</td>
<td>1,024</td>
<td>88.7%</td>
<td>42.4%</td>
</tr>
<tr class="odd">
<td><strong>Path-X</strong></td>
<td><strong>16,384</strong></td>
<td><strong>88.1%</strong></td>
<td><strong>random</strong></td>
</tr>
<tr class="even">
<td>PathFinder</td>
<td>1,024</td>
<td>94.2%</td>
<td>71.4%</td>
</tr>
</tbody>
</table>
<p>Path-X任务特别值得关注：这是一个需要追踪16384长度序列中从起点到终点路径的任务。所有Transformer变体都失败了（等同于随机猜测），而S4成功解决了这个任务。</p>
<div id="fig-s4-lra" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-s4-lra-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-28/original/fig-s4-lra-results.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-s4-lra-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: S4在Long Range Arena基准测试上的性能对比。S4在所有任务上都显著优于之前的方法，特别是首次解决了Path-X任务（序列长度16384），而所有其他模型（包括各种Transformer变体）在该任务上都等同于随机猜测。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Gu et al.&nbsp;(2021) “Efficiently Modeling Long Sequences with Structured State Spaces”, Figure 2</em></p>
</div>
</section>
<section id="mamba的语言建模实验" class="level4" data-number="8.2.2">
<h4 data-number="8.2.2" class="anchored" data-anchor-id="mamba的语言建模实验"><span class="header-section-number">8.2.2</span> Mamba的语言建模实验</h4>
<p>Mamba在语言建模上的成功表明，SSM不仅能处理”人工”的长程任务，还能在真实的NLP数据上与Transformer竞争：</p>
<ul>
<li><strong>Mamba-130M</strong> vs <strong>Transformer-130M</strong>：困惑度更低</li>
<li><strong>Mamba-1.4B</strong> vs <strong>Transformer-1.4B</strong>：困惑度相当</li>
<li><strong>Mamba-3B</strong> vs <strong>Transformer-6B</strong>：下游任务性能相当</li>
</ul>
</section>
</section>
<section id="方法的边界条件" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">8.3</span> 方法的边界条件</h3>
<section id="什么任务ssm表现不好" class="level4" data-number="8.3.1">
<h4 data-number="8.3.1" class="anchored" data-anchor-id="什么任务ssm表现不好"><span class="header-section-number">8.3.1</span> 什么任务SSM表现不好？</h4>
<ol type="1">
<li><strong>需要精确检索的任务</strong>：当任务需要从长序列中精确定位并提取特定信息时，有限容量的状态向量可能不够</li>
<li><strong>复杂推理任务</strong>：多步推理可能需要更灵活的信息交互模式</li>
<li><strong>短序列任务</strong>：当序列很短时，<span class="math inline">\(O(n^2)\)</span>的开销不是问题，Attention的全局交互可能更有优势</li>
</ol>
</section>
<section id="计算效率的边界" class="level4" data-number="8.3.2">
<h4 data-number="8.3.2" class="anchored" data-anchor-id="计算效率的边界"><span class="header-section-number">8.3.2</span> 计算效率的边界</h4>
<p>尽管SSM理论上是<span class="math inline">\(O(n)\)</span>，但实际效率取决于多个因素：</p>
<ul>
<li><strong>状态维度<span class="math inline">\(N\)</span></strong>：更大的<span class="math inline">\(N\)</span>提供更多容量，但计算量也更大</li>
<li><strong>硬件利用率</strong>：Mamba的并行扫描在某些硬件上不如矩阵乘法高效</li>
<li><strong>内存带宽</strong>：SSM的状态更新是memory-bound的，可能受限于内存带宽</li>
</ul>
</section>
</section>
<section id="开放研究问题" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">8.4</span> 开放研究问题</h3>
<ol type="1">
<li><p><strong>SSM vs Attention的本质差异</strong>：它们分别擅长什么？有没有任务类型的理论划分？</p></li>
<li><p><strong>最优的混合比例</strong>：Attention和SSM应该如何组合？1:7的比例是最优的吗？</p></li>
<li><p><strong>更强的选择性机制</strong>：Mamba的选择性机制够强吗？有没有更好的输入依赖设计？</p></li>
<li><p><strong>Scaling Law</strong>：SSM的scaling behavior与Transformer有何不同？更大规模时谁更有优势？</p></li>
<li><p><strong>多模态SSM</strong>：SSM能否像Transformer一样成为多模态的通用架构？</p></li>
</ol>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</h2>
<section id="本方法的局限" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="本方法的局限"><span class="header-section-number">9.1</span> 本方法的局限</h3>
<p><strong>状态容量的限制</strong></p>
<p>SSM的核心是用一个固定维度的状态向量来压缩所有历史信息。当需要记住的信息量超过状态容量时，必然会发生信息损失。</p>
<p>相比之下，Transformer的KV cache可以存储完整的历史信息（尽管代价是<span class="math inline">\(O(n)\)</span>的内存增长）。</p>
<p><strong>并行性的权衡</strong></p>
<p>Mamba的并行扫描算法虽然高效，但仍然不如矩阵乘法那样完美地映射到GPU的tensor core上。在短到中等长度的序列上，Transformer可能实际上更快。</p>
<p><strong>生态系统的成熟度</strong></p>
<p>Transformer有多年的工程优化积累（FlashAttention、vLLM、TensorRT等），而SSM的工具链相对不成熟。这影响了SSM在生产环境中的部署。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">9.2</span> 这些局限导向了什么？</h3>
<p>SSM和Transformer各有所长，这促使我们思考：</p>
<p><strong>未来的序列模型会是什么样的？</strong></p>
<ul>
<li>纯Transformer？</li>
<li>纯SSM？</li>
<li>还是像Jamba那样的混合架构？</li>
</ul>
<p>目前的证据表明，<strong>混合架构可能是更有前景的方向</strong>——结合Attention的全局推理能力和SSM的高效长程建模能力。</p>
<p><strong>是否存在更统一的框架？</strong></p>
<p>Mamba-2的SSD理论表明，SSM和Attention可能只是更一般的序列变换的两种特例。是否存在一个统一的框架，让我们能够在连续的谱系上设计序列模型？</p>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">10</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</h3>
<ol type="1">
<li><p><strong>问题</strong>：Transformer的<span class="math inline">\(O(n^2)\)</span>注意力复杂度在长序列上成为瓶颈。我们需要既能捕获长距离依赖、又有线性复杂度的序列模型。</p></li>
<li><p><strong>洞察</strong>：借鉴控制论中的状态空间模型，通过精心设计的状态转移矩阵（HiPPO初始化），可以实现长程记忆。关键创新是Mamba的选择性机制——让参数依赖于输入，弥补了LTI模型的根本缺陷。</p></li>
<li><p><strong>方法</strong>：</p>
<ul>
<li>SSM将连续时间系统离散化为可训练的递归网络</li>
<li>利用卷积/递归对偶性：训练时用卷积（可并行），推理时用递归（高效）</li>
<li>HiPPO初始化保证长程记忆</li>
<li>选择性机制让模型能够根据内容决定”何时记忆、何时忽略”</li>
</ul></li>
<li><p><strong>意义</strong>：SSM为超越Transformer提供了一条可行路径。Mamba证明了在语言建模等核心任务上，线性复杂度的模型可以与Transformer竞争。混合架构（如Jamba）展示了结合两种范式优势的可能性。</p></li>
</ol>
</section>
<section id="关键公式速查" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">10.2</span> 关键公式速查</h3>
<p><strong>连续状态空间方程</strong>：</p>
<p><span class="math display">\[
\frac{dx(t)}{dt} = Ax(t) + Bu(t), \quad y(t) = Cx(t)
\]</span></p>
<p><strong>离散化（ZOH）</strong>：</p>
<p><span class="math display">\[
\bar{A} = \exp(\Delta A), \quad \bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B
\]</span></p>
<p><strong>离散递归</strong>：</p>
<p><span class="math display">\[
x_k = \bar{A} x_{k-1} + \bar{B} u_k, \quad y_k = C x_k
\]</span></p>
<p><strong>卷积核</strong>：</p>
<p><span class="math display">\[
\bar{K} = (C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B}, \ldots), \quad y = \bar{K} * u
\]</span></p>
<p><strong>选择性参数化（Mamba）</strong>：</p>
<p><span class="math display">\[
B_t = f_B(x_t), \quad C_t = f_C(x_t), \quad \Delta_t = \text{softplus}(f_\Delta(x_t))
\]</span></p>
</section>
<section id="思考题" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">10.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么HiPPO初始化能够帮助SSM建模长距离依赖？如果用随机初始化的<span class="math inline">\(A\)</span>矩阵，会发生什么？</p></li>
<li><p><strong>[数学推导]</strong> 证明：当<span class="math inline">\(A = -aI\)</span>（<span class="math inline">\(a &gt; 0\)</span>）时，离散化后的<span class="math inline">\(\bar{A} = e^{-a\Delta}I\)</span>。这说明状态会以什么速率”遗忘”？<span class="math inline">\(\Delta\)</span>大小如何影响遗忘速率？</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用Hugging Face的<code>transformers</code>库加载一个Mamba模型（如<code>state-spaces/mamba-130m</code>），在相同的文本上比较它与同规模Transformer的推理速度和困惑度。</p></li>
<li><p><strong>[开放思考]</strong> Jamba使用1:7的Attention:Mamba比例。你认为这个比例是否最优？如果让你设计一个混合架构，你会如何决定在哪些层使用Attention、在哪些层使用SSM？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">11</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>Gu et al.&nbsp;(2021)</strong> “Efficiently Modeling Long Sequences with Structured State Spaces (S4)”
<ul>
<li>重点读：Section 3 (S4架构)、Section 4 (HiPPO初始化)</li>
<li>ICLR 2022 Outstanding Paper，现代SSM的奠基之作</li>
</ul></li>
<li><strong>Gu &amp; Dao (2023)</strong> “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”
<ul>
<li>重点读：Section 3 (选择性机制)、Section 3.3 (硬件感知算法)</li>
<li>将SSM带入语言建模主流</li>
</ul></li>
<li><strong>Dao &amp; Gu (2024)</strong> “Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality”
<ul>
<li>重点读：Section 2 (SSD理论)</li>
<li>揭示SSM与Attention的深层联系</li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">11.2</span> 理论基础</h3>
<ul>
<li><strong>Gu et al.&nbsp;(2020)</strong> “HiPPO: Recurrent Memory with Optimal Polynomial Projections”
<ul>
<li>HiPPO初始化的数学原理</li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">11.3</span> 后续发展</h3>
<ul>
<li><strong>Lieber et al.&nbsp;(2024)</strong> “Jamba: A Hybrid Transformer-Mamba Language Model”
<ul>
<li>第一个大规模混合架构</li>
</ul></li>
<li><strong>AI21 (2024)</strong> “Jamba-1.5: Hybrid Transformer-Mamba Models at Scale”
<ul>
<li>Jamba的规模化扩展</li>
</ul></li>
</ul>
</section>
<section id="综述与教程" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">11.4</span> 综述与教程</h3>
<ul>
<li><strong>Goomba Lab Blog</strong> “State Space Duality (Mamba-2)” — Mamba-2的详细解释</li>
<li><strong>The Annotated S4</strong> (srush/annotated-s4) — S4的详细代码注释</li>
</ul>
</section>
<section id="代码资源" class="level3" data-number="11.5">
<h3 data-number="11.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">11.5</span> 代码资源</h3>
<ul>
<li><strong>state-spaces/mamba</strong> — Mamba官方实现</li>
<li><strong>state-spaces/s4</strong> — S4官方实现</li>
<li><strong>Hugging Face</strong> — Mamba模型权重</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">12</span> 历史注脚</h2>
<p>状态空间模型在深度学习中的应用是一个跨学科融合的典范。</p>
<p>Albert Gu在Stanford攻读博士期间，师从Christopher Ré教授。Ré的实验室一直关注如何构建更高效的机器学习系统。Gu最初的研究方向是理解RNN为什么难以建模长距离依赖。</p>
<p>在探索这个问题时，Gu发现了控制论中的状态空间模型。这是一个有几十年历史的理论框架，主要用于分析和设计控制系统（如自动驾驶汽车、机器人等）。控制论研究者早就知道，通过特殊设计的状态转移矩阵，可以让系统保持对久远历史的”记忆”。</p>
<p>HiPPO框架正是这种洞察的数学实现。有趣的是，HiPPO的名字来源于它的核心思想：High-order Polynomial Projection Operators（高阶多项式投影算子）。这个名字也恰好与英文中的”河马”（hippo）同音——一种以记忆力强而闻名的动物。</p>
<p>S4论文的发表引起了广泛关注，因为它首次证明了非Attention模型可以解决长程建模任务。但S4的一个关键限制是它是线性时不变的——这在处理自然语言时是一个严重的缺陷，因为语言理解本质上需要内容感知的推理。</p>
<p>Mamba的关键洞察就是引入选择性机制来解决这个问题。论文的标题”Linear-Time Sequence Modeling with Selective State Spaces”强调了两个核心点：线性时间复杂度和选择性。这两者的结合使得Mamba成为第一个在语言建模上能够与Transformer竞争的SSM。</p>
<p>Tri Dao是Mamba的另一位作者，他以FlashAttention闻名。有趣的是，同一个人既开发了让Attention更高效的技术（FlashAttention），也开发了Attention的替代方案（Mamba）。这反映了一种务实的研究态度：不是”Attention vs SSM”的二选一，而是为不同场景提供最优的工具。</p>
<p>2024年，混合架构的兴起印证了这种务实态度。Jamba、Zamba等模型表明，未来的序列模型可能不是纯Transformer或纯SSM，而是两者的有机结合。</p>
<p>从HiPPO到S4到Mamba到混合架构，这条研究路径展示了一个重要的方法论启示：<strong>有时候，解决深度学习问题的最好方法是从其他领域借鉴成熟的数学工具</strong>。控制论、信号处理、动力系统——这些”传统”领域积累了大量关于序列和时间的知识，等待着被重新发现和应用。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第28章：状态空间模型——序列建模的另一条路"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Beyond Attention: Linear-Time Sequence Modeling with Structured State Spaces"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, SSM, Mamba, 序列建模, 状态空间]</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [SSM, State Space Model, S4, Mamba, Mamba-2, HiPPO, 线性复杂度, 选择性机制, Jamba, 混合架构]</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Transformer的自注意力机制需要O(n²)的计算复杂度，这在处理长序列时成为瓶颈。但O(n²)是序列建模的唯一选择吗？状态空间模型（SSM）提供了一条全新的路径：借鉴控制论中的线性时不变系统，通过结构化的状态转移矩阵实现O(n)或O(n log n)的序列建模。从HiPPO的记忆理论，到S4的结构化参数化，再到Mamba的选择性机制——SSM正在成为Transformer的有力替代者。本章将带你深入这条'从循环到选择'的技术演进之路。"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-28/original/fig1-mamba-overview.png"</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：Transformer的$O(n^2)$注意力复杂度是序列建模的唯一选择吗？能否设计一种既能捕获长距离依赖、又具有线性复杂度的序列模型？</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2020–2024 </span><span class="pp">|</span><span class="at"> HiPPO (2020) → S4 (2021) → Mamba (2023) → Mamba-2 (2024) → Jamba (2024) </span><span class="pp">|</span><span class="at"> 从控制论到深度学习的跨界融合</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gu et al. (2020)** "HiPPO: Recurrent Memory with Optimal Polynomial Projections" (<span class="co">[</span><span class="ot">arXiv:2008.07669</span><span class="co">](https://arxiv.org/abs/2008.07669)</span>) — 参考了 Section 2-4 (HiPPO框架、Legendre多项式投影)；NeurIPS 2020，SSM的理论基础</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gu et al. (2021)** "Efficiently Modeling Long Sequences with Structured State Spaces" (<span class="co">[</span><span class="ot">arXiv:2111.00396</span><span class="co">](https://arxiv.org/abs/2111.00396)</span>) — 参考了 Section 2-4 (S4架构、HiPPO初始化、Cauchy核计算)、Figure 1-3；ICLR 2022 Outstanding Paper</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gu &amp; Dao (2023)** "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" (<span class="co">[</span><span class="ot">arXiv:2312.00752</span><span class="co">](https://arxiv.org/abs/2312.00752)</span>) — 参考了 Section 2-4 (选择性机制、硬件感知算法)、Figure 1-3, 6；从论文提取了4张原图</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dao &amp; Gu (2024)** "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality" (<span class="co">[</span><span class="ot">arXiv:2405.21060</span><span class="co">](https://arxiv.org/abs/2405.21060)</span>) — 参考了 Section 2-4 (状态空间对偶性、SSM与Attention的联系)；ICML 2024</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lieber et al. (2024)** "Jamba: A Hybrid Transformer-Mamba Language Model" (<span class="co">[</span><span class="ot">arXiv:2403.19887</span><span class="co">](https://arxiv.org/abs/2403.19887)</span>) — 参考了 Section 2-3 (混合架构设计、Attention:Mamba比例)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材与博客</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Goomba Lab** "State Space Duality (Mamba-2) Part I &amp; II" — 参考了Mamba-2的详细解释和代码实现</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Princeton PLI** "Mamba-2: Algorithms and Systems" — 参考了SSD算法的解释</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wang et al. (2024)** "State Space Model for New-Generation Network Alternative to Transformers: A Survey" — 参考了SSM演进脉络的梳理</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>上一章我们探讨了Mixture of Experts（MoE）——通过稀疏激活实现参数量与计算量的解耦。MoE让模型可以拥有数千亿甚至万亿参数，而每次推理只激活其中一小部分。这是对Dense Transformer第一个根本挑战（"所有参数对每个token都激活"）的优雅回应。</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>然而，MoE解决的是FFN层的计算效率问题，并没有触及Transformer的另一个核心瓶颈：**自注意力的$O(n^2)$复杂度**。</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>让我们用具体数字来感受这个问题的严重性。假设序列长度$n = 4096$（这在2024年已经算是"短序列"了）：</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>注意力矩阵大小：$4096 \times 4096 = 16.7M$个元素</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>如果序列长度翻倍到$n = 8192$：矩阵大小变为$67M$，增加了4倍</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>如果是GPT-4级别的$n = 128K$：矩阵大小达到$16.4B$个元素</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>这种二次增长意味着：**处理长文档、长对话、代码仓库、甚至整本书时，注意力计算会成为不可逾越的瓶颈**。</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>上一章结尾我们提到，除了让模型"更大"（MoE），还有一条路径是让模型"更高效地利用参数"。这正是状态空间模型（State Space Model, SSM）要探索的方向。</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>SSM的核心问题是：**序列建模一定需要让每个位置都与所有其他位置交互吗？**</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>回想一下RNN——它用一个隐状态$h_t$来压缩所有历史信息，只需要$O(n)$的计算。RNN的问题不是复杂度，而是：</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>顺序计算，无法并行训练</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>梯度消失/爆炸，难以建模长距离依赖</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>如果我们能保留RNN的线性复杂度优势，同时解决它的并行性和长程依赖问题呢？</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>这正是状态空间模型的出发点。SSM借鉴了控制论中研究了几十年的线性时不变系统（Linear Time-Invariant, LTI），通过精心设计的状态转移矩阵，实现了：</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**可并行训练**：通过卷积视角</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**高效推理**：通过递归视角</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**长程依赖**：通过HiPPO初始化</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：序列建模不一定需要attention。通过将连续时间的状态空间方程离散化，并用结构化的方式参数化状态矩阵，可以得到一类既能并行训练、又有线性推理复杂度、还能有效捕获长距离依赖的序列模型。Mamba更进一步，通过引入"选择性机制"让模型参数依赖于输入内容，弥补了LTI模型无法进行内容感知推理的根本缺陷。</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### Attention的二次瓶颈</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>Transformer的自注意力计算如下：</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>这个公式中，$QK^\top$是一个$n \times n$的矩阵，其中$n$是序列长度。这意味着：</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**计算复杂度**：$O(n^2 d)$</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**内存复杂度**：$O(n^2)$（需要存储完整的注意力矩阵）</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>当$n$增大时，这种二次增长很快变得不可接受。例如：</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 序列长度 <span class="pp">|</span> 注意力矩阵大小 <span class="pp">|</span> FP16内存 <span class="pp">|</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|---------------|----------|</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2,048 <span class="pp">|</span> 4.2M <span class="pp">|</span> 8 MB <span class="pp">|</span></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 8,192 <span class="pp">|</span> 67M <span class="pp">|</span> 134 MB <span class="pp">|</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 32,768 <span class="pp">|</span> 1.07B <span class="pp">|</span> 2.1 GB <span class="pp">|</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 131,072 <span class="pp">|</span> 17.2B <span class="pp">|</span> 34 GB <span class="pp">|</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>即使有FlashAttention这样的IO优化，$O(n^2)$的本质没有改变——它只是把常数因子降低了，但增长速度仍然是二次的。</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a><span class="fu">### 重新审视RNN</span></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>在Attention成为主流之前，RNN是序列建模的标准方法。RNN的核心是一个简单的递归方程：</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>h_t = \sigma(W_h h_{t-1} + W_x x_t + b)</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>其中$h_t$是时刻$t$的隐状态，$x_t$是输入。</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>RNN的计算复杂度是$O(n)$——这是线性的！每个时间步只需要常数时间的计算。那为什么RNN被Transformer取代了呢？</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>**问题一：顺序依赖，无法并行**</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>计算$h_t$必须先知道$h_{t-1}$，而$h_{t-1}$必须先知道$h_{t-2}$...这意味着训练时必须按顺序计算，无法利用GPU的并行能力。一个长度为1000的序列，RNN需要串行执行1000步；而Transformer可以一次性并行计算所有位置。</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>**问题二：梯度消失/爆炸**</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>当反向传播经过很多时间步时，梯度要么指数级衰减（消失），要么指数级增长（爆炸）。LSTM和GRU通过门控机制缓解了这个问题，但并没有根本解决。</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>**问题三：信息压缩瓶颈**</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>所有历史信息都被压缩到一个固定维度的向量$h_t$中。当序列很长时，早期信息必然会被"覆盖"或"遗忘"。</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的序列模型？</span></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>理想的序列模型应该同时具备：</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**线性复杂度**：$O(n)$或$O(n \log n)$，而非$O(n^2)$</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**可并行训练**：能够充分利用GPU的并行计算能力</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**长程依赖建模**：能够有效记住和利用远距离的信息</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**高效推理**：生成任务中能够像RNN一样常数时间更新状态</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>状态空间模型（SSM）正是朝着这个目标前进的一次重要尝试。</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从连续到离散：状态空间的基本形式</span></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>状态空间模型起源于控制论，描述的是一类连续时间的动态系统。最简单的形式是线性时不变（LTI）系统：</span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>\frac{dx(t)}{dt} = Ax(t) + Bu(t)</span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>y(t) = Cx(t) + Du(t)</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>其中：</span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$u(t) \in \mathbb{R}$：输入信号（控制输入）</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x(t) \in \mathbb{R}^N$：状态向量（系统的"记忆"）</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y(t) \in \mathbb{R}$：输出信号</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$A \in \mathbb{R}^{N \times N}$：状态转移矩阵（决定状态如何演化）</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$B \in \mathbb{R}^{N \times 1}$：输入矩阵</span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$C \in \mathbb{R}^{1 \times N}$：输出矩阵</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$D \in \mathbb{R}$：直接传递项（通常设为0或忽略）</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>这个系统的直觉是：</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**状态$x(t)$是系统的"记忆"**：它编码了所有历史输入的信息</span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**矩阵$A$决定了"遗忘模式"**：它控制状态如何随时间演化</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**矩阵$B$决定了"输入方式"**：新信息如何进入状态</span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**矩阵$C$决定了"读取方式"**：如何从状态中提取输出</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>用一个简单的类比：想象一个水箱系统。水箱中的水位就是"状态"，进水管道的流量是"输入"，出水管道的流量是"输出"。$A$决定了水箱的"漏水率"（状态的自然衰减），$B$决定了进水管道的效率，$C$决定了出水管道的效率。</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### 离散化：从连续到可计算</span></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>神经网络处理的是离散的序列（token序列），而不是连续的信号。因此需要将连续的状态空间方程**离散化**。</span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a>最常用的方法是**零阶保持（Zero-Order Hold, ZOH）**离散化。假设输入信号在每个采样间隔$\Delta$内保持常数，可以得到离散形式：</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>x_k = \bar{A} x_{k-1} + \bar{B} u_k</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>y_k = C x_k</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>其中离散化参数为：</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>\bar{A} = \exp(\Delta A)</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>\bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>这个形式和RNN非常相似！关键区别在于：</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**矩阵$\bar{A}$不是任意学习的，而是通过连续矩阵$A$和步长$\Delta$推导出来的**</span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**通过特殊的$A$矩阵初始化（如HiPPO），可以获得长程记忆能力**</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a><span class="fu">### 递归视角 vs 卷积视角</span></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>SSM的一个关键优势是它有两种等价的计算方式：</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>**递归视角（Recurrent View）**：</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>逐步计算状态更新：</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>x_k = \bar{A} x_{k-1} + \bar{B} u_k, \quad y_k = C x_k</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>这种方式适合**推理**——每生成一个新token，只需要$O(N)$的计算来更新状态。</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>**卷积视角（Convolutional View）**：</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a>将整个输入序列和一个卷积核进行卷积：</span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>\bar{K} = (C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B}, \ldots, C\bar{A}^{L-1}\bar{B})</span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>y = \bar{K} * u</span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a>这种方式适合**训练**——卷积可以用FFT高效计算，复杂度为$O(n \log n)$，而且可以高度并行。</span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>这种"训练时用卷积，推理时用递归"的双重性质，是SSM相对于传统RNN的关键优势。</span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### HiPPO：长程记忆的数学基础</span></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a>普通的RNN为什么无法建模长距离依赖？根本原因是：当矩阵$A$（或$\bar{A}$）被随机初始化时，状态会快速"遗忘"早期信息。</span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a>HiPPO（High-order Polynomial Projection Operators）提供了一个精妙的解决方案。核心思想是：**用正交多项式来近似历史信号**。</span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a>具体来说，HiPPO将历史输入信号$u(\tau)$（$\tau \leq t$）投影到Legendre多项式的系数上。这些系数就是状态向量$x(t)$的各个分量。</span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a>为什么这样做有效？因为：</span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**正交多项式系数可以增量更新**：当新输入到达时，不需要重新计算所有系数</span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**低阶系数捕获"全局趋势"**：即使序列很长，也能保留关于整体形状的信息</span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**高阶系数捕获"局部细节"**：近期的细节信息也不会丢失</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a>HiPPO-LegS（Legendre Scaled）版本的$A$矩阵有如下形式：</span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a>A_{nk} = -\begin{cases}</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a>(2n+1)^{1/2}(2k+1)^{1/2} &amp; \text{if } n &gt; k <span class="sc">\\</span></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a>n+1 &amp; \text{if } n = k <span class="sc">\\</span></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{if } n &lt; k</span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a>这是一个下三角矩阵，具有特殊的数学性质，能够保证状态向量以最优方式压缩历史信息。</span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a><span class="fu">### S4：结构化状态空间的突破</span></span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a>S4（Structured State Space for Sequences）是将SSM引入深度学习的里程碑工作。它解决了一个关键的计算瓶颈：**如何高效计算卷积核$\bar{K}$？**</span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="al">![S4模型架构概览。左侧展示了状态空间模型的基本形式（连续→离散化→递归/卷积双视角）；中间展示了S4如何通过HiPPO初始化和结构化参数化实现高效计算；右侧是完整的S4层设计。](figures/chapter-28/original/fig-s4-architecture.png)</span>{#fig-s4-arch width=95%}</span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a>*Source: Gu et al. (2021) "Efficiently Modeling Long Sequences with Structured State Spaces", Figure 1*</span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a>朴素方法需要计算$A^k$对于$k = 0, 1, \ldots, L-1$，复杂度是$O(N^2 L)$或$O(N^3 L)$（取决于矩阵乘法方式）——这在长序列上是不可接受的。</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a>S4的关键洞察是：**通过低秩修正将HiPPO矩阵对角化**。</span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a>具体来说，S4假设$A$矩阵可以分解为：</span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a>A = V \Lambda V^{-1} - PQ^\top</span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>其中$\Lambda$是对角矩阵，$P, Q$是低秩矩阵。这种结构化使得：</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**卷积核可以用Cauchy核高效计算**：复杂度降到$O(N + L)$</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**训练时整体复杂度为$O(N + L)\log(N + L)$**：接近线性</span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a>S4在长程竞技场（Long Range Arena）基准测试上取得了突破性成果，首次解决了Path-X任务——这是一个需要关注长度16384序列中首尾信息的分类任务，此前所有模型（包括Transformer）都失败了。</span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mamba：选择性状态空间</span></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a>S4虽然解决了计算效率问题，但它有一个根本性的限制：**参数是时不变的（Time-Invariant）**。</span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>这意味着矩阵$A$、$B$、$C$对所有输入都是相同的——模型无法根据输入内容调整自己的行为。这与Transformer形成鲜明对比：Attention的权重$\text{softmax}(QK^\top)$完全依赖于输入内容。</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a>Mamba的核心创新是引入**选择性机制（Selection Mechanism）**：让参数$B$、$C$和步长$\Delta$成为输入的函数。</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a><span class="al">![Mamba的核心创新：选择性SSM。结构化SSM独立地将每个输入通道映射到输出，通过一个潜在状态实现。选择性机制通过让SSM参数（$\Delta$, $B$, $C$）依赖于输入，为模型添加了输入依赖的动态特性。](figures/chapter-28/original/fig1-mamba-overview.png)</span>{#fig-mamba-overview width=90%}</span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>*Source: Gu &amp; Dao (2023) "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", Figure 1*</span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>具体来说：</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>B_t = \text{Linear}_B(x_t), \quad C_t = \text{Linear}_C(x_t), \quad \Delta_t = \text{softplus}(\text{Linear}_\Delta(x_t))</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>其中$x_t$是时刻$t$的输入。这意味着：</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**$B_t$控制"什么信息进入状态"**：模型可以根据内容决定是否"记住"某个输入</span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**$C_t$控制"什么信息被读取"**：模型可以根据内容决定关注状态的哪些方面</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**$\Delta_t$控制"步长大小"**：可以理解为"时间流逝速度"——步长大意味着更快遗忘旧信息</span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么选择性机制是关键？</span></span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a>让我们用两个具体任务来理解为什么LTI模型不够用：</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a><span class="al">![选择性机制的必要性。上图：选择性复制任务——模型需要记住用特殊标记标记的token（彩色），忽略其他。下图：归纳头任务——模型需要进行关联召回。LTI模型在这两个任务上都会失败。](figures/chapter-28/original/fig2-selective-tasks.png)</span>{#fig-selective-tasks width=85%}</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>*Source: Gu &amp; Dao (2023) "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", Figure 2*</span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a>**选择性复制（Selective Copying）**：</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a>给定序列"A X B X C X"，要求输出"A B C"。中间的"X"是噪声，应该被忽略。</span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a>LTI模型无法完成这个任务，因为它对每个输入的处理方式是相同的——无法区分"有意义的A"和"无意义的X"。</span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a>选择性SSM可以学会：当输入是"X"时，让$\Delta$很小（不更新状态）；当输入是有意义的字母时，让$\Delta$正常大小（写入状态）。</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a>**归纳头（Induction Heads）**：</span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a>给定"...A B ... A ?"，要求预测"B"。这需要：(1) 记住"A后面跟着B"的模式；(2) 在看到第二个"A"时召回这个模式。</span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a>LTI模型的状态更新与输入内容无关，无法实现这种"根据内容检索"的能力。</span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a>选择性SSM可以学会：用$B_t$编码"这是什么内容"，用$C_t$编码"我需要找什么内容"，从而实现关联召回。</span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a><span class="fu">### 硬件感知的选择性扫描算法</span></span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>选择性机制带来了一个计算挑战：参数现在依赖于输入，无法再用FFT-based的卷积高效计算。</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a>Mamba论文提出了一个**硬件感知的并行扫描算法**。核心思想是：</span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**利用并行扫描（Parallel Scan）**：扫描操作可以用$O(\log L)$的并行步骤完成$L$次递归</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**内存优化**：不具体化完整的状态序列，而是在SRAM中逐块计算</span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**内核融合**：将离散化、状态更新、输出计算融合到单个GPU内核中</span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a>这个算法使得Mamba在训练时比S4快40倍以上，同时保持线性的内存复杂度。</span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mamba架构</span></span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a>Mamba不仅仅是一个SSM层，还定义了一个完整的神经网络架构：</span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a><span class="al">![Mamba Block架构。与H3架构相比，Mamba简化了设计：用激活函数替代乘性门，并融合了多个投影步骤。结果是一个更简单、更高效的块结构。](figures/chapter-28/original/fig3-mamba-block.png)</span>{#fig-mamba-block width=70%}</span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>*Source: Gu &amp; Dao (2023) "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", Figure 3*</span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a>Mamba Block的设计借鉴了门控MLP的思想：</span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**输入投影**：将输入扩展到更高维度（通常2倍）</span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**卷积层**：一个短的深度可分离卷积，捕获局部模式</span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**选择性SSM**：核心的序列建模组件</span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**门控乘法**：将SSM输出与另一条分支相乘</span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**输出投影**：投影回原始维度</span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a>关键设计决策：</span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**没有Attention**：整个模型完全基于SSM</span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**没有MLP块**：SSM本身就包含了足够的非线性</span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**状态维度$N$**：通常设为16，远小于隐藏维度</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: Selective SSM Forward Pass (Mamba)</span></span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> selective_ssm(u, A, B, C, delta):</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a><span class="co">    选择性状态空间模型的前向传播</span></span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a><span class="co">        u: [batch, seq_len, d_model] 输入序列</span></span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a><span class="co">        A: [d_inner, N] 状态转移矩阵参数</span></span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a><span class="co">        B: [batch, seq_len, N] 输入依赖的B矩阵</span></span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a><span class="co">        C: [batch, seq_len, N] 输入依赖的C矩阵</span></span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a><span class="co">        delta: [batch, seq_len, d_inner] 输入依赖的步长</span></span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a><span class="co">        y: [batch, seq_len, d_model] 输出序列</span></span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 离散化 (ZOH)</span></span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A_bar = exp(delta * A)</span></span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a>    A_bar <span class="op">=</span> torch.exp(delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> A)  <span class="co"># [batch, seq_len, d_inner, N]</span></span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a>    B_bar <span class="op">=</span> delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> B.unsqueeze(<span class="dv">2</span>)  <span class="co"># [batch, seq_len, d_inner, N]</span></span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 并行扫描计算状态序列</span></span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_k = A_bar_k * x_{k-1} + B_bar_k * u_k</span></span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> parallel_scan(A_bar, B_bar <span class="op">*</span> u.unsqueeze(<span class="op">-</span><span class="dv">1</span>))  <span class="co"># [batch, seq_len, d_inner, N]</span></span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 计算输出</span></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_k = C_k * x_k</span></span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.einsum(<span class="st">'bldn,bln-&gt;bld'</span>, x, C)  <span class="co"># [batch, seq_len, d_inner]</span></span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parallel_scan(A_bar, BU):</span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a><span class="co">    并行扫描算法：O(log L) 并行步骤完成 L 次递归</span></span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a><span class="co">    关键洞察：递归 x_k = A_k * x_{k-1} + b_k 可以重写为</span></span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a><span class="co">    结合律的二元运算 (A_k, b_k) * (A_{k-1}, b_{k-1}) = (A_k*A_{k-1}, A_k*b_{k-1}+b_k)</span></span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a><span class="co">    然后用并行前缀和算法高效计算</span></span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 具体实现涉及GPU内核优化，此处省略</span></span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a>*Source: Adapted from Gu &amp; Dao (2023) "Mamba", Section 3.3*</span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scaling Laws与性能</span></span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a>Mamba在语言建模任务上表现出色：</span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a><span class="al">![Mamba的Scaling Law。在预训练困惑度上，Mamba优于其他subquadratic模型，并与Transformer++（一个强优化的Transformer基线）相当。从125M到1.3B参数规模都保持这种优势。](figures/chapter-28/original/fig6-scaling-laws.png)</span>{#fig-scaling width=90%}</span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a>*Source: Gu &amp; Dao (2023) "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", Figure 6*</span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a>关键发现：</span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mamba-3B在预训练困惑度上与Transformer-3B相当</span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>在下游评估中，Mamba-3B与Transformer-6B相当（2倍参数效率）</span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>推理吞吐量是Transformer的5倍（归功于线性复杂度）</span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数值示例：选择性SSM的状态更新</span></span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a>让我们用一个简化的数值例子来理解选择性SSM的工作方式。</span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>状态维度 $N = 2$</span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入维度 $d = 1$（简化为标量）</span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>3个时间步</span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>**输入序列**：$u = <span class="co">[</span><span class="ot">0.5, 0.8, 0.2</span><span class="co">]</span>$（假设这是一个"重要-重要-不重要"的模式）</span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a>**固定参数**：</span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a>A = \begin{bmatrix} -0.5 &amp; 0 <span class="sc">\\</span> 0 &amp; -1.0 \end{bmatrix} \quad \text{(负值，保证稳定性)}</span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a>**输入依赖的参数**（经过网络计算后）：</span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 时间步 $t$ <span class="pp">|</span> 输入 $u_t$ <span class="pp">|</span> $\Delta_t$ <span class="pp">|</span> $B_t$ <span class="pp">|</span> $C_t$ <span class="pp">|</span></span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-----------|-----------|-------|-------|</span></span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1 <span class="pp">|</span> 0.5 <span class="pp">|</span> 0.8 <span class="pp">|</span> <span class="co">[</span><span class="ot">1.0, 0.5</span><span class="co">]</span> <span class="pp">|</span> <span class="co">[</span><span class="ot">0.6, 0.4</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2 <span class="pp">|</span> 0.8 <span class="pp">|</span> 0.9 <span class="pp">|</span> <span class="co">[</span><span class="ot">1.2, 0.6</span><span class="co">]</span> <span class="pp">|</span> <span class="co">[</span><span class="ot">0.5, 0.5</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 3 <span class="pp">|</span> 0.2 <span class="pp">|</span> 0.1 <span class="pp">|</span> <span class="co">[</span><span class="ot">0.3, 0.1</span><span class="co">]</span> <span class="pp">|</span> <span class="co">[</span><span class="ot">0.7, 0.3</span><span class="co">]</span> <span class="pp">|</span></span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>注意：$\Delta_3 = 0.1$很小，因为输入$u_3 = 0.2$被判断为"不重要"。</span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a>**Step 1：离散化（$t=1$）**</span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a>\bar{A}_1 = \exp(\Delta_1 \cdot A) = \exp\left(0.8 \times \begin{bmatrix} -0.5 &amp; 0 <span class="sc">\\</span> 0 &amp; -1.0 \end{bmatrix}\right) = \begin{bmatrix} e^{-0.4} &amp; 0 <span class="sc">\\</span> 0 &amp; e^{-0.8} \end{bmatrix} \approx \begin{bmatrix} 0.67 &amp; 0 <span class="sc">\\</span> 0 &amp; 0.45 \end{bmatrix}</span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a>\bar{B}_1 \approx \Delta_1 \cdot B_1 = 0.8 \times <span class="co">[</span><span class="ot">1.0, 0.5</span><span class="co">]</span> = <span class="co">[</span><span class="ot">0.8, 0.4</span><span class="co">]</span></span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a>**Step 2：状态更新**</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a>初始状态 $x_0 = <span class="co">[</span><span class="ot">0, 0</span><span class="co">]</span>$</span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a>x_1 = \bar{A}_1 x_0 + \bar{B}_1 u_1 = <span class="co">[</span><span class="ot">0, 0</span><span class="co">]</span> + <span class="co">[</span><span class="ot">0.8, 0.4</span><span class="co">]</span> \times 0.5 = <span class="co">[</span><span class="ot">0.4, 0.2</span><span class="co">]</span></span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a>y_1 = C_1 \cdot x_1 = <span class="co">[</span><span class="ot">0.6, 0.4</span><span class="co">]</span> \cdot <span class="co">[</span><span class="ot">0.4, 0.2</span><span class="co">]</span> = 0.24 + 0.08 = 0.32</span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a>类似地计算 $t=2$：</span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a>x_2 = \bar{A}_2 x_1 + \bar{B}_2 u_2 = <span class="co">[</span><span class="ot">0.64 \times 0.4, 0.41 \times 0.2</span><span class="co">]</span> + <span class="co">[</span><span class="ot">1.08, 0.54</span><span class="co">]</span> \times 0.8</span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.256, 0.082</span><span class="co">]</span> + <span class="co">[</span><span class="ot">0.864, 0.432</span><span class="co">]</span> = <span class="co">[</span><span class="ot">1.12, 0.51</span><span class="co">]</span></span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a>y_2 = C_2 \cdot x_2 = <span class="co">[</span><span class="ot">0.5, 0.5</span><span class="co">]</span> \cdot <span class="co">[</span><span class="ot">1.12, 0.51</span><span class="co">]</span> = 0.82</span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a>对于 $t=3$，由于 $\Delta_3 = 0.1$ 很小：</span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a>\bar{A}_3 = \begin{bmatrix} e^{-0.05} &amp; 0 <span class="sc">\\</span> 0 &amp; e^{-0.1} \end{bmatrix} \approx \begin{bmatrix} 0.95 &amp; 0 <span class="sc">\\</span> 0 &amp; 0.90 \end{bmatrix}</span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a>状态几乎不变：</span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a>x_3 \approx <span class="co">[</span><span class="ot">0.95 \times 1.12, 0.90 \times 0.51</span><span class="co">]</span> + <span class="co">[</span><span class="ot">0.03, 0.01</span><span class="co">]</span> \times 0.2 = <span class="co">[</span><span class="ot">1.07, 0.46</span><span class="co">]</span></span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-515"><a href="#cb7-515" aria-hidden="true" tabindex="-1"></a>**关键观察**：</span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>当输入"重要"（$u_1, u_2$较大）时，$\Delta$较大，状态更新明显</span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>当输入"不重要"（$u_3$较小）时，$\Delta$较小，状态几乎保持不变</span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>这就是"选择性"的含义：模型学会了何时记忆、何时忽略</span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mamba-2与状态空间对偶性</span></span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a><span class="fu">### SSD：结构化状态空间对偶</span></span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a>Mamba-2论文揭示了一个深刻的理论联系：**特定形式的SSM与特定形式的Attention在数学上是等价的**。</span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a>这种联系被称为**状态空间对偶性（Structured State Space Duality, SSD）**。</span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a>具体来说，当SSM的状态矩阵$A$是"标量乘以单位矩阵"的形式时：</span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a>A = -a \cdot I, \quad a &gt; 0</span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a>SSM的输入输出关系可以写成：</span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a>y = M \cdot (L \odot V)</span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a>其中$L$是一个**1-半可分（1-semiseparable）因果掩码矩阵**，$V$是值向量，$M$是一个累乘矩阵。</span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a>这与masked self-attention有惊人的相似性——attention也是对值向量做加权求和，权重由查询-键相似度决定。</span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a><span class="fu">### 两种计算视角的统一</span></span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a>SSD框架的核心洞察是：**同一个序列变换可以用两种算法实现**：</span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**SSM视角**：$O(n)$的递归计算</span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Attention视角**：$O(n^2)$的矩阵乘法计算</span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a>在实践中，我们可以根据硬件特性选择最优实现：</span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**短序列**：使用矩阵乘法（更好地利用GPU的tensor core）</span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**长序列**：使用递归（避免$O(n^2)$的内存开销）</span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**混合策略**：将长序列分成块，块内用矩阵乘法，块间用递归</span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a>Mamba-2利用这种灵活性，实现了比Mamba快2-8倍的训练速度。</span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mamba-2的改进</span></span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a>除了SSD框架，Mamba-2还引入了几个实用改进：</span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**多头结构**：类似Transformer的multi-head attention，Mamba-2将状态空间分成多个"头"，每个头独立处理</span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**归一化**：在SSM后添加RMSNorm，提高训练稳定性</span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**更大的状态维度**：从Mamba的$N=16$增加到$N=64$或更大</span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a><span class="fu">## 混合架构：结合两种范式的优势</span></span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么需要混合？</span></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a>尽管Mamba展示了impressive的性能，但它并非在所有任务上都优于Transformer。研究发现：</span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention擅长**：精确的信息检索、复杂的推理、需要全局视角的任务</span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SSM擅长**：长序列建模、高效推理、流式处理</span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a>一个自然的想法是：**何不把两者结合起来？**</span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a><span class="fu">### Jamba：混合Transformer-Mamba架构</span></span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a>Jamba是AI21 Labs在2024年3月发布的混合架构模型，也是第一个大规模的Transformer-Mamba混合模型。</span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a>**核心设计**：</span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**交替层**：Attention层和Mamba层以1:7的比例交替</span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MoE集成**：每隔一层使用MoE，提供更大的模型容量</span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数**：52B（12B激活参数）</span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**上下文长度**：256K tokens</span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a>**架构细节**：</span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 0: Mamba</span></span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 1: Mamba + MoE</span></span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 2: Mamba</span></span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 3: Attention + MoE</span></span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 4: Mamba</span></span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 5: Mamba + MoE</span></span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 6: Mamba</span></span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a><span class="in">Layer 7: Attention + MoE</span></span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a><span class="in">...（重复）</span></span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a>**关键发现**：</span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Mamba层不需要位置编码**：与Transformer不同，Mamba天然具有位置感知能力（通过递归状态）</span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**少量Attention就够了**：1:7的比例（约12.5%的Attention层）已经足够保留Transformer的优势</span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**内存效率**：由于大部分层是Mamba（无KV cache），整体内存需求远低于纯Transformer</span>
<span id="cb7-613"><a href="#cb7-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-614"><a href="#cb7-614" aria-hidden="true" tabindex="-1"></a>**性能表现**：</span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>在256K上下文窗口内：接近Claude-2的质量</span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>推理速度：显著快于同规模的纯Transformer</span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>内存占用：可以在单个80GB GPU上运行完整的52B模型</span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a><span class="fu">### 混合架构的设计原则</span></span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a>从Jamba和后续工作中，我们可以总结出一些混合架构的设计原则：</span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Attention层放在关键位置**：早期层和晚期层可能更需要全局交互</span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Mamba处理"重复性"计算**：中间层的大量计算可以交给更高效的Mamba</span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**可以结合MoE**：三种技术（Attention、SSM、MoE）可以正交地组合</span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Hugging Face加载Mamba模型</span></span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a>Mamba模型已经集成到Hugging Face的<span class="in">`transformers`</span>库中，可以方便地加载和使用：</span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装依赖</span></span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers torch mamba-ssm causal-conv1d</span></span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> MambaConfig, MambaForCausalLM, AutoTokenizer</span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的Mamba模型</span></span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"state-spaces/mamba-130m-hf"</span></span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MambaForCausalLM.from_pretrained(model_name)</span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置pad token（Mamba使用EOS作为pad）</span></span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a><span class="co"># 文本生成示例</span></span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"The capital of France is"</span></span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成文本</span></span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model.generate(</span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a>        inputs.input_ids,</span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a>        max_new_tokens<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a>        do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generated: </span><span class="sc">{</span>generated_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现简化版SSM</span></span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a>为了深入理解SSM的工作原理，让我们实现一个简化版本：</span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleSSM(nn.Module):</span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a><span class="co">    简化版状态空间模型（用于教学目的）</span></span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a><span class="co">    关键组件：</span></span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a><span class="co">    - A: 状态转移矩阵（对角化，用于稳定性）</span></span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a><span class="co">    - B, C: 输入/输出投影</span></span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a><span class="co">    - delta: 离散化步长</span></span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_state<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_state <span class="op">=</span> d_state</span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A矩阵：使用负值保证稳定性（对角化简化）</span></span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 初始化为HiPPO-like的值</span></span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> torch.arange(<span class="dv">1</span>, d_state <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A_log <span class="op">=</span> nn.Parameter(torch.log(A))  <span class="co"># 学习log(A)保证正值</span></span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B, C投影</span></span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.B_proj <span class="op">=</span> nn.Linear(d_model, d_state, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C_proj <span class="op">=</span> nn.Linear(d_model, d_state, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Delta投影（控制离散化步长）</span></span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.delta_proj <span class="op">=</span> nn.Linear(d_model, d_model, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出投影</span></span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a><span class="co">            x: [batch, seq_len, d_model]</span></span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a><span class="co">            y: [batch, seq_len, d_model]</span></span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a>        batch, seq_len, _ <span class="op">=</span> x.shape</span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算输入依赖的参数（选择性机制的核心）</span></span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a>        B <span class="op">=</span> <span class="va">self</span>.B_proj(x)  <span class="co"># [batch, seq_len, d_state]</span></span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> <span class="va">self</span>.C_proj(x)  <span class="co"># [batch, seq_len, d_state]</span></span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算delta（步长），使用softplus保证正值</span></span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> F.softplus(<span class="va">self</span>.delta_proj(x))  <span class="co"># [batch, seq_len, d_model]</span></span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 获取A（对角矩阵，用向量表示）</span></span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> <span class="op">-</span>torch.exp(<span class="va">self</span>.A_log)  <span class="co"># [d_state]，负值保证稳定</span></span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-736"><a href="#cb7-736" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 离散化：使用简化的ZOH近似</span></span>
<span id="cb7-737"><a href="#cb7-737" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A_bar = exp(delta * A) ≈ 1 + delta * A（一阶近似）</span></span>
<span id="cb7-738"><a href="#cb7-738" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 这里我们对每个通道独立处理</span></span>
<span id="cb7-739"><a href="#cb7-739" aria-hidden="true" tabindex="-1"></a>        delta_A <span class="op">=</span> delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> A  <span class="co"># [batch, seq_len, d_model, d_state]</span></span>
<span id="cb7-740"><a href="#cb7-740" aria-hidden="true" tabindex="-1"></a>        A_bar <span class="op">=</span> torch.exp(delta_A)</span>
<span id="cb7-741"><a href="#cb7-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-742"><a href="#cb7-742" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B_bar = delta * B</span></span>
<span id="cb7-743"><a href="#cb7-743" aria-hidden="true" tabindex="-1"></a>        B_bar <span class="op">=</span> delta.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> B.unsqueeze(<span class="dv">2</span>)  <span class="co"># [batch, seq_len, d_model, d_state]</span></span>
<span id="cb7-744"><a href="#cb7-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-745"><a href="#cb7-745" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 递归计算状态（简化版，实际Mamba使用并行扫描）</span></span>
<span id="cb7-746"><a href="#cb7-746" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.zeros(batch, <span class="va">self</span>.d_model, <span class="va">self</span>.d_state, device<span class="op">=</span>x.device)</span>
<span id="cb7-747"><a href="#cb7-747" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> []</span>
<span id="cb7-748"><a href="#cb7-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-749"><a href="#cb7-749" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-750"><a href="#cb7-750" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 状态更新: h_t = A_bar * h_{t-1} + B_bar * x_t</span></span>
<span id="cb7-751"><a href="#cb7-751" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> A_bar[:, t] <span class="op">*</span> h <span class="op">+</span> B_bar[:, t] <span class="op">*</span> x[:, t:t<span class="op">+</span><span class="dv">1</span>, :].transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb7-752"><a href="#cb7-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-753"><a href="#cb7-753" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 输出: y_t = C_t * h_t</span></span>
<span id="cb7-754"><a href="#cb7-754" aria-hidden="true" tabindex="-1"></a>            y_t <span class="op">=</span> torch.einsum(<span class="st">'bdn,bn-&gt;bd'</span>, h, C[:, t])</span>
<span id="cb7-755"><a href="#cb7-755" aria-hidden="true" tabindex="-1"></a>            outputs.append(y_t)</span>
<span id="cb7-756"><a href="#cb7-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-757"><a href="#cb7-757" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.stack(outputs, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch, seq_len, d_model]</span></span>
<span id="cb7-758"><a href="#cb7-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-759"><a href="#cb7-759" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out_proj(y)</span>
<span id="cb7-760"><a href="#cb7-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-761"><a href="#cb7-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-762"><a href="#cb7-762" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试</span></span>
<span id="cb7-763"><a href="#cb7-763" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb7-764"><a href="#cb7-764" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, d_model <span class="op">=</span> <span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">64</span></span>
<span id="cb7-765"><a href="#cb7-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-766"><a href="#cb7-766" aria-hidden="true" tabindex="-1"></a>    ssm <span class="op">=</span> SimpleSSM(d_model<span class="op">=</span>d_model, d_state<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb7-767"><a href="#cb7-767" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(batch_size, seq_len, d_model)</span>
<span id="cb7-768"><a href="#cb7-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-769"><a href="#cb7-769" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> ssm(x)</span>
<span id="cb7-770"><a href="#cb7-770" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-771"><a href="#cb7-771" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-772"><a href="#cb7-772" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> ssm.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb7-773"><a href="#cb7-773" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-774"><a href="#cb7-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-775"><a href="#cb7-775" aria-hidden="true" tabindex="-1"></a><span class="fu">### 比较Mamba与Transformer的推理速度</span></span>
<span id="cb7-776"><a href="#cb7-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-777"><a href="#cb7-777" aria-hidden="true" tabindex="-1"></a>以下代码展示如何比较两种架构的推理效率：</span>
<span id="cb7-778"><a href="#cb7-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-781"><a href="#cb7-781" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-782"><a href="#cb7-782" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-783"><a href="#cb7-783" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-784"><a href="#cb7-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-785"><a href="#cb7-785" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-786"><a href="#cb7-786" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb7-787"><a href="#cb7-787" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb7-788"><a href="#cb7-788" aria-hidden="true" tabindex="-1"></a>    MambaForCausalLM,</span>
<span id="cb7-789"><a href="#cb7-789" aria-hidden="true" tabindex="-1"></a>    GPT2LMHeadModel,</span>
<span id="cb7-790"><a href="#cb7-790" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer</span>
<span id="cb7-791"><a href="#cb7-791" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-792"><a href="#cb7-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-793"><a href="#cb7-793" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark_generation(model, tokenizer, prompt, max_new_tokens<span class="op">=</span><span class="dv">100</span>, num_runs<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb7-794"><a href="#cb7-794" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""测量生成速度"""</span></span>
<span id="cb7-795"><a href="#cb7-795" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb7-796"><a href="#cb7-796" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb7-797"><a href="#cb7-797" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.cuda()</span>
<span id="cb7-798"><a href="#cb7-798" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> {k: v.cuda() <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()}</span>
<span id="cb7-799"><a href="#cb7-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-800"><a href="#cb7-800" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warmup</span></span>
<span id="cb7-801"><a href="#cb7-801" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-802"><a href="#cb7-802" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> model.generate(inputs[<span class="st">"input_ids"</span>], max_new_tokens<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-803"><a href="#cb7-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-804"><a href="#cb7-804" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Benchmark</span></span>
<span id="cb7-805"><a href="#cb7-805" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> []</span>
<span id="cb7-806"><a href="#cb7-806" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb7-807"><a href="#cb7-807" aria-hidden="true" tabindex="-1"></a>        torch.cuda.synchronize() <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb7-808"><a href="#cb7-808" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> time.perf_counter()</span>
<span id="cb7-809"><a href="#cb7-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-810"><a href="#cb7-810" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-811"><a href="#cb7-811" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model.generate(</span>
<span id="cb7-812"><a href="#cb7-812" aria-hidden="true" tabindex="-1"></a>                inputs[<span class="st">"input_ids"</span>],</span>
<span id="cb7-813"><a href="#cb7-813" aria-hidden="true" tabindex="-1"></a>                max_new_tokens<span class="op">=</span>max_new_tokens,</span>
<span id="cb7-814"><a href="#cb7-814" aria-hidden="true" tabindex="-1"></a>                do_sample<span class="op">=</span><span class="va">False</span>,  <span class="co"># Greedy for reproducibility</span></span>
<span id="cb7-815"><a href="#cb7-815" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-816"><a href="#cb7-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-817"><a href="#cb7-817" aria-hidden="true" tabindex="-1"></a>        torch.cuda.synchronize() <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb7-818"><a href="#cb7-818" aria-hidden="true" tabindex="-1"></a>        times.append(time.perf_counter() <span class="op">-</span> start)</span>
<span id="cb7-819"><a href="#cb7-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-820"><a href="#cb7-820" aria-hidden="true" tabindex="-1"></a>    avg_time <span class="op">=</span> <span class="bu">sum</span>(times) <span class="op">/</span> <span class="bu">len</span>(times)</span>
<span id="cb7-821"><a href="#cb7-821" aria-hidden="true" tabindex="-1"></a>    tokens_per_sec <span class="op">=</span> max_new_tokens <span class="op">/</span> avg_time</span>
<span id="cb7-822"><a href="#cb7-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-823"><a href="#cb7-823" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_time, tokens_per_sec, outputs</span>
<span id="cb7-824"><a href="#cb7-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-825"><a href="#cb7-825" aria-hidden="true" tabindex="-1"></a><span class="co"># 比较Mamba和GPT2（相近规模）</span></span>
<span id="cb7-826"><a href="#cb7-826" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"In the field of artificial intelligence, large language models have"</span></span>
<span id="cb7-827"><a href="#cb7-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-828"><a href="#cb7-828" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb7-829"><a href="#cb7-829" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading Mamba-130M..."</span>)</span>
<span id="cb7-830"><a href="#cb7-830" aria-hidden="true" tabindex="-1"></a>mamba_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"state-spaces/mamba-130m-hf"</span>)</span>
<span id="cb7-831"><a href="#cb7-831" aria-hidden="true" tabindex="-1"></a>mamba_model <span class="op">=</span> MambaForCausalLM.from_pretrained(<span class="st">"state-spaces/mamba-130m-hf"</span>)</span>
<span id="cb7-832"><a href="#cb7-832" aria-hidden="true" tabindex="-1"></a>mamba_tokenizer.pad_token <span class="op">=</span> mamba_tokenizer.eos_token</span>
<span id="cb7-833"><a href="#cb7-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-834"><a href="#cb7-834" aria-hidden="true" tabindex="-1"></a>mamba_time, mamba_tps, mamba_out <span class="op">=</span> benchmark_generation(</span>
<span id="cb7-835"><a href="#cb7-835" aria-hidden="true" tabindex="-1"></a>    mamba_model, mamba_tokenizer, prompt</span>
<span id="cb7-836"><a href="#cb7-836" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-837"><a href="#cb7-837" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mamba-130M: </span><span class="sc">{</span>mamba_time<span class="sc">:.3f}</span><span class="ss">s, </span><span class="sc">{</span>mamba_tps<span class="sc">:.1f}</span><span class="ss"> tokens/s"</span>)</span>
<span id="cb7-838"><a href="#cb7-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-839"><a href="#cb7-839" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Loading GPT2 (124M)..."</span>)</span>
<span id="cb7-840"><a href="#cb7-840" aria-hidden="true" tabindex="-1"></a>gpt2_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb7-841"><a href="#cb7-841" aria-hidden="true" tabindex="-1"></a>gpt2_model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb7-842"><a href="#cb7-842" aria-hidden="true" tabindex="-1"></a>gpt2_tokenizer.pad_token <span class="op">=</span> gpt2_tokenizer.eos_token</span>
<span id="cb7-843"><a href="#cb7-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-844"><a href="#cb7-844" aria-hidden="true" tabindex="-1"></a>gpt2_time, gpt2_tps, gpt2_out <span class="op">=</span> benchmark_generation(</span>
<span id="cb7-845"><a href="#cb7-845" aria-hidden="true" tabindex="-1"></a>    gpt2_model, gpt2_tokenizer, prompt</span>
<span id="cb7-846"><a href="#cb7-846" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-847"><a href="#cb7-847" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT2-124M: </span><span class="sc">{</span>gpt2_time<span class="sc">:.3f}</span><span class="ss">s, </span><span class="sc">{</span>gpt2_tps<span class="sc">:.1f}</span><span class="ss"> tokens/s"</span>)</span>
<span id="cb7-848"><a href="#cb7-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-849"><a href="#cb7-849" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb7-850"><a href="#cb7-850" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>gpt2_time <span class="op">/</span> mamba_time<span class="sc">:.2f}</span><span class="ss">x"</span>)</span>
<span id="cb7-851"><a href="#cb7-851" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb7-852"><a href="#cb7-852" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-853"><a href="#cb7-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-854"><a href="#cb7-854" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb7-855"><a href="#cb7-855" aria-hidden="true" tabindex="-1"></a><span class="fu">## 运行提示</span></span>
<span id="cb7-856"><a href="#cb7-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-857"><a href="#cb7-857" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**GPU推荐**：虽然CPU也能运行，但GPU上性能差异更明显</span>
<span id="cb7-858"><a href="#cb7-858" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**长序列测试**：Mamba的优势在长序列上更显著，可以尝试更长的<span class="in">`max_new_tokens`</span></span>
<span id="cb7-859"><a href="#cb7-859" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**内存监控**：观察KV cache增长（GPT2）vs 固定状态（Mamba）</span>
<span id="cb7-860"><a href="#cb7-860" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-861"><a href="#cb7-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-862"><a href="#cb7-862" aria-hidden="true" tabindex="-1"></a><span class="fu">### 可视化SSM的状态演化</span></span>
<span id="cb7-863"><a href="#cb7-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-864"><a href="#cb7-864" aria-hidden="true" tabindex="-1"></a>理解SSM如何"记忆"信息的一个好方法是可视化状态向量的演化：</span>
<span id="cb7-865"><a href="#cb7-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-868"><a href="#cb7-868" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-869"><a href="#cb7-869" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-870"><a href="#cb7-870" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-871"><a href="#cb7-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-872"><a href="#cb7-872" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-873"><a href="#cb7-873" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-874"><a href="#cb7-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-875"><a href="#cb7-875" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_ssm_state(A_diag, B, C, inputs, title<span class="op">=</span><span class="st">"SSM State Evolution"</span>):</span>
<span id="cb7-876"><a href="#cb7-876" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-877"><a href="#cb7-877" aria-hidden="true" tabindex="-1"></a><span class="co">    可视化SSM状态随时间的演化</span></span>
<span id="cb7-878"><a href="#cb7-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-879"><a href="#cb7-879" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb7-880"><a href="#cb7-880" aria-hidden="true" tabindex="-1"></a><span class="co">        A_diag: 对角A矩阵的值 [d_state]</span></span>
<span id="cb7-881"><a href="#cb7-881" aria-hidden="true" tabindex="-1"></a><span class="co">        B: 输入矩阵 [d_state]</span></span>
<span id="cb7-882"><a href="#cb7-882" aria-hidden="true" tabindex="-1"></a><span class="co">        C: 输出矩阵 [d_state]</span></span>
<span id="cb7-883"><a href="#cb7-883" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: 输入序列 [seq_len]</span></span>
<span id="cb7-884"><a href="#cb7-884" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-885"><a href="#cb7-885" aria-hidden="true" tabindex="-1"></a>    seq_len <span class="op">=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb7-886"><a href="#cb7-886" aria-hidden="true" tabindex="-1"></a>    d_state <span class="op">=</span> <span class="bu">len</span>(A_diag)</span>
<span id="cb7-887"><a href="#cb7-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-888"><a href="#cb7-888" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化</span></span>
<span id="cb7-889"><a href="#cb7-889" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> np.zeros((seq_len <span class="op">+</span> <span class="dv">1</span>, d_state))</span>
<span id="cb7-890"><a href="#cb7-890" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.zeros(seq_len)</span>
<span id="cb7-891"><a href="#cb7-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-892"><a href="#cb7-892" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 模拟SSM</span></span>
<span id="cb7-893"><a href="#cb7-893" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 固定步长</span></span>
<span id="cb7-894"><a href="#cb7-894" aria-hidden="true" tabindex="-1"></a>    A_bar <span class="op">=</span> np.exp(delta <span class="op">*</span> A_diag)</span>
<span id="cb7-895"><a href="#cb7-895" aria-hidden="true" tabindex="-1"></a>    B_bar <span class="op">=</span> delta <span class="op">*</span> B</span>
<span id="cb7-896"><a href="#cb7-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-897"><a href="#cb7-897" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-898"><a href="#cb7-898" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 状态更新</span></span>
<span id="cb7-899"><a href="#cb7-899" aria-hidden="true" tabindex="-1"></a>        states[t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> A_bar <span class="op">*</span> states[t] <span class="op">+</span> B_bar <span class="op">*</span> inputs[t]</span>
<span id="cb7-900"><a href="#cb7-900" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出</span></span>
<span id="cb7-901"><a href="#cb7-901" aria-hidden="true" tabindex="-1"></a>        outputs[t] <span class="op">=</span> np.dot(C, states[t <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb7-902"><a href="#cb7-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-903"><a href="#cb7-903" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 绘图</span></span>
<span id="cb7-904"><a href="#cb7-904" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb7-905"><a href="#cb7-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-906"><a href="#cb7-906" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输入序列</span></span>
<span id="cb7-907"><a href="#cb7-907" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].stem(<span class="bu">range</span>(seq_len), inputs, basefmt<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb7-908"><a href="#cb7-908" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_title(<span class="st">"Input Sequence"</span>)</span>
<span id="cb7-909"><a href="#cb7-909" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_xlabel(<span class="st">"Time step"</span>)</span>
<span id="cb7-910"><a href="#cb7-910" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Input value"</span>)</span>
<span id="cb7-911"><a href="#cb7-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-912"><a href="#cb7-912" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 状态演化（热力图）</span></span>
<span id="cb7-913"><a href="#cb7-913" aria-hidden="true" tabindex="-1"></a>    im <span class="op">=</span> axes[<span class="dv">1</span>].imshow(states[<span class="dv">1</span>:].T, aspect<span class="op">=</span><span class="st">'auto'</span>, cmap<span class="op">=</span><span class="st">'RdBu_r'</span>)</span>
<span id="cb7-914"><a href="#cb7-914" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_title(<span class="st">"State Evolution (each row = one state dimension)"</span>)</span>
<span id="cb7-915"><a href="#cb7-915" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_xlabel(<span class="st">"Time step"</span>)</span>
<span id="cb7-916"><a href="#cb7-916" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_ylabel(<span class="st">"State dimension"</span>)</span>
<span id="cb7-917"><a href="#cb7-917" aria-hidden="true" tabindex="-1"></a>    plt.colorbar(im, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb7-918"><a href="#cb7-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-919"><a href="#cb7-919" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 输出序列</span></span>
<span id="cb7-920"><a href="#cb7-920" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].plot(outputs, <span class="st">'g-o'</span>, markersize<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb7-921"><a href="#cb7-921" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_title(<span class="st">"Output Sequence"</span>)</span>
<span id="cb7-922"><a href="#cb7-922" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_xlabel(<span class="st">"Time step"</span>)</span>
<span id="cb7-923"><a href="#cb7-923" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_ylabel(<span class="st">"Output value"</span>)</span>
<span id="cb7-924"><a href="#cb7-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-925"><a href="#cb7-925" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(title, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-926"><a href="#cb7-926" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb7-927"><a href="#cb7-927" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="st">"ssm_state_evolution.png"</span>, dpi<span class="op">=</span><span class="dv">150</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb7-928"><a href="#cb7-928" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb7-929"><a href="#cb7-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-930"><a href="#cb7-930" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例：观察SSM如何记住脉冲输入</span></span>
<span id="cb7-931"><a href="#cb7-931" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-932"><a href="#cb7-932" aria-hidden="true" tabindex="-1"></a>d_state <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb7-933"><a href="#cb7-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-934"><a href="#cb7-934" aria-hidden="true" tabindex="-1"></a><span class="co"># HiPPO-like初始化：不同的衰减率</span></span>
<span id="cb7-935"><a href="#cb7-935" aria-hidden="true" tabindex="-1"></a>A_diag <span class="op">=</span> <span class="op">-</span>np.arange(<span class="dv">1</span>, d_state <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.5</span>  <span class="co"># 负值，越高维度衰减越快</span></span>
<span id="cb7-936"><a href="#cb7-936" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.ones(d_state) <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb7-937"><a href="#cb7-937" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.random.randn(d_state) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb7-938"><a href="#cb7-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-939"><a href="#cb7-939" aria-hidden="true" tabindex="-1"></a><span class="co"># 输入：一个脉冲 + 一些噪声</span></span>
<span id="cb7-940"><a href="#cb7-940" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.zeros(<span class="dv">50</span>)</span>
<span id="cb7-941"><a href="#cb7-941" aria-hidden="true" tabindex="-1"></a>inputs[<span class="dv">5</span>] <span class="op">=</span> <span class="fl">1.0</span>   <span class="co"># 脉冲1</span></span>
<span id="cb7-942"><a href="#cb7-942" aria-hidden="true" tabindex="-1"></a>inputs[<span class="dv">25</span>] <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># 脉冲2</span></span>
<span id="cb7-943"><a href="#cb7-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-944"><a href="#cb7-944" aria-hidden="true" tabindex="-1"></a>visualize_ssm_state(A_diag, B, C, inputs, <span class="st">"SSM Response to Impulse Inputs"</span>)</span>
<span id="cb7-945"><a href="#cb7-945" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-946"><a href="#cb7-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-947"><a href="#cb7-947" aria-hidden="true" tabindex="-1"></a>这个可视化展示了几个关键点：</span>
<span id="cb7-948"><a href="#cb7-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-949"><a href="#cb7-949" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**不同状态维度的衰减速率不同**：低维度（小的$|A|$）衰减慢，保留长期记忆</span>
<span id="cb7-950"><a href="#cb7-950" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**脉冲响应**：输入的脉冲会在状态中留下"痕迹"，逐渐衰减</span>
<span id="cb7-951"><a href="#cb7-951" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**输出是状态的线性组合**：$C$矩阵决定了如何从状态中"读取"信息</span>
<span id="cb7-952"><a href="#cb7-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-953"><a href="#cb7-953" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-954"><a href="#cb7-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-955"><a href="#cb7-955" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb7-956"><a href="#cb7-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-957"><a href="#cb7-957" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb7-958"><a href="#cb7-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-959"><a href="#cb7-959" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SSM的表达能力</span></span>
<span id="cb7-960"><a href="#cb7-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-961"><a href="#cb7-961" aria-hidden="true" tabindex="-1"></a>从理论上，SSM可以被理解为一类特殊的**线性递归神经网络（Linear RNN）**。线性RNN的表达能力受限于其线性特性——它只能表示线性函数。</span>
<span id="cb7-962"><a href="#cb7-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-963"><a href="#cb7-963" aria-hidden="true" tabindex="-1"></a>然而，SSM通过以下机制获得了更强的表达能力：</span>
<span id="cb7-964"><a href="#cb7-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-965"><a href="#cb7-965" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**非线性激活**：SSM层后通常跟着非线性激活</span>
<span id="cb7-966"><a href="#cb7-966" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**多层堆叠**：深层SSM可以逼近复杂的非线性映射</span>
<span id="cb7-967"><a href="#cb7-967" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**选择性机制**：Mamba的输入依赖参数引入了数据依赖的非线性</span>
<span id="cb7-968"><a href="#cb7-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-969"><a href="#cb7-969" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 与Attention的理论联系</span></span>
<span id="cb7-970"><a href="#cb7-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-971"><a href="#cb7-971" aria-hidden="true" tabindex="-1"></a>Mamba-2的SSD框架揭示了SSM和Attention之间的深层联系。从某种意义上说，SSM可以被看作是一种"结构化的Attention"——它使用特定形式的掩码矩阵（1-semiseparable），而不是任意学习的attention权重。</span>
<span id="cb7-972"><a href="#cb7-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-973"><a href="#cb7-973" aria-hidden="true" tabindex="-1"></a>这种结构化带来的限制是：SSM无法实现任意的长距离信息交互。它必须通过状态向量"传递"信息，而状态向量的容量是有限的。</span>
<span id="cb7-974"><a href="#cb7-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-975"><a href="#cb7-975" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——实证视角</span></span>
<span id="cb7-976"><a href="#cb7-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-977"><a href="#cb7-977" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 长程建模的证据</span></span>
<span id="cb7-978"><a href="#cb7-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-979"><a href="#cb7-979" aria-hidden="true" tabindex="-1"></a>S4在Long Range Arena基准测试上的表现证明了SSM可以有效建模长距离依赖：</span>
<span id="cb7-980"><a href="#cb7-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-981"><a href="#cb7-981" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> 序列长度 <span class="pp">|</span> S4 <span class="pp">|</span> Transformer <span class="pp">|</span></span>
<span id="cb7-982"><a href="#cb7-982" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|-----|-------------|</span></span>
<span id="cb7-983"><a href="#cb7-983" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ListOps <span class="pp">|</span> 2,048 <span class="pp">|</span> 59.6% <span class="pp">|</span> 36.4% <span class="pp">|</span></span>
<span id="cb7-984"><a href="#cb7-984" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Text <span class="pp">|</span> 4,096 <span class="pp">|</span> 86.8% <span class="pp">|</span> 64.3% <span class="pp">|</span></span>
<span id="cb7-985"><a href="#cb7-985" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Retrieval <span class="pp">|</span> 4,000 <span class="pp">|</span> 90.9% <span class="pp">|</span> 53.4% <span class="pp">|</span></span>
<span id="cb7-986"><a href="#cb7-986" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Image <span class="pp">|</span> 1,024 <span class="pp">|</span> 88.7% <span class="pp">|</span> 42.4% <span class="pp">|</span></span>
<span id="cb7-987"><a href="#cb7-987" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Path-X** | **16,384** | **88.1%** | **random** <span class="pp">|</span></span>
<span id="cb7-988"><a href="#cb7-988" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PathFinder <span class="pp">|</span> 1,024 <span class="pp">|</span> 94.2% <span class="pp">|</span> 71.4% <span class="pp">|</span></span>
<span id="cb7-989"><a href="#cb7-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-990"><a href="#cb7-990" aria-hidden="true" tabindex="-1"></a>Path-X任务特别值得关注：这是一个需要追踪16384长度序列中从起点到终点路径的任务。所有Transformer变体都失败了（等同于随机猜测），而S4成功解决了这个任务。</span>
<span id="cb7-991"><a href="#cb7-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-992"><a href="#cb7-992" aria-hidden="true" tabindex="-1"></a><span class="al">![S4在Long Range Arena基准测试上的性能对比。S4在所有任务上都显著优于之前的方法，特别是首次解决了Path-X任务（序列长度16384），而所有其他模型（包括各种Transformer变体）在该任务上都等同于随机猜测。](figures/chapter-28/original/fig-s4-lra-results.png)</span>{#fig-s4-lra width=90%}</span>
<span id="cb7-993"><a href="#cb7-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-994"><a href="#cb7-994" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-995"><a href="#cb7-995" aria-hidden="true" tabindex="-1"></a>*Source: Gu et al. (2021) "Efficiently Modeling Long Sequences with Structured State Spaces", Figure 2*</span>
<span id="cb7-996"><a href="#cb7-996" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-997"><a href="#cb7-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-998"><a href="#cb7-998" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mamba的语言建模实验</span></span>
<span id="cb7-999"><a href="#cb7-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1000"><a href="#cb7-1000" aria-hidden="true" tabindex="-1"></a>Mamba在语言建模上的成功表明，SSM不仅能处理"人工"的长程任务，还能在真实的NLP数据上与Transformer竞争：</span>
<span id="cb7-1001"><a href="#cb7-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1002"><a href="#cb7-1002" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mamba-130M** vs **Transformer-130M**：困惑度更低</span>
<span id="cb7-1003"><a href="#cb7-1003" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mamba-1.4B** vs **Transformer-1.4B**：困惑度相当</span>
<span id="cb7-1004"><a href="#cb7-1004" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mamba-3B** vs **Transformer-6B**：下游任务性能相当</span>
<span id="cb7-1005"><a href="#cb7-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1006"><a href="#cb7-1006" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb7-1007"><a href="#cb7-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1008"><a href="#cb7-1008" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 什么任务SSM表现不好？</span></span>
<span id="cb7-1009"><a href="#cb7-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1010"><a href="#cb7-1010" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**需要精确检索的任务**：当任务需要从长序列中精确定位并提取特定信息时，有限容量的状态向量可能不够</span>
<span id="cb7-1011"><a href="#cb7-1011" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**复杂推理任务**：多步推理可能需要更灵活的信息交互模式</span>
<span id="cb7-1012"><a href="#cb7-1012" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**短序列任务**：当序列很短时，$O(n^2)$的开销不是问题，Attention的全局交互可能更有优势</span>
<span id="cb7-1013"><a href="#cb7-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1014"><a href="#cb7-1014" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 计算效率的边界</span></span>
<span id="cb7-1015"><a href="#cb7-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1016"><a href="#cb7-1016" aria-hidden="true" tabindex="-1"></a>尽管SSM理论上是$O(n)$，但实际效率取决于多个因素：</span>
<span id="cb7-1017"><a href="#cb7-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1018"><a href="#cb7-1018" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**状态维度$N$**：更大的$N$提供更多容量，但计算量也更大</span>
<span id="cb7-1019"><a href="#cb7-1019" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**硬件利用率**：Mamba的并行扫描在某些硬件上不如矩阵乘法高效</span>
<span id="cb7-1020"><a href="#cb7-1020" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**内存带宽**：SSM的状态更新是memory-bound的，可能受限于内存带宽</span>
<span id="cb7-1021"><a href="#cb7-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1022"><a href="#cb7-1022" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb7-1023"><a href="#cb7-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1024"><a href="#cb7-1024" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**SSM vs Attention的本质差异**：它们分别擅长什么？有没有任务类型的理论划分？</span>
<span id="cb7-1025"><a href="#cb7-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1026"><a href="#cb7-1026" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**最优的混合比例**：Attention和SSM应该如何组合？1:7的比例是最优的吗？</span>
<span id="cb7-1027"><a href="#cb7-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1028"><a href="#cb7-1028" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**更强的选择性机制**：Mamba的选择性机制够强吗？有没有更好的输入依赖设计？</span>
<span id="cb7-1029"><a href="#cb7-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1030"><a href="#cb7-1030" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Scaling Law**：SSM的scaling behavior与Transformer有何不同？更大规模时谁更有优势？</span>
<span id="cb7-1031"><a href="#cb7-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1032"><a href="#cb7-1032" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**多模态SSM**：SSM能否像Transformer一样成为多模态的通用架构？</span>
<span id="cb7-1033"><a href="#cb7-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1034"><a href="#cb7-1034" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-1035"><a href="#cb7-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1036"><a href="#cb7-1036" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb7-1037"><a href="#cb7-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1038"><a href="#cb7-1038" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本方法的局限</span></span>
<span id="cb7-1039"><a href="#cb7-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1040"><a href="#cb7-1040" aria-hidden="true" tabindex="-1"></a>**状态容量的限制**</span>
<span id="cb7-1041"><a href="#cb7-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1042"><a href="#cb7-1042" aria-hidden="true" tabindex="-1"></a>SSM的核心是用一个固定维度的状态向量来压缩所有历史信息。当需要记住的信息量超过状态容量时，必然会发生信息损失。</span>
<span id="cb7-1043"><a href="#cb7-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1044"><a href="#cb7-1044" aria-hidden="true" tabindex="-1"></a>相比之下，Transformer的KV cache可以存储完整的历史信息（尽管代价是$O(n)$的内存增长）。</span>
<span id="cb7-1045"><a href="#cb7-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1046"><a href="#cb7-1046" aria-hidden="true" tabindex="-1"></a>**并行性的权衡**</span>
<span id="cb7-1047"><a href="#cb7-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1048"><a href="#cb7-1048" aria-hidden="true" tabindex="-1"></a>Mamba的并行扫描算法虽然高效，但仍然不如矩阵乘法那样完美地映射到GPU的tensor core上。在短到中等长度的序列上，Transformer可能实际上更快。</span>
<span id="cb7-1049"><a href="#cb7-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1050"><a href="#cb7-1050" aria-hidden="true" tabindex="-1"></a>**生态系统的成熟度**</span>
<span id="cb7-1051"><a href="#cb7-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1052"><a href="#cb7-1052" aria-hidden="true" tabindex="-1"></a>Transformer有多年的工程优化积累（FlashAttention、vLLM、TensorRT等），而SSM的工具链相对不成熟。这影响了SSM在生产环境中的部署。</span>
<span id="cb7-1053"><a href="#cb7-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1054"><a href="#cb7-1054" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb7-1055"><a href="#cb7-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1056"><a href="#cb7-1056" aria-hidden="true" tabindex="-1"></a>SSM和Transformer各有所长，这促使我们思考：</span>
<span id="cb7-1057"><a href="#cb7-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1058"><a href="#cb7-1058" aria-hidden="true" tabindex="-1"></a>**未来的序列模型会是什么样的？**</span>
<span id="cb7-1059"><a href="#cb7-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1060"><a href="#cb7-1060" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>纯Transformer？</span>
<span id="cb7-1061"><a href="#cb7-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>纯SSM？</span>
<span id="cb7-1062"><a href="#cb7-1062" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>还是像Jamba那样的混合架构？</span>
<span id="cb7-1063"><a href="#cb7-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1064"><a href="#cb7-1064" aria-hidden="true" tabindex="-1"></a>目前的证据表明，**混合架构可能是更有前景的方向**——结合Attention的全局推理能力和SSM的高效长程建模能力。</span>
<span id="cb7-1065"><a href="#cb7-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1066"><a href="#cb7-1066" aria-hidden="true" tabindex="-1"></a>**是否存在更统一的框架？**</span>
<span id="cb7-1067"><a href="#cb7-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1068"><a href="#cb7-1068" aria-hidden="true" tabindex="-1"></a>Mamba-2的SSD理论表明，SSM和Attention可能只是更一般的序列变换的两种特例。是否存在一个统一的框架，让我们能够在连续的谱系上设计序列模型？</span>
<span id="cb7-1069"><a href="#cb7-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1070"><a href="#cb7-1070" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-1071"><a href="#cb7-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1072"><a href="#cb7-1072" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb7-1073"><a href="#cb7-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1074"><a href="#cb7-1074" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb7-1075"><a href="#cb7-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1076"><a href="#cb7-1076" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：Transformer的$O(n^2)$注意力复杂度在长序列上成为瓶颈。我们需要既能捕获长距离依赖、又有线性复杂度的序列模型。</span>
<span id="cb7-1077"><a href="#cb7-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1078"><a href="#cb7-1078" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**洞察**：借鉴控制论中的状态空间模型，通过精心设计的状态转移矩阵（HiPPO初始化），可以实现长程记忆。关键创新是Mamba的选择性机制——让参数依赖于输入，弥补了LTI模型的根本缺陷。</span>
<span id="cb7-1079"><a href="#cb7-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1080"><a href="#cb7-1080" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**方法**：</span>
<span id="cb7-1081"><a href="#cb7-1081" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>SSM将连续时间系统离散化为可训练的递归网络</span>
<span id="cb7-1082"><a href="#cb7-1082" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>利用卷积/递归对偶性：训练时用卷积（可并行），推理时用递归（高效）</span>
<span id="cb7-1083"><a href="#cb7-1083" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>HiPPO初始化保证长程记忆</span>
<span id="cb7-1084"><a href="#cb7-1084" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>选择性机制让模型能够根据内容决定"何时记忆、何时忽略"</span>
<span id="cb7-1085"><a href="#cb7-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1086"><a href="#cb7-1086" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**意义**：SSM为超越Transformer提供了一条可行路径。Mamba证明了在语言建模等核心任务上，线性复杂度的模型可以与Transformer竞争。混合架构（如Jamba）展示了结合两种范式优势的可能性。</span>
<span id="cb7-1087"><a href="#cb7-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1088"><a href="#cb7-1088" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb7-1089"><a href="#cb7-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1090"><a href="#cb7-1090" aria-hidden="true" tabindex="-1"></a>**连续状态空间方程**：</span>
<span id="cb7-1091"><a href="#cb7-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1092"><a href="#cb7-1092" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1093"><a href="#cb7-1093" aria-hidden="true" tabindex="-1"></a>\frac{dx(t)}{dt} = Ax(t) + Bu(t), \quad y(t) = Cx(t)</span>
<span id="cb7-1094"><a href="#cb7-1094" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1095"><a href="#cb7-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1096"><a href="#cb7-1096" aria-hidden="true" tabindex="-1"></a>**离散化（ZOH）**：</span>
<span id="cb7-1097"><a href="#cb7-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1098"><a href="#cb7-1098" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1099"><a href="#cb7-1099" aria-hidden="true" tabindex="-1"></a>\bar{A} = \exp(\Delta A), \quad \bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B</span>
<span id="cb7-1100"><a href="#cb7-1100" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1101"><a href="#cb7-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1102"><a href="#cb7-1102" aria-hidden="true" tabindex="-1"></a>**离散递归**：</span>
<span id="cb7-1103"><a href="#cb7-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1104"><a href="#cb7-1104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1105"><a href="#cb7-1105" aria-hidden="true" tabindex="-1"></a>x_k = \bar{A} x_{k-1} + \bar{B} u_k, \quad y_k = C x_k</span>
<span id="cb7-1106"><a href="#cb7-1106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1107"><a href="#cb7-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1108"><a href="#cb7-1108" aria-hidden="true" tabindex="-1"></a>**卷积核**：</span>
<span id="cb7-1109"><a href="#cb7-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1110"><a href="#cb7-1110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1111"><a href="#cb7-1111" aria-hidden="true" tabindex="-1"></a>\bar{K} = (C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B}, \ldots), \quad y = \bar{K} * u</span>
<span id="cb7-1112"><a href="#cb7-1112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1113"><a href="#cb7-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1114"><a href="#cb7-1114" aria-hidden="true" tabindex="-1"></a>**选择性参数化（Mamba）**：</span>
<span id="cb7-1115"><a href="#cb7-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1116"><a href="#cb7-1116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1117"><a href="#cb7-1117" aria-hidden="true" tabindex="-1"></a>B_t = f_B(x_t), \quad C_t = f_C(x_t), \quad \Delta_t = \text{softplus}(f_\Delta(x_t))</span>
<span id="cb7-1118"><a href="#cb7-1118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-1119"><a href="#cb7-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1120"><a href="#cb7-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb7-1121"><a href="#cb7-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1122"><a href="#cb7-1122" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么HiPPO初始化能够帮助SSM建模长距离依赖？如果用随机初始化的$A$矩阵，会发生什么？</span>
<span id="cb7-1123"><a href="#cb7-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1124"><a href="#cb7-1124" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明：当$A = -aI$（$a &gt; 0$）时，离散化后的$\bar{A} = e^{-a\Delta}I$。这说明状态会以什么速率"遗忘"？$\Delta$大小如何影响遗忘速率？</span>
<span id="cb7-1125"><a href="#cb7-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1126"><a href="#cb7-1126" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用Hugging Face的<span class="in">`transformers`</span>库加载一个Mamba模型（如<span class="in">`state-spaces/mamba-130m`</span>），在相同的文本上比较它与同规模Transformer的推理速度和困惑度。</span>
<span id="cb7-1127"><a href="#cb7-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1128"><a href="#cb7-1128" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** Jamba使用1<span class="sc">:7的Attention:</span>Mamba比例。你认为这个比例是否最优？如果让你设计一个混合架构，你会如何决定在哪些层使用Attention、在哪些层使用SSM？</span>
<span id="cb7-1129"><a href="#cb7-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1130"><a href="#cb7-1130" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-1131"><a href="#cb7-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1132"><a href="#cb7-1132" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb7-1133"><a href="#cb7-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1134"><a href="#cb7-1134" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb7-1135"><a href="#cb7-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1136"><a href="#cb7-1136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gu et al. (2021)** "Efficiently Modeling Long Sequences with Structured State Spaces (S4)"</span>
<span id="cb7-1137"><a href="#cb7-1137" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3 (S4架构)、Section 4 (HiPPO初始化)</span>
<span id="cb7-1138"><a href="#cb7-1138" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>ICLR 2022 Outstanding Paper，现代SSM的奠基之作</span>
<span id="cb7-1139"><a href="#cb7-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1140"><a href="#cb7-1140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gu &amp; Dao (2023)** "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"</span>
<span id="cb7-1141"><a href="#cb7-1141" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3 (选择性机制)、Section 3.3 (硬件感知算法)</span>
<span id="cb7-1142"><a href="#cb7-1142" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>将SSM带入语言建模主流</span>
<span id="cb7-1143"><a href="#cb7-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1144"><a href="#cb7-1144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dao &amp; Gu (2024)** "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality"</span>
<span id="cb7-1145"><a href="#cb7-1145" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2 (SSD理论)</span>
<span id="cb7-1146"><a href="#cb7-1146" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>揭示SSM与Attention的深层联系</span>
<span id="cb7-1147"><a href="#cb7-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1148"><a href="#cb7-1148" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb7-1149"><a href="#cb7-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1150"><a href="#cb7-1150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gu et al. (2020)** "HiPPO: Recurrent Memory with Optimal Polynomial Projections"</span>
<span id="cb7-1151"><a href="#cb7-1151" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>HiPPO初始化的数学原理</span>
<span id="cb7-1152"><a href="#cb7-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1153"><a href="#cb7-1153" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb7-1154"><a href="#cb7-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1155"><a href="#cb7-1155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lieber et al. (2024)** "Jamba: A Hybrid Transformer-Mamba Language Model"</span>
<span id="cb7-1156"><a href="#cb7-1156" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>第一个大规模混合架构</span>
<span id="cb7-1157"><a href="#cb7-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1158"><a href="#cb7-1158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AI21 (2024)** "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale"</span>
<span id="cb7-1159"><a href="#cb7-1159" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Jamba的规模化扩展</span>
<span id="cb7-1160"><a href="#cb7-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1161"><a href="#cb7-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb7-1162"><a href="#cb7-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1163"><a href="#cb7-1163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Goomba Lab Blog** "State Space Duality (Mamba-2)" — Mamba-2的详细解释</span>
<span id="cb7-1164"><a href="#cb7-1164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**The Annotated S4** (srush/annotated-s4) — S4的详细代码注释</span>
<span id="cb7-1165"><a href="#cb7-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1166"><a href="#cb7-1166" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb7-1167"><a href="#cb7-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1168"><a href="#cb7-1168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**state-spaces/mamba** — Mamba官方实现</span>
<span id="cb7-1169"><a href="#cb7-1169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**state-spaces/s4** — S4官方实现</span>
<span id="cb7-1170"><a href="#cb7-1170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face** — Mamba模型权重</span>
<span id="cb7-1171"><a href="#cb7-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1172"><a href="#cb7-1172" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-1173"><a href="#cb7-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1174"><a href="#cb7-1174" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb7-1175"><a href="#cb7-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1176"><a href="#cb7-1176" aria-hidden="true" tabindex="-1"></a>状态空间模型在深度学习中的应用是一个跨学科融合的典范。</span>
<span id="cb7-1177"><a href="#cb7-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1178"><a href="#cb7-1178" aria-hidden="true" tabindex="-1"></a>Albert Gu在Stanford攻读博士期间，师从Christopher Ré教授。Ré的实验室一直关注如何构建更高效的机器学习系统。Gu最初的研究方向是理解RNN为什么难以建模长距离依赖。</span>
<span id="cb7-1179"><a href="#cb7-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1180"><a href="#cb7-1180" aria-hidden="true" tabindex="-1"></a>在探索这个问题时，Gu发现了控制论中的状态空间模型。这是一个有几十年历史的理论框架，主要用于分析和设计控制系统（如自动驾驶汽车、机器人等）。控制论研究者早就知道，通过特殊设计的状态转移矩阵，可以让系统保持对久远历史的"记忆"。</span>
<span id="cb7-1181"><a href="#cb7-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1182"><a href="#cb7-1182" aria-hidden="true" tabindex="-1"></a>HiPPO框架正是这种洞察的数学实现。有趣的是，HiPPO的名字来源于它的核心思想：High-order Polynomial Projection Operators（高阶多项式投影算子）。这个名字也恰好与英文中的"河马"（hippo）同音——一种以记忆力强而闻名的动物。</span>
<span id="cb7-1183"><a href="#cb7-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1184"><a href="#cb7-1184" aria-hidden="true" tabindex="-1"></a>S4论文的发表引起了广泛关注，因为它首次证明了非Attention模型可以解决长程建模任务。但S4的一个关键限制是它是线性时不变的——这在处理自然语言时是一个严重的缺陷，因为语言理解本质上需要内容感知的推理。</span>
<span id="cb7-1185"><a href="#cb7-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1186"><a href="#cb7-1186" aria-hidden="true" tabindex="-1"></a>Mamba的关键洞察就是引入选择性机制来解决这个问题。论文的标题"Linear-Time Sequence Modeling with Selective State Spaces"强调了两个核心点：线性时间复杂度和选择性。这两者的结合使得Mamba成为第一个在语言建模上能够与Transformer竞争的SSM。</span>
<span id="cb7-1187"><a href="#cb7-1187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1188"><a href="#cb7-1188" aria-hidden="true" tabindex="-1"></a>Tri Dao是Mamba的另一位作者，他以FlashAttention闻名。有趣的是，同一个人既开发了让Attention更高效的技术（FlashAttention），也开发了Attention的替代方案（Mamba）。这反映了一种务实的研究态度：不是"Attention vs SSM"的二选一，而是为不同场景提供最优的工具。</span>
<span id="cb7-1189"><a href="#cb7-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1190"><a href="#cb7-1190" aria-hidden="true" tabindex="-1"></a>2024年，混合架构的兴起印证了这种务实态度。Jamba、Zamba等模型表明，未来的序列模型可能不是纯Transformer或纯SSM，而是两者的有机结合。</span>
<span id="cb7-1191"><a href="#cb7-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-1192"><a href="#cb7-1192" aria-hidden="true" tabindex="-1"></a>从HiPPO到S4到Mamba到混合架构，这条研究路径展示了一个重要的方法论启示：**有时候，解决深度学习问题的最好方法是从其他领域借鉴成熟的数学工具**。控制论、信号处理、动力系统——这些"传统"领域积累了大量关于序列和时间的知识，等待着被重新发现和应用。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>