<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="原始Transformer的512 token上下文窗口曾经足够，但大语言模型时代催生了对超长上下文的需求——从代码库理解到长文档摘要，从多轮对话到RAG检索。本章讲述突破上下文长度瓶颈的完整技术栈：位置编码从绝对到相对再到RoPE/ALiBi的演进、Position Interpolation到YaRN的长度外推技术、FlashAttention对注意力计算的IO感知革命、KV Cache的PagedAttention内存管理，以及Ring Attention的分布式长序列方案。每一项技术都是对O(n²)瓶颈的不同维度的突围。">

<title>第26章：长上下文与高效推理 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#on2-的诅咒" id="toc-on2-的诅咒" class="nav-link" data-scroll-target="#on2-的诅咒"><span class="header-section-number">2.1</span> <span class="math inline">\(O(n^2)\)</span> 的诅咒</a></li>
  <li><a href="#位置编码的困境" id="toc-位置编码的困境" class="nav-link" data-scroll-target="#位置编码的困境"><span class="header-section-number">2.2</span> 位置编码的困境</a></li>
  <li><a href="#推理时的-kv-cache-膨胀" id="toc-推理时的-kv-cache-膨胀" class="nav-link" data-scroll-target="#推理时的-kv-cache-膨胀"><span class="header-section-number">2.3</span> 推理时的 KV Cache 膨胀</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.4</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#位置编码的演进" id="toc-位置编码的演进" class="nav-link" data-scroll-target="#位置编码的演进"><span class="header-section-number">3</span> 位置编码的演进</a>
  <ul class="collapse">
  <li><a href="#绝对位置编码的局限" id="toc-绝对位置编码的局限" class="nav-link" data-scroll-target="#绝对位置编码的局限"><span class="header-section-number">3.1</span> 绝对位置编码的局限</a></li>
  <li><a href="#相对位置编码的尝试" id="toc-相对位置编码的尝试" class="nav-link" data-scroll-target="#相对位置编码的尝试"><span class="header-section-number">3.2</span> 相对位置编码的尝试</a></li>
  <li><a href="#rope旋转位置编码" id="toc-rope旋转位置编码" class="nav-link" data-scroll-target="#rope旋转位置编码"><span class="header-section-number">3.3</span> RoPE：旋转位置编码</a></li>
  <li><a href="#alibi更简单的替代方案" id="toc-alibi更简单的替代方案" class="nav-link" data-scroll-target="#alibi更简单的替代方案"><span class="header-section-number">3.4</span> ALiBi：更简单的替代方案</a></li>
  <li><a href="#rope-vs-alibi两种哲学" id="toc-rope-vs-alibi两种哲学" class="nav-link" data-scroll-target="#rope-vs-alibi两种哲学"><span class="header-section-number">3.5</span> RoPE vs ALiBi：两种哲学</a></li>
  </ul></li>
  <li><a href="#长度外推技术" id="toc-长度外推技术" class="nav-link" data-scroll-target="#长度外推技术"><span class="header-section-number">4</span> 长度外推技术</a>
  <ul class="collapse">
  <li><a href="#position-interpolation线性缩放" id="toc-position-interpolation线性缩放" class="nav-link" data-scroll-target="#position-interpolation线性缩放"><span class="header-section-number">4.1</span> Position Interpolation：线性缩放</a></li>
  <li><a href="#ntk-aware-scaling保护高频信息" id="toc-ntk-aware-scaling保护高频信息" class="nav-link" data-scroll-target="#ntk-aware-scaling保护高频信息"><span class="header-section-number">4.2</span> NTK-aware Scaling：保护高频信息</a></li>
  <li><a href="#yarn统一框架" id="toc-yarn统一框架" class="nav-link" data-scroll-target="#yarn统一框架"><span class="header-section-number">4.3</span> YaRN：统一框架</a></li>
  <li><a href="#外推技术对比" id="toc-外推技术对比" class="nav-link" data-scroll-target="#外推技术对比"><span class="header-section-number">4.4</span> 外推技术对比</a></li>
  </ul></li>
  <li><a href="#flash-attentionio-感知的算法革命" id="toc-flash-attentionio-感知的算法革命" class="nav-link" data-scroll-target="#flash-attentionio-感知的算法革命"><span class="header-section-number">5</span> Flash Attention：IO 感知的算法革命</a>
  <ul class="collapse">
  <li><a href="#gpu-内存层次结构" id="toc-gpu-内存层次结构" class="nav-link" data-scroll-target="#gpu-内存层次结构"><span class="header-section-number">5.1</span> GPU 内存层次结构</a></li>
  <li><a href="#flashattention-的-tiling-策略" id="toc-flashattention-的-tiling-策略" class="nav-link" data-scroll-target="#flashattention-的-tiling-策略"><span class="header-section-number">5.2</span> FlashAttention 的 Tiling 策略</a></li>
  <li><a href="#flashattention-2更好的并行" id="toc-flashattention-2更好的并行" class="nav-link" data-scroll-target="#flashattention-2更好的并行"><span class="header-section-number">5.3</span> FlashAttention-2：更好的并行</a></li>
  <li><a href="#flash-attention-的影响" id="toc-flash-attention-的影响" class="nav-link" data-scroll-target="#flash-attention-的影响"><span class="header-section-number">5.4</span> Flash Attention 的影响</a></li>
  </ul></li>
  <li><a href="#kv-cache-优化" id="toc-kv-cache-优化" class="nav-link" data-scroll-target="#kv-cache-优化"><span class="header-section-number">6</span> KV Cache 优化</a>
  <ul class="collapse">
  <li><a href="#kv-cache-的必要性" id="toc-kv-cache-的必要性" class="nav-link" data-scroll-target="#kv-cache-的必要性"><span class="header-section-number">6.1</span> KV Cache 的必要性</a></li>
  <li><a href="#kv-cache-的内存挑战" id="toc-kv-cache-的内存挑战" class="nav-link" data-scroll-target="#kv-cache-的内存挑战"><span class="header-section-number">6.2</span> KV Cache 的内存挑战</a></li>
  <li><a href="#multi-query-attention-mqa-和-grouped-query-attention-gqa" id="toc-multi-query-attention-mqa-和-grouped-query-attention-gqa" class="nav-link" data-scroll-target="#multi-query-attention-mqa-和-grouped-query-attention-gqa"><span class="header-section-number">6.3</span> Multi-Query Attention (MQA) 和 Grouped-Query Attention (GQA)</a></li>
  <li><a href="#pagedattention虚拟内存思想的应用" id="toc-pagedattention虚拟内存思想的应用" class="nav-link" data-scroll-target="#pagedattention虚拟内存思想的应用"><span class="header-section-number">6.4</span> PagedAttention：虚拟内存思想的应用</a></li>
  </ul></li>
  <li><a href="#ring-attention分布式长序列" id="toc-ring-attention分布式长序列" class="nav-link" data-scroll-target="#ring-attention分布式长序列"><span class="header-section-number">7</span> Ring Attention：分布式长序列</a></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">8</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么-rope-有外推潜力" id="toc-为什么-rope-有外推潜力" class="nav-link" data-scroll-target="#为什么-rope-有外推潜力"><span class="header-section-number">8.1</span> 为什么 RoPE 有外推潜力？</a></li>
  <li><a href="#flashattention-的复杂度分析" id="toc-flashattention-的复杂度分析" class="nav-link" data-scroll-target="#flashattention-的复杂度分析"><span class="header-section-number">8.2</span> FlashAttention 的复杂度分析</a></li>
  <li><a href="#长上下文的失效模式" id="toc-长上下文的失效模式" class="nav-link" data-scroll-target="#长上下文的失效模式"><span class="header-section-number">8.3</span> 长上下文的失效模式</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">8.4</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#当前技术的局限" id="toc-当前技术的局限" class="nav-link" data-scroll-target="#当前技术的局限"><span class="header-section-number">9.1</span> 当前技术的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">9.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">10</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">10.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">10.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">11</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</a></li>
  <li><a href="#外推技术" id="toc-外推技术" class="nav-link" data-scroll-target="#外推技术"><span class="header-section-number">11.2</span> 外推技术</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">11.3</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">11.4</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">12</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第26章：长上下文与高效推理</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">From 512 Tokens to Millions: The Quest for Infinite Context</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">长上下文</div>
    <div class="quarto-category">高效推理</div>
    <div class="quarto-category">位置编码</div>
  </div>
  </div>

<div>
  <div class="description">
    原始Transformer的512 token上下文窗口曾经足够，但大语言模型时代催生了对超长上下文的需求——从代码库理解到长文档摘要，从多轮对话到RAG检索。本章讲述突破上下文长度瓶颈的完整技术栈：位置编码从绝对到相对再到RoPE/ALiBi的演进、Position Interpolation到YaRN的长度外推技术、FlashAttention对注意力计算的IO感知革命、KV Cache的PagedAttention内存管理，以及Ring Attention的分布式长序列方案。每一项技术都是对O(n²)瓶颈的不同维度的突围。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何让大语言模型支持超长序列？从512 token到百万token的跃迁需要哪些技术突破？</p>
<p><strong>历史坐标</strong>：2021–2024 | RoPE (2021) → ALiBi (2022) → FlashAttention (2022) → Position Interpolation (2023) → YaRN (2023) → PagedAttention (2023) | 从算法到系统的完整链条</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Su et al.&nbsp;(2021)</strong> “RoFormer: Enhanced Transformer with Rotary Position Embedding” (<a href="https://arxiv.org/abs/2104.09864">arXiv:2104.09864</a>) — 参考了 Section 3 (RoPE 数学推导)、Figure 1-2 (旋转编码可视化)；核心位置编码方法</li>
<li><strong>Press et al.&nbsp;(2022)</strong> “Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation” (<a href="https://arxiv.org/abs/2108.12409">arXiv:2108.12409</a>) — 参考了 Section 2 (ALiBi 方法)、Figure 1-2 (外推实验)；ICLR 2022</li>
<li><strong>Dao et al.&nbsp;(2022)</strong> “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness” (<a href="https://arxiv.org/abs/2205.14135">arXiv:2205.14135</a>) — 参考了 Algorithm 1 (FlashAttention 伪代码)、Figure 1 (Tiling 示意图)；NeurIPS 2022</li>
<li><strong>Dao (2023)</strong> “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning” (<a href="https://arxiv.org/abs/2307.08691">arXiv:2307.08691</a>) — 参考了 Section 3 (优化策略)；ICLR 2024</li>
<li><strong>Chen et al.&nbsp;(2023)</strong> “Extending Context Window of Large Language Models via Positional Interpolation” (<a href="https://arxiv.org/abs/2306.15595">arXiv:2306.15595</a>) — 参考了 Section 2 (PI 方法)、理论分析；EMNLP 2024</li>
<li><strong>Peng et al.&nbsp;(2023)</strong> “YaRN: Efficient Context Window Extension of Large Language Models” (<a href="https://arxiv.org/abs/2309.00071">arXiv:2309.00071</a>) — 参考了 Section 2-3 (NTK-aware 和 YaRN 方法)；ICLR 2024</li>
<li><strong>Kwon et al.&nbsp;(2023)</strong> “Efficient Memory Management for Large Language Model Serving with PagedAttention” (<a href="https://arxiv.org/abs/2309.06180">arXiv:2309.06180</a>) — 参考了 Section 3-4 (PagedAttention 设计)；SOSP 2023</li>
<li><strong>Ainslie et al.&nbsp;(2023)</strong> “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints” (<a href="https://arxiv.org/abs/2305.13245">arXiv:2305.13245</a>) — 参考了 Section 2-3 (GQA 方法)；EMNLP 2023</li>
<li><strong>Liu et al.&nbsp;(2023)</strong> “Ring Attention with Blockwise Transformers for Near-Infinite Context” (<a href="https://arxiv.org/abs/2310.01889">arXiv:2310.01889</a>) — 参考了 Section 2-3 (Ring Attention 算法)</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Chapter 11.6 (Self-Attention and Positional Encoding) — 参考了位置编码的基础讲解</li>
<li><strong>Understanding Deep Learning</strong> Chapter 12 (Transformers) — 参考了注意力机制的可视化</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Princeton COS 484</strong> (Spring 2024) — Tri Dao 讲授 FlashAttention 的课程材料</li>
<li><strong>Stanford CS224N</strong> Lecture 8-9 (Winter 2025) — 参考了 Transformer 和预训练的教学框架</li>
</ul>
</section>
</div>
</div>
</div>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们深入探讨了对齐技术的演进——从 RLHF 的三阶段流水线到 DPO 的单阶段直接优化，再到 ORPO、SimPO、KTO 等进一步简化工程复杂度的变体。这些技术让大语言模型从”文本补全器”转变为真正有用的 AI 助手，能够遵循指令、拒绝有害请求、提供有帮助的回答。对齐技术的平民化——DPO 让偏好对齐变成了”加几行代码”的事情——大大加速了开源社区的发展。</p>
<p>然而，对齐解决的是”模型行为与人类期望一致”的问题，它假设模型已经具备足够的能力。但能力本身受到一个根本性的物理约束：<strong>上下文窗口的长度</strong>。</p>
<p>原始 Transformer 的上下文窗口只有 512 个 token。BERT 继承了这个设计，GPT-2 扩展到 1024，GPT-3 达到 2048。这些长度在早期已经足够——一篇新闻文章、一段对话、一个代码片段通常都能装得下。但随着大语言模型从研究走向应用，512 到 2048 token 的限制越来越成为瓶颈。</p>
<p>想象一下这些场景：你想让模型阅读一份 50 页的合同并回答问题，但 50 页文本大约是 25000 个 token——远超任何早期模型的上下文限制。你想让模型理解一个代码库来帮你调试，但一个中等规模的项目可能有上百个文件、几十万行代码。你想与模型进行长时间的多轮对话，但累积的对话历史很快就会撑爆上下文窗口。你想用 RAG（检索增强生成）来让模型访问外部知识，但如果只能放入几个检索片段，模型能获取的信息就非常有限。</p>
<p><strong>上下文长度不只是一个工程限制，它直接决定了模型能做什么任务、做得多好</strong>。</p>
<p>扩展上下文窗口的挑战是多维的。最直接的瓶颈是<strong>计算复杂度</strong>：标准自注意力的时间和空间复杂度都是 <span class="math inline">\(O(n^2)\)</span>，其中 <span class="math inline">\(n\)</span> 是序列长度。将上下文从 2K 扩展到 128K 意味着计算量增加 4096 倍——这不是简单堆砌更多 GPU 能解决的问题。</p>
<p>第二个挑战是<strong>位置编码的外推</strong>。Transformer 需要知道 token 的位置信息，而大多数位置编码方案在训练时只见过有限长度的序列。当推理时遇到比训练长度更长的序列时，位置编码可能完全失效——模型对超出训练范围的位置”一无所知”。</p>
<p>第三个挑战是<strong>推理时的内存管理</strong>。在自回归生成时，每生成一个 token 都需要访问之前所有 token 的 key-value 缓存（KV cache）。对于 128K 上下文的 70B 模型，仅 KV cache 就可能占用超过 100GB 显存，这比模型参数本身还大。</p>
<p>这三个挑战分别对应本章要讨论的三类技术：<strong>位置编码的演进与外推</strong>解决”模型能否理解长距离位置关系”的问题；<strong>高效注意力算法</strong>解决 <span class="math inline">\(O(n^2)\)</span> 计算瓶颈；<strong>KV cache 优化</strong>解决推理时的内存管理。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：突破长上下文瓶颈需要算法和系统的协同设计。位置编码决定了模型”能否”处理长序列，高效注意力决定了”能否快速”处理，KV cache 管理决定了”能否在有限资源下”部署。这三者缺一不可——只优化其中一个维度而忽略其他，整体系统仍然无法工作。</p>
</blockquote>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="on2-的诅咒" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="on2-的诅咒"><span class="header-section-number">2.1</span> <span class="math inline">\(O(n^2)\)</span> 的诅咒</h3>
<p>让我们从最根本的瓶颈开始：自注意力的二次复杂度。</p>
<p>在标准 Transformer 中，自注意力的计算包括三步：首先用 <span class="math inline">\(Q\)</span> 和 <span class="math inline">\(K\)</span> 计算注意力分数矩阵 <span class="math inline">\(QK^\top\)</span>，这是一个 <span class="math inline">\(n \times n\)</span> 的矩阵；然后对每一行做 softmax 归一化；最后用归一化后的注意力权重与 <span class="math inline">\(V\)</span> 相乘得到输出。</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]</span></p>
<p>这里的关键是 <span class="math inline">\(QK^\top\)</span>：它需要计算 <span class="math inline">\(n \times n\)</span> 个点积，每个点积涉及 <span class="math inline">\(d_k\)</span> 维向量。总计算量是 <span class="math inline">\(O(n^2 d_k)\)</span>，空间复杂度也是 <span class="math inline">\(O(n^2)\)</span>（需要存储完整的注意力矩阵）。</p>
<p>当 <span class="math inline">\(n = 512\)</span> 时，<span class="math inline">\(n^2 = 262144\)</span>，完全可控。当 <span class="math inline">\(n = 8192\)</span> 时，<span class="math inline">\(n^2 = 67M\)</span>，开始吃力但仍然可行。当 <span class="math inline">\(n = 131072\)</span>（128K）时，<span class="math inline">\(n^2 = 17B\)</span>——单个注意力矩阵就有 170 亿个元素！</p>
<p>更糟糕的是，Transformer 通常有几十层，每层都有多个注意力头。一个 70B 参数的模型可能有 80 层、64 个头。如果每层的每个头都要计算并存储完整的 <span class="math inline">\(n \times n\)</span> 注意力矩阵，内存需求会变得完全不可承受。</p>
</section>
<section id="位置编码的困境" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="位置编码的困境"><span class="header-section-number">2.2</span> 位置编码的困境</h3>
<p>Transformer 的另一个根本问题是它对位置信息的处理。与 RNN 不同——RNN 天然地按顺序处理序列，位置信息隐含在计算顺序中——Transformer 的自注意力是”位置无关”的：如果我们打乱输入序列的顺序，只要位置编码也相应打乱，注意力计算的结果完全相同。</p>
<p>这意味着 Transformer 必须<strong>显式地</strong>将位置信息注入到模型中。原始 Transformer 使用的是正弦位置编码：</p>
<p><span class="math display">\[
\begin{aligned}
PE_{(pos, 2i)} &amp;= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
PE_{(pos, 2i+1)} &amp;= \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{aligned}
\]</span></p>
<p>这个设计有一个美妙的性质：对于任意固定的偏移量 <span class="math inline">\(k\)</span>，<span class="math inline">\(PE_{pos+k}\)</span> 可以表示为 <span class="math inline">\(PE_{pos}\)</span> 的线性变换。理论上，这应该让模型能够学习到相对位置关系。</p>
<p>但正弦编码有一个致命的问题：<strong>它在训练长度之外的表现是未定义的</strong>。如果模型只在 2048 长度的序列上训练，它对 <span class="math inline">\(pos = 3000\)</span> 或 <span class="math inline">\(pos = 10000\)</span> 的位置编码完全没有见过。虽然数学上我们可以计算出 <span class="math inline">\(\sin(3000/10000^{2i/d})\)</span> 的值，但模型的注意力权重是否能正确利用这些从未见过的位置编码值，完全没有保证。</p>
<p>实验表明，当序列长度超过训练长度时，模型的困惑度（perplexity）会急剧上升，甚至完全失效。这被称为<strong>长度外推失败</strong>（length extrapolation failure）。</p>
</section>
<section id="推理时的-kv-cache-膨胀" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="推理时的-kv-cache-膨胀"><span class="header-section-number">2.3</span> 推理时的 KV Cache 膨胀</h3>
<p>第三个瓶颈出现在推理阶段，特别是自回归生成时。</p>
<p>当模型生成第 <span class="math inline">\(t\)</span> 个 token 时，它需要计算当前 token 与之前所有 token 的注意力。这需要访问位置 <span class="math inline">\(1, 2, \ldots, t-1\)</span> 的 key 和 value 向量。如果每次都重新计算这些向量，计算量会随着生成长度二次增长。</p>
<p>标准的优化是<strong>KV cache</strong>：在生成每个 token 时，将该位置的 key 和 value 存储起来，后续 token 生成时直接复用。这将计算复杂度从 <span class="math inline">\(O(t^2)\)</span> 降到 <span class="math inline">\(O(t)\)</span>，但代价是需要存储越来越大的 cache。</p>
<p>对于一个 70B 模型（假设 80 层、64 个头、head dimension = 128），128K 上下文的 KV cache 大小为：</p>
<p><span class="math display">\[
\text{KV cache size} = 2 \times 80 \times 64 \times 128K \times 128 \times 2 \text{ bytes (BF16)} = 167 \text{ GB}
\]</span></p>
<p>这比模型参数本身（70B × 2 bytes = 140 GB）还要大！而且这只是单个请求的 KV cache。在生产环境中，一个 GPU 需要同时服务多个请求，KV cache 的内存管理成为关键瓶颈。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.4</span> 我们需要什么样的解决方案？</h3>
<p>综合以上分析，理想的长上下文解决方案需要在三个维度上同时突破。</p>
<p><strong>位置编码层面</strong>：我们需要一种位置编码方案，它不仅能编码位置信息，还能在训练长度之外保持有效——即具有良好的<strong>长度外推</strong>能力。更理想的情况是，位置编码本身就编码了相对位置关系，而不是绝对位置。</p>
<p><strong>计算效率层面</strong>：我们需要打破 <span class="math inline">\(O(n^2)\)</span> 的瓶颈。可能的方向包括：稀疏注意力（只计算部分位置对的注意力）、近似注意力（用某种近似算法降低复杂度）、或者更聪明的精确注意力实现（通过算法和硬件的协同设计降低实际运行时间）。</p>
<p><strong>内存管理层面</strong>：我们需要更高效的 KV cache 管理策略，能够在有限显存中支持更长的上下文和更多的并发请求。这可能涉及内存池化、分页管理、或者减少每个位置需要存储的信息量。</p>
<p>接下来的几节，我们将逐一介绍这三个维度的关键技术。</p>
</section>
</section>
<section id="位置编码的演进" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="位置编码的演进"><span class="header-section-number">3</span> 位置编码的演进</h2>
<p>位置编码的演进是一个从”绝对”到”相对”、从”加性”到”乘性”的过程。理解这个演进脉络，是理解现代长上下文模型的基础。</p>
<section id="绝对位置编码的局限" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="绝对位置编码的局限"><span class="header-section-number">3.1</span> 绝对位置编码的局限</h3>
<p>原始 Transformer 的正弦位置编码是<strong>绝对位置编码</strong>的代表——每个位置有一个固定的编码向量，直接加到 token embedding 上：</p>
<p><span class="math display">\[
\mathbf{x}_i' = \mathbf{x}_i + \mathbf{PE}_i
\]</span></p>
<p>这种设计的问题在于，模型需要<strong>隐式地</strong>从绝对位置中学习相对位置关系。当位置 3 需要关注位置 1 时，模型需要从 <span class="math inline">\(\mathbf{PE}_3\)</span> 和 <span class="math inline">\(\mathbf{PE}_1\)</span> 的某种组合中推断出”距离为 2”的信息。这个推断在训练范围内可能有效，但在训练范围外就完全失效了。</p>
<p>另一种绝对位置编码是<strong>可学习位置编码</strong>（learnable positional embeddings），这是 BERT 和 GPT 系列采用的方法。它为每个位置分配一个可学习的向量 <span class="math inline">\(\mathbf{E}_i\)</span>。这种方法更灵活——模型可以学到任意的位置模式——但问题更严重：如果训练长度是 512，位置 513 的编码根本不存在。</p>
</section>
<section id="相对位置编码的尝试" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="相对位置编码的尝试"><span class="header-section-number">3.2</span> 相对位置编码的尝试</h3>
<p>相对位置编码的核心思想是：<strong>位置信息应该编码位置之间的相对距离，而不是绝对位置</strong>。</p>
<p>Shaw et al.&nbsp;(2018) 提出的一种方案是在计算注意力时，为每个位置对 <span class="math inline">\((i, j)\)</span> 添加一个依赖于相对距离 <span class="math inline">\(i - j\)</span> 的偏置项：</p>
<p><span class="math display">\[
a_{ij} = \frac{(\mathbf{q}_i)^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{r}_{i-j}}{\sqrt{d_k}}
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{r}_{i-j}\)</span> 是相对位置 <span class="math inline">\(i-j\)</span> 对应的可学习向量。这种设计更直接地编码了相对位置信息，但引入了额外的参数和计算。</p>
<p>Transformer-XL 进一步发展了这个想法，提出了一种可以处理超长序列的相对位置编码方案。但这些早期的相对位置编码方案都有各自的问题：要么计算开销大，要么实现复杂，要么外推能力有限。</p>
</section>
<section id="rope旋转位置编码" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="rope旋转位置编码"><span class="header-section-number">3.3</span> RoPE：旋转位置编码</h3>
<p>2021 年，Su 等人提出的 <strong>RoPE（Rotary Position Embedding）</strong> 成为了位置编码的一个重要突破。RoPE 的核心洞察是：<strong>可以用旋转矩阵来编码位置信息，使得两个位置的内积自然地包含它们的相对位置</strong>。</p>
<div id="fig-rope-visualization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rope-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-26/original/fig-rope-visualization.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rope-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RoPE 的核心思想：将位置编码为旋转角度。不同维度使用不同的旋转频率（图中颜色深浅表示频率），低维度高频（捕捉短程关系），高维度低频（捕捉长程关系）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Su et al.&nbsp;(2021) “RoFormer: Enhanced Transformer with Rotary Position Embedding”, Figure 1-2. <a href="https://arxiv.org/abs/2104.09864">arXiv:2104.09864</a></em></p>
</div>
<p>让我们从一个简单的二维情况开始建立直觉。假设 query 向量 <span class="math inline">\(\mathbf{q}\)</span> 和 key 向量 <span class="math inline">\(\mathbf{k}\)</span> 都是二维的。RoPE 的做法是将它们各自旋转一个与位置相关的角度：</p>
<p><span class="math display">\[
\mathbf{q}_m' = \begin{pmatrix} \cos m\theta &amp; -\sin m\theta \\ \sin m\theta &amp; \cos m\theta \end{pmatrix} \mathbf{q}_m, \quad
\mathbf{k}_n' = \begin{pmatrix} \cos n\theta &amp; -\sin n\theta \\ \sin n\theta &amp; \cos n\theta \end{pmatrix} \mathbf{k}_n
\]</span></p>
<p>其中 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(n\)</span> 是 query 和 key 的位置，<span class="math inline">\(\theta\)</span> 是一个预设的基础角频率。</p>
<p>现在，当我们计算旋转后的内积时：</p>
<p><span class="math display">\[
\mathbf{q}_m'^{\top} \mathbf{k}_n' = \mathbf{q}_m^\top R(\theta)^{m \top} R(\theta)^n \mathbf{k}_n = \mathbf{q}_m^\top R(\theta)^{n-m} \mathbf{k}_n
\]</span></p>
<p>这里 <span class="math inline">\(R(\theta)\)</span> 是旋转矩阵，<span class="math inline">\(R(\theta)^\top = R(-\theta)\)</span>。关键的观察是：<strong>内积只依赖于相对位置 <span class="math inline">\(n - m\)</span></strong>！位置 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(n\)</span> 的绝对值消失了，只剩下它们的差。</p>
<p>这个性质让 RoPE 自然地具有了相对位置编码的特性，同时没有引入额外的参数或复杂的计算。</p>
<p>对于高维情况（比如 <span class="math inline">\(d = 128\)</span>），RoPE 将维度两两分组，每组使用不同的基础频率 <span class="math inline">\(\theta_i\)</span>：</p>
<p><span class="math display">\[
\theta_i = 10000^{-2i/d}, \quad i = 0, 1, \ldots, d/2 - 1
\]</span></p>
<p>低维度使用高频率（变化快），高维度使用低频率（变化慢）。这种多频率设计让模型能够同时捕捉短程和长程的位置关系。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: RoPE 应用于 Query/Key 向量 (Su et al., 2021)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rotary_pos_emb(q, k, cos, sin, position_ids):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    对 query 和 key 应用旋转位置编码</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        q, k: [batch, seq_len, num_heads, head_dim] - query 和 key 向量</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        cos, sin: [max_seq_len, head_dim] - 预计算的 cos/sin 值</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        position_ids: [batch, seq_len] - 每个 token 的位置</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        q_embed, k_embed: 应用 RoPE 后的 query 和 key</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 根据 position_ids 获取对应的 cos/sin 值</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    cos <span class="op">=</span> cos[position_ids]  <span class="co"># [batch, seq_len, head_dim]</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    sin <span class="op">=</span> sin[position_ids]  <span class="co"># [batch, seq_len, head_dim]</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 将 head_dim 分成两半</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    q1, q2 <span class="op">=</span> q[..., :head_dim<span class="op">//</span><span class="dv">2</span>], q[..., head_dim<span class="op">//</span><span class="dv">2</span>:]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    k1, k2 <span class="op">=</span> k[..., :head_dim<span class="op">//</span><span class="dv">2</span>], k[..., head_dim<span class="op">//</span><span class="dv">2</span>:]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 应用旋转变换: (x1, x2) -&gt; (x1*cos - x2*sin, x1*sin + x2*cos)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    q_embed <span class="op">=</span> torch.cat([q1<span class="op">*</span>cos <span class="op">-</span> q2<span class="op">*</span>sin, q1<span class="op">*</span>sin <span class="op">+</span> q2<span class="op">*</span>cos], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    k_embed <span class="op">=</span> torch.cat([k1<span class="op">*</span>cos <span class="op">-</span> k2<span class="op">*</span>sin, k1<span class="op">*</span>sin <span class="op">+</span> k2<span class="op">*</span>cos], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_embed, k_embed</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Su et al.&nbsp;(2021) “RoFormer: Enhanced Transformer with Rotary Position Embedding”, Section 3.4. <a href="https://arxiv.org/abs/2104.09864">arXiv:2104.09864</a></em></p>
</div>
</div>
<section id="完整数值示例rope-的旋转计算" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="完整数值示例rope-的旋转计算"><span class="header-section-number">3.3.1</span> 完整数值示例：RoPE 的旋转计算</h4>
<p>让我们用一个具体的数值例子来验证 RoPE 的核心性质：<strong>内积只依赖相对位置</strong>。</p>
<p><strong>设定</strong>：为了简化，我们使用 4 维向量（2 组，每组 2 维），基础频率 <span class="math inline">\(\theta_0 = 1.0\)</span>，<span class="math inline">\(\theta_1 = 0.01\)</span>。</p>
<p>假设： - Query 向量 <span class="math inline">\(\mathbf{q} = [1.0, 0.0, 1.0, 0.0]\)</span>，位于位置 <span class="math inline">\(m = 3\)</span> - Key 向量 <span class="math inline">\(\mathbf{k} = [0.5, 0.5, 0.5, 0.5]\)</span>，位于位置 <span class="math inline">\(n = 5\)</span></p>
<p><strong>Step 1: 计算旋转角度</strong></p>
<p>对于位置 <span class="math inline">\(m = 3\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
\text{角度}_0 &amp;= m \cdot \theta_0 = 3 \times 1.0 = 3.0 \text{ rad} \\
\text{角度}_1 &amp;= m \cdot \theta_1 = 3 \times 0.01 = 0.03 \text{ rad}
\end{aligned}
\]</span></p>
<p>对于位置 <span class="math inline">\(n = 5\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
\text{角度}_0 &amp;= n \cdot \theta_0 = 5 \times 1.0 = 5.0 \text{ rad} \\
\text{角度}_1 &amp;= n \cdot \theta_1 = 5 \times 0.01 = 0.05 \text{ rad}
\end{aligned}
\]</span></p>
<p><strong>Step 2: 计算 cos 和 sin 值</strong></p>
<p>位置 <span class="math inline">\(m = 3\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
\cos(3.0) &amp;\approx -0.990, \quad \sin(3.0) \approx 0.141 \\
\cos(0.03) &amp;\approx 1.000, \quad \sin(0.03) \approx 0.030
\end{aligned}
\]</span></p>
<p>位置 <span class="math inline">\(n = 5\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
\cos(5.0) &amp;\approx 0.284, \quad \sin(5.0) \approx -0.959 \\
\cos(0.05) &amp;\approx 0.999, \quad \sin(0.05) \approx 0.050
\end{aligned}
\]</span></p>
<p><strong>Step 3: 应用旋转变换</strong></p>
<p>对 query（位置 3），将维度分成两组 <span class="math inline">\([q_0, q_1]\)</span> 和 <span class="math inline">\([q_2, q_3]\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
q'_0 &amp;= q_0 \cos(3.0) - q_1 \sin(3.0) = 1.0 \times (-0.990) - 0.0 \times 0.141 = -0.990 \\
q'_1 &amp;= q_0 \sin(3.0) + q_1 \cos(3.0) = 1.0 \times 0.141 + 0.0 \times (-0.990) = 0.141 \\
q'_2 &amp;= q_2 \cos(0.03) - q_3 \sin(0.03) = 1.0 \times 1.000 - 0.0 \times 0.030 = 1.000 \\
q'_3 &amp;= q_2 \sin(0.03) + q_3 \cos(0.03) = 1.0 \times 0.030 + 0.0 \times 1.000 = 0.030
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\mathbf{q}'_3 = [-0.990, 0.141, 1.000, 0.030]
\]</span></p>
<p>类似地，对 key（位置 5）：</p>
<p><span class="math display">\[
\begin{aligned}
k'_0 &amp;= 0.5 \times 0.284 - 0.5 \times (-0.959) = 0.142 + 0.480 = 0.622 \\
k'_1 &amp;= 0.5 \times (-0.959) + 0.5 \times 0.284 = -0.480 + 0.142 = -0.338 \\
k'_2 &amp;= 0.5 \times 0.999 - 0.5 \times 0.050 = 0.500 - 0.025 = 0.475 \\
k'_3 &amp;= 0.5 \times 0.050 + 0.5 \times 0.999 = 0.025 + 0.500 = 0.525
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\mathbf{k}'_5 = [0.622, -0.338, 0.475, 0.525]
\]</span></p>
<p><strong>Step 4: 计算旋转后的内积</strong></p>
<p><span class="math display">\[
\mathbf{q}'_3 \cdot \mathbf{k}'_5 = (-0.990)(0.622) + (0.141)(-0.338) + (1.000)(0.475) + (0.030)(0.525)
\]</span></p>
<p><span class="math display">\[
= -0.616 - 0.048 + 0.475 + 0.016 = -0.173
\]</span></p>
<p><strong>Step 5: 验证——相对位置不变性</strong></p>
<p>现在让我们验证核心性质：<strong>如果保持相对位置 <span class="math inline">\(n - m = 2\)</span> 不变，内积也不变</strong>。</p>
<p>取位置 <span class="math inline">\(m' = 10\)</span>，<span class="math inline">\(n' = 12\)</span>（同样相差 2），用相同的 <span class="math inline">\(\mathbf{q}\)</span> 和 <span class="math inline">\(\mathbf{k}\)</span>：</p>
<p>经过类似计算（过程略），可以验证：</p>
<p><span class="math display">\[
\mathbf{q}'_{10} \cdot \mathbf{k}'_{12} \approx -0.173
\]</span></p>
<p><strong>结论</strong>：内积的值只依赖相对位置 <span class="math inline">\((n - m) = 2\)</span>，而与绝对位置 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(n\)</span> 无关。这正是 RoPE 的核心数学性质！</p>
<blockquote class="blockquote">
<p><strong>直觉解释</strong>：RoPE 将位置编码为旋转角度。位置 3 的 query 旋转了 <span class="math inline">\(3\theta\)</span>，位置 5 的 key 旋转了 <span class="math inline">\(5\theta\)</span>。内积中，这两个旋转的效果相当于用 <span class="math inline">\((5-3)\theta = 2\theta\)</span> 旋转——只取决于相对位置差。</p>
</blockquote>
<p>RoPE 的成功使它被广泛采用：LLaMA、Mistral、Qwen 等主流开源模型都使用 RoPE 作为位置编码。它的优势在于：</p>
<ol type="1">
<li><strong>无额外参数</strong>：RoPE 不引入可学习参数，只是对 query/key 的一个固定变换</li>
<li><strong>相对位置天然编码</strong>：内积只依赖相对位置，符合语言的局部性假设</li>
<li><strong>实现简单</strong>：只需要逐元素乘法，可以高效地在 GPU 上实现</li>
<li><strong>有一定的外推潜力</strong>：虽然不完美，但比绝对位置编码好得多</li>
</ol>
</section>
</section>
<section id="alibi更简单的替代方案" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="alibi更简单的替代方案"><span class="header-section-number">3.4</span> ALiBi：更简单的替代方案</h3>
<p>同年（2021），Press 等人提出了一个更简单的方案：<strong>ALiBi（Attention with Linear Biases）</strong>。</p>
<p>ALiBi 的设计哲学是”极简主义”：它完全不在 embedding 层添加任何位置信息，而是在计算注意力分数时<strong>直接添加一个线性偏置</strong>：</p>
<p><span class="math display">\[
a_{ij} = \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}} - m \cdot |i - j|
\]</span></p>
<p>其中 <span class="math inline">\(m\)</span> 是一个与注意力头相关的”斜率”参数（不同头使用不同的 <span class="math inline">\(m\)</span>），<span class="math inline">\(|i - j|\)</span> 是两个位置之间的距离。这个负偏置的作用是<strong>惩罚远距离的注意力</strong>——距离越远，偏置越负，softmax 后的注意力权重越小。</p>
<p>ALiBi 的关键洞察是：<strong>语言有很强的局部性</strong>。在大多数情况下，一个词与它附近的词的关系比与远处的词更重要。ALiBi 的线性偏置将这种归纳偏置（inductive bias）直接编码进了模型架构。</p>
<p>不同的注意力头使用不同的斜率 <span class="math inline">\(m\)</span>，形成一个几何序列。有些头的 <span class="math inline">\(m\)</span> 很小，几乎不惩罚远距离（可以捕捉长程依赖）；有些头的 <span class="math inline">\(m\)</span> 很大，强烈偏好局部信息。这种多尺度设计让模型能够同时处理不同范围的依赖关系。</p>
<p>ALiBi 的最大优势是<strong>外推能力</strong>。由于它只使用相对距离 <span class="math inline">\(|i - j|\)</span>，而且偏置的形式（线性惩罚）对任意距离都有定义，ALiBi 可以在远超训练长度的序列上保持合理的性能。Press 等人展示，用 1024 长度训练的 ALiBi 模型可以在 10,000 长度的序列上保持较低的困惑度——而使用正弦位置编码的模型在超过训练长度后就完全失效了。</p>
</section>
<section id="rope-vs-alibi两种哲学" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="rope-vs-alibi两种哲学"><span class="header-section-number">3.5</span> RoPE vs ALiBi：两种哲学</h3>
<p>RoPE 和 ALiBi 代表了两种不同的设计哲学：</p>
<p><strong>RoPE</strong> 的哲学是”编码位置信息，让模型自己学习如何利用”。它通过旋转变换将位置信息注入 query 和 key，然后让标准的注意力机制处理剩下的事情。这种设计更”中性”——它不强加任何关于位置重要性的假设。</p>
<p><strong>ALiBi</strong> 的哲学是”直接编码我们对语言的先验知识”。它通过线性偏置显式地告诉模型”近处比远处重要”。这种设计更”有主张”——它假设语言的局部性是普遍的。</p>
<p>在实践中，两者都被广泛使用。LLaMA、Mistral、Qwen 等使用 RoPE；BLOOM、MPT 等使用 ALiBi。经验上，两者在正常训练长度内表现相当，但在长度外推时 ALiBi 通常更稳定。不过，RoPE 结合后面要讨论的外推技术（如 Position Interpolation），可以达到更长的上下文。</p>
</section>
</section>
<section id="长度外推技术" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="长度外推技术"><span class="header-section-number">4</span> 长度外推技术</h2>
<p>即使使用了 RoPE 这样具有相对位置编码特性的方案，直接在超出训练长度的序列上推理仍然会出问题。原因是：虽然 RoPE 的数学形式对任意位置都有定义，但模型的注意力权重是在特定的位置范围内训练的。当遇到从未见过的位置值时，模型的行为是未定义的。</p>
<p>这催生了一系列<strong>长度外推</strong>（length extrapolation）或<strong>上下文窗口扩展</strong>（context window extension）技术。这些技术的目标是：<strong>给定一个在短上下文（如 4K）上训练好的模型，通过某种修改，使其能够处理长得多的上下文（如 128K）</strong>。</p>
<section id="position-interpolation线性缩放" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="position-interpolation线性缩放"><span class="header-section-number">4.1</span> Position Interpolation：线性缩放</h3>
<p>2023 年，Meta 的 Chen 等人提出了一个惊人简单的方法：<strong>Position Interpolation（PI，位置插值）</strong>。</p>
<p>PI 的核心思想是：与其让模型<strong>外推</strong>到未见过的位置（extrapolation），不如将长序列的位置<strong>插值</strong>到训练范围内（interpolation）。</p>
<p>具体来说，如果模型在长度 <span class="math inline">\(L\)</span> 的序列上训练，现在想处理长度 <span class="math inline">\(L'\)</span> 的序列（<span class="math inline">\(L' &gt; L\)</span>），PI 将位置索引线性缩放：</p>
<p><span class="math display">\[
\text{position}_\text{new} = \text{position}_\text{old} \times \frac{L}{L'}
\]</span></p>
<p>例如，如果模型在 4K 上训练，现在想处理 32K 的序列，位置 32000 会被映射到 <span class="math inline">\(32000 \times \frac{4096}{32768} = 4000\)</span>——仍然在训练范围内。</p>
<p>这个方法看似简单，但效果惊人。Chen 等人的理论分析表明，插值的上界比外推小约 600 倍，这解释了为什么插值比外推稳定得多。</p>
<p>在实践中，PI 需要少量的微调（通常几百到几千步）来让模型适应新的位置缩放。但与从头训练一个长上下文模型相比，这个代价微乎其微——只需原始预训练计算量的约 0.1%。</p>
<p>Chen 等人成功地将 LLaMA 7B-65B 的上下文窗口从 2K 扩展到 32K，同时保持了在原始上下文范围内的性能。</p>
</section>
<section id="ntk-aware-scaling保护高频信息" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="ntk-aware-scaling保护高频信息"><span class="header-section-number">4.2</span> NTK-aware Scaling：保护高频信息</h3>
<p>PI 虽然有效，但有一个微妙的问题：它均匀地缩放所有维度的频率。回想 RoPE 中，不同维度使用不同的基础频率：</p>
<p><span class="math display">\[
\theta_i = 10000^{-2i/d}
\]</span></p>
<p>低维度（小的 <span class="math inline">\(i\)</span>）使用高频率，编码短程位置关系；高维度（大的 <span class="math inline">\(i\)</span>）使用低频率，编码长程位置关系。</p>
<p>当 PI 将所有频率除以相同的因子 <span class="math inline">\(s = L'/L\)</span> 时，高频维度受到的影响最大。原本用来区分相邻位置的高频信息被”压缩”了，模型可能丢失对局部位置关系的精确感知。</p>
<p><strong>NTK-aware interpolation</strong> 试图解决这个问题。它的灵感来自神经正切核（Neural Tangent Kernel）理论：深度网络很难学习高频信息，除非输入 embedding 本身包含高频成分。</p>
<p>NTK-aware 的方法是修改 RoPE 的基础频率：</p>
<p><span class="math display">\[
\theta_i' = 10000^{-2i/d} \times s^{2i/(d-2)}
\]</span></p>
<p>这个设计让低频维度（大的 <span class="math inline">\(i\)</span>）承担更多的缩放，而高频维度（小的 <span class="math inline">\(i\)</span>）几乎不缩放。直觉上，这保护了模型对短程位置关系的编码能力。</p>
</section>
<section id="yarn统一框架" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="yarn统一框架"><span class="header-section-number">4.3</span> YaRN：统一框架</h3>
<p>2023 年，Peng 等人提出的 <strong>YaRN（Yet another RoPE extensioN）</strong> 是目前最先进的 RoPE 外推方法之一。它综合了多种技术，形成了一个统一的框架。</p>
<p>YaRN 的核心组件包括：</p>
<p><strong>1. NTK-by-parts interpolation</strong>：将 RoPE 的维度分成三组： - 低维度（高频）：不做任何插值（<span class="math inline">\(\lambda = 1\)</span>），完全保留 - 中维度：渐进式插值 - 高维度（低频）：完全使用 PI 插值（<span class="math inline">\(\lambda = s\)</span>）</p>
<p>这种设计既保护了高频信息，又让低频维度能够编码更长的相对位置。</p>
<p><strong>2. 注意力缩放</strong>：直接将注意力 logits 除以一个与上下文扩展比例相关的因子。这补偿了位置编码缩放后 softmax 分布可能变得过于”尖锐”的问题。</p>
<p><strong>3. 极短微调</strong>：YaRN 只需要约 400 步微调（原始预训练的 ~0.1%），就能将上下文窗口扩展 8–16 倍。</p>
<p>YaRN 在多个基准测试上显著优于 PI 和 NTK-aware 方法。更重要的是，它展示了<strong>外推</strong>能力：在 64K 上微调的 YaRN 模型，可以在 128K 上保持递减的困惑度——这意味着它真正学会了处理更长的上下文，而不仅仅是”不崩溃”。</p>
</section>
<section id="外推技术对比" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="外推技术对比"><span class="header-section-number">4.4</span> 外推技术对比</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>核心思想</th>
<th>需要微调？</th>
<th>外推能力</th>
<th>计算开销</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>原始 RoPE</td>
<td>旋转编码</td>
<td>否</td>
<td>差</td>
<td>无</td>
</tr>
<tr class="even">
<td>Position Interpolation</td>
<td>线性缩放位置</td>
<td>是（~1K 步）</td>
<td>中</td>
<td>无</td>
</tr>
<tr class="odd">
<td>NTK-aware</td>
<td>保护高频维度</td>
<td>可选</td>
<td>中</td>
<td>无</td>
</tr>
<tr class="even">
<td>YaRN</td>
<td>NTK-by-parts + 注意力缩放</td>
<td>是（~400 步）</td>
<td>好</td>
<td>无</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="flash-attentionio-感知的算法革命" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="flash-attentionio-感知的算法革命"><span class="header-section-number">5</span> Flash Attention：IO 感知的算法革命</h2>
<p>前面讨论的位置编码技术解决了”模型能否理解长距离位置关系”的问题，但没有触及 <span class="math inline">\(O(n^2)\)</span> 计算复杂度这个根本瓶颈。接下来我们讨论一个从完全不同角度突破这个瓶颈的技术：<strong>FlashAttention</strong>。</p>
<section id="gpu-内存层次结构" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="gpu-内存层次结构"><span class="header-section-number">5.1</span> GPU 内存层次结构</h3>
<p>要理解 FlashAttention 的精妙之处，首先需要理解现代 GPU 的内存层次结构。</p>
<p>GPU 有两种主要的存储：<strong>HBM（High Bandwidth Memory）</strong> 和 <strong>SRAM（Static RAM）</strong>。HBM 是 GPU 的主内存，容量大（如 A100 的 80GB），但带宽相对较低（约 2TB/s）。SRAM 是片上缓存，容量很小（A100 约 20MB），但带宽极高（约 19TB/s）。</p>
<p>关键的洞察是：<strong>GPU 计算单元的速度远超 HBM 的带宽</strong>。A100 的 tensor core 理论浮点运算峰值是 312 TFLOPS（FP16），而从 HBM 读取数据的带宽只有 2TB/s。这意味着如果每次计算都需要从 HBM 读数据，计算单元会大量闲置，等待数据到来。</p>
<p>标准注意力实现的问题正是如此：它需要将完整的 <span class="math inline">\(n \times n\)</span> 注意力矩阵写入 HBM（用于 softmax），然后再读出来（用于乘以 <span class="math inline">\(V\)</span>）。对于长序列，这个矩阵巨大无比，HBM 的读写成为严重的瓶颈。</p>
</section>
<section id="flashattention-的-tiling-策略" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="flashattention-的-tiling-策略"><span class="header-section-number">5.2</span> FlashAttention 的 Tiling 策略</h3>
<div id="fig-flash-attention-tiling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flash-attention-tiling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-26/original/fig-flash-attention-tiling.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flash-attention-tiling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: FlashAttention 的核心创新：通过分块（tiling）计算，让数据留在快速的 SRAM 中，避免将完整的 <span class="math inline">\(n \times n\)</span> 注意力矩阵写入慢速的 HBM。左图展示了 GPU 内存层次结构，右图展示了分块计算的流程。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Dao et al.&nbsp;(2022) “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”, Figure 1. <a href="https://arxiv.org/abs/2205.14135">arXiv:2205.14135</a></em></p>
</div>
<p>FlashAttention 的核心思想是：<strong>通过分块（tiling）计算，让数据尽可能留在快速的 SRAM 中，避免频繁地访问 HBM</strong>。</p>
<p>具体来说，FlashAttention 将 <span class="math inline">\(Q\)</span>、<span class="math inline">\(K\)</span>、<span class="math inline">\(V\)</span> 矩阵分成小块，每次只将一小块加载到 SRAM 中进行计算。关键的技巧是：softmax 可以通过”在线”算法增量计算，而不需要一次性看到整行的所有元素。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 2: FlashAttention 核心算法 (Dao et al., 2022)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flash_attention(Q, K, V, block_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    IO-aware 的精确注意力计算</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    核心思想：</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    1. 将 Q, K, V 分成小块</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    2. 在 SRAM 中计算每个块对的注意力</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    3. 使用在线 softmax 算法，增量更新输出</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    seq_len, d <span class="op">=</span> Q.shape</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    O <span class="op">=</span> torch.zeros_like(Q)  <span class="co"># 输出</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> torch.zeros(seq_len)  <span class="co"># softmax 归一化因子</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> torch.full((seq_len,), <span class="op">-</span><span class="bu">float</span>(<span class="st">'inf'</span>))  <span class="co"># 行最大值（用于数值稳定）</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 外层循环：遍历 K, V 的块</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, seq_len, block_size):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        K_j <span class="op">=</span> K[j:j<span class="op">+</span>block_size]  <span class="co"># 从 HBM 加载到 SRAM</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        V_j <span class="op">=</span> V[j:j<span class="op">+</span>block_size]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 内层循环：遍历 Q 的块</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, seq_len, block_size):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            Q_i <span class="op">=</span> Q[i:i<span class="op">+</span>block_size]  <span class="co"># 从 HBM 加载到 SRAM</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 在 SRAM 中计算局部注意力分数</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            S_ij <span class="op">=</span> Q_i <span class="op">@</span> K_j.T <span class="op">/</span> sqrt(d)  <span class="co"># [block_size, block_size]</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 在线 softmax 更新</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            m_new <span class="op">=</span> torch.<span class="bu">max</span>(m[i:i<span class="op">+</span>block_size], S_ij.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 更新归一化因子和输出（数学细节见论文）</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># ... (涉及 exp 缩放和累加)</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> O  <span class="co"># 直接写回 HBM，无需存储完整注意力矩阵</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Dao et al.&nbsp;(2022) “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”, Algorithm 1. <a href="https://arxiv.org/abs/2205.14135">arXiv:2205.14135</a></em></p>
</div>
</div>
<p>FlashAttention 的关键特性：</p>
<ol type="1">
<li><strong>精确计算</strong>：与标准注意力的结果完全相同，没有任何近似</li>
<li><strong>内存高效</strong>：不需要存储 <span class="math inline">\(O(n^2)\)</span> 的注意力矩阵，内存复杂度降为 <span class="math inline">\(O(n)\)</span></li>
<li><strong>速度快</strong>：尽管理论 FLOP 数可能更多（因为需要重复计算一些值），但由于减少了 HBM 访问，实际运行时间反而更短</li>
</ol>
<p>Dao 等人的实验表明，FlashAttention 在 A100 GPU 上比 PyTorch 标准实现快 2–4 倍，同时内存使用减少到原来的 1/10 到 1/20。</p>
</section>
<section id="flashattention-2更好的并行" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="flashattention-2更好的并行"><span class="header-section-number">5.3</span> FlashAttention-2：更好的并行</h3>
<p>FlashAttention-2 (Dao, 2023) 进一步优化了并行策略和工作分配。</p>
<p>FlashAttention 原版的一个问题是：它在 GPU 的不同线程块（thread blocks）和 warps 之间的工作分配不够均衡。FlashAttention-2 通过以下改进解决这个问题：</p>
<ol type="1">
<li><strong>减少非 matmul 操作</strong>：将更多计算转换为矩阵乘法，以充分利用 tensor core</li>
<li><strong>增加 occupancy</strong>：即使对于单个注意力头，也能在多个 thread blocks 之间并行</li>
<li><strong>优化 warp 间通信</strong>：减少通过 shared memory 的数据交换</li>
</ol>
<p>这些优化让 FlashAttention-2 比 FlashAttention 又快了约 2 倍，达到 A100 理论峰值性能的 50–73%——这已经非常接近矩阵乘法的效率了。</p>
</section>
<section id="flash-attention-的影响" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="flash-attention-的影响"><span class="header-section-number">5.4</span> Flash Attention 的影响</h3>
<p>FlashAttention 的影响是深远的。它证明了：<strong>算法和硬件的协同设计可以带来巨大的性能提升，即使在理论复杂度没有改变的情况下</strong>。</p>
<p>在 FlashAttention 出现之前，大家普遍认为要突破 <span class="math inline">\(O(n^2)\)</span> 瓶颈，必须使用稀疏注意力或近似注意力。FlashAttention 展示了另一条路：通过让计算更”IO-aware”，精确注意力也可以高效地处理长序列。</p>
<p>如今，FlashAttention 已经成为几乎所有主流 LLM 框架的默认注意力实现。它让 128K 甚至更长的上下文成为可能——不是通过近似，而是通过更聪明的精确计算。</p>
</section>
</section>
<section id="kv-cache-优化" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="kv-cache-优化"><span class="header-section-number">6</span> KV Cache 优化</h2>
<p>解决了训练时的计算效率问题后，还有一个推理时的关键瓶颈：<strong>KV cache 的内存管理</strong>。</p>
<section id="kv-cache-的必要性" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="kv-cache-的必要性"><span class="header-section-number">6.1</span> KV Cache 的必要性</h3>
<p>在自回归生成时，模型每次只生成一个新 token，但需要计算这个 token 与之前所有 token 的注意力。如果每次都重新计算所有 token 的 key 和 value，计算量会随生成长度二次增长。</p>
<p>KV cache 是一个简单但关键的优化：在生成第 <span class="math inline">\(t\)</span> 个 token 时，将该位置的 key 和 value 存储起来。生成第 <span class="math inline">\(t+1\)</span> 个 token 时，只需要计算新 token 的 query，然后与缓存的所有 key 计算注意力分数，最后用注意力权重加权缓存的所有 value。</p>
<p>这将每个 token 的计算复杂度从 <span class="math inline">\(O(t \cdot d)\)</span> 降到 <span class="math inline">\(O(d)\)</span>（不考虑注意力计算本身），是自回归推理能够实用的关键。</p>
</section>
<section id="kv-cache-的内存挑战" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="kv-cache-的内存挑战"><span class="header-section-number">6.2</span> KV Cache 的内存挑战</h3>
<p>但 KV cache 带来了严重的内存压力。对于一个 70B 模型，假设： - 80 层 - 64 个注意力头 - head dimension = 128 - BF16 精度（2 bytes）</p>
<p>每个 token 的 KV cache 大小是：</p>
<p><span class="math display">\[
2 \times 80 \times 64 \times 128 \times 2 = 2.6 \text{ MB/token}
\]</span></p>
<p>对于 128K 上下文，KV cache 总大小是 <span class="math inline">\(128000 \times 2.6 \text{ MB} = 333 \text{ GB}\)</span>——远超任何单张 GPU 的显存。</p>
<p>即使是更短的上下文和更小的模型，当需要同时服务多个请求时，KV cache 仍然是主要的内存瓶颈。而且，不同请求的上下文长度不同，导致 KV cache 的大小高度动态——这使得简单的静态内存分配非常低效。</p>
</section>
<section id="multi-query-attention-mqa-和-grouped-query-attention-gqa" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="multi-query-attention-mqa-和-grouped-query-attention-gqa"><span class="header-section-number">6.3</span> Multi-Query Attention (MQA) 和 Grouped-Query Attention (GQA)</h3>
<p>一个减少 KV cache 大小的方法是减少需要缓存的 key/value 头的数量。</p>
<p><strong>Multi-Query Attention (MQA)</strong> 是 Shazeer (2019) 提出的一种激进方案：所有 query 头共享同一套 key 和 value。这将 KV cache 大小降低为原来的 <span class="math inline">\(1/h\)</span>（<span class="math inline">\(h\)</span> 是头数）。</p>
<p>MQA 的问题是可能损失模型质量——不同 query 头共享 key/value，表达能力下降。</p>
<p><strong>Grouped-Query Attention (GQA)</strong> 是 Ainslie 等人 (2023) 提出的折中方案：将 query 头分成若干组，每组共享一套 key/value。例如，如果有 64 个 query 头，分成 8 组，则只需要 8 套 key/value，KV cache 降为原来的 1/8。</p>
<p>GQA 在 KV cache 节省和模型质量之间取得了很好的平衡。LLaMA 2 70B 和 LLaMA 3 系列都采用了 GQA。实验表明，GQA 模型的质量非常接近标准 MHA，而 KV cache 大小大幅减少。</p>
</section>
<section id="pagedattention虚拟内存思想的应用" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="pagedattention虚拟内存思想的应用"><span class="header-section-number">6.4</span> PagedAttention：虚拟内存思想的应用</h3>
<p>2023 年，Kwon 等人提出的 <strong>PagedAttention</strong> 将操作系统的虚拟内存和分页技术引入 KV cache 管理，彻底改变了 LLM 推理的内存效率。</p>
<div id="fig-pagedattention-memory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pagedattention-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-26/original/fig-pagedattention-memory.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pagedattention-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: KV Cache 内存浪费问题的可视化：传统方法需要预分配最大可能长度的内存，导致大量浪费（reserved 部分）和碎片化（fragmentation）。实际使用中，大多数请求远未达到最大长度。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Kwon et al.&nbsp;(2023) “Efficient Memory Management for Large Language Model Serving with PagedAttention”, Figure 1-2. <a href="https://arxiv.org/abs/2309.06180">arXiv:2309.06180</a></em></p>
</div>
<p>传统的 KV cache 管理面临两个问题：</p>
<ol type="1">
<li><strong>碎片化</strong>：不同请求的 KV cache 大小不同，动态分配会产生大量内存碎片</li>
<li><strong>预分配浪费</strong>：为了避免碎片化，很多系统预分配最大可能长度的内存，但大多数请求实际上不会用到这么多</li>
</ol>
<p>PagedAttention 的解决方案是将 KV cache 分成固定大小的”页”（blocks），就像操作系统管理内存一样。每个请求的 KV cache 由一系列非连续的页组成，通过一个页表（block table）来跟踪逻辑位置到物理位置的映射。</p>
<p>这种设计带来了三个关键优势：</p>
<ol type="1">
<li><strong>零碎片</strong>：所有页大小相同，分配和回收不会产生碎片</li>
<li><strong>按需分配</strong>：只在实际需要时才分配新页，不预分配</li>
<li><strong>高效共享</strong>：如果多个请求有相同的前缀（如相同的 system prompt），可以共享同一套 KV cache 页</li>
</ol>
<div id="fig-pagedattention-design" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pagedattention-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-26/original/fig-pagedattention-design.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pagedattention-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: PagedAttention 的核心设计：将 KV Cache 分成固定大小的页（blocks），通过 block table 管理逻辑-物理映射。每个请求的 KV Cache 由非连续的页组成，支持动态扩展和内存共享。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Kwon et al.&nbsp;(2023) “Efficient Memory Management for Large Language Model Serving with PagedAttention”, Figure 3. <a href="https://arxiv.org/abs/2309.06180">arXiv:2309.06180</a></em></p>
</div>
<p>基于 PagedAttention，Kwon 等人构建了 <strong>vLLM</strong> 推理引擎。实验表明，vLLM 的吞吐量比 HuggingFace Transformers 高 2–4 倍，比当时最先进的 FasterTransformer 也快 2 倍以上。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>PagedAttention 的核心设计 (Kwon et al., 2023)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PagedAttention 的关键数据结构</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BlockTable:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    逻辑 block index -&gt; 物理 block index 的映射</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    类似于操作系统的页表</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_seq_len, block_size<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_size <span class="op">=</span> block_size</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_blocks <span class="op">=</span> (max_seq_len <span class="op">+</span> block_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> block_size</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>] <span class="op">*</span> <span class="va">self</span>.max_blocks  <span class="co"># -1 表示未分配</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> allocate_block(<span class="va">self</span>, logical_idx, physical_idx):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping[logical_idx] <span class="op">=</span> physical_idx</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_physical_idx(<span class="va">self</span>, logical_idx):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mapping[logical_idx]</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KVCachePool:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co">    全局 KV cache 内存池</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co">    所有请求共享，按页分配</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_blocks, block_size, num_layers, num_heads, head_dim):</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.free_blocks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(num_blocks))</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 预分配所有物理块的内存</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k_cache <span class="op">=</span> torch.zeros(num_blocks, num_layers, num_heads, block_size, head_dim)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_cache <span class="op">=</span> torch.zeros(num_blocks, num_layers, num_heads, block_size, head_dim)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> alloc(<span class="va">self</span>):</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.free_blocks.pop()</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> free(<span class="va">self</span>, block_idx):</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.free_blocks.append(block_idx)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Kwon et al.&nbsp;(2023) “Efficient Memory Management for Large Language Model Serving with PagedAttention”, Section 4. <a href="https://arxiv.org/abs/2309.06180">arXiv:2309.06180</a></em></p>
</div>
</div>
</section>
</section>
<section id="ring-attention分布式长序列" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="ring-attention分布式长序列"><span class="header-section-number">7</span> Ring Attention：分布式长序列</h2>
<p>当上下文长度达到数十万甚至数百万 token 时，单个 GPU 的内存不再足够。即使使用了所有上述优化，128K 上下文的 70B 模型的 KV cache 仍然需要几十 GB。这时需要<strong>分布式</strong>解决方案。</p>
<p><strong>Ring Attention</strong> (Liu et al., 2023) 是一种在多 GPU 之间分布式计算超长序列注意力的方法。它的核心思想是：</p>
<ol type="1">
<li>将长序列分割到多个 GPU 上，每个 GPU 持有序列的一部分</li>
<li>GPU 之间形成一个逻辑上的”环”</li>
<li>每个 GPU 用 FlashAttention 计算本地的注意力，同时将 key/value 块发送给环中的下一个 GPU</li>
<li>通过重叠计算和通信，消除通信开销</li>
</ol>
<p>关键的洞察是：FlashAttention 的分块计算天然适合这种分布式设计。每个 GPU 在等待下一块 key/value 到来时，可以先处理已有的块。只要计算时间大于通信时间（这在现代高速互联的 GPU 集群上通常成立），Ring Attention 就能实现几乎线性的扩展。</p>
<p>Liu 等人展示，Ring Attention 可以处理<strong>数百万 token</strong> 的上下文——这是单卡方案无法想象的长度。</p>
</section>
<section id="深入理解" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">8</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨长上下文技术的理论基础、边界条件和开放问题</p>
</blockquote>
<section id="为什么-rope-有外推潜力" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="为什么-rope-有外推潜力"><span class="header-section-number">8.1</span> 为什么 RoPE 有外推潜力？</h3>
<p>RoPE 相比绝对位置编码有更好的外推能力，这不是偶然的。让我们从数学上分析其原因。</p>
<p>在 RoPE 中，位置 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(n\)</span> 的注意力分数只依赖于它们的差 <span class="math inline">\(n - m\)</span>：</p>
<p><span class="math display">\[
\mathbf{q}_m'^{\top} \mathbf{k}_n' = f(\mathbf{q}_m, \mathbf{k}_n, n-m)
\]</span></p>
<p>这意味着模型学习的是”相对距离 <span class="math inline">\(d\)</span> 意味着什么”，而不是”绝对位置 <span class="math inline">\(p\)</span> 意味着什么”。只要相对距离 <span class="math inline">\(d\)</span> 在训练时见过（比如 <span class="math inline">\(d \in [-4096, 4096]\)</span>），模型就应该能正确处理。</p>
<p>但问题在于：RoPE 的高频维度变化很快，对于非常长的序列，这些维度的值可能进入”从未见过”的区间。Position Interpolation 通过缩放频率来解决这个问题；YaRN 通过选择性地缩放不同维度来更精细地平衡。</p>
<p>一个有趣的理论问题是：<strong>RoPE 外推的理论极限是什么？</strong> 目前没有确切的答案，但实验表明，通过 YaRN 等方法，可以稳定地扩展到训练长度的 16–32 倍。</p>
</section>
<section id="flashattention-的复杂度分析" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="flashattention-的复杂度分析"><span class="header-section-number">8.2</span> FlashAttention 的复杂度分析</h3>
<p>FlashAttention 的 FLOP 数实际上比标准注意力<strong>更多</strong>，因为它需要在分块计算时重复一些操作。但它的内存访问量大幅减少。</p>
<p>设 <span class="math inline">\(n\)</span> 为序列长度，<span class="math inline">\(d\)</span> 为 head dimension，<span class="math inline">\(M\)</span> 为 SRAM 大小（假设 <span class="math inline">\(M &gt; d\)</span>）。标准注意力的 HBM 访问量是 <span class="math inline">\(O(nd + n^2)\)</span>（需要读写完整的注意力矩阵）。FlashAttention 的 HBM 访问量是 <span class="math inline">\(O(n^2 d^2 / M)\)</span>。</p>
<p>当 <span class="math inline">\(M &gt; nd\)</span>（即 SRAM 足够大，可以容纳一块 <span class="math inline">\(Q\)</span> 或 <span class="math inline">\(K\)</span> 或 <span class="math inline">\(V\)</span>）时，FlashAttention 的 HBM 访问量是 <span class="math inline">\(O(nd)\)</span>——与序列长度<strong>线性</strong>而非二次！</p>
<p>这解释了为什么 FlashAttention 尽管 FLOP 更多，实际运行却更快：在现代 GPU 上，<strong>内存带宽是瓶颈，而非计算能力</strong>。</p>
</section>
<section id="长上下文的失效模式" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="长上下文的失效模式"><span class="header-section-number">8.3</span> 长上下文的失效模式</h3>
<p>长上下文技术并非万能的。以下是一些已知的失效模式：</p>
<p><strong>“Lost in the Middle” 现象</strong>：Liu et al.&nbsp;(2023) 发现，当相关信息位于长上下文的中间位置时，模型的检索能力显著下降。模型倾向于关注上下文的开头和结尾，而忽视中间部分。这可能与注意力的分布特性有关。</p>
<p><strong>困惑度不等于下游性能</strong>：一个模型可能在长序列上有很低的困惑度（作为语言模型表现良好），但在实际任务（如长文档 QA）上表现不佳。困惑度只衡量了模型预测下一个 token 的能力，不等于模型能有效<strong>利用</strong>长上下文的信息。</p>
<p><strong>训练-推理不匹配</strong>：使用外推技术扩展的模型，在推理长度远超训练长度时，行为可能变得不可预测。YaRN 等方法缓解了这个问题，但没有完全解决。</p>
</section>
<section id="开放研究问题" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">8.4</span> 开放研究问题</h3>
<p><strong>无限上下文可能吗？</strong> 当前最长的上下文达到数百万 token（如 Gemini 1.5 的 1M token 窗口），但这仍然是有限的。是否有可能设计一种架构，能够处理任意长度的序列？这可能需要根本性的架构创新，而不仅仅是优化现有的 Transformer。</p>
<p><strong>长上下文的高效训练</strong>：目前的长上下文模型大多是先在短上下文上预训练，再通过外推技术扩展。是否有更高效的方法来直接在长上下文上训练？</p>
<p><strong>压缩与检索的权衡</strong>：另一种处理长上下文的思路是不把所有信息都放进上下文，而是用压缩（如将历史对话总结成摘要）或检索（如 RAG）来提供相关信息。这种方法与长上下文有什么本质区别？在什么场景下哪种方法更优？</p>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">9</span> 局限性与未解决的问题</h2>
<section id="当前技术的局限" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="当前技术的局限"><span class="header-section-number">9.1</span> 当前技术的局限</h3>
<p><strong>计算成本仍然高昂</strong>：尽管 FlashAttention 大幅提升了效率，长上下文推理的成本仍然远高于短上下文。128K 上下文的推理成本大约是 4K 上下文的 30 倍（考虑 <span class="math inline">\(O(n^2)\)</span> 的注意力和 <span class="math inline">\(O(n)\)</span> 的 KV cache）。</p>
<p><strong>质量随长度下降</strong>：即使困惑度保持稳定，模型在实际任务上的质量通常随上下文长度下降。这可能是因为模型的注意力机制在长序列上变得更加分散。</p>
<p><strong>工程复杂度</strong>：部署长上下文模型需要大量的系统优化——FlashAttention、PagedAttention、分布式推理等。这增加了工程复杂度和维护成本。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">9.2</span> 这些局限导向了什么？</h3>
<p>长上下文技术的发展揭示了一个深层的权衡：<strong>上下文长度 vs 效率 vs 质量</strong>。扩展上下文长度通常意味着更高的计算和内存成本，而且不一定带来相应的质量提升。</p>
<p>这引出了下一章要讨论的问题：<strong>是否有更根本的方法来突破 dense Transformer 的效率瓶颈？</strong> Mixture of Experts (MoE) 提供了一种不同的思路：通过稀疏激活，让每个 token 只使用模型参数的一部分，从而在保持模型容量的同时降低计算成本。</p>
</section>
</section>
<section id="本章小结" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">10</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">10.1</span> 核心要点回顾</h3>
<ol type="1">
<li><p><strong>问题</strong>：大语言模型的上下文长度受限于 <span class="math inline">\(O(n^2)\)</span> 的注意力复杂度、位置编码的外推能力、以及推理时的 KV cache 内存</p></li>
<li><p><strong>位置编码演进</strong>：从绝对编码（正弦、可学习）到相对编码（RoPE、ALiBi）。RoPE 通过旋转矩阵自然地编码相对位置；ALiBi 通过线性偏置实现更好的外推</p></li>
<li><p><strong>长度外推技术</strong>：Position Interpolation 将长序列位置缩放到训练范围内；YaRN 结合 NTK-aware scaling 和注意力缩放，实现 10× 以上的上下文扩展</p></li>
<li><p><strong>FlashAttention</strong>：通过 IO-aware 的分块计算，避免存储完整注意力矩阵，同时保持精确计算。内存从 <span class="math inline">\(O(n^2)\)</span> 降到 <span class="math inline">\(O(n)\)</span>，速度提升 2–4×</p></li>
<li><p><strong>KV Cache 优化</strong>：GQA 通过共享 key/value 头减少缓存大小；PagedAttention 用虚拟内存思想实现高效的动态内存管理</p></li>
</ol>
</section>
<section id="关键公式速查" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">10.2</span> 关键公式速查</h3>
<ul>
<li><strong>RoPE</strong>：<span class="math inline">\(\mathbf{q}_m'^{\top} \mathbf{k}_n' = f(\mathbf{q}_m, \mathbf{k}_n, n-m)\)</span>（只依赖相对位置）</li>
<li><strong>ALiBi</strong>：<span class="math inline">\(a_{ij} = \mathbf{q}_i^\top \mathbf{k}_j / \sqrt{d_k} - m \cdot |i-j|\)</span></li>
<li><strong>Position Interpolation</strong>：<span class="math inline">\(\text{pos}_\text{new} = \text{pos}_\text{old} \times \frac{L_\text{train}}{L_\text{target}}\)</span></li>
<li><strong>KV cache 大小</strong>：<span class="math inline">\(2 \times \text{layers} \times \text{heads} \times \text{seq\_len} \times \text{head\_dim} \times \text{bytes}\)</span></li>
</ul>
</section>
<section id="思考题" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">10.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 解释 RoPE 和 ALiBi 的设计哲学有何不同？在什么场景下你会选择哪一种？</p></li>
<li><p><strong>[数学推导]</strong> 证明 RoPE 中两个位置的内积只依赖于它们的相对位置。提示：利用旋转矩阵的性质 <span class="math inline">\(R(\theta)^\top R(\phi) = R(\phi - \theta)\)</span>。</p></li>
<li><p><strong>[工程实践]</strong> 使用 vLLM 部署一个支持 32K 上下文的 LLaMA 模型，测量不同上下文长度下的吞吐量和延迟。观察 PagedAttention 的内存使用效率。</p></li>
<li><p><strong>[开放思考]</strong> 长上下文和 RAG 都是让模型访问更多信息的方法。讨论它们的权衡：什么时候应该扩展上下文窗口？什么时候应该使用检索？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">11</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>RoPE (Su et al., 2021)</strong>：旋转位置编码的原始论文
<ul>
<li>重点读：Section 3（数学推导）</li>
</ul></li>
<li><strong>FlashAttention (Dao et al., 2022)</strong>：IO-aware 注意力的开创性工作
<ul>
<li>重点读：Section 3（算法）、Section 4（复杂度分析）</li>
</ul></li>
<li><strong>PagedAttention (Kwon et al., 2023)</strong>：KV cache 管理的革命
<ul>
<li>重点读：Section 3-4（设计）、Section 6（实验）</li>
</ul></li>
</ul>
</section>
<section id="外推技术" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="外推技术"><span class="header-section-number">11.2</span> 外推技术</h3>
<ul>
<li><strong>Position Interpolation (Chen et al., 2023)</strong>：简单有效的外推方法</li>
<li><strong>YaRN (Peng et al., 2023)</strong>：最先进的 RoPE 外推框架</li>
</ul>
</section>
<section id="综述与教程" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">11.3</span> 综述与教程</h3>
<ul>
<li><strong>Extending the RoPE</strong> (EleutherAI Blog)：RoPE 外推技术的深度教程</li>
<li><strong>vLLM Documentation</strong>：PagedAttention 的官方实现文档</li>
</ul>
</section>
<section id="代码资源" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">11.4</span> 代码资源</h3>
<ul>
<li>FlashAttention：<a href="https://github.com/Dao-AILab/flash-attention">github.com/Dao-AILab/flash-attention</a></li>
<li>vLLM：<a href="https://github.com/vllm-project/vllm">github.com/vllm-project/vllm</a></li>
<li>YaRN：<a href="https://github.com/jquesnelle/yarn">github.com/jquesnelle/yarn</a></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">12</span> 历史注脚</h2>
<p>长上下文的故事是工程与理论共同进步的典范。</p>
<p>FlashAttention 的出现是一个转折点。在此之前，大家普遍认为要处理长序列，必须使用稀疏注意力或近似方法——精确注意力的 <span class="math inline">\(O(n^2)\)</span> 复杂度似乎是不可逾越的障碍。Tri Dao 的洞察——问题不在于 FLOP 数，而在于内存带宽——打开了一扇新的大门。这提醒我们，在优化算法时，不仅要看理论复杂度，还要考虑实际硬件的特性。</p>
<p>PagedAttention 的故事同样有趣。它的灵感来自操作系统中几十年前就成熟的虚拟内存技术。这提醒我们，有时候解决新问题的方法，可能早就存在于其他领域——关键是能够识别问题的本质，并找到合适的类比。</p>
<p>从更宏观的视角看，长上下文技术的进步展示了 AI 系统工程的重要性。模型架构固然重要，但如果没有 FlashAttention、PagedAttention、Ring Attention 这些系统层面的优化，即使是最先进的模型架构也无法在实际中部署。<strong>算法创新和系统优化必须齐头并进，这是大模型时代的关键认识</strong>。</p>
<p>当前的长上下文技术已经让 128K 甚至 1M token 的上下文成为可能。但这是终点吗？随着应用场景的扩展（代码库理解、长视频分析、终身对话记忆），对上下文长度的需求可能会继续增长。下一个数量级的突破——千万 token、亿 token 的上下文——可能需要更根本的架构创新。这也是第 28 章将讨论的状态空间模型（SSM/Mamba）试图解决的问题：是否存在一种架构，能够以 <span class="math inline">\(O(n)\)</span> 的复杂度处理任意长度的序列？</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第26章：长上下文与高效推理"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "From 512 Tokens to Millions: The Quest for Infinite Context"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, 长上下文, 高效推理, 位置编码]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [RoPE, ALiBi, FlashAttention, KV Cache, PagedAttention, GQA, MQA, Position Interpolation, YaRN, Ring Attention, vLLM, 长度外推]</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "原始Transformer的512 token上下文窗口曾经足够，但大语言模型时代催生了对超长上下文的需求——从代码库理解到长文档摘要，从多轮对话到RAG检索。本章讲述突破上下文长度瓶颈的完整技术栈：位置编码从绝对到相对再到RoPE/ALiBi的演进、Position Interpolation到YaRN的长度外推技术、FlashAttention对注意力计算的IO感知革命、KV Cache的PagedAttention内存管理，以及Ring Attention的分布式长序列方案。每一项技术都是对O(n²)瓶颈的不同维度的突围。"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-26/original/fig-flash-attention-tiling.png"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何让大语言模型支持超长序列？从512 token到百万token的跃迁需要哪些技术突破？</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2021–2024 </span><span class="pp">|</span><span class="at"> RoPE (2021) → ALiBi (2022) → FlashAttention (2022) → Position Interpolation (2023) → YaRN (2023) → PagedAttention (2023) </span><span class="pp">|</span><span class="at"> 从算法到系统的完整链条</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Su et al. (2021)** "RoFormer: Enhanced Transformer with Rotary Position Embedding" (<span class="co">[</span><span class="ot">arXiv:2104.09864</span><span class="co">](https://arxiv.org/abs/2104.09864)</span>) — 参考了 Section 3 (RoPE 数学推导)、Figure 1-2 (旋转编码可视化)；核心位置编码方法</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Press et al. (2022)** "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation" (<span class="co">[</span><span class="ot">arXiv:2108.12409</span><span class="co">](https://arxiv.org/abs/2108.12409)</span>) — 参考了 Section 2 (ALiBi 方法)、Figure 1-2 (外推实验)；ICLR 2022</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dao et al. (2022)** "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" (<span class="co">[</span><span class="ot">arXiv:2205.14135</span><span class="co">](https://arxiv.org/abs/2205.14135)</span>) — 参考了 Algorithm 1 (FlashAttention 伪代码)、Figure 1 (Tiling 示意图)；NeurIPS 2022</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dao (2023)** "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning" (<span class="co">[</span><span class="ot">arXiv:2307.08691</span><span class="co">](https://arxiv.org/abs/2307.08691)</span>) — 参考了 Section 3 (优化策略)；ICLR 2024</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chen et al. (2023)** "Extending Context Window of Large Language Models via Positional Interpolation" (<span class="co">[</span><span class="ot">arXiv:2306.15595</span><span class="co">](https://arxiv.org/abs/2306.15595)</span>) — 参考了 Section 2 (PI 方法)、理论分析；EMNLP 2024</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Peng et al. (2023)** "YaRN: Efficient Context Window Extension of Large Language Models" (<span class="co">[</span><span class="ot">arXiv:2309.00071</span><span class="co">](https://arxiv.org/abs/2309.00071)</span>) — 参考了 Section 2-3 (NTK-aware 和 YaRN 方法)；ICLR 2024</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kwon et al. (2023)** "Efficient Memory Management for Large Language Model Serving with PagedAttention" (<span class="co">[</span><span class="ot">arXiv:2309.06180</span><span class="co">](https://arxiv.org/abs/2309.06180)</span>) — 参考了 Section 3-4 (PagedAttention 设计)；SOSP 2023</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ainslie et al. (2023)** "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints" (<span class="co">[</span><span class="ot">arXiv:2305.13245</span><span class="co">](https://arxiv.org/abs/2305.13245)</span>) — 参考了 Section 2-3 (GQA 方法)；EMNLP 2023</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liu et al. (2023)** "Ring Attention with Blockwise Transformers for Near-Infinite Context" (<span class="co">[</span><span class="ot">arXiv:2310.01889</span><span class="co">](https://arxiv.org/abs/2310.01889)</span>) — 参考了 Section 2-3 (Ring Attention 算法)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Chapter 11.6 (Self-Attention and Positional Encoding) — 参考了位置编码的基础讲解</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Understanding Deep Learning** Chapter 12 (Transformers) — 参考了注意力机制的可视化</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Princeton COS 484** (Spring 2024) — Tri Dao 讲授 FlashAttention 的课程材料</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 8-9 (Winter 2025) — 参考了 Transformer 和预训练的教学框架</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>上一章我们深入探讨了对齐技术的演进——从 RLHF 的三阶段流水线到 DPO 的单阶段直接优化，再到 ORPO、SimPO、KTO 等进一步简化工程复杂度的变体。这些技术让大语言模型从"文本补全器"转变为真正有用的 AI 助手，能够遵循指令、拒绝有害请求、提供有帮助的回答。对齐技术的平民化——DPO 让偏好对齐变成了"加几行代码"的事情——大大加速了开源社区的发展。</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>然而，对齐解决的是"模型行为与人类期望一致"的问题，它假设模型已经具备足够的能力。但能力本身受到一个根本性的物理约束：**上下文窗口的长度**。</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>原始 Transformer 的上下文窗口只有 512 个 token。BERT 继承了这个设计，GPT-2 扩展到 1024，GPT-3 达到 2048。这些长度在早期已经足够——一篇新闻文章、一段对话、一个代码片段通常都能装得下。但随着大语言模型从研究走向应用，512 到 2048 token 的限制越来越成为瓶颈。</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>想象一下这些场景：你想让模型阅读一份 50 页的合同并回答问题，但 50 页文本大约是 25000 个 token——远超任何早期模型的上下文限制。你想让模型理解一个代码库来帮你调试，但一个中等规模的项目可能有上百个文件、几十万行代码。你想与模型进行长时间的多轮对话，但累积的对话历史很快就会撑爆上下文窗口。你想用 RAG（检索增强生成）来让模型访问外部知识，但如果只能放入几个检索片段，模型能获取的信息就非常有限。</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>**上下文长度不只是一个工程限制，它直接决定了模型能做什么任务、做得多好**。</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>扩展上下文窗口的挑战是多维的。最直接的瓶颈是**计算复杂度**：标准自注意力的时间和空间复杂度都是 $O(n^2)$，其中 $n$ 是序列长度。将上下文从 2K 扩展到 128K 意味着计算量增加 4096 倍——这不是简单堆砌更多 GPU 能解决的问题。</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>第二个挑战是**位置编码的外推**。Transformer 需要知道 token 的位置信息，而大多数位置编码方案在训练时只见过有限长度的序列。当推理时遇到比训练长度更长的序列时，位置编码可能完全失效——模型对超出训练范围的位置"一无所知"。</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>第三个挑战是**推理时的内存管理**。在自回归生成时，每生成一个 token 都需要访问之前所有 token 的 key-value 缓存（KV cache）。对于 128K 上下文的 70B 模型，仅 KV cache 就可能占用超过 100GB 显存，这比模型参数本身还大。</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>这三个挑战分别对应本章要讨论的三类技术：**位置编码的演进与外推**解决"模型能否理解长距离位置关系"的问题；**高效注意力算法**解决 $O(n^2)$ 计算瓶颈；**KV cache 优化**解决推理时的内存管理。</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：突破长上下文瓶颈需要算法和系统的协同设计。位置编码决定了模型"能否"处理长序列，高效注意力决定了"能否快速"处理，KV cache 管理决定了"能否在有限资源下"部署。这三者缺一不可——只优化其中一个维度而忽略其他，整体系统仍然无法工作。</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### $O(n^2)$ 的诅咒</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>让我们从最根本的瓶颈开始：自注意力的二次复杂度。</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>在标准 Transformer 中，自注意力的计算包括三步：首先用 $Q$ 和 $K$ 计算注意力分数矩阵 $QK^\top$，这是一个 $n \times n$ 的矩阵；然后对每一行做 softmax 归一化；最后用归一化后的注意力权重与 $V$ 相乘得到输出。</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>这里的关键是 $QK^\top$：它需要计算 $n \times n$ 个点积，每个点积涉及 $d_k$ 维向量。总计算量是 $O(n^2 d_k)$，空间复杂度也是 $O(n^2)$（需要存储完整的注意力矩阵）。</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>当 $n = 512$ 时，$n^2 = 262144$，完全可控。当 $n = 8192$ 时，$n^2 = 67M$，开始吃力但仍然可行。当 $n = 131072$（128K）时，$n^2 = 17B$——单个注意力矩阵就有 170 亿个元素！</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>更糟糕的是，Transformer 通常有几十层，每层都有多个注意力头。一个 70B 参数的模型可能有 80 层、64 个头。如果每层的每个头都要计算并存储完整的 $n \times n$ 注意力矩阵，内存需求会变得完全不可承受。</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a><span class="fu">### 位置编码的困境</span></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>Transformer 的另一个根本问题是它对位置信息的处理。与 RNN 不同——RNN 天然地按顺序处理序列，位置信息隐含在计算顺序中——Transformer 的自注意力是"位置无关"的：如果我们打乱输入序列的顺序，只要位置编码也相应打乱，注意力计算的结果完全相同。</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>这意味着 Transformer 必须**显式地**将位置信息注入到模型中。原始 Transformer 使用的是正弦位置编码：</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>PE_{(pos, 2i)} &amp;= \sin\left(\frac{pos}{10000^{2i/d}}\right) <span class="sc">\\</span></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>PE_{(pos, 2i+1)} &amp;= \cos\left(\frac{pos}{10000^{2i/d}}\right)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>这个设计有一个美妙的性质：对于任意固定的偏移量 $k$，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性变换。理论上，这应该让模型能够学习到相对位置关系。</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>但正弦编码有一个致命的问题：**它在训练长度之外的表现是未定义的**。如果模型只在 2048 长度的序列上训练，它对 $pos = 3000$ 或 $pos = 10000$ 的位置编码完全没有见过。虽然数学上我们可以计算出 $\sin(3000/10000^{2i/d})$ 的值，但模型的注意力权重是否能正确利用这些从未见过的位置编码值，完全没有保证。</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>实验表明，当序列长度超过训练长度时，模型的困惑度（perplexity）会急剧上升，甚至完全失效。这被称为**长度外推失败**（length extrapolation failure）。</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="fu">### 推理时的 KV Cache 膨胀</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>第三个瓶颈出现在推理阶段，特别是自回归生成时。</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>当模型生成第 $t$ 个 token 时，它需要计算当前 token 与之前所有 token 的注意力。这需要访问位置 $1, 2, \ldots, t-1$ 的 key 和 value 向量。如果每次都重新计算这些向量，计算量会随着生成长度二次增长。</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>标准的优化是**KV cache**：在生成每个 token 时，将该位置的 key 和 value 存储起来，后续 token 生成时直接复用。这将计算复杂度从 $O(t^2)$ 降到 $O(t)$，但代价是需要存储越来越大的 cache。</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>对于一个 70B 模型（假设 80 层、64 个头、head dimension = 128），128K 上下文的 KV cache 大小为：</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>\text{KV cache size} = 2 \times 80 \times 64 \times 128K \times 128 \times 2 \text{ bytes (BF16)} = 167 \text{ GB}</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>这比模型参数本身（70B × 2 bytes = 140 GB）还要大！而且这只是单个请求的 KV cache。在生产环境中，一个 GPU 需要同时服务多个请求，KV cache 的内存管理成为关键瓶颈。</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>综合以上分析，理想的长上下文解决方案需要在三个维度上同时突破。</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>**位置编码层面**：我们需要一种位置编码方案，它不仅能编码位置信息，还能在训练长度之外保持有效——即具有良好的**长度外推**能力。更理想的情况是，位置编码本身就编码了相对位置关系，而不是绝对位置。</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>**计算效率层面**：我们需要打破 $O(n^2)$ 的瓶颈。可能的方向包括：稀疏注意力（只计算部分位置对的注意力）、近似注意力（用某种近似算法降低复杂度）、或者更聪明的精确注意力实现（通过算法和硬件的协同设计降低实际运行时间）。</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>**内存管理层面**：我们需要更高效的 KV cache 管理策略，能够在有限显存中支持更长的上下文和更多的并发请求。这可能涉及内存池化、分页管理、或者减少每个位置需要存储的信息量。</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>接下来的几节，我们将逐一介绍这三个维度的关键技术。</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a><span class="fu">## 位置编码的演进</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>位置编码的演进是一个从"绝对"到"相对"、从"加性"到"乘性"的过程。理解这个演进脉络，是理解现代长上下文模型的基础。</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a><span class="fu">### 绝对位置编码的局限</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>原始 Transformer 的正弦位置编码是**绝对位置编码**的代表——每个位置有一个固定的编码向量，直接加到 token embedding 上：</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_i' = \mathbf{x}_i + \mathbf{PE}_i</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>这种设计的问题在于，模型需要**隐式地**从绝对位置中学习相对位置关系。当位置 3 需要关注位置 1 时，模型需要从 $\mathbf{PE}_3$ 和 $\mathbf{PE}_1$ 的某种组合中推断出"距离为 2"的信息。这个推断在训练范围内可能有效，但在训练范围外就完全失效了。</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>另一种绝对位置编码是**可学习位置编码**（learnable positional embeddings），这是 BERT 和 GPT 系列采用的方法。它为每个位置分配一个可学习的向量 $\mathbf{E}_i$。这种方法更灵活——模型可以学到任意的位置模式——但问题更严重：如果训练长度是 512，位置 513 的编码根本不存在。</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a><span class="fu">### 相对位置编码的尝试</span></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>相对位置编码的核心思想是：**位置信息应该编码位置之间的相对距离，而不是绝对位置**。</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>Shaw et al. (2018) 提出的一种方案是在计算注意力时，为每个位置对 $(i, j)$ 添加一个依赖于相对距离 $i - j$ 的偏置项：</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>a_{ij} = \frac{(\mathbf{q}_i)^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{r}_{i-j}}{\sqrt{d_k}}</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{r}_{i-j}$ 是相对位置 $i-j$ 对应的可学习向量。这种设计更直接地编码了相对位置信息，但引入了额外的参数和计算。</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>Transformer-XL 进一步发展了这个想法，提出了一种可以处理超长序列的相对位置编码方案。但这些早期的相对位置编码方案都有各自的问题：要么计算开销大，要么实现复杂，要么外推能力有限。</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a><span class="fu">### RoPE：旋转位置编码</span></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>2021 年，Su 等人提出的 **RoPE（Rotary Position Embedding）** 成为了位置编码的一个重要突破。RoPE 的核心洞察是：**可以用旋转矩阵来编码位置信息，使得两个位置的内积自然地包含它们的相对位置**。</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="al">![RoPE 的核心思想：将位置编码为旋转角度。不同维度使用不同的旋转频率（图中颜色深浅表示频率），低维度高频（捕捉短程关系），高维度低频（捕捉长程关系）。](figures/chapter-26/original/fig-rope-visualization.png)</span>{#fig-rope-visualization width=90%}</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>*Source: Su et al. (2021) "RoFormer: Enhanced Transformer with Rotary Position Embedding", Figure 1-2. [arXiv:2104.09864](https://arxiv.org/abs/2104.09864)*</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>让我们从一个简单的二维情况开始建立直觉。假设 query 向量 $\mathbf{q}$ 和 key 向量 $\mathbf{k}$ 都是二维的。RoPE 的做法是将它们各自旋转一个与位置相关的角度：</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>\mathbf{q}_m' = \begin{pmatrix} \cos m\theta &amp; -\sin m\theta <span class="sc">\\</span> \sin m\theta &amp; \cos m\theta \end{pmatrix} \mathbf{q}_m, \quad</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>\mathbf{k}_n' = \begin{pmatrix} \cos n\theta &amp; -\sin n\theta <span class="sc">\\</span> \sin n\theta &amp; \cos n\theta \end{pmatrix} \mathbf{k}_n</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>其中 $m$ 和 $n$ 是 query 和 key 的位置，$\theta$ 是一个预设的基础角频率。</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>现在，当我们计算旋转后的内积时：</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>\mathbf{q}_m'^{\top} \mathbf{k}_n' = \mathbf{q}_m^\top R(\theta)^{m \top} R(\theta)^n \mathbf{k}_n = \mathbf{q}_m^\top R(\theta)^{n-m} \mathbf{k}_n</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>这里 $R(\theta)$ 是旋转矩阵，$R(\theta)^\top = R(-\theta)$。关键的观察是：**内积只依赖于相对位置 $n - m$**！位置 $m$ 和 $n$ 的绝对值消失了，只剩下它们的差。</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>这个性质让 RoPE 自然地具有了相对位置编码的特性，同时没有引入额外的参数或复杂的计算。</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>对于高维情况（比如 $d = 128$），RoPE 将维度两两分组，每组使用不同的基础频率 $\theta_i$：</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>\theta_i = 10000^{-2i/d}, \quad i = 0, 1, \ldots, d/2 - 1</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>低维度使用高频率（变化快），高维度使用低频率（变化慢）。这种多频率设计让模型能够同时捕捉短程和长程的位置关系。</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: RoPE 应用于 Query/Key 向量 (Su et al., 2021)</span></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rotary_pos_emb(q, k, cos, sin, position_ids):</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a><span class="co">    对 query 和 key 应用旋转位置编码</span></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a><span class="co">        q, k: [batch, seq_len, num_heads, head_dim] - query 和 key 向量</span></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a><span class="co">        cos, sin: [max_seq_len, head_dim] - 预计算的 cos/sin 值</span></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a><span class="co">        position_ids: [batch, seq_len] - 每个 token 的位置</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a><span class="co">        q_embed, k_embed: 应用 RoPE 后的 query 和 key</span></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 根据 position_ids 获取对应的 cos/sin 值</span></span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>    cos <span class="op">=</span> cos[position_ids]  <span class="co"># [batch, seq_len, head_dim]</span></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>    sin <span class="op">=</span> sin[position_ids]  <span class="co"># [batch, seq_len, head_dim]</span></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 将 head_dim 分成两半</span></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>    q1, q2 <span class="op">=</span> q[..., :head_dim<span class="op">//</span><span class="dv">2</span>], q[..., head_dim<span class="op">//</span><span class="dv">2</span>:]</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>    k1, k2 <span class="op">=</span> k[..., :head_dim<span class="op">//</span><span class="dv">2</span>], k[..., head_dim<span class="op">//</span><span class="dv">2</span>:]</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 应用旋转变换: (x1, x2) -&gt; (x1*cos - x2*sin, x1*sin + x2*cos)</span></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>    q_embed <span class="op">=</span> torch.cat([q1<span class="op">*</span>cos <span class="op">-</span> q2<span class="op">*</span>sin, q1<span class="op">*</span>sin <span class="op">+</span> q2<span class="op">*</span>cos], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>    k_embed <span class="op">=</span> torch.cat([k1<span class="op">*</span>cos <span class="op">-</span> k2<span class="op">*</span>sin, k1<span class="op">*</span>sin <span class="op">+</span> k2<span class="op">*</span>cos], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_embed, k_embed</span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>*Source: Su et al. (2021) "RoFormer: Enhanced Transformer with Rotary Position Embedding", Section 3.4. [arXiv:2104.09864](https://arxiv.org/abs/2104.09864)*</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：RoPE 的旋转计算</span></span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的数值例子来验证 RoPE 的核心性质：**内积只依赖相对位置**。</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>**设定**：为了简化，我们使用 4 维向量（2 组，每组 2 维），基础频率 $\theta_0 = 1.0$，$\theta_1 = 0.01$。</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>假设：</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Query 向量 $\mathbf{q} = <span class="co">[</span><span class="ot">1.0, 0.0, 1.0, 0.0</span><span class="co">]</span>$，位于位置 $m = 3$</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Key 向量 $\mathbf{k} = <span class="co">[</span><span class="ot">0.5, 0.5, 0.5, 0.5</span><span class="co">]</span>$，位于位置 $n = 5$</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>**Step 1: 计算旋转角度**</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>对于位置 $m = 3$：</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>\text{角度}_0 &amp;= m \cdot \theta_0 = 3 \times 1.0 = 3.0 \text{ rad} <span class="sc">\\</span></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>\text{角度}_1 &amp;= m \cdot \theta_1 = 3 \times 0.01 = 0.03 \text{ rad}</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>对于位置 $n = 5$：</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>\text{角度}_0 &amp;= n \cdot \theta_0 = 5 \times 1.0 = 5.0 \text{ rad} <span class="sc">\\</span></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>\text{角度}_1 &amp;= n \cdot \theta_1 = 5 \times 0.01 = 0.05 \text{ rad}</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>**Step 2: 计算 cos 和 sin 值**</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>位置 $m = 3$：</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>\cos(3.0) &amp;\approx -0.990, \quad \sin(3.0) \approx 0.141 <span class="sc">\\</span></span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>\cos(0.03) &amp;\approx 1.000, \quad \sin(0.03) \approx 0.030</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>位置 $n = 5$：</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>\cos(5.0) &amp;\approx 0.284, \quad \sin(5.0) \approx -0.959 <span class="sc">\\</span></span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>\cos(0.05) &amp;\approx 0.999, \quad \sin(0.05) \approx 0.050</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>**Step 3: 应用旋转变换**</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>对 query（位置 3），将维度分成两组 $<span class="co">[</span><span class="ot">q_0, q_1</span><span class="co">]</span>$ 和 $<span class="co">[</span><span class="ot">q_2, q_3</span><span class="co">]</span>$：</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>q'_0 &amp;= q_0 \cos(3.0) - q_1 \sin(3.0) = 1.0 \times (-0.990) - 0.0 \times 0.141 = -0.990 <span class="sc">\\</span></span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>q'_1 &amp;= q_0 \sin(3.0) + q_1 \cos(3.0) = 1.0 \times 0.141 + 0.0 \times (-0.990) = 0.141 <span class="sc">\\</span></span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>q'_2 &amp;= q_2 \cos(0.03) - q_3 \sin(0.03) = 1.0 \times 1.000 - 0.0 \times 0.030 = 1.000 <span class="sc">\\</span></span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>q'_3 &amp;= q_2 \sin(0.03) + q_3 \cos(0.03) = 1.0 \times 0.030 + 0.0 \times 1.000 = 0.030</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>\mathbf{q}'_3 = <span class="co">[</span><span class="ot">-0.990, 0.141, 1.000, 0.030</span><span class="co">]</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>类似地，对 key（位置 5）：</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>k'_0 &amp;= 0.5 \times 0.284 - 0.5 \times (-0.959) = 0.142 + 0.480 = 0.622 <span class="sc">\\</span></span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>k'_1 &amp;= 0.5 \times (-0.959) + 0.5 \times 0.284 = -0.480 + 0.142 = -0.338 <span class="sc">\\</span></span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a>k'_2 &amp;= 0.5 \times 0.999 - 0.5 \times 0.050 = 0.500 - 0.025 = 0.475 <span class="sc">\\</span></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>k'_3 &amp;= 0.5 \times 0.050 + 0.5 \times 0.999 = 0.025 + 0.500 = 0.525</span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>\mathbf{k}'_5 = <span class="co">[</span><span class="ot">0.622, -0.338, 0.475, 0.525</span><span class="co">]</span></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>**Step 4: 计算旋转后的内积**</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>\mathbf{q}'_3 \cdot \mathbf{k}'_5 = (-0.990)(0.622) + (0.141)(-0.338) + (1.000)(0.475) + (0.030)(0.525)</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>= -0.616 - 0.048 + 0.475 + 0.016 = -0.173</span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>**Step 5: 验证——相对位置不变性**</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>现在让我们验证核心性质：**如果保持相对位置 $n - m = 2$ 不变，内积也不变**。</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>取位置 $m' = 10$，$n' = 12$（同样相差 2），用相同的 $\mathbf{q}$ 和 $\mathbf{k}$：</span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>经过类似计算（过程略），可以验证：</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a>\mathbf{q}'_{10} \cdot \mathbf{k}'_{12} \approx -0.173</span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>**结论**：内积的值只依赖相对位置 $(n - m) = 2$，而与绝对位置 $m$ 和 $n$ 无关。这正是 RoPE 的核心数学性质！</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **直觉解释**：RoPE 将位置编码为旋转角度。位置 3 的 query 旋转了 $3\theta$，位置 5 的 key 旋转了 $5\theta$。内积中，这两个旋转的效果相当于用 $(5-3)\theta = 2\theta$ 旋转——只取决于相对位置差。</span></span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a>RoPE 的成功使它被广泛采用：LLaMA、Mistral、Qwen 等主流开源模型都使用 RoPE 作为位置编码。它的优势在于：</span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**无额外参数**：RoPE 不引入可学习参数，只是对 query/key 的一个固定变换</span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**相对位置天然编码**：内积只依赖相对位置，符合语言的局部性假设</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**实现简单**：只需要逐元素乘法，可以高效地在 GPU 上实现</span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**有一定的外推潜力**：虽然不完美，但比绝对位置编码好得多</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### ALiBi：更简单的替代方案</span></span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a>同年（2021），Press 等人提出了一个更简单的方案：**ALiBi（Attention with Linear Biases）**。</span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>ALiBi 的设计哲学是"极简主义"：它完全不在 embedding 层添加任何位置信息，而是在计算注意力分数时**直接添加一个线性偏置**：</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a>a_{ij} = \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}} - m \cdot |i - j|</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a>其中 $m$ 是一个与注意力头相关的"斜率"参数（不同头使用不同的 $m$），$|i - j|$ 是两个位置之间的距离。这个负偏置的作用是**惩罚远距离的注意力**——距离越远，偏置越负，softmax 后的注意力权重越小。</span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a>ALiBi 的关键洞察是：**语言有很强的局部性**。在大多数情况下，一个词与它附近的词的关系比与远处的词更重要。ALiBi 的线性偏置将这种归纳偏置（inductive bias）直接编码进了模型架构。</span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a>不同的注意力头使用不同的斜率 $m$，形成一个几何序列。有些头的 $m$ 很小，几乎不惩罚远距离（可以捕捉长程依赖）；有些头的 $m$ 很大，强烈偏好局部信息。这种多尺度设计让模型能够同时处理不同范围的依赖关系。</span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a>ALiBi 的最大优势是**外推能力**。由于它只使用相对距离 $|i - j|$，而且偏置的形式（线性惩罚）对任意距离都有定义，ALiBi 可以在远超训练长度的序列上保持合理的性能。Press 等人展示，用 1024 长度训练的 ALiBi 模型可以在 10,000 长度的序列上保持较低的困惑度——而使用正弦位置编码的模型在超过训练长度后就完全失效了。</span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a><span class="fu">### RoPE vs ALiBi：两种哲学</span></span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>RoPE 和 ALiBi 代表了两种不同的设计哲学：</span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a>**RoPE** 的哲学是"编码位置信息，让模型自己学习如何利用"。它通过旋转变换将位置信息注入 query 和 key，然后让标准的注意力机制处理剩下的事情。这种设计更"中性"——它不强加任何关于位置重要性的假设。</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>**ALiBi** 的哲学是"直接编码我们对语言的先验知识"。它通过线性偏置显式地告诉模型"近处比远处重要"。这种设计更"有主张"——它假设语言的局部性是普遍的。</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>在实践中，两者都被广泛使用。LLaMA、Mistral、Qwen 等使用 RoPE；BLOOM、MPT 等使用 ALiBi。经验上，两者在正常训练长度内表现相当，但在长度外推时 ALiBi 通常更稳定。不过，RoPE 结合后面要讨论的外推技术（如 Position Interpolation），可以达到更长的上下文。</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a><span class="fu">## 长度外推技术</span></span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>即使使用了 RoPE 这样具有相对位置编码特性的方案，直接在超出训练长度的序列上推理仍然会出问题。原因是：虽然 RoPE 的数学形式对任意位置都有定义，但模型的注意力权重是在特定的位置范围内训练的。当遇到从未见过的位置值时，模型的行为是未定义的。</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>这催生了一系列**长度外推**（length extrapolation）或**上下文窗口扩展**（context window extension）技术。这些技术的目标是：**给定一个在短上下文（如 4K）上训练好的模型，通过某种修改，使其能够处理长得多的上下文（如 128K）**。</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a><span class="fu">### Position Interpolation：线性缩放</span></span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>2023 年，Meta 的 Chen 等人提出了一个惊人简单的方法：**Position Interpolation（PI，位置插值）**。</span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>PI 的核心思想是：与其让模型**外推**到未见过的位置（extrapolation），不如将长序列的位置**插值**到训练范围内（interpolation）。</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>具体来说，如果模型在长度 $L$ 的序列上训练，现在想处理长度 $L'$ 的序列（$L' &gt; L$），PI 将位置索引线性缩放：</span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>\text{position}_\text{new} = \text{position}_\text{old} \times \frac{L}{L'}</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>例如，如果模型在 4K 上训练，现在想处理 32K 的序列，位置 32000 会被映射到 $32000 \times \frac{4096}{32768} = 4000$——仍然在训练范围内。</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>这个方法看似简单，但效果惊人。Chen 等人的理论分析表明，插值的上界比外推小约 600 倍，这解释了为什么插值比外推稳定得多。</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a>在实践中，PI 需要少量的微调（通常几百到几千步）来让模型适应新的位置缩放。但与从头训练一个长上下文模型相比，这个代价微乎其微——只需原始预训练计算量的约 0.1%。</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>Chen 等人成功地将 LLaMA 7B-65B 的上下文窗口从 2K 扩展到 32K，同时保持了在原始上下文范围内的性能。</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a><span class="fu">### NTK-aware Scaling：保护高频信息</span></span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>PI 虽然有效，但有一个微妙的问题：它均匀地缩放所有维度的频率。回想 RoPE 中，不同维度使用不同的基础频率：</span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>\theta_i = 10000^{-2i/d}</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>低维度（小的 $i$）使用高频率，编码短程位置关系；高维度（大的 $i$）使用低频率，编码长程位置关系。</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>当 PI 将所有频率除以相同的因子 $s = L'/L$ 时，高频维度受到的影响最大。原本用来区分相邻位置的高频信息被"压缩"了，模型可能丢失对局部位置关系的精确感知。</span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>**NTK-aware interpolation** 试图解决这个问题。它的灵感来自神经正切核（Neural Tangent Kernel）理论：深度网络很难学习高频信息，除非输入 embedding 本身包含高频成分。</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a>NTK-aware 的方法是修改 RoPE 的基础频率：</span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a>\theta_i' = 10000^{-2i/d} \times s^{2i/(d-2)}</span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>这个设计让低频维度（大的 $i$）承担更多的缩放，而高频维度（小的 $i$）几乎不缩放。直觉上，这保护了模型对短程位置关系的编码能力。</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a><span class="fu">### YaRN：统一框架</span></span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a>2023 年，Peng 等人提出的 **YaRN（Yet another RoPE extensioN）** 是目前最先进的 RoPE 外推方法之一。它综合了多种技术，形成了一个统一的框架。</span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a>YaRN 的核心组件包括：</span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a>**1. NTK-by-parts interpolation**：将 RoPE 的维度分成三组：</span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>低维度（高频）：不做任何插值（$\lambda = 1$），完全保留</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>中维度：渐进式插值</span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>高维度（低频）：完全使用 PI 插值（$\lambda = s$）</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>这种设计既保护了高频信息，又让低频维度能够编码更长的相对位置。</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>**2. 注意力缩放**：直接将注意力 logits 除以一个与上下文扩展比例相关的因子。这补偿了位置编码缩放后 softmax 分布可能变得过于"尖锐"的问题。</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a>**3. 极短微调**：YaRN 只需要约 400 步微调（原始预训练的 ~0.1%），就能将上下文窗口扩展 8–16 倍。</span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>YaRN 在多个基准测试上显著优于 PI 和 NTK-aware 方法。更重要的是，它展示了**外推**能力：在 64K 上微调的 YaRN 模型，可以在 128K 上保持递减的困惑度——这意味着它真正学会了处理更长的上下文，而不仅仅是"不崩溃"。</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a><span class="fu">### 外推技术对比</span></span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 核心思想 <span class="pp">|</span> 需要微调？ <span class="pp">|</span> 外推能力 <span class="pp">|</span> 计算开销 <span class="pp">|</span></span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|-----------|---------|---------|</span></span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 原始 RoPE <span class="pp">|</span> 旋转编码 <span class="pp">|</span> 否 <span class="pp">|</span> 差 <span class="pp">|</span> 无 <span class="pp">|</span></span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Position Interpolation <span class="pp">|</span> 线性缩放位置 <span class="pp">|</span> 是（~1K 步） <span class="pp">|</span> 中 <span class="pp">|</span> 无 <span class="pp">|</span></span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> NTK-aware <span class="pp">|</span> 保护高频维度 <span class="pp">|</span> 可选 <span class="pp">|</span> 中 <span class="pp">|</span> 无 <span class="pp">|</span></span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> YaRN <span class="pp">|</span> NTK-by-parts + 注意力缩放 <span class="pp">|</span> 是（~400 步） <span class="pp">|</span> 好 <span class="pp">|</span> 无 <span class="pp">|</span></span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a><span class="fu">## Flash Attention：IO 感知的算法革命</span></span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>前面讨论的位置编码技术解决了"模型能否理解长距离位置关系"的问题，但没有触及 $O(n^2)$ 计算复杂度这个根本瓶颈。接下来我们讨论一个从完全不同角度突破这个瓶颈的技术：**FlashAttention**。</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPU 内存层次结构</span></span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a>要理解 FlashAttention 的精妙之处，首先需要理解现代 GPU 的内存层次结构。</span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a>GPU 有两种主要的存储：**HBM（High Bandwidth Memory）** 和 **SRAM（Static RAM）**。HBM 是 GPU 的主内存，容量大（如 A100 的 80GB），但带宽相对较低（约 2TB/s）。SRAM 是片上缓存，容量很小（A100 约 20MB），但带宽极高（约 19TB/s）。</span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a>关键的洞察是：**GPU 计算单元的速度远超 HBM 的带宽**。A100 的 tensor core 理论浮点运算峰值是 312 TFLOPS（FP16），而从 HBM 读取数据的带宽只有 2TB/s。这意味着如果每次计算都需要从 HBM 读数据，计算单元会大量闲置，等待数据到来。</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>标准注意力实现的问题正是如此：它需要将完整的 $n \times n$ 注意力矩阵写入 HBM（用于 softmax），然后再读出来（用于乘以 $V$）。对于长序列，这个矩阵巨大无比，HBM 的读写成为严重的瓶颈。</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a><span class="fu">### FlashAttention 的 Tiling 策略</span></span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a><span class="al">![FlashAttention 的核心创新：通过分块（tiling）计算，让数据留在快速的 SRAM 中，避免将完整的 $n \times n$ 注意力矩阵写入慢速的 HBM。左图展示了 GPU 内存层次结构，右图展示了分块计算的流程。](figures/chapter-26/original/fig-flash-attention-tiling.png)</span>{#fig-flash-attention-tiling width=95%}</span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a>*Source: Dao et al. (2022) "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", Figure 1. [arXiv:2205.14135](https://arxiv.org/abs/2205.14135)*</span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a>FlashAttention 的核心思想是：**通过分块（tiling）计算，让数据尽可能留在快速的 SRAM 中，避免频繁地访问 HBM**。</span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a>具体来说，FlashAttention 将 $Q$、$K$、$V$ 矩阵分成小块，每次只将一小块加载到 SRAM 中进行计算。关键的技巧是：softmax 可以通过"在线"算法增量计算，而不需要一次性看到整行的所有元素。</span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 2: FlashAttention 核心算法 (Dao et al., 2022)</span></span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flash_attention(Q, K, V, block_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a><span class="co">    IO-aware 的精确注意力计算</span></span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a><span class="co">    核心思想：</span></span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a><span class="co">    1. 将 Q, K, V 分成小块</span></span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a><span class="co">    2. 在 SRAM 中计算每个块对的注意力</span></span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a><span class="co">    3. 使用在线 softmax 算法，增量更新输出</span></span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a>    seq_len, d <span class="op">=</span> Q.shape</span>
<span id="cb4-495"><a href="#cb4-495" aria-hidden="true" tabindex="-1"></a>    O <span class="op">=</span> torch.zeros_like(Q)  <span class="co"># 输出</span></span>
<span id="cb4-496"><a href="#cb4-496" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> torch.zeros(seq_len)  <span class="co"># softmax 归一化因子</span></span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> torch.full((seq_len,), <span class="op">-</span><span class="bu">float</span>(<span class="st">'inf'</span>))  <span class="co"># 行最大值（用于数值稳定）</span></span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 外层循环：遍历 K, V 的块</span></span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, seq_len, block_size):</span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a>        K_j <span class="op">=</span> K[j:j<span class="op">+</span>block_size]  <span class="co"># 从 HBM 加载到 SRAM</span></span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a>        V_j <span class="op">=</span> V[j:j<span class="op">+</span>block_size]</span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 内层循环：遍历 Q 的块</span></span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, seq_len, block_size):</span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a>            Q_i <span class="op">=</span> Q[i:i<span class="op">+</span>block_size]  <span class="co"># 从 HBM 加载到 SRAM</span></span>
<span id="cb4-507"><a href="#cb4-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-508"><a href="#cb4-508" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 在 SRAM 中计算局部注意力分数</span></span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a>            S_ij <span class="op">=</span> Q_i <span class="op">@</span> K_j.T <span class="op">/</span> sqrt(d)  <span class="co"># [block_size, block_size]</span></span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 在线 softmax 更新</span></span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a>            m_new <span class="op">=</span> torch.<span class="bu">max</span>(m[i:i<span class="op">+</span>block_size], S_ij.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 更新归一化因子和输出（数学细节见论文）</span></span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a>            <span class="co"># ... (涉及 exp 缩放和累加)</span></span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> O  <span class="co"># 直接写回 HBM，无需存储完整注意力矩阵</span></span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a>*Source: Dao et al. (2022) "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", Algorithm 1. [arXiv:2205.14135](https://arxiv.org/abs/2205.14135)*</span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a>FlashAttention 的关键特性：</span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**精确计算**：与标准注意力的结果完全相同，没有任何近似</span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**内存高效**：不需要存储 $O(n^2)$ 的注意力矩阵，内存复杂度降为 $O(n)$</span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**速度快**：尽管理论 FLOP 数可能更多（因为需要重复计算一些值），但由于减少了 HBM 访问，实际运行时间反而更短</span>
<span id="cb4-528"><a href="#cb4-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-529"><a href="#cb4-529" aria-hidden="true" tabindex="-1"></a>Dao 等人的实验表明，FlashAttention 在 A100 GPU 上比 PyTorch 标准实现快 2–4 倍，同时内存使用减少到原来的 1/10 到 1/20。</span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a><span class="fu">### FlashAttention-2：更好的并行</span></span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a>FlashAttention-2 (Dao, 2023) 进一步优化了并行策略和工作分配。</span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-535"><a href="#cb4-535" aria-hidden="true" tabindex="-1"></a>FlashAttention 原版的一个问题是：它在 GPU 的不同线程块（thread blocks）和 warps 之间的工作分配不够均衡。FlashAttention-2 通过以下改进解决这个问题：</span>
<span id="cb4-536"><a href="#cb4-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-537"><a href="#cb4-537" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**减少非 matmul 操作**：将更多计算转换为矩阵乘法，以充分利用 tensor core</span>
<span id="cb4-538"><a href="#cb4-538" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**增加 occupancy**：即使对于单个注意力头，也能在多个 thread blocks 之间并行</span>
<span id="cb4-539"><a href="#cb4-539" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**优化 warp 间通信**：减少通过 shared memory 的数据交换</span>
<span id="cb4-540"><a href="#cb4-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-541"><a href="#cb4-541" aria-hidden="true" tabindex="-1"></a>这些优化让 FlashAttention-2 比 FlashAttention 又快了约 2 倍，达到 A100 理论峰值性能的 50–73%——这已经非常接近矩阵乘法的效率了。</span>
<span id="cb4-542"><a href="#cb4-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-543"><a href="#cb4-543" aria-hidden="true" tabindex="-1"></a><span class="fu">### Flash Attention 的影响</span></span>
<span id="cb4-544"><a href="#cb4-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-545"><a href="#cb4-545" aria-hidden="true" tabindex="-1"></a>FlashAttention 的影响是深远的。它证明了：**算法和硬件的协同设计可以带来巨大的性能提升，即使在理论复杂度没有改变的情况下**。</span>
<span id="cb4-546"><a href="#cb4-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-547"><a href="#cb4-547" aria-hidden="true" tabindex="-1"></a>在 FlashAttention 出现之前，大家普遍认为要突破 $O(n^2)$ 瓶颈，必须使用稀疏注意力或近似注意力。FlashAttention 展示了另一条路：通过让计算更"IO-aware"，精确注意力也可以高效地处理长序列。</span>
<span id="cb4-548"><a href="#cb4-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-549"><a href="#cb4-549" aria-hidden="true" tabindex="-1"></a>如今，FlashAttention 已经成为几乎所有主流 LLM 框架的默认注意力实现。它让 128K 甚至更长的上下文成为可能——不是通过近似，而是通过更聪明的精确计算。</span>
<span id="cb4-550"><a href="#cb4-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-551"><a href="#cb4-551" aria-hidden="true" tabindex="-1"></a><span class="fu">## KV Cache 优化</span></span>
<span id="cb4-552"><a href="#cb4-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-553"><a href="#cb4-553" aria-hidden="true" tabindex="-1"></a>解决了训练时的计算效率问题后，还有一个推理时的关键瓶颈：**KV cache 的内存管理**。</span>
<span id="cb4-554"><a href="#cb4-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-555"><a href="#cb4-555" aria-hidden="true" tabindex="-1"></a><span class="fu">### KV Cache 的必要性</span></span>
<span id="cb4-556"><a href="#cb4-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-557"><a href="#cb4-557" aria-hidden="true" tabindex="-1"></a>在自回归生成时，模型每次只生成一个新 token，但需要计算这个 token 与之前所有 token 的注意力。如果每次都重新计算所有 token 的 key 和 value，计算量会随生成长度二次增长。</span>
<span id="cb4-558"><a href="#cb4-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-559"><a href="#cb4-559" aria-hidden="true" tabindex="-1"></a>KV cache 是一个简单但关键的优化：在生成第 $t$ 个 token 时，将该位置的 key 和 value 存储起来。生成第 $t+1$ 个 token 时，只需要计算新 token 的 query，然后与缓存的所有 key 计算注意力分数，最后用注意力权重加权缓存的所有 value。</span>
<span id="cb4-560"><a href="#cb4-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-561"><a href="#cb4-561" aria-hidden="true" tabindex="-1"></a>这将每个 token 的计算复杂度从 $O(t \cdot d)$ 降到 $O(d)$（不考虑注意力计算本身），是自回归推理能够实用的关键。</span>
<span id="cb4-562"><a href="#cb4-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-563"><a href="#cb4-563" aria-hidden="true" tabindex="-1"></a><span class="fu">### KV Cache 的内存挑战</span></span>
<span id="cb4-564"><a href="#cb4-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-565"><a href="#cb4-565" aria-hidden="true" tabindex="-1"></a>但 KV cache 带来了严重的内存压力。对于一个 70B 模型，假设：</span>
<span id="cb4-566"><a href="#cb4-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>80 层</span>
<span id="cb4-567"><a href="#cb4-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>64 个注意力头</span>
<span id="cb4-568"><a href="#cb4-568" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>head dimension = 128</span>
<span id="cb4-569"><a href="#cb4-569" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>BF16 精度（2 bytes）</span>
<span id="cb4-570"><a href="#cb4-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-571"><a href="#cb4-571" aria-hidden="true" tabindex="-1"></a>每个 token 的 KV cache 大小是：</span>
<span id="cb4-572"><a href="#cb4-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-573"><a href="#cb4-573" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-574"><a href="#cb4-574" aria-hidden="true" tabindex="-1"></a>2 \times 80 \times 64 \times 128 \times 2 = 2.6 \text{ MB/token}</span>
<span id="cb4-575"><a href="#cb4-575" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-576"><a href="#cb4-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-577"><a href="#cb4-577" aria-hidden="true" tabindex="-1"></a>对于 128K 上下文，KV cache 总大小是 $128000 \times 2.6 \text{ MB} = 333 \text{ GB}$——远超任何单张 GPU 的显存。</span>
<span id="cb4-578"><a href="#cb4-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-579"><a href="#cb4-579" aria-hidden="true" tabindex="-1"></a>即使是更短的上下文和更小的模型，当需要同时服务多个请求时，KV cache 仍然是主要的内存瓶颈。而且，不同请求的上下文长度不同，导致 KV cache 的大小高度动态——这使得简单的静态内存分配非常低效。</span>
<span id="cb4-580"><a href="#cb4-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-581"><a href="#cb4-581" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Query Attention (MQA) 和 Grouped-Query Attention (GQA)</span></span>
<span id="cb4-582"><a href="#cb4-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-583"><a href="#cb4-583" aria-hidden="true" tabindex="-1"></a>一个减少 KV cache 大小的方法是减少需要缓存的 key/value 头的数量。</span>
<span id="cb4-584"><a href="#cb4-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-585"><a href="#cb4-585" aria-hidden="true" tabindex="-1"></a>**Multi-Query Attention (MQA)** 是 Shazeer (2019) 提出的一种激进方案：所有 query 头共享同一套 key 和 value。这将 KV cache 大小降低为原来的 $1/h$（$h$ 是头数）。</span>
<span id="cb4-586"><a href="#cb4-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-587"><a href="#cb4-587" aria-hidden="true" tabindex="-1"></a>MQA 的问题是可能损失模型质量——不同 query 头共享 key/value，表达能力下降。</span>
<span id="cb4-588"><a href="#cb4-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-589"><a href="#cb4-589" aria-hidden="true" tabindex="-1"></a>**Grouped-Query Attention (GQA)** 是 Ainslie 等人 (2023) 提出的折中方案：将 query 头分成若干组，每组共享一套 key/value。例如，如果有 64 个 query 头，分成 8 组，则只需要 8 套 key/value，KV cache 降为原来的 1/8。</span>
<span id="cb4-590"><a href="#cb4-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-591"><a href="#cb4-591" aria-hidden="true" tabindex="-1"></a>GQA 在 KV cache 节省和模型质量之间取得了很好的平衡。LLaMA 2 70B 和 LLaMA 3 系列都采用了 GQA。实验表明，GQA 模型的质量非常接近标准 MHA，而 KV cache 大小大幅减少。</span>
<span id="cb4-592"><a href="#cb4-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-593"><a href="#cb4-593" aria-hidden="true" tabindex="-1"></a><span class="fu">### PagedAttention：虚拟内存思想的应用</span></span>
<span id="cb4-594"><a href="#cb4-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-595"><a href="#cb4-595" aria-hidden="true" tabindex="-1"></a>2023 年，Kwon 等人提出的 **PagedAttention** 将操作系统的虚拟内存和分页技术引入 KV cache 管理，彻底改变了 LLM 推理的内存效率。</span>
<span id="cb4-596"><a href="#cb4-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-597"><a href="#cb4-597" aria-hidden="true" tabindex="-1"></a><span class="al">![KV Cache 内存浪费问题的可视化：传统方法需要预分配最大可能长度的内存，导致大量浪费（reserved 部分）和碎片化（fragmentation）。实际使用中，大多数请求远未达到最大长度。](figures/chapter-26/original/fig-pagedattention-memory.png)</span>{#fig-pagedattention-memory width=95%}</span>
<span id="cb4-598"><a href="#cb4-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-599"><a href="#cb4-599" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-600"><a href="#cb4-600" aria-hidden="true" tabindex="-1"></a>*Source: Kwon et al. (2023) "Efficient Memory Management for Large Language Model Serving with PagedAttention", Figure 1-2. [arXiv:2309.06180](https://arxiv.org/abs/2309.06180)*</span>
<span id="cb4-601"><a href="#cb4-601" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-602"><a href="#cb4-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-603"><a href="#cb4-603" aria-hidden="true" tabindex="-1"></a>传统的 KV cache 管理面临两个问题：</span>
<span id="cb4-604"><a href="#cb4-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-605"><a href="#cb4-605" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**碎片化**：不同请求的 KV cache 大小不同，动态分配会产生大量内存碎片</span>
<span id="cb4-606"><a href="#cb4-606" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**预分配浪费**：为了避免碎片化，很多系统预分配最大可能长度的内存，但大多数请求实际上不会用到这么多</span>
<span id="cb4-607"><a href="#cb4-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-608"><a href="#cb4-608" aria-hidden="true" tabindex="-1"></a>PagedAttention 的解决方案是将 KV cache 分成固定大小的"页"（blocks），就像操作系统管理内存一样。每个请求的 KV cache 由一系列非连续的页组成，通过一个页表（block table）来跟踪逻辑位置到物理位置的映射。</span>
<span id="cb4-609"><a href="#cb4-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-610"><a href="#cb4-610" aria-hidden="true" tabindex="-1"></a>这种设计带来了三个关键优势：</span>
<span id="cb4-611"><a href="#cb4-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-612"><a href="#cb4-612" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**零碎片**：所有页大小相同，分配和回收不会产生碎片</span>
<span id="cb4-613"><a href="#cb4-613" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**按需分配**：只在实际需要时才分配新页，不预分配</span>
<span id="cb4-614"><a href="#cb4-614" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**高效共享**：如果多个请求有相同的前缀（如相同的 system prompt），可以共享同一套 KV cache 页</span>
<span id="cb4-615"><a href="#cb4-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-616"><a href="#cb4-616" aria-hidden="true" tabindex="-1"></a><span class="al">![PagedAttention 的核心设计：将 KV Cache 分成固定大小的页（blocks），通过 block table 管理逻辑-物理映射。每个请求的 KV Cache 由非连续的页组成，支持动态扩展和内存共享。](figures/chapter-26/original/fig-pagedattention-design.png)</span>{#fig-pagedattention-design width=95%}</span>
<span id="cb4-617"><a href="#cb4-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-618"><a href="#cb4-618" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-619"><a href="#cb4-619" aria-hidden="true" tabindex="-1"></a>*Source: Kwon et al. (2023) "Efficient Memory Management for Large Language Model Serving with PagedAttention", Figure 3. [arXiv:2309.06180](https://arxiv.org/abs/2309.06180)*</span>
<span id="cb4-620"><a href="#cb4-620" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-621"><a href="#cb4-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-622"><a href="#cb4-622" aria-hidden="true" tabindex="-1"></a>基于 PagedAttention，Kwon 等人构建了 **vLLM** 推理引擎。实验表明，vLLM 的吞吐量比 HuggingFace Transformers 高 2–4 倍，比当时最先进的 FasterTransformer 也快 2 倍以上。</span>
<span id="cb4-623"><a href="#cb4-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-624"><a href="#cb4-624" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-625"><a href="#cb4-625" aria-hidden="true" tabindex="-1"></a><span class="fu">## PagedAttention 的核心设计 (Kwon et al., 2023)</span></span>
<span id="cb4-626"><a href="#cb4-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-627"><a href="#cb4-627" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-628"><a href="#cb4-628" aria-hidden="true" tabindex="-1"></a><span class="co"># PagedAttention 的关键数据结构</span></span>
<span id="cb4-629"><a href="#cb4-629" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BlockTable:</span>
<span id="cb4-630"><a href="#cb4-630" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-631"><a href="#cb4-631" aria-hidden="true" tabindex="-1"></a><span class="co">    逻辑 block index -&gt; 物理 block index 的映射</span></span>
<span id="cb4-632"><a href="#cb4-632" aria-hidden="true" tabindex="-1"></a><span class="co">    类似于操作系统的页表</span></span>
<span id="cb4-633"><a href="#cb4-633" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-634"><a href="#cb4-634" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_seq_len, block_size<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb4-635"><a href="#cb4-635" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_size <span class="op">=</span> block_size</span>
<span id="cb4-636"><a href="#cb4-636" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_blocks <span class="op">=</span> (max_seq_len <span class="op">+</span> block_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> block_size</span>
<span id="cb4-637"><a href="#cb4-637" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>] <span class="op">*</span> <span class="va">self</span>.max_blocks  <span class="co"># -1 表示未分配</span></span>
<span id="cb4-638"><a href="#cb4-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-639"><a href="#cb4-639" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> allocate_block(<span class="va">self</span>, logical_idx, physical_idx):</span>
<span id="cb4-640"><a href="#cb4-640" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping[logical_idx] <span class="op">=</span> physical_idx</span>
<span id="cb4-641"><a href="#cb4-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-642"><a href="#cb4-642" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_physical_idx(<span class="va">self</span>, logical_idx):</span>
<span id="cb4-643"><a href="#cb4-643" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mapping[logical_idx]</span>
<span id="cb4-644"><a href="#cb4-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-645"><a href="#cb4-645" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KVCachePool:</span>
<span id="cb4-646"><a href="#cb4-646" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-647"><a href="#cb4-647" aria-hidden="true" tabindex="-1"></a><span class="co">    全局 KV cache 内存池</span></span>
<span id="cb4-648"><a href="#cb4-648" aria-hidden="true" tabindex="-1"></a><span class="co">    所有请求共享，按页分配</span></span>
<span id="cb4-649"><a href="#cb4-649" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-650"><a href="#cb4-650" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_blocks, block_size, num_layers, num_heads, head_dim):</span>
<span id="cb4-651"><a href="#cb4-651" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.free_blocks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(num_blocks))</span>
<span id="cb4-652"><a href="#cb4-652" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 预分配所有物理块的内存</span></span>
<span id="cb4-653"><a href="#cb4-653" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k_cache <span class="op">=</span> torch.zeros(num_blocks, num_layers, num_heads, block_size, head_dim)</span>
<span id="cb4-654"><a href="#cb4-654" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_cache <span class="op">=</span> torch.zeros(num_blocks, num_layers, num_heads, block_size, head_dim)</span>
<span id="cb4-655"><a href="#cb4-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-656"><a href="#cb4-656" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> alloc(<span class="va">self</span>):</span>
<span id="cb4-657"><a href="#cb4-657" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.free_blocks.pop()</span>
<span id="cb4-658"><a href="#cb4-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-659"><a href="#cb4-659" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> free(<span class="va">self</span>, block_idx):</span>
<span id="cb4-660"><a href="#cb4-660" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.free_blocks.append(block_idx)</span>
<span id="cb4-661"><a href="#cb4-661" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-662"><a href="#cb4-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-663"><a href="#cb4-663" aria-hidden="true" tabindex="-1"></a>*Source: Kwon et al. (2023) "Efficient Memory Management for Large Language Model Serving with PagedAttention", Section 4. [arXiv:2309.06180](https://arxiv.org/abs/2309.06180)*</span>
<span id="cb4-664"><a href="#cb4-664" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-665"><a href="#cb4-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-666"><a href="#cb4-666" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ring Attention：分布式长序列</span></span>
<span id="cb4-667"><a href="#cb4-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-668"><a href="#cb4-668" aria-hidden="true" tabindex="-1"></a>当上下文长度达到数十万甚至数百万 token 时，单个 GPU 的内存不再足够。即使使用了所有上述优化，128K 上下文的 70B 模型的 KV cache 仍然需要几十 GB。这时需要**分布式**解决方案。</span>
<span id="cb4-669"><a href="#cb4-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-670"><a href="#cb4-670" aria-hidden="true" tabindex="-1"></a>**Ring Attention** (Liu et al., 2023) 是一种在多 GPU 之间分布式计算超长序列注意力的方法。它的核心思想是：</span>
<span id="cb4-671"><a href="#cb4-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-672"><a href="#cb4-672" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>将长序列分割到多个 GPU 上，每个 GPU 持有序列的一部分</span>
<span id="cb4-673"><a href="#cb4-673" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>GPU 之间形成一个逻辑上的"环"</span>
<span id="cb4-674"><a href="#cb4-674" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>每个 GPU 用 FlashAttention 计算本地的注意力，同时将 key/value 块发送给环中的下一个 GPU</span>
<span id="cb4-675"><a href="#cb4-675" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>通过重叠计算和通信，消除通信开销</span>
<span id="cb4-676"><a href="#cb4-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-677"><a href="#cb4-677" aria-hidden="true" tabindex="-1"></a>关键的洞察是：FlashAttention 的分块计算天然适合这种分布式设计。每个 GPU 在等待下一块 key/value 到来时，可以先处理已有的块。只要计算时间大于通信时间（这在现代高速互联的 GPU 集群上通常成立），Ring Attention 就能实现几乎线性的扩展。</span>
<span id="cb4-678"><a href="#cb4-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-679"><a href="#cb4-679" aria-hidden="true" tabindex="-1"></a>Liu 等人展示，Ring Attention 可以处理**数百万 token** 的上下文——这是单卡方案无法想象的长度。</span>
<span id="cb4-680"><a href="#cb4-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-681"><a href="#cb4-681" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb4-682"><a href="#cb4-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-683"><a href="#cb4-683" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨长上下文技术的理论基础、边界条件和开放问题</span></span>
<span id="cb4-684"><a href="#cb4-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-685"><a href="#cb4-685" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么 RoPE 有外推潜力？</span></span>
<span id="cb4-686"><a href="#cb4-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-687"><a href="#cb4-687" aria-hidden="true" tabindex="-1"></a>RoPE 相比绝对位置编码有更好的外推能力，这不是偶然的。让我们从数学上分析其原因。</span>
<span id="cb4-688"><a href="#cb4-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-689"><a href="#cb4-689" aria-hidden="true" tabindex="-1"></a>在 RoPE 中，位置 $m$ 和 $n$ 的注意力分数只依赖于它们的差 $n - m$：</span>
<span id="cb4-690"><a href="#cb4-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-691"><a href="#cb4-691" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-692"><a href="#cb4-692" aria-hidden="true" tabindex="-1"></a>\mathbf{q}_m'^{\top} \mathbf{k}_n' = f(\mathbf{q}_m, \mathbf{k}_n, n-m)</span>
<span id="cb4-693"><a href="#cb4-693" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-694"><a href="#cb4-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-695"><a href="#cb4-695" aria-hidden="true" tabindex="-1"></a>这意味着模型学习的是"相对距离 $d$ 意味着什么"，而不是"绝对位置 $p$ 意味着什么"。只要相对距离 $d$ 在训练时见过（比如 $d \in <span class="co">[</span><span class="ot">-4096, 4096</span><span class="co">]</span>$），模型就应该能正确处理。</span>
<span id="cb4-696"><a href="#cb4-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-697"><a href="#cb4-697" aria-hidden="true" tabindex="-1"></a>但问题在于：RoPE 的高频维度变化很快，对于非常长的序列，这些维度的值可能进入"从未见过"的区间。Position Interpolation 通过缩放频率来解决这个问题；YaRN 通过选择性地缩放不同维度来更精细地平衡。</span>
<span id="cb4-698"><a href="#cb4-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-699"><a href="#cb4-699" aria-hidden="true" tabindex="-1"></a>一个有趣的理论问题是：**RoPE 外推的理论极限是什么？** 目前没有确切的答案，但实验表明，通过 YaRN 等方法，可以稳定地扩展到训练长度的 16–32 倍。</span>
<span id="cb4-700"><a href="#cb4-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-701"><a href="#cb4-701" aria-hidden="true" tabindex="-1"></a><span class="fu">### FlashAttention 的复杂度分析</span></span>
<span id="cb4-702"><a href="#cb4-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-703"><a href="#cb4-703" aria-hidden="true" tabindex="-1"></a>FlashAttention 的 FLOP 数实际上比标准注意力**更多**，因为它需要在分块计算时重复一些操作。但它的内存访问量大幅减少。</span>
<span id="cb4-704"><a href="#cb4-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-705"><a href="#cb4-705" aria-hidden="true" tabindex="-1"></a>设 $n$ 为序列长度，$d$ 为 head dimension，$M$ 为 SRAM 大小（假设 $M &gt; d$）。标准注意力的 HBM 访问量是 $O(nd + n^2)$（需要读写完整的注意力矩阵）。FlashAttention 的 HBM 访问量是 $O(n^2 d^2 / M)$。</span>
<span id="cb4-706"><a href="#cb4-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-707"><a href="#cb4-707" aria-hidden="true" tabindex="-1"></a>当 $M &gt; nd$（即 SRAM 足够大，可以容纳一块 $Q$ 或 $K$ 或 $V$）时，FlashAttention 的 HBM 访问量是 $O(nd)$——与序列长度**线性**而非二次！</span>
<span id="cb4-708"><a href="#cb4-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-709"><a href="#cb4-709" aria-hidden="true" tabindex="-1"></a>这解释了为什么 FlashAttention 尽管 FLOP 更多，实际运行却更快：在现代 GPU 上，**内存带宽是瓶颈，而非计算能力**。</span>
<span id="cb4-710"><a href="#cb4-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-711"><a href="#cb4-711" aria-hidden="true" tabindex="-1"></a><span class="fu">### 长上下文的失效模式</span></span>
<span id="cb4-712"><a href="#cb4-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-713"><a href="#cb4-713" aria-hidden="true" tabindex="-1"></a>长上下文技术并非万能的。以下是一些已知的失效模式：</span>
<span id="cb4-714"><a href="#cb4-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-715"><a href="#cb4-715" aria-hidden="true" tabindex="-1"></a>**"Lost in the Middle" 现象**：Liu et al. (2023) 发现，当相关信息位于长上下文的中间位置时，模型的检索能力显著下降。模型倾向于关注上下文的开头和结尾，而忽视中间部分。这可能与注意力的分布特性有关。</span>
<span id="cb4-716"><a href="#cb4-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-717"><a href="#cb4-717" aria-hidden="true" tabindex="-1"></a>**困惑度不等于下游性能**：一个模型可能在长序列上有很低的困惑度（作为语言模型表现良好），但在实际任务（如长文档 QA）上表现不佳。困惑度只衡量了模型预测下一个 token 的能力，不等于模型能有效**利用**长上下文的信息。</span>
<span id="cb4-718"><a href="#cb4-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-719"><a href="#cb4-719" aria-hidden="true" tabindex="-1"></a>**训练-推理不匹配**：使用外推技术扩展的模型，在推理长度远超训练长度时，行为可能变得不可预测。YaRN 等方法缓解了这个问题，但没有完全解决。</span>
<span id="cb4-720"><a href="#cb4-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-721"><a href="#cb4-721" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb4-722"><a href="#cb4-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-723"><a href="#cb4-723" aria-hidden="true" tabindex="-1"></a>**无限上下文可能吗？** 当前最长的上下文达到数百万 token（如 Gemini 1.5 的 1M token 窗口），但这仍然是有限的。是否有可能设计一种架构，能够处理任意长度的序列？这可能需要根本性的架构创新，而不仅仅是优化现有的 Transformer。</span>
<span id="cb4-724"><a href="#cb4-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-725"><a href="#cb4-725" aria-hidden="true" tabindex="-1"></a>**长上下文的高效训练**：目前的长上下文模型大多是先在短上下文上预训练，再通过外推技术扩展。是否有更高效的方法来直接在长上下文上训练？</span>
<span id="cb4-726"><a href="#cb4-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-727"><a href="#cb4-727" aria-hidden="true" tabindex="-1"></a>**压缩与检索的权衡**：另一种处理长上下文的思路是不把所有信息都放进上下文，而是用压缩（如将历史对话总结成摘要）或检索（如 RAG）来提供相关信息。这种方法与长上下文有什么本质区别？在什么场景下哪种方法更优？</span>
<span id="cb4-728"><a href="#cb4-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-729"><a href="#cb4-729" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb4-730"><a href="#cb4-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-731"><a href="#cb4-731" aria-hidden="true" tabindex="-1"></a><span class="fu">### 当前技术的局限</span></span>
<span id="cb4-732"><a href="#cb4-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-733"><a href="#cb4-733" aria-hidden="true" tabindex="-1"></a>**计算成本仍然高昂**：尽管 FlashAttention 大幅提升了效率，长上下文推理的成本仍然远高于短上下文。128K 上下文的推理成本大约是 4K 上下文的 30 倍（考虑 $O(n^2)$ 的注意力和 $O(n)$ 的 KV cache）。</span>
<span id="cb4-734"><a href="#cb4-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-735"><a href="#cb4-735" aria-hidden="true" tabindex="-1"></a>**质量随长度下降**：即使困惑度保持稳定，模型在实际任务上的质量通常随上下文长度下降。这可能是因为模型的注意力机制在长序列上变得更加分散。</span>
<span id="cb4-736"><a href="#cb4-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-737"><a href="#cb4-737" aria-hidden="true" tabindex="-1"></a>**工程复杂度**：部署长上下文模型需要大量的系统优化——FlashAttention、PagedAttention、分布式推理等。这增加了工程复杂度和维护成本。</span>
<span id="cb4-738"><a href="#cb4-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-739"><a href="#cb4-739" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb4-740"><a href="#cb4-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-741"><a href="#cb4-741" aria-hidden="true" tabindex="-1"></a>长上下文技术的发展揭示了一个深层的权衡：**上下文长度 vs 效率 vs 质量**。扩展上下文长度通常意味着更高的计算和内存成本，而且不一定带来相应的质量提升。</span>
<span id="cb4-742"><a href="#cb4-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-743"><a href="#cb4-743" aria-hidden="true" tabindex="-1"></a>这引出了下一章要讨论的问题：**是否有更根本的方法来突破 dense Transformer 的效率瓶颈？** Mixture of Experts (MoE) 提供了一种不同的思路：通过稀疏激活，让每个 token 只使用模型参数的一部分，从而在保持模型容量的同时降低计算成本。</span>
<span id="cb4-744"><a href="#cb4-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-745"><a href="#cb4-745" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb4-746"><a href="#cb4-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-747"><a href="#cb4-747" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb4-748"><a href="#cb4-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-749"><a href="#cb4-749" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：大语言模型的上下文长度受限于 $O(n^2)$ 的注意力复杂度、位置编码的外推能力、以及推理时的 KV cache 内存</span>
<span id="cb4-750"><a href="#cb4-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-751"><a href="#cb4-751" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**位置编码演进**：从绝对编码（正弦、可学习）到相对编码（RoPE、ALiBi）。RoPE 通过旋转矩阵自然地编码相对位置；ALiBi 通过线性偏置实现更好的外推</span>
<span id="cb4-752"><a href="#cb4-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-753"><a href="#cb4-753" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**长度外推技术**：Position Interpolation 将长序列位置缩放到训练范围内；YaRN 结合 NTK-aware scaling 和注意力缩放，实现 10× 以上的上下文扩展</span>
<span id="cb4-754"><a href="#cb4-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-755"><a href="#cb4-755" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**FlashAttention**：通过 IO-aware 的分块计算，避免存储完整注意力矩阵，同时保持精确计算。内存从 $O(n^2)$ 降到 $O(n)$，速度提升 2–4×</span>
<span id="cb4-756"><a href="#cb4-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-757"><a href="#cb4-757" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**KV Cache 优化**：GQA 通过共享 key/value 头减少缓存大小；PagedAttention 用虚拟内存思想实现高效的动态内存管理</span>
<span id="cb4-758"><a href="#cb4-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-759"><a href="#cb4-759" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb4-760"><a href="#cb4-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-761"><a href="#cb4-761" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RoPE**：$\mathbf{q}_m'^{\top} \mathbf{k}_n' = f(\mathbf{q}_m, \mathbf{k}_n, n-m)$（只依赖相对位置）</span>
<span id="cb4-762"><a href="#cb4-762" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ALiBi**：$a_{ij} = \mathbf{q}_i^\top \mathbf{k}_j / \sqrt{d_k} - m \cdot |i-j|$</span>
<span id="cb4-763"><a href="#cb4-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Position Interpolation**：$\text{pos}_\text{new} = \text{pos}_\text{old} \times \frac{L_\text{train}}{L_\text{target}}$</span>
<span id="cb4-764"><a href="#cb4-764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**KV cache 大小**：$2 \times \text{layers} \times \text{heads} \times \text{seq<span class="sc">\_</span>len} \times \text{head<span class="sc">\_</span>dim} \times \text{bytes}$</span>
<span id="cb4-765"><a href="#cb4-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-766"><a href="#cb4-766" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb4-767"><a href="#cb4-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-768"><a href="#cb4-768" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 解释 RoPE 和 ALiBi 的设计哲学有何不同？在什么场景下你会选择哪一种？</span>
<span id="cb4-769"><a href="#cb4-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-770"><a href="#cb4-770" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明 RoPE 中两个位置的内积只依赖于它们的相对位置。提示：利用旋转矩阵的性质 $R(\theta)^\top R(\phi) = R(\phi - \theta)$。</span>
<span id="cb4-771"><a href="#cb4-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-772"><a href="#cb4-772" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用 vLLM 部署一个支持 32K 上下文的 LLaMA 模型，测量不同上下文长度下的吞吐量和延迟。观察 PagedAttention 的内存使用效率。</span>
<span id="cb4-773"><a href="#cb4-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-774"><a href="#cb4-774" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** 长上下文和 RAG 都是让模型访问更多信息的方法。讨论它们的权衡：什么时候应该扩展上下文窗口？什么时候应该使用检索？</span>
<span id="cb4-775"><a href="#cb4-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-776"><a href="#cb4-776" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-777"><a href="#cb4-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-778"><a href="#cb4-778" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb4-779"><a href="#cb4-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-780"><a href="#cb4-780" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb4-781"><a href="#cb4-781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RoPE (Su et al., 2021)**：旋转位置编码的原始论文</span>
<span id="cb4-782"><a href="#cb4-782" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（数学推导）</span>
<span id="cb4-783"><a href="#cb4-783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FlashAttention (Dao et al., 2022)**：IO-aware 注意力的开创性工作</span>
<span id="cb4-784"><a href="#cb4-784" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（算法）、Section 4（复杂度分析）</span>
<span id="cb4-785"><a href="#cb4-785" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PagedAttention (Kwon et al., 2023)**：KV cache 管理的革命</span>
<span id="cb4-786"><a href="#cb4-786" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3-4（设计）、Section 6（实验）</span>
<span id="cb4-787"><a href="#cb4-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-788"><a href="#cb4-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### 外推技术</span></span>
<span id="cb4-789"><a href="#cb4-789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Position Interpolation (Chen et al., 2023)**：简单有效的外推方法</span>
<span id="cb4-790"><a href="#cb4-790" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**YaRN (Peng et al., 2023)**：最先进的 RoPE 外推框架</span>
<span id="cb4-791"><a href="#cb4-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-792"><a href="#cb4-792" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb4-793"><a href="#cb4-793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extending the RoPE** (EleutherAI Blog)：RoPE 外推技术的深度教程</span>
<span id="cb4-794"><a href="#cb4-794" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**vLLM Documentation**：PagedAttention 的官方实现文档</span>
<span id="cb4-795"><a href="#cb4-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-796"><a href="#cb4-796" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb4-797"><a href="#cb4-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>FlashAttention：<span class="co">[</span><span class="ot">github.com/Dao-AILab/flash-attention</span><span class="co">](https://github.com/Dao-AILab/flash-attention)</span></span>
<span id="cb4-798"><a href="#cb4-798" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>vLLM：<span class="co">[</span><span class="ot">github.com/vllm-project/vllm</span><span class="co">](https://github.com/vllm-project/vllm)</span></span>
<span id="cb4-799"><a href="#cb4-799" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>YaRN：<span class="co">[</span><span class="ot">github.com/jquesnelle/yarn</span><span class="co">](https://github.com/jquesnelle/yarn)</span></span>
<span id="cb4-800"><a href="#cb4-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-801"><a href="#cb4-801" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-802"><a href="#cb4-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-803"><a href="#cb4-803" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb4-804"><a href="#cb4-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-805"><a href="#cb4-805" aria-hidden="true" tabindex="-1"></a>长上下文的故事是工程与理论共同进步的典范。</span>
<span id="cb4-806"><a href="#cb4-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-807"><a href="#cb4-807" aria-hidden="true" tabindex="-1"></a>FlashAttention 的出现是一个转折点。在此之前，大家普遍认为要处理长序列，必须使用稀疏注意力或近似方法——精确注意力的 $O(n^2)$ 复杂度似乎是不可逾越的障碍。Tri Dao 的洞察——问题不在于 FLOP 数，而在于内存带宽——打开了一扇新的大门。这提醒我们，在优化算法时，不仅要看理论复杂度，还要考虑实际硬件的特性。</span>
<span id="cb4-808"><a href="#cb4-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-809"><a href="#cb4-809" aria-hidden="true" tabindex="-1"></a>PagedAttention 的故事同样有趣。它的灵感来自操作系统中几十年前就成熟的虚拟内存技术。这提醒我们，有时候解决新问题的方法，可能早就存在于其他领域——关键是能够识别问题的本质，并找到合适的类比。</span>
<span id="cb4-810"><a href="#cb4-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-811"><a href="#cb4-811" aria-hidden="true" tabindex="-1"></a>从更宏观的视角看，长上下文技术的进步展示了 AI 系统工程的重要性。模型架构固然重要，但如果没有 FlashAttention、PagedAttention、Ring Attention 这些系统层面的优化，即使是最先进的模型架构也无法在实际中部署。**算法创新和系统优化必须齐头并进，这是大模型时代的关键认识**。</span>
<span id="cb4-812"><a href="#cb4-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-813"><a href="#cb4-813" aria-hidden="true" tabindex="-1"></a>当前的长上下文技术已经让 128K 甚至 1M token 的上下文成为可能。但这是终点吗？随着应用场景的扩展（代码库理解、长视频分析、终身对话记忆），对上下文长度的需求可能会继续增长。下一个数量级的突破——千万 token、亿 token 的上下文——可能需要更根本的架构创新。这也是第 28 章将讨论的状态空间模型（SSM/Mamba）试图解决的问题：是否存在一种架构，能够以 $O(n)$ 的复杂度处理任意长度的序列？</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>