<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-29">
<meta name="description" content="当模型参数量从亿级跃升到千亿级，全参数微调成为不可承受之重——一个70B模型的微调需要超过500GB显存。2019年起，研究者探索出多条参数高效微调（PEFT）路线：Adapter在Transformer层中插入小型瓶颈模块；Prefix/Prompt Tuning在输入端学习软提示；而2021年的LoRA另辟蹊径，通过低秩矩阵分解实现了训练参数减少10000倍、显存降低3倍、推理零额外延迟的完美平衡。本章系统梳理PEFT方法的演进脉络，从Adapter的插入式设计到LoRA的低秩分解，从QLoRA的4-bit量化到DoRA的权重分解，揭示为什么’少就是多’在大模型时代成为可能。">

<title>第30章：高效微调技术的演进 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#重新审视微调" id="toc-重新审视微调" class="nav-link" data-scroll-target="#重新审视微调"><span class="header-section-number">2.1</span> 重新审视”微调”</a></li>
  <li><a href="#三种设计哲学" id="toc-三种设计哲学" class="nav-link" data-scroll-target="#三种设计哲学"><span class="header-section-number">2.2</span> 三种设计哲学</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#adapter瓶颈式插入" id="toc-adapter瓶颈式插入" class="nav-link" data-scroll-target="#adapter瓶颈式插入"><span class="header-section-number">3.1</span> Adapter：瓶颈式插入</a></li>
  <li><a href="#prefix-tuning-与-prompt-tuning软提示的力量" id="toc-prefix-tuning-与-prompt-tuning软提示的力量" class="nav-link" data-scroll-target="#prefix-tuning-与-prompt-tuning软提示的力量"><span class="header-section-number">3.2</span> Prefix Tuning 与 Prompt Tuning：软提示的力量</a></li>
  <li><a href="#lora低秩适配的优雅" id="toc-lora低秩适配的优雅" class="nav-link" data-scroll-target="#lora低秩适配的优雅"><span class="header-section-number">3.3</span> LoRA：低秩适配的优雅</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#lora-的数学原理为什么低秩有效" id="toc-lora-的数学原理为什么低秩有效" class="nav-link" data-scroll-target="#lora-的数学原理为什么低秩有效"><span class="header-section-number">4.1</span> LoRA 的数学原理：为什么低秩有效？</a></li>
  <li><a href="#lora-的设计决策" id="toc-lora-的设计决策" class="nav-link" data-scroll-target="#lora-的设计决策"><span class="header-section-number">4.2</span> LoRA 的设计决策</a></li>
  <li><a href="#完整数值示例lora-前向传播" id="toc-完整数值示例lora-前向传播" class="nav-link" data-scroll-target="#完整数值示例lora-前向传播"><span class="header-section-number">4.3</span> 完整数值示例：LoRA 前向传播</a></li>
  <li><a href="#qlora量化与低秩的结合" id="toc-qlora量化与低秩的结合" class="nav-link" data-scroll-target="#qlora量化与低秩的结合"><span class="header-section-number">4.4</span> QLoRA：量化与低秩的结合</a></li>
  <li><a href="#dora权重分解的新视角" id="toc-dora权重分解的新视角" class="nav-link" data-scroll-target="#dora权重分解的新视角"><span class="header-section-number">4.5</span> DoRA：权重分解的新视角</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#从零实现-lora" id="toc-从零实现-lora" class="nav-link" data-scroll-target="#从零实现-lora"><span class="header-section-number">5.1</span> 从零实现 LoRA</a></li>
  <li><a href="#使用-hugging-face-peft-库" id="toc-使用-hugging-face-peft-库" class="nav-link" data-scroll-target="#使用-hugging-face-peft-库"><span class="header-section-number">5.2</span> 使用 Hugging Face PEFT 库</a></li>
  <li><a href="#qlora-微调示例" id="toc-qlora-微调示例" class="nav-link" data-scroll-target="#qlora-微调示例"><span class="header-section-number">5.3</span> QLoRA 微调示例</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.2</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#本方法的局限" id="toc-本方法的局限" class="nav-link" data-scroll-target="#本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#方法对比速查" id="toc-方法对比速查" class="nav-link" data-scroll-target="#方法对比速查"><span class="header-section-number">8.3</span> 方法对比速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.4</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.2</span> 后续发展</a></li>
  <li><a href="#代码资源-1" id="toc-代码资源-1" class="nav-link" data-scroll-target="#代码资源-1"><span class="header-section-number">9.3</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第30章：高效微调技术的演进</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">From Full Fine-tuning to LoRA: The Art of Adapting Giants with Minimal Parameters</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">PEFT</div>
    <div class="quarto-category">LoRA</div>
    <div class="quarto-category">Adapter</div>
    <div class="quarto-category">Fine-tuning</div>
  </div>
  </div>

<div>
  <div class="description">
    当模型参数量从亿级跃升到千亿级，全参数微调成为不可承受之重——一个70B模型的微调需要超过500GB显存。2019年起，研究者探索出多条参数高效微调（PEFT）路线：Adapter在Transformer层中插入小型瓶颈模块；Prefix/Prompt Tuning在输入端学习软提示；而2021年的LoRA另辟蹊径，通过低秩矩阵分解实现了训练参数减少10000倍、显存降低3倍、推理零额外延迟的完美平衡。本章系统梳理PEFT方法的演进脉络，从Adapter的插入式设计到LoRA的低秩分解，从QLoRA的4-bit量化到DoRA的权重分解，揭示为什么’少就是多’在大模型时代成为可能。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 29, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：当大语言模型的参数量达到数百亿甚至数千亿时，如何用极少的可训练参数（&lt;1%）实现与全参数微调相当的性能？不同PEFT方法的设计哲学和适用场景有何差异？</p>
<p><strong>历史坐标</strong>：2019–2024 | Adapter (Houlsby et al., 2019), Prefix Tuning (Li &amp; Liang, 2021), LoRA (Hu et al., 2021), QLoRA (Dettmers et al., 2023), DoRA (Liu et al., 2024) | 从全参数微调到参数高效适配的范式转变</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Houlsby et al.&nbsp;(2019)</strong> “Parameter-Efficient Transfer Learning for NLP” (<a href="https://arxiv.org/abs/1902.00751">arXiv:1902.00751</a>) — 参考了 Section 3 的 Adapter 架构设计、Figure 2（Adapter 模块结构）、Table 1（GLUE 实验结果）；从论文提取了1张架构图</li>
<li><strong>Li &amp; Liang (2021)</strong> “Prefix-Tuning: Optimizing Continuous Prompts for Generation” (<a href="https://arxiv.org/abs/2101.00190">arXiv:2101.00190</a>) — 参考了 Section 2-3 的 Prefix Tuning 方法、与 Fine-tuning 的对比实验</li>
<li><strong>Lester et al.&nbsp;(2021)</strong> “The Power of Scale for Parameter-Efficient Prompt Tuning” (<a href="https://arxiv.org/abs/2104.08691">arXiv:2104.08691</a>) — 参考了 Prompt Tuning 的设计和规模效应分析</li>
<li><strong>Hu et al.&nbsp;(2021)</strong> “LoRA: Low-Rank Adaptation of Large Language Models” (<a href="https://arxiv.org/abs/2106.09685">arXiv:2106.09685</a>) — 参考了 Section 2-4 的低秩分解方法、Figure 1（LoRA 架构）、Section 7 的 rank 分析；本章核心技术参考，从论文提取了2张图</li>
<li><strong>Dettmers et al.&nbsp;(2023)</strong> “QLoRA: Efficient Finetuning of Quantized LLMs” (<a href="https://arxiv.org/abs/2305.14314">arXiv:2305.14314</a>) — 参考了 Section 3 的 NF4 量化、Double Quantization、Paged Optimizers</li>
<li><strong>Liu et al.&nbsp;(2024)</strong> “DoRA: Weight-Decomposed Low-Rank Adaptation” (<a href="https://arxiv.org/abs/2402.09353">arXiv:2402.09353</a>) — 参考了权重分解分析和 DoRA 设计（ICML 2024 Oral）</li>
<li><strong>Hayou et al.&nbsp;(2024)</strong> “LoRA+: Efficient Low Rank Adaptation of Large Models” (<a href="https://arxiv.org/abs/2402.12354">arXiv:2402.12354</a>) — 参考了 A 和 B 矩阵不同学习率的设计</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.2</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 11: Efficient Adaptation (Winter 2024) — 参考了 PEFT 方法的教学组织框架</li>
</ul>
</section>
<section id="代码资源" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">0.3</span> 代码资源</h3>
<ul>
<li><a href="https://github.com/microsoft/LoRA">microsoft/LoRA</a> — LoRA 官方实现</li>
<li><a href="https://github.com/huggingface/peft">huggingface/peft</a> — Hugging Face PEFT 库</li>
<li><a href="https://github.com/artidoro/qlora">artidoro/qlora</a> — QLoRA 官方实现</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了开源大模型的蓬勃发展——从 LLaMA 开创先河，到 LLaMA 2/3 的持续进化，再到 Mistral、Qwen、DeepSeek 等模型的多极化竞争。开源生态的繁荣让每个人都能获得与 GPT-3.5 相当甚至更强的模型。但获得模型只是第一步，真正的挑战在于<strong>如何让这些通用模型适配你的具体任务</strong>。</p>
<p>传统的方法是<strong>全参数微调</strong>（Full Fine-tuning）：加载预训练模型，在下游任务的数据上继续训练，更新所有参数。这种方法在 BERT 时代（1.1亿参数）完全可行——任何一张消费级 GPU 都能装下模型和梯度。但当模型规模跃升到 GPT-3 级别（1750亿参数），情况发生了质变。让我们做一个简单的算术：</p>
<p>一个 175B 参数的模型：</p>
<ul>
<li><strong>模型参数</strong>：<span class="math inline">\(175 \times 10^9 \times 2 \text{ bytes (FP16)} = 350 \text{ GB}\)</span></li>
<li><strong>梯度</strong>：与参数同等大小，<span class="math inline">\(350 \text{ GB}\)</span></li>
<li><strong>优化器状态</strong>（Adam）：参数的2倍（一阶和二阶动量），<span class="math inline">\(700 \text{ GB}\)</span></li>
<li><strong>激活值</strong>：取决于 batch size 和序列长度，通常数百 GB</li>
</ul>
<p>总计：全参数微调 GPT-3 需要约 <strong>1.5 TB</strong> 显存——这大约需要 20 张 80GB 的 A100 GPU。即使是”小”一点的 LLaMA 70B，全参数微调也需要超过 500GB 显存。对于大多数研究者和开发者来说，这是不可承受之重。</p>
<p>更糟糕的是，全参数微调还有一个实际问题：每个下游任务都需要保存一份完整的模型副本。如果你有 10 个任务，就需要存储 10 份 175B 参数——这是 3.5 TB 的磁盘空间。部署时，每个任务都需要单独加载一个完整模型，无法共享计算资源。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）的核心思想是——<strong>预训练模型已经学到了强大的通用表示，适配下游任务不需要改变所有参数，只需要在关键位置做小幅调整</strong>。不同的 PEFT 方法探索了不同的”关键位置”：Adapter 在层间插入小型瓶颈模块，Prefix/Prompt Tuning 在输入端添加可学习的软提示，而 LoRA 则发现权重更新本身具有低秩结构，可以用两个小矩阵的乘积来近似。这些方法将可训练参数从 100% 降到 0.01%–1%，却能达到接近全参数微调的效果。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="重新审视微调" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="重新审视微调"><span class="header-section-number">2.1</span> 重新审视”微调”</h3>
<p>让我们退后一步，思考微调究竟在做什么。预训练后，模型拥有了一组权重 <span class="math inline">\(W_0\)</span>，这些权重编码了从海量数据中学到的语言知识。微调的目标是找到一个权重更新 <span class="math inline">\(\Delta W\)</span>，使得 <span class="math inline">\(W = W_0 + \Delta W\)</span> 在下游任务上表现更好。</p>
<p>全参数微调假设 <span class="math inline">\(\Delta W\)</span> 可以是任意的——它与 <span class="math inline">\(W_0\)</span> 有相同的维度，每个参数都可以自由变化。但这个假设合理吗？</p>
<p>想象一下：GPT-3 在万亿 token 上预训练，学习了从语法到事实知识到推理能力的广泛能力。现在你要用它来做情感分析——判断电影评论是正面还是负面的。这个任务需要”重新学习”175B 个参数吗？直觉上，情感分析需要的”新知识”相当有限：理解一些情感词汇的极性、掌握否定的处理方式、识别讽刺等。这些应该只需要对模型做<strong>微小的调整</strong>，而不是从头改写。</p>
<p>这个直觉得到了实证支持。研究发现，微调后的权重更新 <span class="math inline">\(\Delta W\)</span> 具有<strong>低秩结构</strong>——它可以用远低于原始维度的矩阵来近似。换句话说，<span class="math inline">\(\Delta W\)</span> 虽然形式上是一个高维矩阵，但它的”本征自由度”（intrinsic dimensionality）很低。这就是 PEFT 方法的理论基础：我们不需要更新所有参数，因为真正有效的更新本就存在于一个低维子空间中。</p>
</section>
<section id="三种设计哲学" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="三种设计哲学"><span class="header-section-number">2.2</span> 三种设计哲学</h3>
<p>从 2019 年到 2024 年，研究者探索了多种 PEFT 方法，它们大致可以归为三类设计哲学：</p>
<p><strong>哲学一：在模型中插入可训练模块</strong></p>
<p>Adapter 的思路是在 Transformer 的现有层之间插入小型的”适配器”模块。预训练权重完全冻结，只训练这些新增的模块。这相当于在一座建好的大厦里增设几部电梯——原有结构不变，新模块负责”适配”。</p>
<p><strong>哲学二：在输入端添加可学习的提示</strong></p>
<p>Prefix Tuning 和 Prompt Tuning 的思路是在输入序列前面添加一些可学习的”软提示”（soft prompts）。这些软提示是连续的向量，而不是离散的 token。模型在处理输入时会”看到”这些提示，从而调整其行为。这相当于给模型一个”隐形的系统提示”，但这个提示是通过反向传播学习的，而不是人工设计的。</p>
<p><strong>哲学三：用低秩矩阵近似权重更新</strong></p>
<p>LoRA 的思路是直接参数化权重更新 <span class="math inline">\(\Delta W = BA\)</span>，其中 <span class="math inline">\(B \in \mathbb{R}^{d \times r}\)</span>，<span class="math inline">\(A \in \mathbb{R}^{r \times k}\)</span>，<span class="math inline">\(r \ll \min(d, k)\)</span>。通过强制 <span class="math inline">\(\Delta W\)</span> 是低秩的，LoRA 将可训练参数从 <span class="math inline">\(d \times k\)</span> 降到 <span class="math inline">\(r \times (d + k)\)</span>。这相当于说：权重更新虽然看起来是高维的，但实际上只在一个低维子空间里变化。</p>
<p>每种哲学都有其优缺点，我们将逐一深入探讨。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="adapter瓶颈式插入" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="adapter瓶颈式插入"><span class="header-section-number">3.1</span> Adapter：瓶颈式插入</h3>
<p>Adapter 的核心洞察是：<strong>任务适配需要的新信息可以用少量参数表示</strong>。</p>
<div id="fig-adapter-module" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adapter-module-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-30/original/fig-adapter-module.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adapter-module-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Adapter 模块结构：左图显示 Adapter 在 Transformer 层中的位置，右图显示 Adapter 内部的瓶颈结构
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Houlsby et al.&nbsp;(2019) “Parameter-Efficient Transfer Learning for NLP”, Figure 2</em></p>
</div>
<p>如上图所示，Adapter 模块采用<strong>瓶颈架构</strong>（bottleneck architecture）：</p>
<ol type="1">
<li><strong>下投影</strong>（Down-projection）：将隐藏状态从维度 <span class="math inline">\(d\)</span> 压缩到 <span class="math inline">\(m\)</span>（<span class="math inline">\(m \ll d\)</span>）</li>
<li><strong>非线性激活</strong>：通常使用 ReLU 或 GELU</li>
<li><strong>上投影</strong>（Up-projection）：从维度 <span class="math inline">\(m\)</span> 恢复到 <span class="math inline">\(d\)</span></li>
<li><strong>残差连接</strong>：将输出与输入相加</li>
</ol>
<p>用数学公式表示：</p>
<p><span class="math display">\[
\text{Adapter}(h) = h + f(h W_{\text{down}}) W_{\text{up}}
\]</span></p>
<p>其中 <span class="math inline">\(W_{\text{down}} \in \mathbb{R}^{d \times m}\)</span>，<span class="math inline">\(W_{\text{up}} \in \mathbb{R}^{m \times d}\)</span>，<span class="math inline">\(f\)</span> 是非线性激活函数。</p>
<p>为什么叫”瓶颈”？因为信息在中间被压缩到一个更低维的空间。如果 <span class="math inline">\(d = 1024\)</span>，<span class="math inline">\(m = 64\)</span>，那么信息必须通过一个只有 64 维的”瓶颈”。这种设计有两个好处：</p>
<ul>
<li><strong>参数效率</strong>：Adapter 的参数量是 <span class="math inline">\(2 \times d \times m\)</span>，而不是 <span class="math inline">\(d \times d\)</span>。如果 <span class="math inline">\(m = 64\)</span>，<span class="math inline">\(d = 1024\)</span>，Adapter 只需要 131K 参数，而全连接层需要 1M 参数。</li>
<li><strong>正则化效果</strong>：瓶颈强制模型学习任务最关键的信息，避免过拟合。</li>
</ul>
<p>在 Transformer 中，Adapter 被插入到两个位置：多头注意力之后、前馈网络之后。每个 Transformer 层增加两个 Adapter，整个模型增加 <span class="math inline">\(2L\)</span> 个 Adapter（<span class="math inline">\(L\)</span> 是层数）。</p>
<p><strong>Adapter 的局限</strong>：Adapter 引入了额外的推理延迟。即使只增加了 3% 的参数，它也改变了模型的计算图——每个 token 必须经过额外的矩阵乘法。在低 batch size 的在线推理场景中，这可能导致 20–30% 的延迟增加。</p>
</section>
<section id="prefix-tuning-与-prompt-tuning软提示的力量" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="prefix-tuning-与-prompt-tuning软提示的力量"><span class="header-section-number">3.2</span> Prefix Tuning 与 Prompt Tuning：软提示的力量</h3>
<p>Prefix Tuning 的核心洞察是：<strong>通过在注意力机制中注入可学习的”虚拟 token”，可以引导模型的行为</strong>。</p>
<p>想象你要让 GPT 生成一段莎士比亚风格的文本。一种方法是在输入前加上提示 “Write in the style of Shakespeare:”。但这个提示是离散的 token，它的表示是固定的（来自预训练的词嵌入）。Prefix Tuning 的想法是：为什么不直接学习这个提示应该是什么向量？</p>
<p>具体来说，Prefix Tuning 在每一层的 Key 和 Value 前面添加可学习的前缀向量：</p>
<p><span class="math display">\[
\text{head}_i = \text{Attention}(Q, [P_K^{(i)}; K], [P_V^{(i)}; V])
\]</span></p>
<p>其中 <span class="math inline">\(P_K^{(i)}, P_V^{(i)} \in \mathbb{R}^{l \times d}\)</span> 是第 <span class="math inline">\(i\)</span> 层的前缀，<span class="math inline">\(l\)</span> 是前缀长度。注意力计算时，Query 会同时关注原始的 Key/Value 和前缀 Key/Value。</p>
<p>Prefix Tuning 的参数量是 <span class="math inline">\(l \times d \times 2L\)</span>（每层两个前缀，共 <span class="math inline">\(L\)</span> 层）。对于 <span class="math inline">\(l = 10\)</span>，<span class="math inline">\(d = 1024\)</span>，<span class="math inline">\(L = 24\)</span>，这大约是 500K 参数——仅占 BERT-base 的 0.5%。</p>
<p><strong>Prompt Tuning</strong> 是 Prefix Tuning 的简化版本。它只在输入层添加可学习的软提示，而不是每一层都添加。参数量更少（只有 <span class="math inline">\(l \times d\)</span>），但效果在小模型上稍差。有趣的是，Lester et al.&nbsp;(2021) 发现 <strong>Prompt Tuning 的效果随模型规模提升</strong>：在 T5-Small 上远不如微调，但在 T5-XXL（11B 参数）上几乎与微调持平。</p>
<p>这个发现意味着：大模型已经学到了足够强大的表示，只需要一点”引导”就能适配新任务；而小模型的表示能力有限，需要更多的参数调整。</p>
</section>
<section id="lora低秩适配的优雅" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="lora低秩适配的优雅"><span class="header-section-number">3.3</span> LoRA：低秩适配的优雅</h3>
<p>LoRA（Low-Rank Adaptation）的核心洞察是：<strong>权重更新本身具有低秩结构，可以用两个小矩阵的乘积来近似</strong>。</p>
<div id="fig-lora-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lora-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-30/original/fig-lora-reparametrization.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lora-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LoRA 重参数化：左侧是预训练权重 <span class="math inline">\(W\)</span>，右侧是低秩更新 <span class="math inline">\(BA\)</span>。推理时可以合并为 <span class="math inline">\(W + BA\)</span>
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Hu et al.&nbsp;(2021) “LoRA: Low-Rank Adaptation of Large Language Models”, Figure 1</em></p>
</div>
<p>LoRA 的设计极其简洁。对于预训练权重 <span class="math inline">\(W_0 \in \mathbb{R}^{d \times k}\)</span>，LoRA 不直接学习完整的更新 <span class="math inline">\(\Delta W\)</span>，而是将其分解为：</p>
<p><span class="math display">\[
\Delta W = BA
\]</span></p>
<p>其中 <span class="math inline">\(B \in \mathbb{R}^{d \times r}\)</span>，<span class="math inline">\(A \in \mathbb{R}^{r \times k}\)</span>，<span class="math inline">\(r\)</span> 是秩（rank），通常取 1–64，远小于 <span class="math inline">\(d\)</span> 和 <span class="math inline">\(k\)</span>。</p>
<p>前向传播变为：</p>
<p><span class="math display">\[
h = W_0 x + \Delta W \cdot x = W_0 x + BA \cdot x
\]</span></p>
<p>训练时，<span class="math inline">\(W_0\)</span> 冻结，只更新 <span class="math inline">\(B\)</span> 和 <span class="math inline">\(A\)</span>。初始化时，<span class="math inline">\(A\)</span> 使用高斯初始化，<span class="math inline">\(B\)</span> 初始化为零，这样训练开始时 <span class="math inline">\(\Delta W = 0\)</span>，模型输出与预训练模型相同。</p>
<p>LoRA 的参数量是 <span class="math inline">\(r \times (d + k)\)</span>。如果 <span class="math inline">\(d = k = 4096\)</span>，<span class="math inline">\(r = 8\)</span>，LoRA 只需要 65K 参数，而原始权重有 16M 参数——减少了 <strong>250 倍</strong>。</p>
<p><strong>LoRA 的独特优势：推理零额外延迟</strong></p>
<p>与 Adapter 不同，LoRA 在推理时可以<strong>合并到原始权重</strong>中：</p>
<p><span class="math display">\[
W = W_0 + BA
\]</span></p>
<p>合并后，模型的计算图与原始模型完全相同——没有额外的层，没有额外的矩阵乘法。这意味着 LoRA 微调后的模型在推理时与原始模型有完全相同的延迟。如果需要切换任务，只需替换 <span class="math inline">\(BA\)</span> 矩阵（几十 MB），而不需要重新加载整个模型（几百 GB）。</p>
<p>这个特性使得 LoRA 成为多任务部署的理想选择：加载一次基座模型，然后根据请求动态切换不同任务的 LoRA 权重。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<p>在深入分析各方法的理论基础之前，我们先用伪代码明确其核心算法流程。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: LoRA Forward Pass (Hu et al., 2021)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Input: x ∈ ℝ^(batch × seq × d_in)     # 输入张量
       W₀ ∈ ℝ^(d_in × d_out)          # 冻结的预训练权重
       A ∈ ℝ^(d_in × r)               # LoRA 下投影矩阵
       B ∈ ℝ^(r × d_out)              # LoRA 上投影矩阵
       α                               # 缩放因子
       r                               # 秩

Initialization:
    A ← Gaussian(0, σ²)               # 高斯初始化
    B ← 0                             # 零初始化（确保ΔW初始为0）

Forward:
    h_base = x @ W₀                   # 原始前向传播
    h_lora = x @ A @ B                # LoRA 增量
    h = h_base + (α/r) · h_lora       # 合并输出
    return h

Merge (for inference):
    W = W₀ + (α/r) · A @ B            # 权重合并，推理零延迟</code></pre>
<p><em>Source: 改编自 Hu et al.&nbsp;(2021) “LoRA: Low-Rank Adaptation of Large Language Models”, Section 4.1</em></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 2: Adapter Module (Houlsby et al., 2019)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Input: h ∈ ℝ^(batch × seq × d)        # 隐藏状态
       W_down ∈ ℝ^(d × m)             # 下投影矩阵 (d → m, m ≪ d)
       W_up ∈ ℝ^(m × d)               # 上投影矩阵 (m → d)

Initialization:
    W_down, W_up ← near-zero          # 近零初始化

Forward:
    # 瓶颈结构：下投影 → 非线性 → 上投影
    h_down = h @ W_down               # [batch, seq, m]
    h_act = GELU(h_down)              # 非线性激活
    h_up = h_act @ W_up               # [batch, seq, d]

    # 残差连接
    output = h + h_up
    return output

# 在 Transformer 中的位置：
# 每层插入两次：
#   1. Multi-Head Attention 输出后
#   2. Feed-Forward Network 输出后</code></pre>
<p><em>Source: 改编自 Houlsby et al.&nbsp;(2019) “Parameter-Efficient Transfer Learning for NLP”, Section 3.1</em></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 3: QLoRA Training (Dettmers et al., 2023)
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Input: Model with weights W₀
       Training data D

Quantization (preprocessing):
    W_nf4 = NF4_Quantize(W₀)          # 4-bit NormalFloat 量化
    scales = Quantize_FP8(scales)     # 双重量化：量化常数再量化

Training loop:
    for batch in D:
        # 反量化到 BF16 用于计算
        W_bf16 = Dequantize(W_nf4, scales)

        # 前向传播（LoRA 部分用 BF16）
        output = W_bf16 @ x + (A @ B) @ x
        loss = compute_loss(output, targets)

        # 反向传播（只更新 A, B）
        gradients = backward(loss, [A, B])

        # 处理显存峰值
        if memory_pressure &gt; threshold:
            page_to_cpu(optimizer_states)  # Paged Optimizers

        update(A, B, gradients)</code></pre>
<p><em>Source: 改编自 Dettmers et al.&nbsp;(2023) “QLoRA: Efficient Finetuning of Quantized LLMs”, Section 3</em></p>
</div>
</div>
<section id="lora-的数学原理为什么低秩有效" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="lora-的数学原理为什么低秩有效"><span class="header-section-number">4.1</span> LoRA 的数学原理：为什么低秩有效？</h3>
<p>LoRA 的有效性基于一个关键假设：<strong>微调引起的权重变化位于一个低维子空间中</strong>。这个假设有理论和实证的双重支持。</p>
<p><strong>理论视角：本征维度（Intrinsic Dimensionality）</strong></p>
<p>Aghajanyan et al.&nbsp;(2020) 研究了预训练模型的”本征维度”——在保持 90% 性能的前提下，微调需要的最小自由度。他们发现，RoBERTa-base 的本征维度只有约 200，远小于其 1.25 亿参数。这意味着微调的有效更新集中在一个极低维的子空间中。</p>
<p><strong>实证视角：权重变化的奇异值分布</strong></p>
<p>Hu et al.&nbsp;分析了微调前后的权重差异 <span class="math inline">\(\Delta W = W_{\text{finetuned}} - W_0\)</span>，发现其奇异值快速衰减——前几个奇异值占主导，其余接近于零。这说明 <span class="math inline">\(\Delta W\)</span> 确实是低秩的，用 <span class="math inline">\(r = 4\)</span> 或 <span class="math inline">\(r = 8\)</span> 的近似就能捕获大部分信息。</p>
<p><strong>为什么预训练模型会有这种性质？</strong></p>
<p>一个直觉解释：预训练已经学习了通用的语言表示，任务适配只需要在这个表示空间中做”微调”——选择性地增强或抑制某些方向。这种调整本质上是线性的、低维的，不需要”重新发明轮子”。</p>
</section>
<section id="lora-的设计决策" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="lora-的设计决策"><span class="header-section-number">4.2</span> LoRA 的设计决策</h3>
<p><strong>应该对哪些矩阵应用 LoRA？</strong></p>
<p>Transformer 中有多种权重矩阵：<span class="math inline">\(W_Q, W_K, W_V\)</span>（注意力的 Query、Key、Value 投影）、<span class="math inline">\(W_O\)</span>（注意力的输出投影）、<span class="math inline">\(W_{\text{up}}, W_{\text{down}}\)</span>（FFN 层）。Hu et al.&nbsp;的实验表明：</p>
<ul>
<li>只对 <span class="math inline">\(W_Q\)</span> 和 <span class="math inline">\(W_V\)</span> 应用 LoRA 就能获得大部分收益</li>
<li>同时对所有注意力矩阵应用效果最佳</li>
<li>FFN 层的收益相对较小</li>
</ul>
<p>这个发现的直觉是：注意力层决定了”关注什么信息”，是任务适配的关键；FFN 层更多是”处理信息”，在预训练中已经学得很好。</p>
<p><strong>秩 <span class="math inline">\(r\)</span> 应该选多大？</strong></p>
<p>这是 LoRA 最重要的超参数。Hu et al.&nbsp;发现：</p>
<ul>
<li><span class="math inline">\(r = 1\)</span> 就能在很多任务上超过全参数微调的 baseline</li>
<li><span class="math inline">\(r = 4\)</span> 到 <span class="math inline">\(r = 8\)</span> 通常是性能-效率的甜点</li>
<li><span class="math inline">\(r\)</span> 超过 16 后，增益递减</li>
</ul>
<p>令人惊讶的是，<span class="math inline">\(r\)</span> 的选择对性能影响不大——即使 <span class="math inline">\(r = 1\)</span>，性能也只比 <span class="math inline">\(r = 64\)</span> 差一点点。这进一步证实了权重更新的低秩假设。</p>
</section>
<section id="完整数值示例lora-前向传播" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="完整数值示例lora-前向传播"><span class="header-section-number">4.3</span> 完整数值示例：LoRA 前向传播</h3>
<p>让我们用一个具体的数值例子来理解 LoRA 的计算过程。</p>
<p><strong>设定</strong>：输入维度 <span class="math inline">\(d = 4\)</span>，输出维度 <span class="math inline">\(k = 4\)</span>，秩 <span class="math inline">\(r = 2\)</span>。</p>
<p><strong>Step 1：预训练权重</strong></p>
<p><span class="math display">\[
W_0 = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.5 &amp; 0.6 &amp; 0.7 &amp; 0.8 \\ 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\ 0.6 &amp; 0.7 &amp; 0.8 &amp; 0.9 \end{bmatrix}
\]</span></p>
<p><strong>Step 2：LoRA 矩阵</strong>（假设已训练好）</p>
<p><span class="math display">\[
B = \begin{bmatrix} 0.1 &amp; -0.1 \\ 0.2 &amp; 0.0 \\ -0.1 &amp; 0.1 \\ 0.0 &amp; 0.2 \end{bmatrix}, \quad
A = \begin{bmatrix} 0.5 &amp; 0.3 &amp; -0.2 &amp; 0.1 \\ 0.2 &amp; -0.4 &amp; 0.1 &amp; 0.3 \end{bmatrix}
\]</span></p>
<p><strong>Step 3：计算 <span class="math inline">\(\Delta W = BA\)</span></strong></p>
<p><span class="math display">\[
\Delta W = BA = \begin{bmatrix} 0.03 &amp; 0.07 &amp; -0.03 &amp; -0.02 \\ 0.10 &amp; 0.06 &amp; -0.04 &amp; 0.02 \\ -0.03 &amp; -0.07 &amp; 0.03 &amp; 0.02 \\ 0.04 &amp; -0.08 &amp; 0.02 &amp; 0.06 \end{bmatrix}
\]</span></p>
<p><strong>Step 4：合并权重</strong></p>
<p><span class="math display">\[
W = W_0 + \alpha \cdot \Delta W
\]</span></p>
<p>其中 <span class="math inline">\(\alpha\)</span> 是缩放因子（通常设为 <span class="math inline">\(\alpha / r\)</span>，用于稳定训练）。</p>
<p><strong>Step 5：输入向量</strong></p>
<p><span class="math display">\[
x = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.5 \\ 0.0 \end{bmatrix}
\]</span></p>
<p><strong>Step 6：计算输出</strong></p>
<p><span class="math display">\[
h = W \cdot x = (W_0 + \alpha \cdot BA) \cdot x
\]</span></p>
<p>在推理时，我们只需要一次矩阵乘法（因为 <span class="math inline">\(W_0 + BA\)</span> 已经预先合并）。</p>
</section>
<section id="qlora量化与低秩的结合" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="qlora量化与低秩的结合"><span class="header-section-number">4.4</span> QLoRA：量化与低秩的结合</h3>
<p>QLoRA 的核心创新是：<strong>在 4-bit 量化的冻结模型上应用 LoRA</strong>。</p>
<p>全精度模型需要大量显存，而量化可以将模型压缩到原来的 1/4 甚至 1/8。但传统量化后无法微调（梯度需要高精度）。QLoRA 解决了这个问题。</p>
<p><strong>QLoRA 的三个核心技术</strong></p>
<p><strong>1. 4-bit NormalFloat (NF4)</strong></p>
<p>NF4 是一种新的 4-bit 数据类型，专门为正态分布的权重设计。它的量化区间不是均匀的，而是根据正态分布的分位数设置，使得量化误差在信息论意义上最小。</p>
<p><strong>2. 双重量化（Double Quantization）</strong></p>
<p>量化需要存储量化常数（每个 block 一个 scale 和 zero point）。QLoRA 对这些量化常数再做一次量化，进一步减少内存占用——平均每个参数节省 0.37 bit。</p>
<p><strong>3. 分页优化器（Paged Optimizers）</strong></p>
<p>长序列训练时，激活值会导致显存峰值。QLoRA 使用 NVIDIA 统一内存，在 GPU 显存不足时自动将数据转移到 CPU 内存，避免 OOM。</p>
<p><strong>QLoRA 的惊人结果</strong></p>
<p>使用 QLoRA，Dettmers et al.&nbsp;在<strong>单张 48GB GPU</strong> 上微调了 65B 参数的模型。他们的最佳模型 Guanaco 在 Vicuna 基准上达到了 ChatGPT 性能的 99.3%——仅用 24 小时训练。这使得大模型微调从”需要一个集群”变成了”一张消费级 GPU 就够”。</p>
</section>
<section id="dora权重分解的新视角" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="dora权重分解的新视角"><span class="header-section-number">4.5</span> DoRA：权重分解的新视角</h3>
<p>DoRA（Weight-Decomposed Low-Rank Adaptation）是 2024 年的最新进展，被选为 ICML 2024 Oral。</p>
<p>DoRA 的核心观察是：<strong>LoRA 和全参数微调的权重更新模式不同</strong>。全参数微调倾向于改变权重的<strong>方向</strong>（direction）而较少改变<strong>幅度</strong>（magnitude），而 LoRA 同时改变两者。</p>
<p>基于这个观察，DoRA 将权重分解为幅度和方向两部分：</p>
<p><span class="math display">\[
W = m \cdot \frac{V}{\|V\|_c}
\]</span></p>
<p>其中 <span class="math inline">\(m\)</span> 是幅度向量，<span class="math inline">\(V\)</span> 是方向矩阵，<span class="math inline">\(\|\cdot\|_c\)</span> 是列归一化。</p>
<p>DoRA 对方向部分使用 LoRA 更新：</p>
<p><span class="math display">\[
W' = m \cdot \frac{V + \Delta V}{\|V + \Delta V\|_c}, \quad \Delta V = BA
\]</span></p>
<p>这种设计使得 DoRA 的学习动态更接近全参数微调，在多个任务上超越了 LoRA。</p>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="从零实现-lora" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="从零实现-lora"><span class="header-section-number">5.1</span> 从零实现 LoRA</h3>
<div id="af97f342" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRALayer(nn.Module):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""LoRA 层：对线性层进行低秩适配"""</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        in_features: <span class="bu">int</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        out_features: <span class="bu">int</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        rank: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        alpha: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rank <span class="op">=</span> rank</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scaling <span class="op">=</span> alpha <span class="op">/</span> rank</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 低秩矩阵 A 和 B</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A: 下投影，初始化为高斯分布</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_A <span class="op">=</span> nn.Parameter(torch.randn(in_features, rank) <span class="op">*</span> <span class="fl">0.01</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B: 上投影，初始化为零（确保训练开始时 ΔW = 0）</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_B <span class="op">=</span> nn.Parameter(torch.zeros(rank, out_features))</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""计算 LoRA 的增量输出"""</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: [batch, seq_len, in_features]</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算 x @ A @ B，得到 [batch, seq_len, out_features]</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (x <span class="op">@</span> <span class="va">self</span>.lora_A <span class="op">@</span> <span class="va">self</span>.lora_B) <span class="op">*</span> <span class="va">self</span>.scaling</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearWithLoRA(nn.Module):</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""带 LoRA 的线性层：冻结原始权重，只训练 LoRA 参数"""</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        linear: nn.Linear,</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        rank: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        alpha: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> linear</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora <span class="op">=</span> LoRALayer(</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>            linear.in_features,</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>            linear.out_features,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>            rank<span class="op">=</span>rank,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span>alpha,</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 冻结原始权重</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.linear.parameters():</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 原始输出 + LoRA 增量</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear(x) <span class="op">+</span> <span class="va">self</span>.lora(x)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> merge_weights(<span class="va">self</span>):</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""将 LoRA 权重合并到原始权重中（用于推理优化）"""</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>            <span class="co"># W' = W + scaling * B @ A</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.linear.weight.data <span class="op">+=</span> (</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.lora.scaling <span class="op">*</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.lora.lora_B.T <span class="op">@</span> <span class="va">self</span>.lora.lora_A.T</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并后可以删除 LoRA 参数</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>代码解读</strong>：</p>
<ol type="1">
<li><code>LoRALayer</code> 实现低秩分解 <span class="math inline">\(\Delta W = BA\)</span>，其中 <span class="math inline">\(A\)</span> 用高斯初始化，<span class="math inline">\(B\)</span> 用零初始化</li>
<li><code>scaling = alpha / rank</code> 是一个稳定训练的技巧：当增大 rank 时，自动减小每个方向的贡献</li>
<li><code>merge_weights()</code> 方法将 LoRA 权重合并到原始权重中，实现推理零额外延迟</li>
</ol>
</section>
<section id="使用-hugging-face-peft-库" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="使用-hugging-face-peft-库"><span class="header-section-number">5.2</span> 使用 Hugging Face PEFT 库</h3>
<div id="8a6ce1b1" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model, TaskType</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载基座模型</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-2-7b-hf"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置 LoRA</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span>TaskType.CAUSAL_LM,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">8</span>,                      <span class="co"># 秩</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>,            <span class="co"># alpha 参数</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,        <span class="co"># Dropout 防止过拟合</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[          <span class="co"># 应用 LoRA 的模块</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"o_proj"</span>,  <span class="co"># 注意力</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">"gate_proj"</span>, <span class="st">"up_proj"</span>, <span class="st">"down_proj"</span>,      <span class="co"># FFN</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,              <span class="co"># 不训练 bias</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 PEFT 模型</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>peft_model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看可训练参数</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>peft_model.print_trainable_parameters()</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出类似: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="qlora-微调示例" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="qlora-微调示例"><span class="header-section-number">5.3</span> QLoRA 微调示例</h3>
<div id="234012ff" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> prepare_model_for_kbit_training</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BitsAndBytesConfig</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> bitsandbytes <span class="im">as</span> bnb</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 4-bit 量化配置</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>bnb_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">"nf4"</span>,           <span class="co"># NormalFloat 量化</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_use_double_quant<span class="op">=</span><span class="va">True</span>,      <span class="co"># 双重量化</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载量化模型</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"meta-llama/Llama-2-70b-hf"</span>,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>bnb_config,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备模型用于 k-bit 训练</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> prepare_model_for_kbit_training(model)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置 LoRA（与上面相同）</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">64</span>,                     <span class="co"># QLoRA 通常用更大的 rank</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>],</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span>TaskType.CAUSAL_LM,</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>peft_model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 现在可以在单张 GPU 上微调 70B 模型！</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<section id="为什么有效理论视角" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</h3>
<p><strong>过参数化与本征维度</strong></p>
<p>现代神经网络是严重过参数化的：参数数量远超训练样本数量，却能泛化得很好。这种现象的一个解释是<strong>双下降</strong>（double descent）：模型复杂度超过某个阈值后，测试误差反而下降。</p>
<p>PEFT 方法利用了过参数化的另一面：既然模型的有效自由度远小于参数数量，那么微调时也只需要调整少量”关键”参数。本征维度的研究表明，这个关键参数集合往往可以表示为原始参数空间的一个低维子空间。</p>
<p><strong>线性子空间假设</strong></p>
<p>LoRA 的有效性暗示了一个更强的假设：不仅微调的有效更新是低维的，而且它是一个<strong>线性子空间</strong>。这意味着最优的 <span class="math inline">\(\Delta W\)</span> 可以写成少数几个基向量的线性组合。</p>
<p>Hu et al.&nbsp;的分析支持这个假设：他们发现不同初始化种子训练出的 LoRA 矩阵，其列向量张成的子空间有很高的重叠。这说明这个”有效更新子空间”是任务本身的性质，而非随机初始化的结果。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.2</span> 方法的边界条件</h3>
<p><strong>LoRA 什么时候不够？</strong></p>
<ol type="1">
<li><p><strong>任务与预训练差异太大</strong>：如果下游任务需要的知识完全不在预训练分布内（比如用英文模型做中文任务），LoRA 可能不足以捕获所需的变化。这时可能需要更大的 rank 或全参数微调。</p></li>
<li><p><strong>数据量非常大</strong>：当下游数据量接近预训练数据量时，LoRA 的低秩约束可能成为瓶颈。研究表明，在超大规模持续预训练场景下，全参数微调仍有优势。</p></li>
<li><p><strong>需要”遗忘”预训练知识</strong>：LoRA 本质上是”加性”更新，难以完全覆盖预训练权重。如果任务需要模型”忘记”某些预训练行为，可能需要其他技术。</p></li>
</ol>
<p><strong>秩选择的经验法则</strong></p>
<ul>
<li><strong>任务简单/数据少</strong>：<span class="math inline">\(r = 4\)</span> 或 <span class="math inline">\(r = 8\)</span></li>
<li><strong>任务复杂/数据多</strong>：<span class="math inline">\(r = 16\)</span> 到 <span class="math inline">\(r = 64\)</span></li>
<li><strong>极端效率需求</strong>：<span class="math inline">\(r = 1\)</span>（惊人地，这通常也能工作！）</li>
</ul>
</section>
<section id="开放研究问题" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.3</span> 开放研究问题</h3>
<ol type="1">
<li><p><strong>最优 PEFT 架构</strong>：是否存在一种理论上最优的 PEFT 设计？目前的方法都是启发式的，缺乏理论指导。</p></li>
<li><p><strong>组合多个 LoRA</strong>：能否通过组合多个任务的 LoRA 权重实现零样本的任务泛化？初步研究表明 LoRA 权重可以线性插值，但理论尚不清楚。</p></li>
<li><p><strong>LoRA 的表达能力边界</strong>：给定 rank <span class="math inline">\(r\)</span>，LoRA 能表示的权重更新有多大范围？这个问题与矩阵低秩近似的理论密切相关。</p></li>
</ol>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="本方法的局限" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</h3>
<p><strong>局限 1：对超参数敏感</strong></p>
<p>不同的模型、任务、数据量需要不同的 LoRA 配置（rank、alpha、target_modules）。目前没有通用的选择原则，需要实验调参。</p>
<p><strong>局限 2：与全参数微调仍有差距</strong></p>
<p>虽然 LoRA 在大多数任务上接近全参数微调，但在某些复杂任务上仍有 1–3% 的性能差距。对于追求极致性能的场景，全参数微调仍是更安全的选择。</p>
<p><strong>局限 3：训练不稳定</strong></p>
<p>QLoRA 等方法在低精度下训练，可能面临数值稳定性问题。训练过程需要仔细监控，可能需要多次实验。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</h3>
<p>PEFT 方法解决了”如何廉价地适配大模型”，但模型部署还面临另一个挑战：<strong>推理效率</strong>。即使用 LoRA 微调只需要一张 GPU，推理一个 70B 模型仍然需要大量显存和计算。下一章，我们将探讨<strong>推理优化</strong>技术——量化、投机解码、持续批处理等——它们让大模型的部署成本进一步降低。</p>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<ol type="1">
<li><strong>问题</strong>：全参数微调在大模型时代不可行——175B 参数需要 1.5TB 显存</li>
<li><strong>洞察</strong>：权重更新具有低秩结构，大部分变化集中在低维子空间</li>
<li><strong>方法</strong>：
<ul>
<li><strong>Adapter</strong>：插入瓶颈模块，增加 3% 参数</li>
<li><strong>Prefix/Prompt Tuning</strong>：学习软提示，0.1% 参数</li>
<li><strong>LoRA</strong>：低秩矩阵分解，0.01%–1% 参数，推理零延迟</li>
<li><strong>QLoRA</strong>：4-bit 量化 + LoRA，单 GPU 微调 70B 模型</li>
<li><strong>DoRA</strong>：权重分解，更接近全参数微调的学习动态</li>
</ul></li>
<li><strong>意义</strong>：将大模型微调从”需要一个集群”变成”一张消费级 GPU”</li>
</ol>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<ul>
<li><strong>LoRA 更新</strong>：<span class="math inline">\(h = W_0 x + BA \cdot x\)</span>，其中 <span class="math inline">\(B \in \mathbb{R}^{d \times r}\)</span>，<span class="math inline">\(A \in \mathbb{R}^{r \times k}\)</span></li>
<li><strong>参数量</strong>：<span class="math inline">\(r \times (d + k)\)</span>，比原始 <span class="math inline">\(d \times k\)</span> 减少约 <span class="math inline">\(\frac{d+k}{r}\)</span> 倍</li>
<li><strong>Adapter 瓶颈</strong>：<span class="math inline">\(\text{Adapter}(h) = h + f(h W_{\text{down}}) W_{\text{up}}\)</span></li>
</ul>
</section>
<section id="方法对比速查" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="方法对比速查"><span class="header-section-number">8.3</span> 方法对比速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>参数量</th>
<th>推理延迟</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adapter</td>
<td>3–5%</td>
<td>+20–30%</td>
<td>多任务学习</td>
</tr>
<tr class="even">
<td>Prefix Tuning</td>
<td>0.1–1%</td>
<td>无（序列变长）</td>
<td>生成任务</td>
</tr>
<tr class="odd">
<td>Prompt Tuning</td>
<td>0.01%</td>
<td>无</td>
<td>超大模型、低数据</td>
</tr>
<tr class="even">
<td>LoRA</td>
<td>0.01–1%</td>
<td><strong>0%</strong></td>
<td>通用场景（推荐）</td>
</tr>
<tr class="odd">
<td>QLoRA</td>
<td>0.01–1%</td>
<td>0%</td>
<td>显存受限</td>
</tr>
<tr class="even">
<td>DoRA</td>
<td>0.01–1%</td>
<td>0%</td>
<td>追求极致性能</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.4</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么 LoRA 的 <span class="math inline">\(B\)</span> 矩阵初始化为零？如果用随机初始化会怎样？</p></li>
<li><p><strong>[数学推导]</strong> 证明 LoRA 参数量公式。如果对 Transformer 的 <span class="math inline">\(W_Q, W_K, W_V, W_O\)</span> 四个矩阵都应用 rank-8 的 LoRA，一层增加多少参数？</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用 PEFT 库对 LLaMA-7B 进行指令微调，比较 rank=4 和 rank=16 的效果差异。</p></li>
<li><p><strong>[开放思考]</strong> LoRA 能否用于持续预训练（continual pre-training）？与微调相比，持续预训练对 rank 的需求是更高还是更低？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<ul>
<li><strong><a href="https://arxiv.org/abs/2106.09685">LoRA</a></strong>：Hu et al.&nbsp;(2021)，本章核心
<ul>
<li>重点读：Section 4（方法）、Section 7（rank 分析）</li>
<li>可跳过：Appendix 的详细实验表格</li>
</ul></li>
<li><strong><a href="https://arxiv.org/abs/2305.14314">QLoRA</a></strong>：Dettmers et al.&nbsp;(2023)
<ul>
<li>重点读：Section 3（NF4、双重量化）、Section 5（Guanaco 实验）</li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.2</span> 后续发展</h3>
<ul>
<li><strong><a href="https://arxiv.org/abs/2402.09353">DoRA</a></strong>：Liu et al.&nbsp;(2024)，权重分解视角，ICML 2024 Oral</li>
<li><strong><a href="https://arxiv.org/abs/2402.12354">LoRA+</a></strong>：Hayou et al.&nbsp;(2024)，A 和 B 使用不同学习率</li>
<li><strong><a href="https://arxiv.org/abs/2403.03507">GaLore</a></strong>：Zhao et al.&nbsp;(2024)，梯度低秩投影</li>
</ul>
</section>
<section id="代码资源-1" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="代码资源-1"><span class="header-section-number">9.3</span> 代码资源</h3>
<ul>
<li><strong><a href="https://github.com/microsoft/LoRA">microsoft/LoRA</a></strong>：官方实现</li>
<li><strong><a href="https://github.com/huggingface/peft">huggingface/peft</a></strong>：统一的 PEFT 库，支持 LoRA、Prefix Tuning、Prompt Tuning、Adapter 等</li>
<li><strong><a href="https://github.com/artidoro/qlora">artidoro/qlora</a></strong>：QLoRA 官方实现</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>LoRA 的灵感来自一个更早的观察：矩阵乘法是神经网络的核心运算，而矩阵分解是线性代数的核心技术。早在 2016 年，研究者就尝试用低秩矩阵加速推理（如 SVD 压缩）。但 LoRA 的创新在于将低秩约束用于<strong>训练时的更新</strong>而非<strong>推理时的近似</strong>——这个视角转换是关键。</p>
<p>Hu 等人最初在 GPT-3 上验证 LoRA，但由于 GPT-3 不开源，无法直接发布代码。讽刺的是，LoRA 真正流行是在 LLaMA 开源之后——开源模型的繁荣为 PEFT 方法提供了广阔的应用舞台。今天，几乎所有的开源模型社区都默认使用 LoRA 进行微调，它已经成为大模型时代的”标准配置”。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第30章：高效微调技术的演进"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "From Full Fine-tuning to LoRA: The Art of Adapting Giants with Minimal Parameters"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-29"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, PEFT, LoRA, Adapter, Fine-tuning]</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [LoRA, QLoRA, DoRA, Adapter, Prefix Tuning, Prompt Tuning, PEFT, 参数高效微调, 低秩适配]</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "当模型参数量从亿级跃升到千亿级，全参数微调成为不可承受之重——一个70B模型的微调需要超过500GB显存。2019年起，研究者探索出多条参数高效微调（PEFT）路线：Adapter在Transformer层中插入小型瓶颈模块；Prefix/Prompt Tuning在输入端学习软提示；而2021年的LoRA另辟蹊径，通过低秩矩阵分解实现了训练参数减少10000倍、显存降低3倍、推理零额外延迟的完美平衡。本章系统梳理PEFT方法的演进脉络，从Adapter的插入式设计到LoRA的低秩分解，从QLoRA的4-bit量化到DoRA的权重分解，揭示为什么'少就是多'在大模型时代成为可能。"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-30/original/fig-lora-reparametrization.png"</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：当大语言模型的参数量达到数百亿甚至数千亿时，如何用极少的可训练参数（&lt;1%）实现与全参数微调相当的性能？不同PEFT方法的设计哲学和适用场景有何差异？</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2019–2024 </span><span class="pp">|</span><span class="at"> Adapter (Houlsby et al., 2019), Prefix Tuning (Li &amp; Liang, 2021), LoRA (Hu et al., 2021), QLoRA (Dettmers et al., 2023), DoRA (Liu et al., 2024) </span><span class="pp">|</span><span class="at"> 从全参数微调到参数高效适配的范式转变</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Houlsby et al. (2019)** "Parameter-Efficient Transfer Learning for NLP" (<span class="co">[</span><span class="ot">arXiv:1902.00751</span><span class="co">](https://arxiv.org/abs/1902.00751)</span>) — 参考了 Section 3 的 Adapter 架构设计、Figure 2（Adapter 模块结构）、Table 1（GLUE 实验结果）；从论文提取了1张架构图</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Li &amp; Liang (2021)** "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (<span class="co">[</span><span class="ot">arXiv:2101.00190</span><span class="co">](https://arxiv.org/abs/2101.00190)</span>) — 参考了 Section 2-3 的 Prefix Tuning 方法、与 Fine-tuning 的对比实验</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lester et al. (2021)** "The Power of Scale for Parameter-Efficient Prompt Tuning" (<span class="co">[</span><span class="ot">arXiv:2104.08691</span><span class="co">](https://arxiv.org/abs/2104.08691)</span>) — 参考了 Prompt Tuning 的设计和规模效应分析</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hu et al. (2021)** "LoRA: Low-Rank Adaptation of Large Language Models" (<span class="co">[</span><span class="ot">arXiv:2106.09685</span><span class="co">](https://arxiv.org/abs/2106.09685)</span>) — 参考了 Section 2-4 的低秩分解方法、Figure 1（LoRA 架构）、Section 7 的 rank 分析；本章核心技术参考，从论文提取了2张图</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dettmers et al. (2023)** "QLoRA: Efficient Finetuning of Quantized LLMs" (<span class="co">[</span><span class="ot">arXiv:2305.14314</span><span class="co">](https://arxiv.org/abs/2305.14314)</span>) — 参考了 Section 3 的 NF4 量化、Double Quantization、Paged Optimizers</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liu et al. (2024)** "DoRA: Weight-Decomposed Low-Rank Adaptation" (<span class="co">[</span><span class="ot">arXiv:2402.09353</span><span class="co">](https://arxiv.org/abs/2402.09353)</span>) — 参考了权重分解分析和 DoRA 设计（ICML 2024 Oral）</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hayou et al. (2024)** "LoRA+: Efficient Low Rank Adaptation of Large Models" (<span class="co">[</span><span class="ot">arXiv:2402.12354</span><span class="co">](https://arxiv.org/abs/2402.12354)</span>) — 参考了 A 和 B 矩阵不同学习率的设计</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 11: Efficient Adaptation (Winter 2024) — 参考了 PEFT 方法的教学组织框架</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">microsoft/LoRA</span><span class="co">](https://github.com/microsoft/LoRA)</span> — LoRA 官方实现</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">huggingface/peft</span><span class="co">](https://github.com/huggingface/peft)</span> — Hugging Face PEFT 库</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">artidoro/qlora</span><span class="co">](https://github.com/artidoro/qlora)</span> — QLoRA 官方实现</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>上一章我们见证了开源大模型的蓬勃发展——从 LLaMA 开创先河，到 LLaMA 2/3 的持续进化，再到 Mistral、Qwen、DeepSeek 等模型的多极化竞争。开源生态的繁荣让每个人都能获得与 GPT-3.5 相当甚至更强的模型。但获得模型只是第一步，真正的挑战在于**如何让这些通用模型适配你的具体任务**。</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>传统的方法是**全参数微调**（Full Fine-tuning）：加载预训练模型，在下游任务的数据上继续训练，更新所有参数。这种方法在 BERT 时代（1.1亿参数）完全可行——任何一张消费级 GPU 都能装下模型和梯度。但当模型规模跃升到 GPT-3 级别（1750亿参数），情况发生了质变。让我们做一个简单的算术：</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>一个 175B 参数的模型：</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**模型参数**：$175 \times 10^9 \times 2 \text{ bytes (FP16)} = 350 \text{ GB}$</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**梯度**：与参数同等大小，$350 \text{ GB}$</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**优化器状态**（Adam）：参数的2倍（一阶和二阶动量），$700 \text{ GB}$</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**激活值**：取决于 batch size 和序列长度，通常数百 GB</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>总计：全参数微调 GPT-3 需要约 **1.5 TB** 显存——这大约需要 20 张 80GB 的 A100 GPU。即使是"小"一点的 LLaMA 70B，全参数微调也需要超过 500GB 显存。对于大多数研究者和开发者来说，这是不可承受之重。</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>更糟糕的是，全参数微调还有一个实际问题：每个下游任务都需要保存一份完整的模型副本。如果你有 10 个任务，就需要存储 10 份 175B 参数——这是 3.5 TB 的磁盘空间。部署时，每个任务都需要单独加载一个完整模型，无法共享计算资源。</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）的核心思想是——**预训练模型已经学到了强大的通用表示，适配下游任务不需要改变所有参数，只需要在关键位置做小幅调整**。不同的 PEFT 方法探索了不同的"关键位置"：Adapter 在层间插入小型瓶颈模块，Prefix/Prompt Tuning 在输入端添加可学习的软提示，而 LoRA 则发现权重更新本身具有低秩结构，可以用两个小矩阵的乘积来近似。这些方法将可训练参数从 100% 降到 0.01%–1%，却能达到接近全参数微调的效果。</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a><span class="fu">### 重新审视"微调"</span></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>让我们退后一步，思考微调究竟在做什么。预训练后，模型拥有了一组权重 $W_0$，这些权重编码了从海量数据中学到的语言知识。微调的目标是找到一个权重更新 $\Delta W$，使得 $W = W_0 + \Delta W$ 在下游任务上表现更好。</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>全参数微调假设 $\Delta W$ 可以是任意的——它与 $W_0$ 有相同的维度，每个参数都可以自由变化。但这个假设合理吗？</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>想象一下：GPT-3 在万亿 token 上预训练，学习了从语法到事实知识到推理能力的广泛能力。现在你要用它来做情感分析——判断电影评论是正面还是负面的。这个任务需要"重新学习"175B 个参数吗？直觉上，情感分析需要的"新知识"相当有限：理解一些情感词汇的极性、掌握否定的处理方式、识别讽刺等。这些应该只需要对模型做**微小的调整**，而不是从头改写。</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>这个直觉得到了实证支持。研究发现，微调后的权重更新 $\Delta W$ 具有**低秩结构**——它可以用远低于原始维度的矩阵来近似。换句话说，$\Delta W$ 虽然形式上是一个高维矩阵，但它的"本征自由度"（intrinsic dimensionality）很低。这就是 PEFT 方法的理论基础：我们不需要更新所有参数，因为真正有效的更新本就存在于一个低维子空间中。</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a><span class="fu">### 三种设计哲学</span></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>从 2019 年到 2024 年，研究者探索了多种 PEFT 方法，它们大致可以归为三类设计哲学：</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>**哲学一：在模型中插入可训练模块**</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>Adapter 的思路是在 Transformer 的现有层之间插入小型的"适配器"模块。预训练权重完全冻结，只训练这些新增的模块。这相当于在一座建好的大厦里增设几部电梯——原有结构不变，新模块负责"适配"。</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>**哲学二：在输入端添加可学习的提示**</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>Prefix Tuning 和 Prompt Tuning 的思路是在输入序列前面添加一些可学习的"软提示"（soft prompts）。这些软提示是连续的向量，而不是离散的 token。模型在处理输入时会"看到"这些提示，从而调整其行为。这相当于给模型一个"隐形的系统提示"，但这个提示是通过反向传播学习的，而不是人工设计的。</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>**哲学三：用低秩矩阵近似权重更新**</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>LoRA 的思路是直接参数化权重更新 $\Delta W = BA$，其中 $B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$，$r \ll \min(d, k)$。通过强制 $\Delta W$ 是低秩的，LoRA 将可训练参数从 $d \times k$ 降到 $r \times (d + k)$。这相当于说：权重更新虽然看起来是高维的，但实际上只在一个低维子空间里变化。</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>每种哲学都有其优缺点，我们将逐一深入探讨。</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adapter：瓶颈式插入</span></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>Adapter 的核心洞察是：**任务适配需要的新信息可以用少量参数表示**。</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a><span class="al">![Adapter 模块结构：左图显示 Adapter 在 Transformer 层中的位置，右图显示 Adapter 内部的瓶颈结构](figures/chapter-30/original/fig-adapter-module.png)</span>{#fig-adapter-module width=60%}</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>*Source: Houlsby et al. (2019) "Parameter-Efficient Transfer Learning for NLP", Figure 2*</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>如上图所示，Adapter 模块采用**瓶颈架构**（bottleneck architecture）：</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**下投影**（Down-projection）：将隐藏状态从维度 $d$ 压缩到 $m$（$m \ll d$）</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**非线性激活**：通常使用 ReLU 或 GELU</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**上投影**（Up-projection）：从维度 $m$ 恢复到 $d$</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**残差连接**：将输出与输入相加</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>用数学公式表示：</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>\text{Adapter}(h) = h + f(h W_{\text{down}}) W_{\text{up}}</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>其中 $W_{\text{down}} \in \mathbb{R}^{d \times m}$，$W_{\text{up}} \in \mathbb{R}^{m \times d}$，$f$ 是非线性激活函数。</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>为什么叫"瓶颈"？因为信息在中间被压缩到一个更低维的空间。如果 $d = 1024$，$m = 64$，那么信息必须通过一个只有 64 维的"瓶颈"。这种设计有两个好处：</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**参数效率**：Adapter 的参数量是 $2 \times d \times m$，而不是 $d \times d$。如果 $m = 64$，$d = 1024$，Adapter 只需要 131K 参数，而全连接层需要 1M 参数。</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**正则化效果**：瓶颈强制模型学习任务最关键的信息，避免过拟合。</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>在 Transformer 中，Adapter 被插入到两个位置：多头注意力之后、前馈网络之后。每个 Transformer 层增加两个 Adapter，整个模型增加 $2L$ 个 Adapter（$L$ 是层数）。</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>**Adapter 的局限**：Adapter 引入了额外的推理延迟。即使只增加了 3% 的参数，它也改变了模型的计算图——每个 token 必须经过额外的矩阵乘法。在低 batch size 的在线推理场景中，这可能导致 20–30% 的延迟增加。</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prefix Tuning 与 Prompt Tuning：软提示的力量</span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>Prefix Tuning 的核心洞察是：**通过在注意力机制中注入可学习的"虚拟 token"，可以引导模型的行为**。</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>想象你要让 GPT 生成一段莎士比亚风格的文本。一种方法是在输入前加上提示 "Write in the style of Shakespeare:"。但这个提示是离散的 token，它的表示是固定的（来自预训练的词嵌入）。Prefix Tuning 的想法是：为什么不直接学习这个提示应该是什么向量？</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>具体来说，Prefix Tuning 在每一层的 Key 和 Value 前面添加可学习的前缀向量：</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>\text{head}_i = \text{Attention}(Q, <span class="co">[</span><span class="ot">P_K^{(i)}; K</span><span class="co">]</span>, <span class="co">[</span><span class="ot">P_V^{(i)}; V</span><span class="co">]</span>)</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>其中 $P_K^{(i)}, P_V^{(i)} \in \mathbb{R}^{l \times d}$ 是第 $i$ 层的前缀，$l$ 是前缀长度。注意力计算时，Query 会同时关注原始的 Key/Value 和前缀 Key/Value。</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>Prefix Tuning 的参数量是 $l \times d \times 2L$（每层两个前缀，共 $L$ 层）。对于 $l = 10$，$d = 1024$，$L = 24$，这大约是 500K 参数——仅占 BERT-base 的 0.5%。</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>**Prompt Tuning** 是 Prefix Tuning 的简化版本。它只在输入层添加可学习的软提示，而不是每一层都添加。参数量更少（只有 $l \times d$），但效果在小模型上稍差。有趣的是，Lester et al. (2021) 发现 **Prompt Tuning 的效果随模型规模提升**：在 T5-Small 上远不如微调，但在 T5-XXL（11B 参数）上几乎与微调持平。</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>这个发现意味着：大模型已经学到了足够强大的表示，只需要一点"引导"就能适配新任务；而小模型的表示能力有限，需要更多的参数调整。</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a><span class="fu">### LoRA：低秩适配的优雅</span></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>LoRA（Low-Rank Adaptation）的核心洞察是：**权重更新本身具有低秩结构，可以用两个小矩阵的乘积来近似**。</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a><span class="al">![LoRA 重参数化：左侧是预训练权重 $W$，右侧是低秩更新 $BA$。推理时可以合并为 $W + BA$](figures/chapter-30/original/fig-lora-reparametrization.png)</span>{#fig-lora-arch width=50%}</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a>*Source: Hu et al. (2021) "LoRA: Low-Rank Adaptation of Large Language Models", Figure 1*</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>LoRA 的设计极其简洁。对于预训练权重 $W_0 \in \mathbb{R}^{d \times k}$，LoRA 不直接学习完整的更新 $\Delta W$，而是将其分解为：</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a>\Delta W = BA</span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>其中 $B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$，$r$ 是秩（rank），通常取 1–64，远小于 $d$ 和 $k$。</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>前向传播变为：</span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>h = W_0 x + \Delta W \cdot x = W_0 x + BA \cdot x</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>训练时，$W_0$ 冻结，只更新 $B$ 和 $A$。初始化时，$A$ 使用高斯初始化，$B$ 初始化为零，这样训练开始时 $\Delta W = 0$，模型输出与预训练模型相同。</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>LoRA 的参数量是 $r \times (d + k)$。如果 $d = k = 4096$，$r = 8$，LoRA 只需要 65K 参数，而原始权重有 16M 参数——减少了 **250 倍**。</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>**LoRA 的独特优势：推理零额外延迟**</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>与 Adapter 不同，LoRA 在推理时可以**合并到原始权重**中：</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>W = W_0 + BA</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>合并后，模型的计算图与原始模型完全相同——没有额外的层，没有额外的矩阵乘法。这意味着 LoRA 微调后的模型在推理时与原始模型有完全相同的延迟。如果需要切换任务，只需替换 $BA$ 矩阵（几十 MB），而不需要重新加载整个模型（几百 GB）。</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>这个特性使得 LoRA 成为多任务部署的理想选择：加载一次基座模型，然后根据请求动态切换不同任务的 LoRA 权重。</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>在深入分析各方法的理论基础之前，我们先用伪代码明确其核心算法流程。</span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: LoRA Forward Pass (Hu et al., 2021)</span></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a><span class="in">Input: x ∈ ℝ^(batch × seq × d_in)     # 输入张量</span></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a><span class="in">       W₀ ∈ ℝ^(d_in × d_out)          # 冻结的预训练权重</span></span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a><span class="in">       A ∈ ℝ^(d_in × r)               # LoRA 下投影矩阵</span></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a><span class="in">       B ∈ ℝ^(r × d_out)              # LoRA 上投影矩阵</span></span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a><span class="in">       α                               # 缩放因子</span></span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a><span class="in">       r                               # 秩</span></span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a><span class="in">Initialization:</span></span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a><span class="in">    A ← Gaussian(0, σ²)               # 高斯初始化</span></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a><span class="in">    B ← 0                             # 零初始化（确保ΔW初始为0）</span></span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a><span class="in">Forward:</span></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a><span class="in">    h_base = x @ W₀                   # 原始前向传播</span></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a><span class="in">    h_lora = x @ A @ B                # LoRA 增量</span></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a><span class="in">    h = h_base + (α/r) · h_lora       # 合并输出</span></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a><span class="in">    return h</span></span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a><span class="in">Merge (for inference):</span></span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a><span class="in">    W = W₀ + (α/r) · A @ B            # 权重合并，推理零延迟</span></span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a>*Source: 改编自 Hu et al. (2021) "LoRA: Low-Rank Adaptation of Large Language Models", Section 4.1*</span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 2: Adapter Module (Houlsby et al., 2019)</span></span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a><span class="in">Input: h ∈ ℝ^(batch × seq × d)        # 隐藏状态</span></span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a><span class="in">       W_down ∈ ℝ^(d × m)             # 下投影矩阵 (d → m, m ≪ d)</span></span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a><span class="in">       W_up ∈ ℝ^(m × d)               # 上投影矩阵 (m → d)</span></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a><span class="in">Initialization:</span></span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a><span class="in">    W_down, W_up ← near-zero          # 近零初始化</span></span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a><span class="in">Forward:</span></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a><span class="in">    # 瓶颈结构：下投影 → 非线性 → 上投影</span></span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a><span class="in">    h_down = h @ W_down               # [batch, seq, m]</span></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a><span class="in">    h_act = GELU(h_down)              # 非线性激活</span></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a><span class="in">    h_up = h_act @ W_up               # [batch, seq, d]</span></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a><span class="in">    # 残差连接</span></span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="in">    output = h + h_up</span></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a><span class="in">    return output</span></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a><span class="in"># 在 Transformer 中的位置：</span></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a><span class="in"># 每层插入两次：</span></span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a><span class="in">#   1. Multi-Head Attention 输出后</span></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a><span class="in">#   2. Feed-Forward Network 输出后</span></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a>*Source: 改编自 Houlsby et al. (2019) "Parameter-Efficient Transfer Learning for NLP", Section 3.1*</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 3: QLoRA Training (Dettmers et al., 2023)</span></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="in">Input: Model with weights W₀</span></span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a><span class="in">       Training data D</span></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a><span class="in">Quantization (preprocessing):</span></span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a><span class="in">    W_nf4 = NF4_Quantize(W₀)          # 4-bit NormalFloat 量化</span></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a><span class="in">    scales = Quantize_FP8(scales)     # 双重量化：量化常数再量化</span></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a><span class="in">Training loop:</span></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a><span class="in">    for batch in D:</span></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a><span class="in">        # 反量化到 BF16 用于计算</span></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a><span class="in">        W_bf16 = Dequantize(W_nf4, scales)</span></span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a><span class="in">        # 前向传播（LoRA 部分用 BF16）</span></span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a><span class="in">        output = W_bf16 @ x + (A @ B) @ x</span></span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a><span class="in">        loss = compute_loss(output, targets)</span></span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a><span class="in">        # 反向传播（只更新 A, B）</span></span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a><span class="in">        gradients = backward(loss, [A, B])</span></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a><span class="in">        # 处理显存峰值</span></span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a><span class="in">        if memory_pressure &gt; threshold:</span></span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a><span class="in">            page_to_cpu(optimizer_states)  # Paged Optimizers</span></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a><span class="in">        update(A, B, gradients)</span></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a>*Source: 改编自 Dettmers et al. (2023) "QLoRA: Efficient Finetuning of Quantized LLMs", Section 3*</span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a><span class="fu">### LoRA 的数学原理：为什么低秩有效？</span></span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a>LoRA 的有效性基于一个关键假设：**微调引起的权重变化位于一个低维子空间中**。这个假设有理论和实证的双重支持。</span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>**理论视角：本征维度（Intrinsic Dimensionality）**</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a>Aghajanyan et al. (2020) 研究了预训练模型的"本征维度"——在保持 90% 性能的前提下，微调需要的最小自由度。他们发现，RoBERTa-base 的本征维度只有约 200，远小于其 1.25 亿参数。这意味着微调的有效更新集中在一个极低维的子空间中。</span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>**实证视角：权重变化的奇异值分布**</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>Hu et al. 分析了微调前后的权重差异 $\Delta W = W_{\text{finetuned}} - W_0$，发现其奇异值快速衰减——前几个奇异值占主导，其余接近于零。这说明 $\Delta W$ 确实是低秩的，用 $r = 4$ 或 $r = 8$ 的近似就能捕获大部分信息。</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>**为什么预训练模型会有这种性质？**</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>一个直觉解释：预训练已经学习了通用的语言表示，任务适配只需要在这个表示空间中做"微调"——选择性地增强或抑制某些方向。这种调整本质上是线性的、低维的，不需要"重新发明轮子"。</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a><span class="fu">### LoRA 的设计决策</span></span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a>**应该对哪些矩阵应用 LoRA？**</span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a>Transformer 中有多种权重矩阵：$W_Q, W_K, W_V$（注意力的 Query、Key、Value 投影）、$W_O$（注意力的输出投影）、$W_{\text{up}}, W_{\text{down}}$（FFN 层）。Hu et al. 的实验表明：</span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>只对 $W_Q$ 和 $W_V$ 应用 LoRA 就能获得大部分收益</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>同时对所有注意力矩阵应用效果最佳</span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>FFN 层的收益相对较小</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a>这个发现的直觉是：注意力层决定了"关注什么信息"，是任务适配的关键；FFN 层更多是"处理信息"，在预训练中已经学得很好。</span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a>**秩 $r$ 应该选多大？**</span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a>这是 LoRA 最重要的超参数。Hu et al. 发现：</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r = 1$ 就能在很多任务上超过全参数微调的 baseline</span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r = 4$ 到 $r = 8$ 通常是性能-效率的甜点</span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r$ 超过 16 后，增益递减</span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a>令人惊讶的是，$r$ 的选择对性能影响不大——即使 $r = 1$，性能也只比 $r = 64$ 差一点点。这进一步证实了权重更新的低秩假设。</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：LoRA 前向传播</span></span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的数值例子来理解 LoRA 的计算过程。</span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a>**设定**：输入维度 $d = 4$，输出维度 $k = 4$，秩 $r = 2$。</span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a>**Step 1：预训练权重**</span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a>W_0 = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 <span class="sc">\\</span> 0.5 &amp; 0.6 &amp; 0.7 &amp; 0.8 <span class="sc">\\</span> 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 <span class="sc">\\</span> 0.6 &amp; 0.7 &amp; 0.8 &amp; 0.9 \end{bmatrix}</span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a>**Step 2：LoRA 矩阵**（假设已训练好）</span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a>B = \begin{bmatrix} 0.1 &amp; -0.1 <span class="sc">\\</span> 0.2 &amp; 0.0 <span class="sc">\\</span> -0.1 &amp; 0.1 <span class="sc">\\</span> 0.0 &amp; 0.2 \end{bmatrix}, \quad</span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a>A = \begin{bmatrix} 0.5 &amp; 0.3 &amp; -0.2 &amp; 0.1 <span class="sc">\\</span> 0.2 &amp; -0.4 &amp; 0.1 &amp; 0.3 \end{bmatrix}</span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a>**Step 3：计算 $\Delta W = BA$**</span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a>\Delta W = BA = \begin{bmatrix} 0.03 &amp; 0.07 &amp; -0.03 &amp; -0.02 <span class="sc">\\</span> 0.10 &amp; 0.06 &amp; -0.04 &amp; 0.02 <span class="sc">\\</span> -0.03 &amp; -0.07 &amp; 0.03 &amp; 0.02 <span class="sc">\\</span> 0.04 &amp; -0.08 &amp; 0.02 &amp; 0.06 \end{bmatrix}</span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a>**Step 4：合并权重**</span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>W = W_0 + \alpha \cdot \Delta W</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a>其中 $\alpha$ 是缩放因子（通常设为 $\alpha / r$，用于稳定训练）。</span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a>**Step 5：输入向量**</span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a>x = \begin{bmatrix} 1.0 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> 0.0 \end{bmatrix}</span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a>**Step 6：计算输出**</span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a>h = W \cdot x = (W_0 + \alpha \cdot BA) \cdot x</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a>在推理时，我们只需要一次矩阵乘法（因为 $W_0 + BA$ 已经预先合并）。</span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a><span class="fu">### QLoRA：量化与低秩的结合</span></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a>QLoRA 的核心创新是：**在 4-bit 量化的冻结模型上应用 LoRA**。</span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a>全精度模型需要大量显存，而量化可以将模型压缩到原来的 1/4 甚至 1/8。但传统量化后无法微调（梯度需要高精度）。QLoRA 解决了这个问题。</span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a>**QLoRA 的三个核心技术**</span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a>**1. 4-bit NormalFloat (NF4)**</span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a>NF4 是一种新的 4-bit 数据类型，专门为正态分布的权重设计。它的量化区间不是均匀的，而是根据正态分布的分位数设置，使得量化误差在信息论意义上最小。</span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a>**2. 双重量化（Double Quantization）**</span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a>量化需要存储量化常数（每个 block 一个 scale 和 zero point）。QLoRA 对这些量化常数再做一次量化，进一步减少内存占用——平均每个参数节省 0.37 bit。</span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a>**3. 分页优化器（Paged Optimizers）**</span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a>长序列训练时，激活值会导致显存峰值。QLoRA 使用 NVIDIA 统一内存，在 GPU 显存不足时自动将数据转移到 CPU 内存，避免 OOM。</span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a>**QLoRA 的惊人结果**</span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a>使用 QLoRA，Dettmers et al. 在**单张 48GB GPU** 上微调了 65B 参数的模型。他们的最佳模型 Guanaco 在 Vicuna 基准上达到了 ChatGPT 性能的 99.3%——仅用 24 小时训练。这使得大模型微调从"需要一个集群"变成了"一张消费级 GPU 就够"。</span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a><span class="fu">### DoRA：权重分解的新视角</span></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>DoRA（Weight-Decomposed Low-Rank Adaptation）是 2024 年的最新进展，被选为 ICML 2024 Oral。</span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a>DoRA 的核心观察是：**LoRA 和全参数微调的权重更新模式不同**。全参数微调倾向于改变权重的**方向**（direction）而较少改变**幅度**（magnitude），而 LoRA 同时改变两者。</span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a>基于这个观察，DoRA 将权重分解为幅度和方向两部分：</span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a>W = m \cdot \frac{V}{<span class="sc">\|</span>V<span class="sc">\|</span>_c}</span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a>其中 $m$ 是幅度向量，$V$ 是方向矩阵，$<span class="sc">\|</span>\cdot<span class="sc">\|</span>_c$ 是列归一化。</span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>DoRA 对方向部分使用 LoRA 更新：</span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a>W' = m \cdot \frac{V + \Delta V}{<span class="sc">\|</span>V + \Delta V<span class="sc">\|</span>_c}, \quad \Delta V = BA</span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a>这种设计使得 DoRA 的学习动态更接近全参数微调，在多个任务上超越了 LoRA。</span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现 LoRA</span></span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRALayer(nn.Module):</span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""LoRA 层：对线性层进行低秩适配"""</span></span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a>        in_features: <span class="bu">int</span>,</span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a>        out_features: <span class="bu">int</span>,</span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a>        rank: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a>        alpha: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rank <span class="op">=</span> rank</span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scaling <span class="op">=</span> alpha <span class="op">/</span> rank</span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 低秩矩阵 A 和 B</span></span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A: 下投影，初始化为高斯分布</span></span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_A <span class="op">=</span> nn.Parameter(torch.randn(in_features, rank) <span class="op">*</span> <span class="fl">0.01</span>)</span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B: 上投影，初始化为零（确保训练开始时 ΔW = 0）</span></span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora_B <span class="op">=</span> nn.Parameter(torch.zeros(rank, out_features))</span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""计算 LoRA 的增量输出"""</span></span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: [batch, seq_len, in_features]</span></span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算 x @ A @ B，得到 [batch, seq_len, out_features]</span></span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (x <span class="op">@</span> <span class="va">self</span>.lora_A <span class="op">@</span> <span class="va">self</span>.lora_B) <span class="op">*</span> <span class="va">self</span>.scaling</span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearWithLoRA(nn.Module):</span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""带 LoRA 的线性层：冻结原始权重，只训练 LoRA 参数"""</span></span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a>        linear: nn.Linear,</span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a>        rank: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a>        alpha: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> linear</span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lora <span class="op">=</span> LoRALayer(</span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a>            linear.in_features,</span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a>            linear.out_features,</span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a>            rank<span class="op">=</span>rank,</span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span>alpha,</span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 冻结原始权重</span></span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.linear.parameters():</span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 原始输出 + LoRA 增量</span></span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear(x) <span class="op">+</span> <span class="va">self</span>.lora(x)</span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> merge_weights(<span class="va">self</span>):</span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""将 LoRA 权重合并到原始权重中（用于推理优化）"""</span></span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a>            <span class="co"># W' = W + scaling * B @ A</span></span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.linear.weight.data <span class="op">+=</span> (</span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.lora.scaling <span class="op">*</span></span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.lora.lora_B.T <span class="op">@</span> <span class="va">self</span>.lora.lora_A.T</span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并后可以删除 LoRA 参数</span></span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear</span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a>**代码解读**：</span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="in">`LoRALayer`</span> 实现低秩分解 $\Delta W = BA$，其中 $A$ 用高斯初始化，$B$ 用零初始化</span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="in">`scaling = alpha / rank`</span> 是一个稳定训练的技巧：当增大 rank 时，自动减小每个方向的贡献</span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="in">`merge_weights()`</span> 方法将 LoRA 权重合并到原始权重中，实现推理零额外延迟</span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用 Hugging Face PEFT 库</span></span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model, TaskType</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载基座模型</span></span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-2-7b-hf"</span></span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置 LoRA</span></span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span>TaskType.CAUSAL_LM,</span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">8</span>,                      <span class="co"># 秩</span></span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>,            <span class="co"># alpha 参数</span></span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,        <span class="co"># Dropout 防止过拟合</span></span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[          <span class="co"># 应用 LoRA 的模块</span></span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a>        <span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"o_proj"</span>,  <span class="co"># 注意力</span></span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a>        <span class="st">"gate_proj"</span>, <span class="st">"up_proj"</span>, <span class="st">"down_proj"</span>,      <span class="co"># FFN</span></span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,              <span class="co"># 不训练 bias</span></span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 PEFT 模型</span></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a>peft_model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看可训练参数</span></span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a>peft_model.print_trainable_parameters()</span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a><span class="co"># 输出类似: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622</span></span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a><span class="fu">### QLoRA 微调示例</span></span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> prepare_model_for_kbit_training</span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BitsAndBytesConfig</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> bitsandbytes <span class="im">as</span> bnb</span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a><span class="co"># 4-bit 量化配置</span></span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a>bnb_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">"nf4"</span>,           <span class="co"># NormalFloat 量化</span></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_use_double_quant<span class="op">=</span><span class="va">True</span>,      <span class="co"># 双重量化</span></span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载量化模型</span></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a>    <span class="st">"meta-llama/Llama-2-70b-hf"</span>,</span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>bnb_config,</span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备模型用于 k-bit 训练</span></span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> prepare_model_for_kbit_training(model)</span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a><span class="co"># 配置 LoRA（与上面相同）</span></span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">64</span>,                     <span class="co"># QLoRA 通常用更大的 rank</span></span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>],</span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span>TaskType.CAUSAL_LM,</span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a>peft_model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a><span class="co"># 现在可以在单张 GPU 上微调 70B 模型！</span></span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a>**过参数化与本征维度**</span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a>现代神经网络是严重过参数化的：参数数量远超训练样本数量，却能泛化得很好。这种现象的一个解释是**双下降**（double descent）：模型复杂度超过某个阈值后，测试误差反而下降。</span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a>PEFT 方法利用了过参数化的另一面：既然模型的有效自由度远小于参数数量，那么微调时也只需要调整少量"关键"参数。本征维度的研究表明，这个关键参数集合往往可以表示为原始参数空间的一个低维子空间。</span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a>**线性子空间假设**</span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a>LoRA 的有效性暗示了一个更强的假设：不仅微调的有效更新是低维的，而且它是一个**线性子空间**。这意味着最优的 $\Delta W$ 可以写成少数几个基向量的线性组合。</span>
<span id="cb7-613"><a href="#cb7-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-614"><a href="#cb7-614" aria-hidden="true" tabindex="-1"></a>Hu et al. 的分析支持这个假设：他们发现不同初始化种子训练出的 LoRA 矩阵，其列向量张成的子空间有很高的重叠。这说明这个"有效更新子空间"是任务本身的性质，而非随机初始化的结果。</span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a>**LoRA 什么时候不够？**</span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**任务与预训练差异太大**：如果下游任务需要的知识完全不在预训练分布内（比如用英文模型做中文任务），LoRA 可能不足以捕获所需的变化。这时可能需要更大的 rank 或全参数微调。</span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**数据量非常大**：当下游数据量接近预训练数据量时，LoRA 的低秩约束可能成为瓶颈。研究表明，在超大规模持续预训练场景下，全参数微调仍有优势。</span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**需要"遗忘"预训练知识**：LoRA 本质上是"加性"更新，难以完全覆盖预训练权重。如果任务需要模型"忘记"某些预训练行为，可能需要其他技术。</span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a>**秩选择的经验法则**</span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**任务简单/数据少**：$r = 4$ 或 $r = 8$</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**任务复杂/数据多**：$r = 16$ 到 $r = 64$</span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**极端效率需求**：$r = 1$（惊人地，这通常也能工作！）</span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**最优 PEFT 架构**：是否存在一种理论上最优的 PEFT 设计？目前的方法都是启发式的，缺乏理论指导。</span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**组合多个 LoRA**：能否通过组合多个任务的 LoRA 权重实现零样本的任务泛化？初步研究表明 LoRA 权重可以线性插值，但理论尚不清楚。</span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**LoRA 的表达能力边界**：给定 rank $r$，LoRA 能表示的权重更新有多大范围？这个问题与矩阵低秩近似的理论密切相关。</span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本方法的局限</span></span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a>**局限 1：对超参数敏感**</span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a>不同的模型、任务、数据量需要不同的 LoRA 配置（rank、alpha、target_modules）。目前没有通用的选择原则，需要实验调参。</span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a>**局限 2：与全参数微调仍有差距**</span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a>虽然 LoRA 在大多数任务上接近全参数微调，但在某些复杂任务上仍有 1–3% 的性能差距。对于追求极致性能的场景，全参数微调仍是更安全的选择。</span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a>**局限 3：训练不稳定**</span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a>QLoRA 等方法在低精度下训练，可能面临数值稳定性问题。训练过程需要仔细监控，可能需要多次实验。</span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a>PEFT 方法解决了"如何廉价地适配大模型"，但模型部署还面临另一个挑战：**推理效率**。即使用 LoRA 微调只需要一张 GPU，推理一个 70B 模型仍然需要大量显存和计算。下一章，我们将探讨**推理优化**技术——量化、投机解码、持续批处理等——它们让大模型的部署成本进一步降低。</span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：全参数微调在大模型时代不可行——175B 参数需要 1.5TB 显存</span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**洞察**：权重更新具有低秩结构，大部分变化集中在低维子空间</span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**方法**：</span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Adapter**：插入瓶颈模块，增加 3% 参数</span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Prefix/Prompt Tuning**：学习软提示，0.1% 参数</span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**LoRA**：低秩矩阵分解，0.01%–1% 参数，推理零延迟</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**QLoRA**：4-bit 量化 + LoRA，单 GPU 微调 70B 模型</span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**DoRA**：权重分解，更接近全参数微调的学习动态</span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**意义**：将大模型微调从"需要一个集群"变成"一张消费级 GPU"</span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LoRA 更新**：$h = W_0 x + BA \cdot x$，其中 $B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$</span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**参数量**：$r \times (d + k)$，比原始 $d \times k$ 减少约 $\frac{d+k}{r}$ 倍</span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Adapter 瓶颈**：$\text{Adapter}(h) = h + f(h W_{\text{down}}) W_{\text{up}}$</span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法对比速查</span></span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 参数量 <span class="pp">|</span> 推理延迟 <span class="pp">|</span> 适用场景 <span class="pp">|</span></span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|----------|----------|</span></span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Adapter <span class="pp">|</span> 3–5% <span class="pp">|</span> +20–30% <span class="pp">|</span> 多任务学习 <span class="pp">|</span></span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Prefix Tuning <span class="pp">|</span> 0.1–1% <span class="pp">|</span> 无（序列变长） <span class="pp">|</span> 生成任务 <span class="pp">|</span></span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Prompt Tuning <span class="pp">|</span> 0.01% <span class="pp">|</span> 无 <span class="pp">|</span> 超大模型、低数据 <span class="pp">|</span></span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LoRA <span class="pp">|</span> 0.01–1% <span class="pp">|</span> **0%** <span class="pp">|</span> 通用场景（推荐） <span class="pp">|</span></span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> QLoRA <span class="pp">|</span> 0.01–1% <span class="pp">|</span> 0% <span class="pp">|</span> 显存受限 <span class="pp">|</span></span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> DoRA <span class="pp">|</span> 0.01–1% <span class="pp">|</span> 0% <span class="pp">|</span> 追求极致性能 <span class="pp">|</span></span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么 LoRA 的 $B$ 矩阵初始化为零？如果用随机初始化会怎样？</span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明 LoRA 参数量公式。如果对 Transformer 的 $W_Q, W_K, W_V, W_O$ 四个矩阵都应用 rank-8 的 LoRA，一层增加多少参数？</span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用 PEFT 库对 LLaMA-7B 进行指令微调，比较 rank=4 和 rank=16 的效果差异。</span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** LoRA 能否用于持续预训练（continual pre-training）？与微调相比，持续预训练对 rank 的需求是更高还是更低？</span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[LoRA](https://arxiv.org/abs/2106.09685)**：Hu et al. (2021)，本章核心</span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 4（方法）、Section 7（rank 分析）</span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>可跳过：Appendix 的详细实验表格</span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[QLoRA](https://arxiv.org/abs/2305.14314)**：Dettmers et al. (2023)</span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（NF4、双重量化）、Section 5（Guanaco 实验）</span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[DoRA](https://arxiv.org/abs/2402.09353)**：Liu et al. (2024)，权重分解视角，ICML 2024 Oral</span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[LoRA+](https://arxiv.org/abs/2402.12354)**：Hayou et al. (2024)，A 和 B 使用不同学习率</span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[GaLore](https://arxiv.org/abs/2403.03507)**：Zhao et al. (2024)，梯度低秩投影</span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[microsoft/LoRA](https://github.com/microsoft/LoRA)**：官方实现</span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[huggingface/peft](https://github.com/huggingface/peft)**：统一的 PEFT 库，支持 LoRA、Prefix Tuning、Prompt Tuning、Adapter 等</span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[artidoro/qlora](https://github.com/artidoro/qlora)**：QLoRA 官方实现</span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a>LoRA 的灵感来自一个更早的观察：矩阵乘法是神经网络的核心运算，而矩阵分解是线性代数的核心技术。早在 2016 年，研究者就尝试用低秩矩阵加速推理（如 SVD 压缩）。但 LoRA 的创新在于将低秩约束用于**训练时的更新**而非**推理时的近似**——这个视角转换是关键。</span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-736"><a href="#cb7-736" aria-hidden="true" tabindex="-1"></a>Hu 等人最初在 GPT-3 上验证 LoRA，但由于 GPT-3 不开源，无法直接发布代码。讽刺的是，LoRA 真正流行是在 LLaMA 开源之后——开源模型的繁荣为 PEFT 方法提供了广阔的应用舞台。今天，几乎所有的开源模型社区都默认使用 LoRA 进行微调，它已经成为大模型时代的"标准配置"。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>