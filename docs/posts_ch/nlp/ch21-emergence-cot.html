<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="上一章我们见证了GPT-3的In-Context Learning——不需要梯度更新，仅凭输入中的几个示例就能完成新任务。但ICL有一个致命弱点：它在需要多步推理的任务上几乎完全失败。2022年，Wei等人发现了一个出奇简单的解决方案：在few-shot示例中不仅给出答案，还给出完整的推理过程——Chain-of-Thought prompting。这个看似微小的改动带来了数学和逻辑推理上的飞跃式提升，更引发了关于LLM’涌现能力’的激烈争论：这些能力是真实的相变，还是度量方式制造的假象？本章系统讲述CoT及其变体，探讨涌现现象的本质，并追问一个根本问题：LLM真的在’推理’吗？">

<title>第21章：涌现能力与思维链推理 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#为什么直接回答会失败" id="toc-为什么直接回答会失败" class="nav-link" data-scroll-target="#为什么直接回答会失败"><span class="header-section-number">2.1</span> 为什么直接回答会失败？</a></li>
  <li><a href="#人类解题的启示" id="toc-人类解题的启示" class="nav-link" data-scroll-target="#人类解题的启示"><span class="header-section-number">2.2</span> 人类解题的启示</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#关键洞察展示你的工作过程" id="toc-关键洞察展示你的工作过程" class="nav-link" data-scroll-target="#关键洞察展示你的工作过程"><span class="header-section-number">3.1</span> 关键洞察：“展示你的工作过程”</a></li>
  <li><a href="#一个类比开卷考试-vs-闭卷考试" id="toc-一个类比开卷考试-vs-闭卷考试" class="nav-link" data-scroll-target="#一个类比开卷考试-vs-闭卷考试"><span class="header-section-number">3.2</span> 一个类比：开卷考试 vs 闭卷考试</a></li>
  <li><a href="#为什么只有大模型受益" id="toc-为什么只有大模型受益" class="nav-link" data-scroll-target="#为什么只有大模型受益"><span class="header-section-number">3.3</span> 为什么只有大模型受益？</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#chain-of-thought-prompting-wei-et-al.-2022" id="toc-chain-of-thought-prompting-wei-et-al.-2022" class="nav-link" data-scroll-target="#chain-of-thought-prompting-wei-et-al.-2022"><span class="header-section-number">4.1</span> Chain-of-Thought Prompting (Wei et al., 2022)</a></li>
  <li><a href="#zero-shot-cot-kojima-et-al.-2022" id="toc-zero-shot-cot-kojima-et-al.-2022" class="nav-link" data-scroll-target="#zero-shot-cot-kojima-et-al.-2022"><span class="header-section-number">4.2</span> Zero-shot CoT (Kojima et al., 2022)</a></li>
  <li><a href="#self-consistency-wang-et-al.-2022" id="toc-self-consistency-wang-et-al.-2022" class="nav-link" data-scroll-target="#self-consistency-wang-et-al.-2022"><span class="header-section-number">4.3</span> Self-Consistency (Wang et al., 2022)</a></li>
  <li><a href="#tree-of-thoughts-yao-et-al.-2023" id="toc-tree-of-thoughts-yao-et-al.-2023" class="nav-link" data-scroll-target="#tree-of-thoughts-yao-et-al.-2023"><span class="header-section-number">4.4</span> Tree of Thoughts (Yao et al., 2023)</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#实现cot推理" id="toc-实现cot推理" class="nav-link" data-scroll-target="#实现cot推理"><span class="header-section-number">5.1</span> 实现CoT推理</a></li>
  <li><a href="#实现self-consistency" id="toc-实现self-consistency" class="nav-link" data-scroll-target="#实现self-consistency"><span class="header-section-number">5.2</span> 实现Self-Consistency</a></li>
  <li><a href="#实现zero-shot-cot" id="toc-实现zero-shot-cot" class="nav-link" data-scroll-target="#实现zero-shot-cot"><span class="header-section-number">5.3</span> 实现Zero-shot CoT</a></li>
  <li><a href="#工程注意事项" id="toc-工程注意事项" class="nav-link" data-scroll-target="#工程注意事项"><span class="header-section-number">5.4</span> 工程注意事项</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#涌现能力规模带来相变-wei-et-al.-2022b" id="toc-涌现能力规模带来相变-wei-et-al.-2022b" class="nav-link" data-scroll-target="#涌现能力规模带来相变-wei-et-al.-2022b"><span class="header-section-number">6.1</span> 涌现能力：规模带来相变？ (Wei et al., 2022b)</a></li>
  <li><a href="#涌现是真实的还是海市蜃楼-schaeffer-et-al.-2023" id="toc-涌现是真实的还是海市蜃楼-schaeffer-et-al.-2023" class="nav-link" data-scroll-target="#涌现是真实的还是海市蜃楼-schaeffer-et-al.-2023"><span class="header-section-number">6.2</span> 涌现是真实的还是”海市蜃楼”？ (Schaeffer et al., 2023)</a></li>
  <li><a href="#围绕涌现的持续争论" id="toc-围绕涌现的持续争论" class="nav-link" data-scroll-target="#围绕涌现的持续争论"><span class="header-section-number">6.3</span> 围绕涌现的持续争论</a></li>
  <li><a href="#llm真的在推理吗" id="toc-llm真的在推理吗" class="nav-link" data-scroll-target="#llm真的在推理吗"><span class="header-section-number">6.4</span> LLM真的在”推理”吗？</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.5</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#本方法的局限" id="toc-本方法的局限" class="nav-link" data-scroll-target="#本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键对比速查" id="toc-关键对比速查" class="nav-link" data-scroll-target="#关键对比速查"><span class="header-section-number">8.2</span> 关键对比速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#方法改进" id="toc-方法改进" class="nav-link" data-scroll-target="#方法改进"><span class="header-section-number">9.2</span> 方法改进</a></li>
  <li><a href="#理论与分析" id="toc-理论与分析" class="nav-link" data-scroll-target="#理论与分析"><span class="header-section-number">9.3</span> 理论与分析</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">9.4</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第21章：涌现能力与思维链推理</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Emergence, Chain-of-Thought, and the Boundaries of LLM Reasoning</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">Chain-of-Thought</div>
    <div class="quarto-category">Emergence</div>
    <div class="quarto-category">Reasoning</div>
  </div>
  </div>

<div>
  <div class="description">
    上一章我们见证了GPT-3的In-Context Learning——不需要梯度更新，仅凭输入中的几个示例就能完成新任务。但ICL有一个致命弱点：它在需要多步推理的任务上几乎完全失败。2022年，Wei等人发现了一个出奇简单的解决方案：在few-shot示例中不仅给出答案，还给出完整的推理过程——Chain-of-Thought prompting。这个看似微小的改动带来了数学和逻辑推理上的飞跃式提升，更引发了关于LLM’涌现能力’的激烈争论：这些能力是真实的相变，还是度量方式制造的假象？本章系统讲述CoT及其变体，探讨涌现现象的本质，并追问一个根本问题：LLM真的在’推理’吗？
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：大语言模型能否进行多步推理？如何通过提示策略释放这种潜力？随规模增长而”涌现”的能力是真实的相变，还是度量方式制造的假象？</p>
<p><strong>历史坐标</strong>：2022 | Wei et al.&nbsp;“Chain-of-Thought Prompting” &amp; “Emergent Abilities of Large Language Models” | 从模式匹配到（疑似）推理</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Wei et al.&nbsp;(2022a)</strong> “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models” (arXiv:2201.11903) — 参考了 Section 2-3（CoT方法定义与实验）、Figure 1（CoT示例对比图）、Figure 2（模型规模与CoT效果的关系）、Table 1-5（GSM8K等benchmark结果）；提取了 Figure 1 作为论文原图</li>
<li><strong>Kojima et al.&nbsp;(2022)</strong> “Large Language Models are Zero-Shot Reasoners” (arXiv:2205.11916) — 参考了 Section 3（Zero-shot CoT方法）、Figure 1（Zero-shot CoT流程图）、Table 1-2（MultiArith/GSM8K等实验结果）</li>
<li><strong>Wang et al.&nbsp;(2022)</strong> “Self-Consistency Improves Chain of Thought Reasoning in Language Models” (arXiv:2203.11171) — 参考了 Section 2（Self-Consistency方法定义）、Figure 1（方法示意图）、Table 1-3（数学推理benchmark结果）；提取了 Figure 1 作为论文原图</li>
<li><strong>Yao et al.&nbsp;(2023)</strong> “Tree of Thoughts: Deliberate Problem Solving with Large Language Models” (arXiv:2305.10601) — 参考了 Section 2-3（ToT框架定义）、Figure 1（四种prompting方法对比图）、Table 1（24点游戏实验结果）；提取了 Figure 1 作为论文原图</li>
<li><strong>Wei et al.&nbsp;(2022b)</strong> “Emergent Abilities of Large Language Models” (arXiv:2206.07682) — 参考了 Section 2（涌现定义）、Figure 1-2（涌现能力的经典”相变”图）、Table 1（涌现能力列表）；提取了 Figure 2 作为论文原图</li>
<li><strong>Schaeffer et al.&nbsp;(2023)</strong> “Are Emergent Abilities of Large Language Models a Mirage?” (arXiv:2304.15004, NeurIPS 2023 Best Paper) — 参考了 Section 2-3（度量选择对涌现的影响）、Figure 1（相同数据不同度量的对比）</li>
<li><strong>Zhou et al.&nbsp;(2022)</strong> “Least-to-Most Prompting Enables Complex Reasoning in Large Language Models” (arXiv:2205.10625) — 参考了分解策略</li>
<li><strong>Suzgun et al.&nbsp;(2022)</strong> “Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them” (arXiv:2210.09261, BIG-Bench Hard) — 参考了 CoT 在困难任务上的系统评估</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li>SLP3 Chapter 12 (Prompting, In-Context Learning, and Instruct Tuning) — 参考了 CoT prompting 的教学组织</li>
<li>D2L Chapter 11 — 参考了注意力机制在推理中的作用讨论</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li>Stanford CS224N Lecture 11-12 (Winter 2025) — 参考了 Prompting 和 Reasoning 的教学框架</li>
<li>CMU 11-711 ANLP (Fall 2024) “Prompting &amp; In-context Learning” — 参考了 CoT 及其变体的系统化讲解</li>
<li>Princeton COS 597R (Fall 2024) “Deep Dive into LLMs” — 参考了涌现能力的讨论</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们详细讲述了GPT-3和In-Context Learning的故事。175B参数的GPT-3展示了一种令人震惊的能力：不需要任何梯度更新，仅通过在输入中提供几个示例，模型就能”学会”翻译、分类、问答等各种任务。这种能力随规模增长而增强——大模型不仅绝对性能更高，还更擅长利用上下文示例——这暗示着规模本身可能是通向通用智能的关键。</p>
<p>然而，上一章结尾也揭示了ICL的一个致命弱点：<strong>它在需要多步推理的任务上几乎完全失败</strong>。</p>
<p>考虑这样一个小学数学题：“Roger有5个网球。他又买了2罐网球。每罐有3个网球。他现在一共有多少个网球？”一个人类学生会这样思考：Roger原来有5个，买了2罐×3个=6个，所以一共有5+6=11个。但当你把这个问题直接扔给GPT-3——即使给了几个类似问题的输入-输出示例——模型经常直接输出一个错误的数字，跳过所有中间推理步骤。</p>
<p>这不是因为GPT-3”不知道”加法或乘法。它在预训练中见过海量的数学文本，对基本运算有相当的”知识”。问题在于：<strong>标准的ICL只展示了最终答案，没有展示到达答案的思考过程</strong>。模型被训练去模仿”问题→答案”的直接映射，而不是”问题→推理步骤→答案”的逐步求解。</p>
<p>2022年初，Google Brain的Jason Wei等人发现了一个出奇简单的解决方案——简单到让整个社区都感到不可思议。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：Chain-of-Thought（思维链）prompting的核心思想是：在few-shot示例中不仅给出最终答案，还给出完整的中间推理过程。这个看似微小的改动——相当于在数学考试中要求学生”展示你的解题过程”——让大语言模型在数学推理、逻辑推理和常识推理上获得了飞跃式提升。更令人惊讶的是，这种推理能力呈现出”涌现”特征：小模型几乎不受益于CoT，但当模型规模超过某个阈值时，CoT的效果突然爆发。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="为什么直接回答会失败" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="为什么直接回答会失败"><span class="header-section-number">2.1</span> 为什么直接回答会失败？</h3>
<p>让我们用一个更具体的例子来剖析ICL在推理任务上失败的根本原因。考虑GSM8K数据集（Grade School Math 8K）中的一道典型题：</p>
<blockquote class="blockquote">
<p><em>咖啡店周一卖了37杯拿铁和13杯卡布奇诺。周二拿铁卖了周一的两倍，但卡布奇诺只卖了周一的一半（四舍五入取整）。两天一共卖了多少杯咖啡？</em></p>
</blockquote>
<p>要正确回答这个问题，需要四步推理：</p>
<ol type="1">
<li>周二拿铁：37 × 2 = 74杯</li>
<li>周二卡布奇诺：13 ÷ 2 ≈ 7杯（四舍五入）</li>
<li>周一总计：37 + 13 = 50杯</li>
<li>两天总计：50 + 74 + 7 = 131杯</li>
</ol>
<p>标准的few-shot ICL如何处理这类问题？在传统范式下，few-shot示例只包含”问题→答案”的映射：</p>
<pre><code>Q: 农场有23只鸡和7只鸭。农场一共有多少只家禽？
A: 30

Q: 咖啡店周一卖了37杯拿铁和13杯卡布奇诺...
A:</code></pre>
<p>模型看到的模式是：给定一个数学问题，直接输出一个数字。但从”问题文本”到”最终数字”之间有一条复杂的推理链，这条链在示例中是<strong>完全不可见的</strong>。模型被要求做的不是逐步计算，而是”一步到位”地猜出答案——本质上是在做一个困难的端到端映射。</p>
<p>这里有一个关键的认知错位：人类在看到”答案是30”时，脑中自动补全了”23+7=30”的推理过程。但对语言模型而言，它只看到了一个从长文本到单个数字的黑箱映射。</p>
</section>
<section id="人类解题的启示" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="人类解题的启示"><span class="header-section-number">2.2</span> 人类解题的启示</h3>
<p>反观人类如何解决数学问题。没有哪个数学老师会告诉学生：“看完题目后直接写出答案。”相反，从小学开始，数学教育就强调<strong>“展示你的解题过程”</strong>（show your work）。这不仅是为了方便老师打分——更重要的是，解题过程本身就是思考的载体。</p>
<p>写下中间步骤至少有三个好处：</p>
<p>第一，<strong>分解复杂性</strong>。一个需要四步推理的问题，如果必须一步到位地得出答案，相当于要求大脑同时处理四个运算并正确组合结果。而如果把它分解成四个独立的步骤，每一步只需要做一个简单的运算。</p>
<p>第二，<strong>提供工作记忆</strong>。人类的工作记忆容量有限（著名的”7±2”规则），写下中间结果相当于用纸笔扩展了工作记忆。类似地，语言模型的”工作记忆”就是它能生成和回读的文本——如果中间步骤被写在输出中，模型就可以在后续步骤中引用这些中间结果。</p>
<p>第三，<strong>使错误可检测</strong>。如果只看到最终答案是错的，你无从知道哪里出了问题。但如果有完整的推理链，就能精确定位错误发生在哪一步。</p>
<p>这个来自人类教育的朴素洞察，正是Chain-of-Thought prompting的核心灵感。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>基于上述分析，理想的解决方案应该具备几个特性。首先，它应该能让模型生成中间推理步骤，而不是直接跳到最终答案。其次，它不应该需要修改模型架构或重新训练——GPT-3这样的大模型训练一次就要数百万美元，我们希望在推理阶段（inference time）就能提升性能。第三，它应该是通用的，适用于各种需要推理的任务，而不仅限于数学。</p>
<p>Chain-of-Thought prompting恰好满足了这三个要求：它通过改变prompt的构造方式——在示例中加入推理过程——引导模型也生成类似的中间步骤，而完全不需要修改模型本身。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="关键洞察展示你的工作过程" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="关键洞察展示你的工作过程"><span class="header-section-number">3.1</span> 关键洞察：“展示你的工作过程”</h3>
<p>Chain-of-Thought prompting的核心思想可以用一句话概括：<strong>在few-shot示例中展示推理过程，模型就会学着也展示推理过程</strong>。</p>
<div id="fig-cot-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cot-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-21/original/fig1-cot-example.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cot-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Chain-of-Thought prompting与标准prompting的对比。左侧：标准few-shot只给出最终答案，模型直接输出（通常错误的）数字。右侧：CoT在示例中展示完整的推理过程，引导模型也生成中间步骤，最终得到正确答案。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Wei et al.&nbsp;(2022) “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”, Figure 1</em></p>
</div>
<p>对比标准few-shot和CoT：</p>
<p><strong>标准 few-shot</strong>（只给答案）：</p>
<pre><code>Q: Roger有5个网球。他又买了2罐网球。每罐有3个网球。
   他现在一共有多少个网球？
A: 答案是11。</code></pre>
<p><strong>Chain-of-Thought</strong>（给出推理过程）：</p>
<pre><code>Q: Roger有5个网球。他又买了2罐网球。每罐有3个网球。
   他现在一共有多少个网球？
A: Roger一开始有5个网球。2罐网球一共有2×3=6个网球。
   5+6=11。答案是11。</code></pre>
<p>区别仅仅在于：答案前面多了几句解释——“Roger一开始有5个网球。2罐网球一共有2×3=6个网球。5+6=11。”这就是全部的技巧。</p>
<p>为什么这么简单的改动会有如此大的效果？回到In-Context Learning的本质：模型从示例中学习的不仅是”任务内容”，更重要的是<strong>输出格式和模式</strong>。当示例中的答案只有一个数字时，模型学到的模式是”问题→数字”；当示例中的答案包含推理链条时，模型学到的模式变成了”问题→推理步骤→数字”。后者给了模型一个”思考的空间”——通过生成中间文本，模型可以一步一步地分解问题，每一步只需要做一个相对简单的操作。</p>
</section>
<section id="一个类比开卷考试-vs-闭卷考试" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="一个类比开卷考试-vs-闭卷考试"><span class="header-section-number">3.2</span> 一个类比：开卷考试 vs 闭卷考试</h3>
<p>标准ICL就像一场<strong>闭卷考试</strong>：学生（模型）看完题目后必须直接在答题卡上填写最终答案，不允许在草稿纸上写任何计算过程。对于简单的事实性问题（“法国的首都是哪里？”），这没有问题。但对于需要多步推理的数学题，这就像要求学生在脑中完成所有计算，不犯一个错误。</p>
<p>CoT则像是给了学生一张<strong>草稿纸</strong>：你可以在上面写下中间步骤、画出辅助线、记录临时结果，最后再把答案写到答题卡上。这不是在”降低考试难度”——题目没有变，需要的知识也没有变——你只是给了解题过程一个被记录和引用的空间。</p>
<p>对语言模型来说，这张”草稿纸”就是模型生成的文本本身。自回归模型在生成每个token时，可以”看到”它之前生成的所有token。如果模型先生成了”2×3=6”，那么在生成下一步时，“6”这个中间结果就出现在了模型的上下文中，可以被直接引用来计算”5+6=11”。本质上，CoT将模型的输出空间从”答案空间”扩展到了”推理链+答案空间”，让模型可以利用自己的生成结果作为”工作记忆”。</p>
</section>
<section id="为什么只有大模型受益" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="为什么只有大模型受益"><span class="header-section-number">3.3</span> 为什么只有大模型受益？</h3>
<p>CoT最令人困惑的特征之一是它的<strong>规模依赖性</strong>：只有足够大的模型（通常≥100B参数）才能从CoT中显著受益，小模型即使给了CoT示例也无法生成连贯的推理链条。</p>
<p>一个直觉性的解释是：生成正确的CoT需要模型同时具备多种能力——理解数学关系、执行算术运算、保持逻辑连贯、遵循示例格式——这些能力可能各自需要一定的模型容量才能涌现。小模型在每一项上都只有勉强及格的水平，组合起来就完全不够用了。而大模型在各项上都有充足的”余量”，让它们可以流畅地生成多步推理链。</p>
<p>这与人类认知发展有一个有趣的平行：皮亚杰的认知发展理论指出，儿童在某个年龄阶段之前无法执行”形式运算”（如假设推理），不是因为他们不知道某个具体知识，而是因为他们的认知系统还没有复杂到足以支持这种运算模式。类似地，小语言模型可能”知道”2×3=6和5+6=11，但无法将这些知识组织成一个连贯的多步推理链。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="chain-of-thought-prompting-wei-et-al.-2022" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="chain-of-thought-prompting-wei-et-al.-2022"><span class="header-section-number">4.1</span> Chain-of-Thought Prompting (Wei et al., 2022)</h3>
<section id="方法定义" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="方法定义"><span class="header-section-number">4.1.1</span> 方法定义</h4>
<p>Chain-of-Thought prompting的方法极其简单，不涉及任何算法创新或模型修改。它唯一的改变是<strong>few-shot示例的构造方式</strong>：在每个示例的答案中加入中间推理步骤（reasoning chain），形成”问题→推理链→答案”的格式。</p>
<p>形式化地，设 <span class="math inline">\(x\)</span> 为输入问题，<span class="math inline">\(y\)</span> 为最终答案。标准few-shot的示例格式为 <span class="math inline">\((x, y)\)</span>，而CoT的示例格式为 <span class="math inline">\((x, c, y)\)</span>，其中 <span class="math inline">\(c = (c_1, c_2, \ldots, c_m)\)</span> 是一系列中间推理步骤。在推理时，给定新问题 <span class="math inline">\(x_{\text{query}}\)</span>，模型被引导先生成推理链 <span class="math inline">\(\hat{c}\)</span>，再基于推理链生成最终答案 <span class="math inline">\(\hat{y}\)</span>。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>CoT Prompting（Wei et al., 2022）方法概要
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入：语言模型 LM，K 个带推理链的示例 {(x₁, c₁, y₁), ..., (xₖ, cₖ, yₖ)}，
      查询问题 x_query
输出：推理链 ĉ 和最终答案 ŷ

1. 构造 prompt：
     prompt ← ""
     for k = 1 to K:
         prompt ← prompt + "Q: " + xₖ + "\n"
         prompt ← prompt + "A: " + cₖ + " 答案是 " + yₖ + "\n\n"
     prompt ← prompt + "Q: " + x_query + "\n"
     prompt ← prompt + "A: "

2. 前向推理（无梯度更新）：
     output ← LM.generate(prompt)

3. 从 output 中提取推理链 ĉ 和最终答案 ŷ

4. return (ĉ, ŷ)</code></pre>
<p><em>改编自 Wei et al.&nbsp;(2022) “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”, Section 2. <a href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a></em></p>
</div>
</div>
<p>值得注意的是，这个方法<strong>没有任何可训练的参数</strong>。推理链是手工编写的、放在few-shot示例中的自然语言文本。模型不是被”训练”去推理，而是被”引导”去模仿示例中的推理格式。</p>
</section>
<section id="完整数值示例cot如何解决数学题" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="完整数值示例cot如何解决数学题"><span class="header-section-number">4.1.2</span> 完整数值示例：CoT如何解决数学题</h4>
<p><strong>设定</strong>：使用PaLM 540B模型，在GSM8K数据集上进行few-shot CoT推理。</p>
<p><strong>Step 1: 构造CoT示例</strong></p>
<p>我们手工编写8个数学题的推理链作为few-shot示例。以下展示其中一个：</p>
<pre><code>Q: Shawn有5个玩具。圣诞节他从妈妈和爸爸那里各收到了2个玩具。
   他现在一共有多少个玩具？
A: Shawn一开始有5个玩具。他从妈妈那里收到2个，从爸爸那里也
   收到2个，所以他一共收到了2+2=4个新玩具。现在他一共有
   5+4=9个玩具。答案是9。</code></pre>
<p><strong>Step 2: 加入查询问题</strong></p>
<p>将上述8个CoT示例拼接成prompt，然后附上新问题：</p>
<pre><code>Q: 咖啡店周一卖了37杯拿铁和13杯卡布奇诺。周二拿铁卖了
   周一的两倍，但卡布奇诺只卖了周一的一半（四舍五入取整）。
   两天一共卖了多少杯咖啡？
A:</code></pre>
<p><strong>Step 3: 模型生成推理链</strong></p>
<p>PaLM 540B生成如下输出：</p>
<pre><code>周一卖了37杯拿铁和13杯卡布奇诺，周一总共卖了37+13=50杯。
周二拿铁卖了37×2=74杯。周二卡布奇诺卖了13÷2=6.5，
四舍五入得7杯。周二总共卖了74+7=81杯。
两天总共卖了50+81=131杯。答案是131。</code></pre>
<p><strong>Step 4: 提取答案</strong></p>
<p>从生成文本中提取”答案是131”，得到最终答案131。✓</p>
<p><strong>解读</strong>：注意模型的推理链几乎完美地复现了人类的解题过程——分步计算周一总量、周二各品类数量、周二总量，最后求和。每一步都是一个简单的算术运算，模型可以可靠地执行。如果没有CoT，模型需要一步从题目文本直接映射到”131”，这对模型来说是一个极难的黑箱映射。</p>
</section>
<section id="关键实验结果" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="关键实验结果"><span class="header-section-number">4.1.3</span> 关键实验结果</h4>
<p>Wei et al.&nbsp;(2022) 在三类推理benchmark上评估了CoT的效果：</p>
<p><strong>数学推理</strong>（GSM8K — 小学数学）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>标准few-shot</th>
<th>CoT few-shot</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3 175B</td>
<td>18.0%</td>
<td>49.6%</td>
<td>+31.6</td>
</tr>
<tr class="even">
<td>PaLM 540B</td>
<td>17.9%</td>
<td><strong>56.9%</strong></td>
<td>+39.0</td>
</tr>
<tr class="odd">
<td>微调SOTA（当时）</td>
<td>—</td>
<td>55.0%</td>
<td>—</td>
</tr>
</tbody>
</table>
<p>PaLM 540B + CoT不仅远超标准few-shot（56.9% vs 17.9%），还<strong>超过了当时的微调SOTA</strong>（55.0%）。这个结果意味着：一个足够大的通用语言模型，配合简单的CoT提示，可以在不经过任何任务特定训练的情况下，超越精心微调的专用模型。</p>
<p><strong>符号推理</strong>（如字母串拼接）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>任务</th>
<th>PaLM 540B标准</th>
<th>PaLM 540B CoT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>字母串拼接 (in-domain)</td>
<td>88.0%</td>
<td><strong>99.6%</strong></td>
</tr>
<tr class="even">
<td>字母串拼接 (OOD, 更长)</td>
<td>34.0%</td>
<td><strong>98.8%</strong></td>
</tr>
</tbody>
</table>
<p>CoT在域外（Out-of-Distribution）泛化上的提升尤为惊人：从34%跳到98.8%。这暗示CoT不仅帮助模型”做对已知模式的题”，还帮助它”学会了推理规则本身”——至少在某种程度上。</p>
<p><strong>常识推理</strong>（StrategyQA — 需要隐式推理的是/否问题）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>标准few-shot</th>
<th>CoT few-shot</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PaLM 540B</td>
<td>73.9%</td>
<td><strong>77.8%</strong></td>
</tr>
</tbody>
</table>
<p>在常识推理上，CoT的提升相对温和（+3.9个百分点），这可能是因为常识推理不像数学那样容易分解成明确的逻辑步骤。</p>
</section>
<section id="规模效应cot的涌现特征" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="规模效应cot的涌现特征"><span class="header-section-number">4.1.4</span> 规模效应：CoT的”涌现”特征</h4>
<p>CoT最引人注目的实验发现是它与模型规模之间的非线性关系。Wei et al.&nbsp;在不同规模的模型上测试了标准few-shot和CoT的效果：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型规模</th>
<th>标准few-shot (GSM8K)</th>
<th>CoT (GSM8K)</th>
<th>CoT增益</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>~8B</td>
<td>~5%</td>
<td>~5%</td>
<td>≈0</td>
</tr>
<tr class="even">
<td>~62B</td>
<td>~12%</td>
<td>~18%</td>
<td>+6</td>
</tr>
<tr class="odd">
<td>~540B</td>
<td>~18%</td>
<td>~57%</td>
<td><strong>+39</strong></td>
</tr>
</tbody>
</table>
<div id="fig-cot-scaling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cot-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-21/original/fig2-cot-scaling.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cot-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: CoT效果与模型规模的关系。在数学推理任务上，小模型（<sub>8B）几乎不受益于CoT，而大模型（</sub>540B）获得巨大提升。CoT的效果呈现出类似”涌现”的非线性跳跃。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Wei et al.&nbsp;(2022) “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”, Figure 2</em></p>
</div>
<p>规律极其清晰：<strong>小模型几乎不受益于CoT，中等模型有一些受益，大模型获得巨大提升</strong>。更精确地说，CoT的效果不是线性增长的，而是呈现出类似”相变”（phase transition）的急剧跳跃。在约100B参数处，CoT的效果似乎突然”爆发”了。</p>
<p>这个发现引出了一个更大的话题——涌现能力——我们将在本章后半部分详细讨论。</p>
</section>
</section>
<section id="zero-shot-cot-kojima-et-al.-2022" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="zero-shot-cot-kojima-et-al.-2022"><span class="header-section-number">4.2</span> Zero-shot CoT (Kojima et al., 2022)</h3>
<p>Wei et al.的CoT方法需要手工编写推理链示例，这虽然简单但仍需要一定的人工成本。2022年5月，Kojima et al.&nbsp;发现了一个令人震惊的结果：只需要在prompt末尾加上一句<strong>“Let’s think step by step”</strong>（让我们一步一步想），模型就能自动生成推理链——完全不需要任何few-shot示例。</p>
<p>这就是<strong>Zero-shot CoT</strong>：一种不需要任何示例的思维链推理方法。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Zero-shot CoT 两阶段推理流程（Kojima et al., 2022）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入：语言模型 LM，查询问题 x_query
输出：最终答案 ŷ

阶段1 — 推理提取（Reasoning Extraction）：
  prompt₁ ← x_query + "\nLet's think step by step."
  reasoning ← LM.generate(prompt₁)

阶段2 — 答案提取（Answer Extraction）：
  prompt₂ ← prompt₁ + reasoning +
             "\nTherefore, the answer (arabic numerals) is"
  ŷ ← LM.generate(prompt₂)

return ŷ</code></pre>
<p><em>Source: Kojima et al.&nbsp;(2022) “Large Language Models are Zero-Shot Reasoners”, Section 3. <a href="https://arxiv.org/abs/2205.11916">arXiv:2205.11916</a></em></p>
</div>
</div>
<p>注意这个方法分两个阶段：第一阶段让模型自由生成推理过程，第二阶段将推理过程作为上下文，引导模型给出最终答案。分两阶段的原因是：如果只用一个prompt，模型可能在生成推理链后”忘记”给出一个清晰的最终答案，或者答案被淹没在冗长的推理文本中。</p>
<section id="为什么这六个词如此有效" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="为什么这六个词如此有效"><span class="header-section-number">4.2.1</span> 为什么这六个词如此有效？</h4>
<p>“Let’s think step by step”为什么能work？这个问题至今没有完全令人满意的答案，但有几个可能的解释。</p>
<p>第一个解释与<strong>预训练数据</strong>有关。GPT-3等模型在预训练阶段看过了海量的教学文本、论坛答案、教程等。在这些文本中，“let’s think step by step”经常出现在详细的解题过程之前。当模型看到这个短语时，它被”激活”了一种特定的生成模式——生成逐步推理的文本。</p>
<p>第二个解释与<strong>生成分布的偏移</strong>有关。标准zero-shot prompt（如”Q: … A:“）暗示模型应该直接输出答案。而”Let’s think step by step”将期望输出从”简短答案”转移到了”详细解释”，这改变了模型生成的整个token分布。</p>
<p>Kojima et al.&nbsp;还测试了其他”触发短语”（trigger phrases）的效果：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>触发短语</th>
<th>MultiArith准确率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>（无触发）</em></td>
<td>17.7%</td>
</tr>
<tr class="even">
<td>“Let’s think step by step”</td>
<td><strong>78.7%</strong></td>
</tr>
<tr class="odd">
<td>“Let’s think about this logically”</td>
<td>72.6%</td>
</tr>
<tr class="even">
<td>“Let’s solve this problem by splitting it into steps”</td>
<td>74.5%</td>
</tr>
<tr class="odd">
<td>“First,”</td>
<td>77.3%</td>
</tr>
<tr class="even">
<td>“The answer is”</td>
<td>0.0%</td>
</tr>
</tbody>
</table>
<p>“Let’s think step by step”是效果最好的触发短语，但其他鼓励逐步思考的短语也有类似效果。而”The answer is”反而导致了0%的准确率——因为它引导模型直接输出答案，跳过了推理过程。</p>
</section>
<section id="实验结果" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="实验结果"><span class="header-section-number">4.2.2</span> 实验结果</h4>
<p>在MultiArith（多步算术推理）数据集上：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>GPT-3 175B准确率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Zero-shot（直接问）</td>
<td>17.7%</td>
</tr>
<tr class="even">
<td>Zero-shot CoT（+“Let’s think step by step”）</td>
<td><strong>78.7%</strong></td>
</tr>
<tr class="odd">
<td>Few-shot（8个示例，无推理链）</td>
<td>33.7%</td>
</tr>
<tr class="even">
<td>Few-shot CoT（8个示例+推理链）</td>
<td><strong>93.0%</strong></td>
</tr>
</tbody>
</table>
<p>Zero-shot CoT（78.7%）不仅远超标准zero-shot（17.7%），甚至超过了标准few-shot（33.7%）。这意味着<strong>“一句话的魔力”超过了”8个精心选择的示例”</strong>。当然，few-shot CoT（93.0%）仍然是最好的——但Zero-shot CoT的简便性使它在实践中极具吸引力。</p>
</section>
</section>
<section id="self-consistency-wang-et-al.-2022" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="self-consistency-wang-et-al.-2022"><span class="header-section-number">4.3</span> Self-Consistency (Wang et al., 2022)</h3>
<p>CoT的一个局限是它使用贪心解码（greedy decoding）：模型只生成一条推理路径，然后基于这条路径给出答案。但任何一条推理路径都可能包含错误——也许第二步的计算出了问题，也许推理方向完全错了。有没有办法提高CoT的可靠性？</p>
<p>Wang et al.&nbsp;(2022) 提出了一个直觉上非常自然的改进：<strong>Self-Consistency</strong>（自一致性）。核心思想是：对同一个问题，采样多条不同的推理路径，然后对最终答案进行<strong>多数投票</strong>（majority voting）。如果多条独立的推理路径都指向同一个答案，那这个答案大概率是对的。</p>
<div id="fig-self-consistency" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-self-consistency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-21/original/fig4-self-consistency.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-self-consistency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Self-Consistency方法示意图。对同一个问题采样多条不同的推理路径（通过非零温度），然后对最终答案进行多数投票，选择出现次数最多的答案。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Wang et al.&nbsp;(2022) “Self-Consistency Improves Chain of Thought Reasoning in Language Models”, Figure 1</em></p>
</div>
<p>这个思想来自一个简单的观察：<strong>正确答案通常可以通过多种不同的路径到达，而错误答案则各有各的错法</strong>。考虑一个数学题，正确答案是42。三条推理路径可能分别通过”先算A再算B”、“先算B再算A”、“用代数方法”得到42，而一条错误路径可能因为算错了一步得到38。多数投票自然会选出42。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Self-Consistency（Wang et al., 2022）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入：语言模型 LM，CoT prompt P，查询问题 x_query，
      采样次数 N，采样温度 T
输出：最终答案 ŷ

1. 构造完整 prompt：
     full_prompt ← P + x_query

2. 采样多条推理路径：
     for i = 1 to N:
         (cᵢ, yᵢ) ← LM.sample(full_prompt, temperature=T)
         # cᵢ 是推理链，yᵢ 是从推理链中提取的答案

3. 多数投票：
     ŷ ← mode({y₁, y₂, ..., yₙ})  # 选择出现次数最多的答案

4. return ŷ</code></pre>
<p><em>Source: Wang et al.&nbsp;(2022) “Self-Consistency Improves Chain of Thought Reasoning in Language Models”, Section 2. <a href="https://arxiv.org/abs/2203.11171">arXiv:2203.11171</a></em></p>
</div>
</div>
<section id="完整数值示例self-consistency如何工作" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="完整数值示例self-consistency如何工作"><span class="header-section-number">4.3.1</span> 完整数值示例：Self-Consistency如何工作</h4>
<p><strong>设定</strong>：使用CoT prompt对同一道数学题采样5条推理路径。</p>
<p><strong>问题</strong>：“一家书店有120本书。第一周卖了总量的1/3，第二周卖了剩余的1/4。还剩多少本？”</p>
<p><strong>正确推理</strong>：第一周卖 120×(1/3)=40本，剩80本。第二周卖 80×(1/4)=20本，剩60本。</p>
<p><strong>采样5条路径</strong>（temperature=0.7）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>路径</th>
<th>推理过程</th>
<th>最终答案</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>路径1</td>
<td>第一周：120/3=40，剩80；第二周：80/4=20，剩60</td>
<td><strong>60</strong> ✓</td>
</tr>
<tr class="even">
<td>路径2</td>
<td>先算两周共卖：120/3+120/4=40+30=70，剩120-70=50</td>
<td>50 ✗</td>
</tr>
<tr class="odd">
<td>路径3</td>
<td>第一周卖40本剩80，第二周卖80÷4=20，80-20=60</td>
<td><strong>60</strong> ✓</td>
</tr>
<tr class="even">
<td>路径4</td>
<td>120×(1/3)=40剩80，80×(1/4)=20，剩下80-20=60</td>
<td><strong>60</strong> ✓</td>
</tr>
<tr class="odd">
<td>路径5</td>
<td>第一周：120/3=40；第二周：120/4=30，120-40-30=50</td>
<td>50 ✗</td>
</tr>
</tbody>
</table>
<p><strong>多数投票</strong>：答案60出现3次，答案50出现2次。选择60。✓</p>
<p><strong>解读</strong>：路径2和路径5犯了同样的错误——它们把”剩余的1/4”错误理解为”总量的1/4”。但另外三条路径正确理解了”剩余”的含义，各自独立地得到了60。多数投票成功过滤了错误。</p>
</section>
<section id="实验结果-1" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="实验结果-1"><span class="header-section-number">4.3.2</span> 实验结果</h4>
<p>Self-Consistency在各个推理benchmark上持续改善CoT的效果：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Benchmark</th>
<th>CoT (贪心)</th>
<th>Self-Consistency (40路径)</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GSM8K (PaLM 540B)</td>
<td>56.9%</td>
<td><strong>74.4%</strong></td>
<td>+17.5</td>
</tr>
<tr class="even">
<td>SVAMP (PaLM 540B)</td>
<td>79.0%</td>
<td><strong>86.6%</strong></td>
<td>+7.6</td>
</tr>
<tr class="odd">
<td>AQuA (PaLM 540B)</td>
<td>35.8%</td>
<td><strong>48.0%</strong></td>
<td>+12.2</td>
</tr>
</tbody>
</table>
<p>在GSM8K上，Self-Consistency将PaLM 540B的CoT准确率从56.9%提升到74.4%——提升了17.5个百分点，相当于在CoT基础上又减少了约40%的错误。</p>
</section>
</section>
<section id="tree-of-thoughts-yao-et-al.-2023" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="tree-of-thoughts-yao-et-al.-2023"><span class="header-section-number">4.4</span> Tree of Thoughts (Yao et al., 2023)</h3>
<p>CoT和Self-Consistency都假设推理是一个<strong>线性</strong>过程：从问题出发，一步一步走到答案。Self-Consistency虽然采样了多条路径，但每条路径内部仍然是线性的，而且不同路径之间互不交流。</p>
<p>但有些问题的求解过程更像是<strong>探索一棵搜索树</strong>：你在某一步做出选择，如果发现走不通就需要回溯（backtrack），尝试另一个分支。这正是经典人工智能中搜索算法（BFS、DFS）擅长的事情。</p>
<p>Yao et al.&nbsp;(2023) 提出的 <strong>Tree of Thoughts (ToT)</strong> 将CoT从线性推理扩展到了树状搜索。核心思想是：</p>
<ol type="1">
<li>将推理过程分解为多个”思维步骤”（thought steps）</li>
<li>在每一步，生成多个候选思维</li>
<li>用LLM自身来评估每个候选思维的”前景”（是否有希望到达正确答案）</li>
<li>使用BFS或DFS来系统地探索思维树</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Tree of Thoughts 框架（Yao et al., 2023）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入：语言模型 LM，问题 x，思维生成器 G，
      状态评估器 V，搜索算法 (BFS 或 DFS)
输出：最终解 ŷ

定义：
  s = [x] 为初始状态（只包含问题描述）
  T 为思维步骤的最大深度

BFS 搜索过程：
  S₀ ← {s}  # 初始候选集
  for t = 1 to T:
      # 1. 生成候选思维
      candidates ← {}
      for s in Sₜ₋₁:
          thoughts ← G(LM, s, k)  # 生成 k 个候选思维
          candidates ← candidates ∪ {[s, z] : z ∈ thoughts}

      # 2. 评估候选状态
      for s' in candidates:
          V(s') ← LM.evaluate(s')  # 用 LM 评估前景

      # 3. 选择最优的 b 个状态（beam width = b）
      Sₜ ← top_b(candidates, key=V)

  return best solution from Sₜ</code></pre>
<p><em>Source: Yao et al.&nbsp;(2023) “Tree of Thoughts: Deliberate Problem Solving with Large Language Models”, Section 2-3. <a href="https://arxiv.org/abs/2305.10601">arXiv:2305.10601</a></em></p>
</div>
</div>
<section id="四种方法的对比" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="四种方法的对比"><span class="header-section-number">4.4.1</span> 四种方法的对比</h4>
<div id="fig-tot-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tot-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-21/original/fig5-tree-of-thoughts.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tot-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: 四种prompting方法的推理结构对比。从左到右：标准IO（一步到位）、CoT（线性链）、CoT-SC（多条线性链+投票）、ToT（树状搜索+评估+回溯）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Yao et al.&nbsp;(2023) “Tree of Thoughts: Deliberate Problem Solving with Large Language Models”, Figure 1</em></p>
</div>
<p>从标准ICL到Tree of Thoughts，推理结构经历了一个清晰的演进：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>推理结构</th>
<th>路径数</th>
<th>是否有评估</th>
<th>是否回溯</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>标准ICL (IO)</strong></td>
<td>无推理</td>
<td>1</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr class="even">
<td><strong>CoT</strong></td>
<td>线性链</td>
<td>1</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr class="odd">
<td><strong>CoT + Self-Consistency</strong></td>
<td>多条线性链</td>
<td>N</td>
<td>✗（仅投票）</td>
<td>✗</td>
</tr>
<tr class="even">
<td><strong>Tree of Thoughts</strong></td>
<td>树状搜索</td>
<td>动态</td>
<td>✓（LLM评估）</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>标准ICL是”一步到位”，CoT是”一条直线”，Self-Consistency是”多条直线取众数”，ToT是”一棵搜索树”。</p>
</section>
<section id="tot的标志性实验24点游戏" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="tot的标志性实验24点游戏"><span class="header-section-number">4.4.2</span> ToT的标志性实验：24点游戏</h4>
<p>Yao et al.&nbsp;选择了一个巧妙的评测任务来展示ToT的优势：<strong>24点游戏</strong>。给定4个数字（1-13），使用加减乘除（每个数字恰好用一次），使结果等于24。例如，给定 {1, 2, 3, 4}，一个解是 <span class="math inline">\(1 \times 2 \times 3 \times 4 = 24\)</span>。</p>
<p>这个任务非常适合树搜索，因为：(a) 它需要探索不同的运算组合；(b) 某些中间结果一看就走不通（比如中间算出了一个很大的数）；(c) 有明确的成功标准（等于24）。</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>24点成功率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>标准ICL (IO)</td>
<td>7.3%</td>
</tr>
<tr class="even">
<td>CoT</td>
<td>4.0%</td>
</tr>
<tr class="odd">
<td>CoT + Self-Consistency (100路径)</td>
<td>9.0%</td>
</tr>
<tr class="even">
<td><strong>ToT (BFS, b=5)</strong></td>
<td><strong>74.0%</strong></td>
</tr>
</tbody>
</table>
<p>结果令人震惊：ToT（74%）比其他所有方法高出一个数量级。有趣的是，CoT（4%）甚至比标准ICL（7.3%）更差——这是因为CoT生成的线性推理链容易在早期犯错并将错误传递到后续步骤，而标准ICL至少还有”蒙对”的可能。ToT通过评估和回溯避免了这个问题。</p>
<p>然而，需要注意的是，ToT的计算成本远高于CoT——每个问题需要多次LLM调用（生成候选+评估），在24点游戏中大约需要数十次API调用。这使得它在大规模应用中成本高昂。</p>
<hr>
</section>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="实现cot推理" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="实现cot推理"><span class="header-section-number">5.1</span> 实现CoT推理</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cot_solve(question: <span class="bu">str</span>, cot_examples: <span class="bu">list</span>[<span class="bu">dict</span>], model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-4"</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    使用 Chain-of-Thought 解决推理问题。</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">        question: 待解决的问题</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">        cot_examples: CoT示例列表，每个元素包含 'question', 'reasoning', 'answer'</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">        model: 模型名称</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">        (reasoning, answer) 元组</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构造 CoT prompt</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="st">"请一步一步地解决以下数学问题。</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ex <span class="kw">in</span> cot_examples:</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>ex[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"A: </span><span class="sc">{</span>ex[<span class="st">'reasoning'</span>]<span class="sc">}</span><span class="ss"> 答案是</span><span class="sc">{</span>ex[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">。</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="st">"A: "</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 调用模型（贪心解码）</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,   <span class="co"># 贪心解码</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取答案（假设格式为"答案是X"）</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"答案是"</span> <span class="kw">in</span> output:</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> output.split(<span class="st">"答案是"</span>)[<span class="op">-</span><span class="dv">1</span>].strip().rstrip(<span class="st">"。"</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, answer</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>cot_examples <span class="op">=</span> [</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: <span class="st">"Shawn有5个玩具。圣诞节从妈妈和爸爸各收到2个。现在有几个？"</span>,</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reasoning"</span>: <span class="st">"Shawn原有5个。妈妈给了2个，爸爸给了2个，共收到2+2=4个。"</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>                     <span class="st">"5+4=9。"</span>,</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: <span class="st">"9"</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: <span class="st">"花园有36朵玫瑰。园丁又种了18朵，然后摘了9朵送人。现在有多少？"</span>,</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reasoning"</span>: <span class="st">"原来36朵，种了18朵变成36+18=54朵。摘了9朵后剩54-9=45朵。"</span>,</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: <span class="st">"45"</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>reasoning, answer <span class="op">=</span> cot_solve(</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"一家书店有120本书。第一周卖了总量的1/3，第二周卖了剩余的1/4。还剩多少？"</span>,</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>    cot_examples</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"推理过程: </span><span class="sc">{</span>reasoning<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"最终答案: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="实现self-consistency" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="实现self-consistency"><span class="header-section-number">5.2</span> 实现Self-Consistency</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_consistency_solve(</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    question: <span class="bu">str</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    cot_examples: <span class="bu">list</span>[<span class="bu">dict</span>],</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-4"</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">    使用 Self-Consistency 提升 CoT 推理的可靠性。</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">    核心思想：采样多条推理路径，对最终答案进行多数投票。</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构造 CoT prompt（与上面相同）</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="st">"请一步一步地解决以下数学问题。</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ex <span class="kw">in</span> cot_examples:</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>ex[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"A: </span><span class="sc">{</span>ex[<span class="st">'reasoning'</span>]<span class="sc">}</span><span class="ss"> 答案是</span><span class="sc">{</span>ex[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">。</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">A: "</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 采样多条推理路径</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    answers <span class="op">=</span> []</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    reasonings <span class="op">=</span> []</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span>model,</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span>temperature,  <span class="co"># 非零温度以获得多样性</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        reasonings.append(output)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 提取答案</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"答案是"</span> <span class="kw">in</span> output:</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>            ans <span class="op">=</span> output.split(<span class="st">"答案是"</span>)[<span class="op">-</span><span class="dv">1</span>].strip().rstrip(<span class="st">"。"</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>            answers.append(ans)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 多数投票</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> answers:</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span>, []</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    vote_counts <span class="op">=</span> Counter(answers)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    final_answer <span class="op">=</span> vote_counts.most_common(<span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    confidence <span class="op">=</span> vote_counts[final_answer] <span class="op">/</span> <span class="bu">len</span>(answers)</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_answer, confidence, <span class="bu">list</span>(<span class="bu">zip</span>(reasonings, answers))</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>answer, confidence, paths <span class="op">=</span> self_consistency_solve(</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"一家书店有120本书。第一周卖了总量的1/3，第二周卖了剩余的1/4。还剩多少？"</span>,</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    cot_examples,</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.7</span></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"最终答案: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss"> (置信度: </span><span class="sc">{</span>confidence<span class="sc">:.0%}</span><span class="ss">)"</span>)</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"各路径答案分布: </span><span class="sc">{</span>Counter([a <span class="cf">for</span> _, a <span class="kw">in</span> paths])<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="实现zero-shot-cot" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="实现zero-shot-cot"><span class="header-section-number">5.3</span> 实现Zero-shot CoT</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> zero_shot_cot_solve(question: <span class="bu">str</span>, model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-4"</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Zero-shot CoT：只需要一句"Let's think step by step"。</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    分两阶段：先提取推理，再提取答案。</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 阶段1：推理提取</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    prompt_stage1 <span class="op">=</span> <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">A: Let's think step by step."</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    response1 <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt_stage1}],</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    reasoning <span class="op">=</span> response1.choices[<span class="dv">0</span>].message.content</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 阶段2：答案提取</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    prompt_stage2 <span class="op">=</span> (</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"A: Let's think step by step. </span><span class="sc">{</span>reasoning<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Therefore, the answer (arabic numerals) is"</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    response2 <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt_stage2}],</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> response2.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> reasoning, answer</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="工程注意事项" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="工程注意事项"><span class="header-section-number">5.4</span> 工程注意事项</h3>
<p><strong>温度设置的策略</strong>：CoT推理有两种使用场景，它们需要不同的温度设置。当使用贪心CoT（单路径）时，应设置 <span class="math inline">\(\text{temperature} = 0\)</span> 以获得最确定性的推理链。当使用Self-Consistency（多路径投票）时，需要设置 <span class="math inline">\(\text{temperature} \in [0.5, 1.0]\)</span> 以获得推理路径的多样性——如果温度为0，所有路径都会相同，多数投票就失去了意义。</p>
<p><strong>答案提取的鲁棒性</strong>：从模型输出中提取最终答案是CoT工程中最容易出问题的环节。模型可能以各种格式输出答案：“答案是42”、“所以结果为42”、“42。”、“<span class="math inline">\(42\)</span>”。建议使用正则表达式或二次LLM调用来鲁棒地提取答案。</p>
<p><strong>成本考量</strong>：Self-Consistency需要多次模型调用，成本正比于采样次数。Wang et al.&nbsp;的实验显示，大约10-40条路径就能获得接近最优的效果——更多路径的边际收益递减。在实践中，10条路径（10倍成本）通常是性价比最高的选择。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨涌现能力的本质、CoT的机制解释、以及LLM推理能力的根本争论。</p>
</blockquote>
<section id="涌现能力规模带来相变-wei-et-al.-2022b" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="涌现能力规模带来相变-wei-et-al.-2022b"><span class="header-section-number">6.1</span> 涌现能力：规模带来相变？ (Wei et al., 2022b)</h3>
<p>Chain-of-Thought的规模依赖性只是一个更大现象的缩影。2022年6月，Jason Wei等人发表了一篇影响深远的论文——“Emergent Abilities of Large Language Models”——系统性地研究了一类特殊现象：某些能力在小模型中完全不存在，但当模型规模超过某个临界点时突然出现。</p>
<p>他们将<strong>涌现能力</strong>（emergent ability）定义为：“一种在小模型中不存在、但在大模型中出现的能力。”更精确地说，如果我们画出能力（如某benchmark的准确率）随模型规模变化的曲线，涌现能力呈现出一种<strong>不可预测的跳跃</strong>：在某个规模之前，性能接近随机猜测或零；在某个规模之后，性能突然跃升到远高于随机的水平。这与物理学中的相变（phase transition）——如水在0°C突然从液态变为固态——有惊人的相似性。</p>
<div id="fig-emergent-abilities" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emergent-abilities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-21/original/fig3-emergent-abilities.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emergent-abilities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 涌现能力的经典图示。横轴是模型规模（FLOPs或参数量），纵轴是任务准确率。在某个规模阈值之前，性能接近随机水平；超过阈值后，性能急剧跃升——呈现出类似物理学中”相变”的特征。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Wei et al.&nbsp;(2022) “Emergent Abilities of Large Language Models”, Figure 2</em></p>
</div>
<p>Wei et al.&nbsp;(2022b) 从两个角度收集了涌现能力的证据。</p>
<p><strong>BIG-Bench</strong>上的涌现：BIG-Bench是一个包含200+语言任务的大型评测集。在许多任务上，小模型的表现接近随机，但当模型规模增加到某个阈值后，准确率突然飙升。例如，在三步算术推理任务上，模型从10B到100B几乎没有进展（约等于随机水平），但从100B到540B时准确率从近0%跳到超过40%。</p>
<p><strong>多模型系列的涌现</strong>：Wei et al.&nbsp;还跨越多个模型系列（GPT-3、LaMDA、PaLM、Chinchilla）展示了涌现现象。在不同的任务上，涌现发生的规模阈值不同，但模式一致：先是一段长长的”沉默期”（接近随机性能），然后是一个急剧的跳跃。</p>
<p>这个发现的影响是深远的。如果涌现是真实的，它意味着<strong>我们无法通过研究小模型来预测大模型的能力</strong>——因为质变不是量变的简单延伸。这对整个AI安全和对齐领域都有重大影响：如果我们不知道更大的模型会涌现出什么能力（包括可能有害的能力），那么”先训练再评估”的策略就存在根本性的风险。</p>
</section>
<section id="涌现是真实的还是海市蜃楼-schaeffer-et-al.-2023" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="涌现是真实的还是海市蜃楼-schaeffer-et-al.-2023"><span class="header-section-number">6.2</span> 涌现是真实的还是”海市蜃楼”？ (Schaeffer et al., 2023)</h3>
<p>涌现能力的叙事在2023年受到了一次强有力的挑战。Schaeffer et al.&nbsp;在NeurIPS 2023最佳论文”Are Emergent Abilities of Large Language Models a Mirage?“中论证：<strong>涌现可能不是模型行为的内在属性，而是我们选择的度量方式（metric）制造的假象</strong>。</p>
<p>他们的核心论点可以用一个精彩的实验来说明。考虑一个分类任务，模型的”真实”准确率随规模平滑增长：</p>
<p><span class="math display">\[
\text{真实准确率}(N) = 0.1 + 0.5 \times \log_{10}(N / 10^8)
\]</span></p>
<p>其中 <span class="math inline">\(N\)</span> 是参数量。当 <span class="math inline">\(N\)</span> 从 <span class="math inline">\(10^8\)</span> 增长到 <span class="math inline">\(10^{12}\)</span> 时，真实准确率从0.1平滑增长到0.9——没有任何相变。</p>
<p>但是，如果我们用<strong>精确匹配</strong>（Exact Match, EM）作为度量指标，情况就完全不同了。EM是一个非线性度量：如果模型的输出与标准答案完全一致就得1分，否则0分。对于需要多步输出的任务，即使模型的”接近正确”的能力在平滑增长，只要它还不够”完全正确”，EM就会给出0。这意味着用EM衡量时，性能曲线可能呈现出阶跃状的”涌现”——但这只是EM度量的非线性造成的，不是模型能力的真实突变。</p>
<div id="fig-emergence-mirage" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emergence-mirage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-21/original/fig6-emergence-mirage.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emergence-mirage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: 涌现的”海市蜃楼”。同一组模型在同一任务上的表现，使用非线性度量（如精确匹配，左图）时呈现出”涌现”的阶跃，但使用连续度量（如token级准确率，右图）时变为平滑增长——暗示涌现可能是度量方式的产物。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Schaeffer et al.&nbsp;(2023) “Are Emergent Abilities of Large Language Models a Mirage?”, Figure 1. NeurIPS 2023 Best Paper</em></p>
</div>
<p>Schaeffer et al.&nbsp;提供了以下关键证据：</p>
<p><strong>证据1：更换度量，涌现消失</strong>。在多个被Wei et al.&nbsp;标记为”涌现”的任务上，如果将度量从EM换成连续度量（如token级准确率、Brier Score、log-likelihood），相变就消失了——取而代之的是平滑、可预测的性能曲线。</p>
<p><strong>证据2：InverseBench实验</strong>。他们构造了一个人工任务，让模型输出一个数字序列。用EM衡量，小模型的EM接近0（因为没有完全匹配），大模型的EM突然跳到高值——看起来像涌现。但用部分匹配度量，性能增长是完全平滑的。</p>
<p><strong>证据3：统计理论分析</strong>。他们从理论上证明：对于非线性度量函数 <span class="math inline">\(f\)</span>（如EM的阶跃函数），即使底层的概率 <span class="math inline">\(p(N)\)</span> 是 <span class="math inline">\(N\)</span> 的连续函数，<span class="math inline">\(f(p(N))\)</span> 也可能呈现出阶跃状行为。涌现本质上可能是度量函数的非线性与有限样本效应的共同作用。</p>
</section>
<section id="围绕涌现的持续争论" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="围绕涌现的持续争论"><span class="header-section-number">6.3</span> 围绕涌现的持续争论</h3>
<p>Schaeffer et al.&nbsp;的工作并没有完全终结涌现争论。支持涌现真实性的研究者提出了几个反驳。</p>
<p>第一，<strong>并非所有涌现都可以通过更换度量来消除</strong>。在某些任务上（如BIG-Bench中的部分推理任务），即使用连续度量，性能曲线仍然呈现出非线性的加速增长，虽然不是完美的阶跃，但也远非线性增长。</p>
<p>第二，<strong>度量的选择本身是有意义的</strong>。在实际应用中，我们关心的往往就是”模型能否给出完全正确的答案”，而不是”模型有多接近正确答案”。如果一个数学模型的每一步都有99%的概率正确，但一个10步问题的端到端正确率只有 <span class="math inline">\(0.99^{10} \approx 90\%\)</span>，而一个20步问题只有 <span class="math inline">\(0.99^{20} \approx 82\%\)</span>——从用户视角看，模型的”可靠性”确实呈现出非线性的下降。在这个意义上，涌现可能不是”假象”，而是”复合效应”的合理体现。</p>
<p>第三，<strong>CoT的规模依赖性不容易用度量论解释</strong>。CoT的涌现不仅是”从不正确到正确”，而是”从无法生成连贯推理链到能够生成连贯推理链”——这是一种质的变化，不只是精度的量变。小模型生成的”推理链”通常是语无伦次的，而大模型生成的推理链在逻辑上是连贯的，即使最终答案偶尔出错。</p>
<p><strong>目前的共识</strong>（如果有的话）大致是：涌现能力的观测是真实的（在特定度量下，性能确实呈现出非线性跳跃），但其底层机制可能是平滑的（模型的”原始能力”在连续增长），非线性主要来自度量选择和任务的组合性。这并不意味着涌现”不重要”——它仍然意味着我们很难从小模型的表现外推大模型的行为，这对AI安全和能力预测都有重大意义。</p>
</section>
<section id="llm真的在推理吗" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="llm真的在推理吗"><span class="header-section-number">6.4</span> LLM真的在”推理”吗？</h3>
<p>CoT的成功引发了一个更根本的哲学和科学问题：当大语言模型生成了一个看起来合理的推理链并得出正确答案时，它真的在”推理”吗？</p>
<p><strong>支持”推理”的证据</strong>：</p>
<p>首先，CoT生成的推理链在很多情况下是<strong>正确且有逻辑的</strong>——不仅最终答案对了，中间步骤也是对的。如果模型只是在做”高级模式匹配”，为什么它能在从未见过的新问题上产生正确的多步推理？</p>
<p>其次，CoT的<strong>泛化能力</strong>表明模型学到了某种”规则”而非仅仅记忆。Wei et al.&nbsp;在OOD（out-of-distribution）设置中——测试问题比训练示例更长——发现CoT仍然有效。如果模型只是在复制训练分布中的模式，OOD性能应该大幅下降。</p>
<p>第三，Self-Consistency的有效性（不同推理路径指向同一答案）暗示模型能够从多个角度”验证”一个结论，这是推理的一个重要特征。</p>
<p><strong>反对”推理”的证据</strong>：</p>
<p>然而，也有大量证据表明LLM的”推理”与人类推理有本质区别。</p>
<p>最关键的反面证据是<strong>CoT的不忠实性</strong>（unfaithfulness）。多项研究（如Turpin et al., 2024; Lanham et al., 2023）发现，模型生成的推理链可能<strong>不是它真正使用的推理过程</strong>。在某些实验中，如果在prompt中加入一个错误的”提示”（如”Professor Smith说答案是X”），模型会改变自己的推理过程来”论证”X是对的——推理链变成了事后合理化（post-hoc rationalization），而非真正的逐步推导。</p>
<p>另一个关键发现是：LLM在推理中对<strong>问题的表述方式</strong>极度敏感。同一个逻辑问题，如果用不同的词语描述，或者改变无关的上下文信息，模型的答案可能截然不同。人类的逻辑推理通常对问题的表面描述是鲁棒的（至少在受过训练后），但LLM的”推理”似乎更多地依赖于文本的表面模式而非深层逻辑结构。</p>
<p>此外，LLM在需要<strong>系统性规则遵循</strong>的任务上（如棋盘游戏、精确计算大数、逻辑谜题的所有推理步骤都必须正确）表现仍然不可靠，这暗示它们缺乏人类式的”规则应用”机制。</p>
<p><strong>一个可能的折衷立场</strong>是：LLM所做的既不是人类意义上的”推理”，也不是纯粹的”模式匹配”，而是一种介于两者之间的新型信息处理。它能够利用预训练中学到的统计规律来近似推理步骤，在很多情况下这种近似足够准确，但它缺乏人类推理的系统性和可靠性。</p>
</section>
<section id="开放研究问题" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.5</span> 开放研究问题</h3>
<p>如果你要在这个方向写一篇论文，以下几个问题仍然值得深入探索。</p>
<p>第一个方向是<strong>推理的忠实性</strong>。如何确保模型生成的推理链确实是它得出答案的原因，而非事后合理化？如何设计实验来区分”真正的推理”和”可信的推理模拟”？这对AI可信度和安全性有直接影响。</p>
<p>第二个方向是<strong>推理能力的可扩展性</strong>。CoT在小学数学上效果很好，但在更复杂的推理任务（竞赛数学、定理证明、多步策略规划）上表现有限。推理能力是否有理论上限？通过增加模型规模或改进推理策略，能否不断提升？</p>
<p>第三个方向是<strong>涌现的预测</strong>。如果涌现是真实的（或至少在某种度量下是真实的），我们能否预测下一代模型会涌现出什么新能力？这对AI安全和政策制定至关重要——如果我们训练了一个10倍于GPT-4的模型，它会突然获得什么我们没预料到的能力？</p>
<p>第四个方向是<strong>推理时间计算</strong>（test-time compute）。CoT、Self-Consistency、ToT都是在推理阶段投入更多计算来提升性能的策略。推理时间计算与训练时间计算之间是否存在可预测的换算关系？能否为推理时间计算建立类似Scaling Laws的定量理论？</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="本方法的局限" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</h3>
<p>Chain-of-Thought及其变体虽然在推理任务上取得了突破性进展，但它们面临着几个根本性的局限。</p>
<p>第一个局限是<strong>小模型无法受益</strong>。CoT只在≥100B参数的模型上才显著有效。对于资源受限的场景（如在手机上运行的模型），CoT带来的提升非常有限。虽然后续的工作（如知识蒸馏、CoT微调）试图将大模型的推理能力迁移到小模型，但效果仍有差距。</p>
<p>第二个局限是<strong>推理链的不忠实性</strong>。如前所述，模型生成的推理链可能不是它真正的”思考过程”。这意味着我们不能简单地通过检查推理链来验证模型的推理是否正确——一个看起来完美的推理链可能对应一个错误的内部过程。</p>
<p>第三个局限是<strong>计算成本</strong>。CoT增加了输出长度（从几个token到几百个token），Self-Consistency将成本乘以采样次数（通常10-40倍），ToT更是需要数十次LLM调用。在大规模应用中，这些成本可能是不可接受的。</p>
<p>第四个局限是<strong>对prompt的敏感性</strong>。CoT示例的质量、数量、措辞都显著影响最终效果。手工编写高质量的CoT示例需要领域知识，而自动生成的推理链质量参差不齐。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</h3>
<p>CoT和涌现能力的讨论自然引出了一个更宏观的问题：<strong>我们如何可靠地评估和衡量大语言模型的能力？</strong></p>
<p>Schaeffer et al.&nbsp;的工作已经表明，度量的选择可以根本性地改变我们对模型能力的判断。如果连”涌现”这个核心概念的真实性都取决于度量方式，那么我们对LLM能力的所有评估都需要更加审慎。</p>
<p>这正是下一章——<strong>评测方法论</strong>——将要系统讨论的问题。如何设计不被”刷分”的benchmark？如何区分”真正的能力提升”和”度量假象”？如何在模型能力快速迭代的时代建立可靠的评测体系？这些看似”元问题”（关于方法的方法、关于评估的评估），实际上是确保AI研究科学性的基石。</p>
<blockquote class="blockquote">
<p>下一章预告：第22章将聚焦<strong>评测方法论</strong>。从GLUE到SuperGLUE到MMLU到LLM-as-Judge，评测基准经历了怎样的演进？静态benchmark的数据泄漏问题如何解决？LLM-as-Judge这种”用模型评模型”的方法有什么偏差？如何在”涌现”和”假象”之间建立可靠的科学判断？</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>本章围绕两个相互交织的主题——<strong>思维链推理</strong>和<strong>涌现能力</strong>——讲述了大语言模型在推理能力上的突破与争论。</p>
<p><strong>Chain-of-Thought Prompting</strong>（Wei et al., 2022）通过一个极其简单的改动——在few-shot示例中展示推理过程——让大语言模型在数学推理上获得了飞跃式提升（GSM8K：17.9% → 56.9%）。CoT的核心洞察是：通过引导模型生成中间推理步骤，将一个困难的端到端映射分解成多个简单的步骤，同时利用模型自己的生成文本作为”工作记忆”。</p>
<p><strong>Zero-shot CoT</strong>（Kojima et al., 2022）发现仅在prompt末尾添加”Let’s think step by step”就能触发推理模式，在MultiArith上将准确率从17.7%提升到78.7%，证明了推理能力已经”内嵌”在大模型中，只需要正确的触发方式。</p>
<p><strong>Self-Consistency</strong>（Wang et al., 2022）通过采样多条推理路径并进行多数投票，进一步将CoT的GSM8K准确率从56.9%提升到74.4%，利用了”正确答案可以通过多种路径到达”的统计规律。</p>
<p><strong>Tree of Thoughts</strong>（Yao et al., 2023）将推理从线性链扩展到树状搜索，在24点游戏等需要探索和回溯的任务上，从7.3%（标准ICL）提升到74.0%，展示了结构化推理策略的潜力。</p>
<p><strong>涌现能力</strong>（Wei et al., 2022b）描述了一种令人不安的现象：某些能力在小模型中完全不存在，但在模型规模超过阈值后突然出现。Schaeffer et al.（2023）的反驳表明，涌现可能部分是度量方式制造的假象——在非线性度量（如精确匹配）下呈现的”阶跃”，在连续度量下可能是平滑增长。这场争论尚未完全解决，但它深刻地提醒我们：<strong>评估方式本身可能扭曲我们对模型能力的认知</strong>。</p>
</section>
<section id="关键对比速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键对比速查"><span class="header-section-number">8.2</span> 关键对比速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方法</th>
<th>推理结构</th>
<th>关键改进</th>
<th>GSM8K (PaLM 540B)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>标准 ICL</td>
<td>无中间步骤</td>
<td>—</td>
<td>~18%</td>
</tr>
<tr class="even">
<td>CoT (few-shot)</td>
<td>线性链 × 1</td>
<td>展示推理过程</td>
<td>56.9%</td>
</tr>
<tr class="odd">
<td>Zero-shot CoT</td>
<td>线性链 × 1</td>
<td>“Let’s think step by step”</td>
<td>~47%*</td>
</tr>
<tr class="even">
<td>Self-Consistency</td>
<td>线性链 × N</td>
<td>多数投票</td>
<td>74.4%</td>
</tr>
<tr class="odd">
<td>Tree of Thoughts</td>
<td>树状搜索</td>
<td>LLM评估 + 回溯</td>
<td>N/A（不同任务）</td>
</tr>
</tbody>
</table>
<p>*Zero-shot CoT在GSM8K上的具体数字因模型版本而异，此处为近似值。</p>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么Chain-of-Thought只在大模型（≥100B参数）上有效，而小模型几乎不受益？试从”模型需要同时具备多种子能力”的角度给出解释，并讨论这是否与Schaeffer et al.关于涌现是”度量假象”的论点矛盾。</p></li>
<li><p><strong>[数学推导]</strong> Self-Consistency使用多数投票来选择最终答案。假设模型对每条推理路径独立地以概率 <span class="math inline">\(p\)</span> 给出正确答案（<span class="math inline">\(p &gt; 0.5\)</span>），共采样 <span class="math inline">\(N\)</span> 条路径。(a) 推导多数投票给出正确答案的概率表达式（提示：二项分布）。(b) 当 <span class="math inline">\(p = 0.6\)</span>，<span class="math inline">\(N = 10\)</span> 时，多数投票的正确率是多少？与单条路径（<span class="math inline">\(p = 0.6\)</span>）相比提升了多少？(c) 当 <span class="math inline">\(p &lt; 0.5\)</span> 时会发生什么？</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用任一LLM API，在以下三种设置下解决10道数学推理题（可使用GSM8K样本）：(a) 标准few-shot（只给答案），(b) CoT few-shot（给推理过程），(c) Self-Consistency（CoT + 10次采样 + 多数投票）。比较三种方法的准确率和API成本。</p></li>
<li><p><strong>[研究思考]</strong> Schaeffer et al.&nbsp;(2023) 论证涌现可能是度量方式的假象。但考虑Chain-of-Thought的规模依赖性：小模型生成的”推理链”通常是语无伦次的，而大模型生成的推理链是连贯的。这种<strong>质的变化</strong>能否被”度量假象”论完全解释？设计一个实验来检验你的假设。</p></li>
<li><p><strong>[开放思考]</strong> 如果LLM的”推理”确实只是”高级模式匹配”（而非人类意义上的逻辑推理），这对AI的实际应用意味着什么？在哪些应用场景中”可靠的模式匹配”就够了？在哪些场景中我们需要”真正的推理”？如何判断一个任务属于哪类？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Wei, J. et al.&nbsp;(2022). “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”</strong>。CoT的原始论文，本章最核心的参考。重点阅读：Section 2（方法定义，只有半页但定义了一个时代）、Section 3（实验结果）。可快速浏览：Appendix中的完整推理链示例。<a href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a></p>
<p><strong>Wei, J. et al.&nbsp;(2022). “Emergent Abilities of Large Language Models”</strong>。涌现能力的系统性研究，引发了整个领域关于”规模vs能力”的讨论。重点阅读：Section 2（涌现的定义）、Figure 1-2（经典的涌现曲线）。<a href="https://arxiv.org/abs/2206.07682">arXiv:2206.07682</a></p>
<p><strong>Schaeffer, R. et al.&nbsp;(2023). “Are Emergent Abilities of Large Language Models a Mirage?”</strong>。NeurIPS 2023最佳论文，对涌现叙事的强有力挑战。重点阅读：Section 2-3（度量论证）。这是一篇”改变你思考方式”的论文。<a href="https://arxiv.org/abs/2304.15004">arXiv:2304.15004</a></p>
</section>
<section id="方法改进" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="方法改进"><span class="header-section-number">9.2</span> 方法改进</h3>
<p><strong>Kojima, S. et al.&nbsp;(2022). “Large Language Models are Zero-Shot Reasoners”</strong>。发现”Let’s think step by step”的神奇效果。重点阅读：Section 3（方法）、Table 4（不同触发短语的对比）。<a href="https://arxiv.org/abs/2205.11916">arXiv:2205.11916</a></p>
<p><strong>Wang, X. et al.&nbsp;(2022). “Self-Consistency Improves Chain of Thought Reasoning”</strong>。Self-Consistency方法。重点阅读：Section 2（方法，优雅简洁）。<a href="https://arxiv.org/abs/2203.11171">arXiv:2203.11171</a></p>
<p><strong>Yao, S. et al.&nbsp;(2023). “Tree of Thoughts”</strong>。将推理从线性扩展到树状搜索。重点阅读：Section 2（框架定义）、Section 4（24点游戏实验）。<a href="https://arxiv.org/abs/2305.10601">arXiv:2305.10601</a></p>
<p><strong>Zhou, D. et al.&nbsp;(2022). “Least-to-Most Prompting Enables Complex Reasoning”</strong>。一种将复杂问题分解为子问题、从简单到复杂逐步求解的策略。重点阅读：Section 2-3。<a href="https://arxiv.org/abs/2205.10625">arXiv:2205.10625</a></p>
</section>
<section id="理论与分析" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="理论与分析"><span class="header-section-number">9.3</span> 理论与分析</h3>
<p><strong>Turpin, M. et al.&nbsp;(2024). “Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting”</strong>。揭示CoT推理链可能不忠实于模型的真实”推理过程”。<a href="https://arxiv.org/abs/2305.04388">arXiv:2305.04388</a></p>
<p><strong>Suzgun, M. et al.&nbsp;(2022). “Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them”</strong>。BIG-Bench Hard（BBH）：精选的23个CoT对其有显著帮助的困难任务。<a href="https://arxiv.org/abs/2210.09261">arXiv:2210.09261</a></p>
</section>
<section id="综述与教程" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">9.4</span> 综述与教程</h3>
<p><strong>Qiao, S. et al.&nbsp;(2023). “Reasoning with Language Model Prompting: A Survey”</strong>。推理提示方法的全面综述。<a href="https://arxiv.org/abs/2212.09597">arXiv:2212.09597</a></p>
<p><strong>Huang, J. &amp; Chang, K. (2023). “Towards Reasoning in Large Language Models: A Survey”</strong>。LLM推理能力的全面综述。<a href="https://arxiv.org/abs/2212.10403">arXiv:2212.10403</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.5</span> 代码资源</h3>
<ul>
<li><strong>LangChain</strong>：<a href="https://www.langchain.com/">langchain.com</a>（内置CoT和推理链工具）</li>
<li><strong>Tree of Thoughts实现</strong>：<a href="https://github.com/princeton-nlp/tree-of-thought-llm">github.com/princeton-nlp/tree-of-thought-llm</a></li>
<li><strong>GSM8K数据集</strong>：<a href="https://github.com/openai/grade-school-math">github.com/openai/grade-school-math</a></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>Chain-of-Thought prompting的故事充满了戏剧性的巧合和意外发现。</p>
<p>2021年底，Google Brain的Jason Wei在研究PaLM模型的few-shot能力时，偶然发现了一个有趣的现象：如果在few-shot示例中加入简单的解题步骤，模型的数学推理能力会大幅提升。这个发现最初并没有被当作一个重要的研究贡献——毕竟，它的方法实在太简单了，简单到让人怀疑”这能发论文吗？“Wei后来在采访中回忆，论文提交过程中遇到了不少质疑：”你就是换了个prompt格式，这算什么贡献？”</p>
<p>但Wei坚持认为这个发现的意义不在于方法的复杂度，而在于它揭示的现象：大语言模型具有通过中间步骤进行推理的潜力，只是需要正确的”激活方式”。事后看来，这篇论文确实改变了整个领域对LLM推理能力的认知，被引用超过5000次。</p>
<p>Kojima et al.&nbsp;的Zero-shot CoT故事同样引人入胜。“Let’s think step by step”这六个英文单词成为了AI历史上最著名的”咒语”之一。Kojima等人测试了超过50个不同的触发短语，发现这句话的效果最好——但他们自己也无法完全解释为什么。这六个词的发现几乎可以说是”实验驱动”而非”理论驱动”的——先发现了现象，然后才开始思考解释。</p>
<p>涌现能力的讨论则触及了AI研究中一个更深层的张力：乐观主义与怀疑主义。Wei et al.的涌现论文被乐观主义者视为”规模就是通往AGI的道路”的证据，被怀疑主义者视为”没有控制好度量的可疑发现”。Schaeffer et al.的反驳论文在NeurIPS 2023获得最佳论文奖，这本身就是一个信号：学术界认为对热门叙事的严谨质疑同样值得褒奖。</p>
<p>一个历史的讽刺是：2022年是CoT和涌现论文发表的年份，也是ChatGPT发布的年份（2022年11月30日）。ChatGPT的成功很大程度上依赖于指令微调和RLHF，而不是CoT——但正是CoT等研究揭示的LLM推理潜力，为ChatGPT的应用场景（编程、分析、解题）提供了信心基础。从某种意义上说，CoT为LLM从”有趣的研究工具”变成”改变世界的产品”铺平了认知上的道路。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第21章：涌现能力与思维链推理"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Emergence, Chain-of-Thought, and the Boundaries of LLM Reasoning"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, Chain-of-Thought, Emergence, Reasoning]</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [Chain-of-Thought, CoT, Zero-shot CoT, Self-Consistency, Tree of Thoughts, 涌现能力, 推理, Prompt Engineering, Wei, Kojima, Wang, Yao]</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "上一章我们见证了GPT-3的In-Context Learning——不需要梯度更新，仅凭输入中的几个示例就能完成新任务。但ICL有一个致命弱点：它在需要多步推理的任务上几乎完全失败。2022年，Wei等人发现了一个出奇简单的解决方案：在few-shot示例中不仅给出答案，还给出完整的推理过程——Chain-of-Thought prompting。这个看似微小的改动带来了数学和逻辑推理上的飞跃式提升，更引发了关于LLM'涌现能力'的激烈争论：这些能力是真实的相变，还是度量方式制造的假象？本章系统讲述CoT及其变体，探讨涌现现象的本质，并追问一个根本问题：LLM真的在'推理'吗？"</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-21/original/fig1-cot-example.png"</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：大语言模型能否进行多步推理？如何通过提示策略释放这种潜力？随规模增长而"涌现"的能力是真实的相变，还是度量方式制造的假象？</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2022 </span><span class="pp">|</span><span class="at"> Wei et al. "Chain-of-Thought Prompting" &amp; "Emergent Abilities of Large Language Models" </span><span class="pp">|</span><span class="at"> 从模式匹配到（疑似）推理</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wei et al. (2022a)** "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (arXiv:2201.11903) — 参考了 Section 2-3（CoT方法定义与实验）、Figure 1（CoT示例对比图）、Figure 2（模型规模与CoT效果的关系）、Table 1-5（GSM8K等benchmark结果）；提取了 Figure 1 作为论文原图</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kojima et al. (2022)** "Large Language Models are Zero-Shot Reasoners" (arXiv:2205.11916) — 参考了 Section 3（Zero-shot CoT方法）、Figure 1（Zero-shot CoT流程图）、Table 1-2（MultiArith/GSM8K等实验结果）</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wang et al. (2022)** "Self-Consistency Improves Chain of Thought Reasoning in Language Models" (arXiv:2203.11171) — 参考了 Section 2（Self-Consistency方法定义）、Figure 1（方法示意图）、Table 1-3（数学推理benchmark结果）；提取了 Figure 1 作为论文原图</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Yao et al. (2023)** "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" (arXiv:2305.10601) — 参考了 Section 2-3（ToT框架定义）、Figure 1（四种prompting方法对比图）、Table 1（24点游戏实验结果）；提取了 Figure 1 作为论文原图</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wei et al. (2022b)** "Emergent Abilities of Large Language Models" (arXiv:2206.07682) — 参考了 Section 2（涌现定义）、Figure 1-2（涌现能力的经典"相变"图）、Table 1（涌现能力列表）；提取了 Figure 2 作为论文原图</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Schaeffer et al. (2023)** "Are Emergent Abilities of Large Language Models a Mirage?" (arXiv:2304.15004, NeurIPS 2023 Best Paper) — 参考了 Section 2-3（度量选择对涌现的影响）、Figure 1（相同数据不同度量的对比）</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zhou et al. (2022)** "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models" (arXiv:2205.10625) — 参考了分解策略</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Suzgun et al. (2022)** "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them" (arXiv:2210.09261, BIG-Bench Hard) — 参考了 CoT 在困难任务上的系统评估</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SLP3 Chapter 12 (Prompting, In-Context Learning, and Instruct Tuning) — 参考了 CoT prompting 的教学组织</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>D2L Chapter 11 — 参考了注意力机制在推理中的作用讨论</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stanford CS224N Lecture 11-12 (Winter 2025) — 参考了 Prompting 和 Reasoning 的教学框架</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CMU 11-711 ANLP (Fall 2024) "Prompting &amp; In-context Learning" — 参考了 CoT 及其变体的系统化讲解</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Princeton COS 597R (Fall 2024) "Deep Dive into LLMs" — 参考了涌现能力的讨论</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>上一章我们详细讲述了GPT-3和In-Context Learning的故事。175B参数的GPT-3展示了一种令人震惊的能力：不需要任何梯度更新，仅通过在输入中提供几个示例，模型就能"学会"翻译、分类、问答等各种任务。这种能力随规模增长而增强——大模型不仅绝对性能更高，还更擅长利用上下文示例——这暗示着规模本身可能是通向通用智能的关键。</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>然而，上一章结尾也揭示了ICL的一个致命弱点：**它在需要多步推理的任务上几乎完全失败**。</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>考虑这样一个小学数学题："Roger有5个网球。他又买了2罐网球。每罐有3个网球。他现在一共有多少个网球？"一个人类学生会这样思考：Roger原来有5个，买了2罐×3个=6个，所以一共有5+6=11个。但当你把这个问题直接扔给GPT-3——即使给了几个类似问题的输入-输出示例——模型经常直接输出一个错误的数字，跳过所有中间推理步骤。</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>这不是因为GPT-3"不知道"加法或乘法。它在预训练中见过海量的数学文本，对基本运算有相当的"知识"。问题在于：**标准的ICL只展示了最终答案，没有展示到达答案的思考过程**。模型被训练去模仿"问题→答案"的直接映射，而不是"问题→推理步骤→答案"的逐步求解。</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>2022年初，Google Brain的Jason Wei等人发现了一个出奇简单的解决方案——简单到让整个社区都感到不可思议。</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：Chain-of-Thought（思维链）prompting的核心思想是：在few-shot示例中不仅给出最终答案，还给出完整的中间推理过程。这个看似微小的改动——相当于在数学考试中要求学生"展示你的解题过程"——让大语言模型在数学推理、逻辑推理和常识推理上获得了飞跃式提升。更令人惊讶的是，这种推理能力呈现出"涌现"特征：小模型几乎不受益于CoT，但当模型规模超过某个阈值时，CoT的效果突然爆发。</span></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么直接回答会失败？</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>让我们用一个更具体的例子来剖析ICL在推理任务上失败的根本原因。考虑GSM8K数据集（Grade School Math 8K）中的一道典型题：</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *咖啡店周一卖了37杯拿铁和13杯卡布奇诺。周二拿铁卖了周一的两倍，但卡布奇诺只卖了周一的一半（四舍五入取整）。两天一共卖了多少杯咖啡？*</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>要正确回答这个问题，需要四步推理：</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>周二拿铁：37 × 2 = 74杯</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>周二卡布奇诺：13 ÷ 2 ≈ 7杯（四舍五入）</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>周一总计：37 + 13 = 50杯</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>两天总计：50 + 74 + 7 = 131杯</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>标准的few-shot ICL如何处理这类问题？在传统范式下，few-shot示例只包含"问题→答案"的映射：</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="in">Q: 农场有23只鸡和7只鸭。农场一共有多少只家禽？</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a><span class="in">A: 30</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="in">Q: 咖啡店周一卖了37杯拿铁和13杯卡布奇诺...</span></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="in">A:</span></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>模型看到的模式是：给定一个数学问题，直接输出一个数字。但从"问题文本"到"最终数字"之间有一条复杂的推理链，这条链在示例中是**完全不可见的**。模型被要求做的不是逐步计算，而是"一步到位"地猜出答案——本质上是在做一个困难的端到端映射。</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>这里有一个关键的认知错位：人类在看到"答案是30"时，脑中自动补全了"23+7=30"的推理过程。但对语言模型而言，它只看到了一个从长文本到单个数字的黑箱映射。</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a><span class="fu">### 人类解题的启示</span></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>反观人类如何解决数学问题。没有哪个数学老师会告诉学生："看完题目后直接写出答案。"相反，从小学开始，数学教育就强调**"展示你的解题过程"**（show your work）。这不仅是为了方便老师打分——更重要的是，解题过程本身就是思考的载体。</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>写下中间步骤至少有三个好处：</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>第一，**分解复杂性**。一个需要四步推理的问题，如果必须一步到位地得出答案，相当于要求大脑同时处理四个运算并正确组合结果。而如果把它分解成四个独立的步骤，每一步只需要做一个简单的运算。</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>第二，**提供工作记忆**。人类的工作记忆容量有限（著名的"7±2"规则），写下中间结果相当于用纸笔扩展了工作记忆。类似地，语言模型的"工作记忆"就是它能生成和回读的文本——如果中间步骤被写在输出中，模型就可以在后续步骤中引用这些中间结果。</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>第三，**使错误可检测**。如果只看到最终答案是错的，你无从知道哪里出了问题。但如果有完整的推理链，就能精确定位错误发生在哪一步。</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>这个来自人类教育的朴素洞察，正是Chain-of-Thought prompting的核心灵感。</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>基于上述分析，理想的解决方案应该具备几个特性。首先，它应该能让模型生成中间推理步骤，而不是直接跳到最终答案。其次，它不应该需要修改模型架构或重新训练——GPT-3这样的大模型训练一次就要数百万美元，我们希望在推理阶段（inference time）就能提升性能。第三，它应该是通用的，适用于各种需要推理的任务，而不仅限于数学。</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>Chain-of-Thought prompting恰好满足了这三个要求：它通过改变prompt的构造方式——在示例中加入推理过程——引导模型也生成类似的中间步骤，而完全不需要修改模型本身。</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键洞察："展示你的工作过程"</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a>Chain-of-Thought prompting的核心思想可以用一句话概括：**在few-shot示例中展示推理过程，模型就会学着也展示推理过程**。</span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a><span class="al">![Chain-of-Thought prompting与标准prompting的对比。左侧：标准few-shot只给出最终答案，模型直接输出（通常错误的）数字。右侧：CoT在示例中展示完整的推理过程，引导模型也生成中间步骤，最终得到正确答案。](figures/chapter-21/original/fig1-cot-example.png)</span>{#fig-cot-example width=90%}</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>*Source: Wei et al. (2022) "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", Figure 1*</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>对比标准few-shot和CoT：</span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>**标准 few-shot**（只给答案）：</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="in">Q: Roger有5个网球。他又买了2罐网球。每罐有3个网球。</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a><span class="in">   他现在一共有多少个网球？</span></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="in">A: 答案是11。</span></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a>**Chain-of-Thought**（给出推理过程）：</span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="in">Q: Roger有5个网球。他又买了2罐网球。每罐有3个网球。</span></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a><span class="in">   他现在一共有多少个网球？</span></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a><span class="in">A: Roger一开始有5个网球。2罐网球一共有2×3=6个网球。</span></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a><span class="in">   5+6=11。答案是11。</span></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>区别仅仅在于：答案前面多了几句解释——"Roger一开始有5个网球。2罐网球一共有2×3=6个网球。5+6=11。"这就是全部的技巧。</span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a>为什么这么简单的改动会有如此大的效果？回到In-Context Learning的本质：模型从示例中学习的不仅是"任务内容"，更重要的是**输出格式和模式**。当示例中的答案只有一个数字时，模型学到的模式是"问题→数字"；当示例中的答案包含推理链条时，模型学到的模式变成了"问题→推理步骤→数字"。后者给了模型一个"思考的空间"——通过生成中间文本，模型可以一步一步地分解问题，每一步只需要做一个相对简单的操作。</span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个类比：开卷考试 vs 闭卷考试</span></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>标准ICL就像一场**闭卷考试**：学生（模型）看完题目后必须直接在答题卡上填写最终答案，不允许在草稿纸上写任何计算过程。对于简单的事实性问题（"法国的首都是哪里？"），这没有问题。但对于需要多步推理的数学题，这就像要求学生在脑中完成所有计算，不犯一个错误。</span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>CoT则像是给了学生一张**草稿纸**：你可以在上面写下中间步骤、画出辅助线、记录临时结果，最后再把答案写到答题卡上。这不是在"降低考试难度"——题目没有变，需要的知识也没有变——你只是给了解题过程一个被记录和引用的空间。</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>对语言模型来说，这张"草稿纸"就是模型生成的文本本身。自回归模型在生成每个token时，可以"看到"它之前生成的所有token。如果模型先生成了"2×3=6"，那么在生成下一步时，"6"这个中间结果就出现在了模型的上下文中，可以被直接引用来计算"5+6=11"。本质上，CoT将模型的输出空间从"答案空间"扩展到了"推理链+答案空间"，让模型可以利用自己的生成结果作为"工作记忆"。</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么只有大模型受益？</span></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a>CoT最令人困惑的特征之一是它的**规模依赖性**：只有足够大的模型（通常≥100B参数）才能从CoT中显著受益，小模型即使给了CoT示例也无法生成连贯的推理链条。</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>一个直觉性的解释是：生成正确的CoT需要模型同时具备多种能力——理解数学关系、执行算术运算、保持逻辑连贯、遵循示例格式——这些能力可能各自需要一定的模型容量才能涌现。小模型在每一项上都只有勉强及格的水平，组合起来就完全不够用了。而大模型在各项上都有充足的"余量"，让它们可以流畅地生成多步推理链。</span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>这与人类认知发展有一个有趣的平行：皮亚杰的认知发展理论指出，儿童在某个年龄阶段之前无法执行"形式运算"（如假设推理），不是因为他们不知道某个具体知识，而是因为他们的认知系统还没有复杂到足以支持这种运算模式。类似地，小语言模型可能"知道"2×3=6和5+6=11，但无法将这些知识组织成一个连贯的多步推理链。</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chain-of-Thought Prompting (Wei et al., 2022)</span></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 方法定义</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a>Chain-of-Thought prompting的方法极其简单，不涉及任何算法创新或模型修改。它唯一的改变是**few-shot示例的构造方式**：在每个示例的答案中加入中间推理步骤（reasoning chain），形成"问题→推理链→答案"的格式。</span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>形式化地，设 $x$ 为输入问题，$y$ 为最终答案。标准few-shot的示例格式为 $(x, y)$，而CoT的示例格式为 $(x, c, y)$，其中 $c = (c_1, c_2, \ldots, c_m)$ 是一系列中间推理步骤。在推理时，给定新问题 $x_{\text{query}}$，模型被引导先生成推理链 $\hat{c}$，再基于推理链生成最终答案 $\hat{y}$。</span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="fu">## CoT Prompting（Wei et al., 2022）方法概要</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="in">输入：语言模型 LM，K 个带推理链的示例 {(x₁, c₁, y₁), ..., (xₖ, cₖ, yₖ)}，</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a><span class="in">      查询问题 x_query</span></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a><span class="in">输出：推理链 ĉ 和最终答案 ŷ</span></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a><span class="in">1. 构造 prompt：</span></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a><span class="in">     prompt ← ""</span></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a><span class="in">     for k = 1 to K:</span></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a><span class="in">         prompt ← prompt + "Q: " + xₖ + "\n"</span></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a><span class="in">         prompt ← prompt + "A: " + cₖ + " 答案是 " + yₖ + "\n\n"</span></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a><span class="in">     prompt ← prompt + "Q: " + x_query + "\n"</span></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a><span class="in">     prompt ← prompt + "A: "</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a><span class="in">2. 前向推理（无梯度更新）：</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a><span class="in">     output ← LM.generate(prompt)</span></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a><span class="in">3. 从 output 中提取推理链 ĉ 和最终答案 ŷ</span></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a><span class="in">4. return (ĉ, ŷ)</span></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>*改编自 Wei et al. (2022) "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", Section 2. [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)*</span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>值得注意的是，这个方法**没有任何可训练的参数**。推理链是手工编写的、放在few-shot示例中的自然语言文本。模型不是被"训练"去推理，而是被"引导"去模仿示例中的推理格式。</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：CoT如何解决数学题</span></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>**设定**：使用PaLM 540B模型，在GSM8K数据集上进行few-shot CoT推理。</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a>**Step 1: 构造CoT示例**</span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>我们手工编写8个数学题的推理链作为few-shot示例。以下展示其中一个：</span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a><span class="in">Q: Shawn有5个玩具。圣诞节他从妈妈和爸爸那里各收到了2个玩具。</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a><span class="in">   他现在一共有多少个玩具？</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="in">A: Shawn一开始有5个玩具。他从妈妈那里收到2个，从爸爸那里也</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a><span class="in">   收到2个，所以他一共收到了2+2=4个新玩具。现在他一共有</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a><span class="in">   5+4=9个玩具。答案是9。</span></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>**Step 2: 加入查询问题**</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>将上述8个CoT示例拼接成prompt，然后附上新问题：</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a><span class="in">Q: 咖啡店周一卖了37杯拿铁和13杯卡布奇诺。周二拿铁卖了</span></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="in">   周一的两倍，但卡布奇诺只卖了周一的一半（四舍五入取整）。</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a><span class="in">   两天一共卖了多少杯咖啡？</span></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="in">A:</span></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>**Step 3: 模型生成推理链**</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a>PaLM 540B生成如下输出：</span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a><span class="in">周一卖了37杯拿铁和13杯卡布奇诺，周一总共卖了37+13=50杯。</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="in">周二拿铁卖了37×2=74杯。周二卡布奇诺卖了13÷2=6.5，</span></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="in">四舍五入得7杯。周二总共卖了74+7=81杯。</span></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a><span class="in">两天总共卖了50+81=131杯。答案是131。</span></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>**Step 4: 提取答案**</span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a>从生成文本中提取"答案是131"，得到最终答案131。✓</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a>**解读**：注意模型的推理链几乎完美地复现了人类的解题过程——分步计算周一总量、周二各品类数量、周二总量，最后求和。每一步都是一个简单的算术运算，模型可以可靠地执行。如果没有CoT，模型需要一步从题目文本直接映射到"131"，这对模型来说是一个极难的黑箱映射。</span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 关键实验结果</span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a>Wei et al. (2022) 在三类推理benchmark上评估了CoT的效果：</span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a>**数学推理**（GSM8K — 小学数学）：</span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 标准few-shot <span class="pp">|</span> CoT few-shot <span class="pp">|</span> 提升 <span class="pp">|</span></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------------|-------------|------|</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 175B <span class="pp">|</span> 18.0% <span class="pp">|</span> 49.6% <span class="pp">|</span> +31.6 <span class="pp">|</span></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PaLM 540B <span class="pp">|</span> 17.9% <span class="pp">|</span> **56.9%** <span class="pp">|</span> +39.0 <span class="pp">|</span></span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 微调SOTA（当时） <span class="pp">|</span> — <span class="pp">|</span> 55.0% <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>PaLM 540B + CoT不仅远超标准few-shot（56.9% vs 17.9%），还**超过了当时的微调SOTA**（55.0%）。这个结果意味着：一个足够大的通用语言模型，配合简单的CoT提示，可以在不经过任何任务特定训练的情况下，超越精心微调的专用模型。</span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>**符号推理**（如字母串拼接）：</span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> PaLM 540B标准 <span class="pp">|</span> PaLM 540B CoT <span class="pp">|</span></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------------|-------------|</span></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 字母串拼接 (in-domain) <span class="pp">|</span> 88.0% <span class="pp">|</span> **99.6%** <span class="pp">|</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 字母串拼接 (OOD, 更长) <span class="pp">|</span> 34.0% <span class="pp">|</span> **98.8%** <span class="pp">|</span></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>CoT在域外（Out-of-Distribution）泛化上的提升尤为惊人：从34%跳到98.8%。这暗示CoT不仅帮助模型"做对已知模式的题"，还帮助它"学会了推理规则本身"——至少在某种程度上。</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>**常识推理**（StrategyQA — 需要隐式推理的是/否问题）：</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 标准few-shot <span class="pp">|</span> CoT few-shot <span class="pp">|</span></span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------------|-------------|</span></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PaLM 540B <span class="pp">|</span> 73.9% <span class="pp">|</span> **77.8%** <span class="pp">|</span></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>在常识推理上，CoT的提升相对温和（+3.9个百分点），这可能是因为常识推理不像数学那样容易分解成明确的逻辑步骤。</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 规模效应：CoT的"涌现"特征</span></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>CoT最引人注目的实验发现是它与模型规模之间的非线性关系。Wei et al. 在不同规模的模型上测试了标准few-shot和CoT的效果：</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型规模 <span class="pp">|</span> 标准few-shot (GSM8K) <span class="pp">|</span> CoT (GSM8K) <span class="pp">|</span> CoT增益 <span class="pp">|</span></span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a><span class="pp">|---------|---------------------|-------------|---------|</span></span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~8B <span class="pp">|</span> ~5% <span class="pp">|</span> ~5% <span class="pp">|</span> ≈0 <span class="pp">|</span></span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~62B <span class="pp">|</span> ~12% <span class="pp">|</span> ~18% <span class="pp">|</span> +6 <span class="pp">|</span></span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~540B <span class="pp">|</span> ~18% <span class="pp">|</span> ~57% <span class="pp">|</span> **+39** <span class="pp">|</span></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a><span class="al">![CoT效果与模型规模的关系。在数学推理任务上，小模型（~8B）几乎不受益于CoT，而大模型（~540B）获得巨大提升。CoT的效果呈现出类似"涌现"的非线性跳跃。](figures/chapter-21/original/fig2-cot-scaling.png)</span>{#fig-cot-scaling width=85%}</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>*Source: Wei et al. (2022) "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", Figure 2*</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>规律极其清晰：**小模型几乎不受益于CoT，中等模型有一些受益，大模型获得巨大提升**。更精确地说，CoT的效果不是线性增长的，而是呈现出类似"相变"（phase transition）的急剧跳跃。在约100B参数处，CoT的效果似乎突然"爆发"了。</span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>这个发现引出了一个更大的话题——涌现能力——我们将在本章后半部分详细讨论。</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a><span class="fu">### Zero-shot CoT (Kojima et al., 2022)</span></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a>Wei et al.的CoT方法需要手工编写推理链示例，这虽然简单但仍需要一定的人工成本。2022年5月，Kojima et al. 发现了一个令人震惊的结果：只需要在prompt末尾加上一句**"Let's think step by step"**（让我们一步一步想），模型就能自动生成推理链——完全不需要任何few-shot示例。</span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a>这就是**Zero-shot CoT**：一种不需要任何示例的思维链推理方法。</span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="fu">## Zero-shot CoT 两阶段推理流程（Kojima et al., 2022）</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a><span class="in">输入：语言模型 LM，查询问题 x_query</span></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a><span class="in">输出：最终答案 ŷ</span></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a><span class="in">阶段1 — 推理提取（Reasoning Extraction）：</span></span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a><span class="in">  prompt₁ ← x_query + "\nLet's think step by step."</span></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a><span class="in">  reasoning ← LM.generate(prompt₁)</span></span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a><span class="in">阶段2 — 答案提取（Answer Extraction）：</span></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a><span class="in">  prompt₂ ← prompt₁ + reasoning +</span></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a><span class="in">             "\nTherefore, the answer (arabic numerals) is"</span></span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a><span class="in">  ŷ ← LM.generate(prompt₂)</span></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a><span class="in">return ŷ</span></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a>*Source: Kojima et al. (2022) "Large Language Models are Zero-Shot Reasoners", Section 3. [arXiv:2205.11916](https://arxiv.org/abs/2205.11916)*</span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>注意这个方法分两个阶段：第一阶段让模型自由生成推理过程，第二阶段将推理过程作为上下文，引导模型给出最终答案。分两阶段的原因是：如果只用一个prompt，模型可能在生成推理链后"忘记"给出一个清晰的最终答案，或者答案被淹没在冗长的推理文本中。</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 为什么这六个词如此有效？</span></span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a>"Let's think step by step"为什么能work？这个问题至今没有完全令人满意的答案，但有几个可能的解释。</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>第一个解释与**预训练数据**有关。GPT-3等模型在预训练阶段看过了海量的教学文本、论坛答案、教程等。在这些文本中，"let's think step by step"经常出现在详细的解题过程之前。当模型看到这个短语时，它被"激活"了一种特定的生成模式——生成逐步推理的文本。</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>第二个解释与**生成分布的偏移**有关。标准zero-shot prompt（如"Q: ... A:"）暗示模型应该直接输出答案。而"Let's think step by step"将期望输出从"简短答案"转移到了"详细解释"，这改变了模型生成的整个token分布。</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>Kojima et al. 还测试了其他"触发短语"（trigger phrases）的效果：</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 触发短语 <span class="pp">|</span> MultiArith准确率 <span class="pp">|</span></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a><span class="pp">|---------|----------------|</span></span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> *（无触发）* <span class="pp">|</span> 17.7% <span class="pp">|</span></span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "Let's think step by step" <span class="pp">|</span> **78.7%** <span class="pp">|</span></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "Let's think about this logically" <span class="pp">|</span> 72.6% <span class="pp">|</span></span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "Let's solve this problem by splitting it into steps" <span class="pp">|</span> 74.5% <span class="pp">|</span></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "First," <span class="pp">|</span> 77.3% <span class="pp">|</span></span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> "The answer is" <span class="pp">|</span> 0.0% <span class="pp">|</span></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a>"Let's think step by step"是效果最好的触发短语，但其他鼓励逐步思考的短语也有类似效果。而"The answer is"反而导致了0%的准确率——因为它引导模型直接输出答案，跳过了推理过程。</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 实验结果</span></span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a>在MultiArith（多步算术推理）数据集上：</span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> GPT-3 175B准确率 <span class="pp">|</span></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------------|</span></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Zero-shot（直接问） <span class="pp">|</span> 17.7% <span class="pp">|</span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Zero-shot CoT（+"Let's think step by step"） <span class="pp">|</span> **78.7%** <span class="pp">|</span></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Few-shot（8个示例，无推理链） <span class="pp">|</span> 33.7% <span class="pp">|</span></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Few-shot CoT（8个示例+推理链） <span class="pp">|</span> **93.0%** <span class="pp">|</span></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a>Zero-shot CoT（78.7%）不仅远超标准zero-shot（17.7%），甚至超过了标准few-shot（33.7%）。这意味着**"一句话的魔力"超过了"8个精心选择的示例"**。当然，few-shot CoT（93.0%）仍然是最好的——但Zero-shot CoT的简便性使它在实践中极具吸引力。</span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Consistency (Wang et al., 2022)</span></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>CoT的一个局限是它使用贪心解码（greedy decoding）：模型只生成一条推理路径，然后基于这条路径给出答案。但任何一条推理路径都可能包含错误——也许第二步的计算出了问题，也许推理方向完全错了。有没有办法提高CoT的可靠性？</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>Wang et al. (2022) 提出了一个直觉上非常自然的改进：**Self-Consistency**（自一致性）。核心思想是：对同一个问题，采样多条不同的推理路径，然后对最终答案进行**多数投票**（majority voting）。如果多条独立的推理路径都指向同一个答案，那这个答案大概率是对的。</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a><span class="al">![Self-Consistency方法示意图。对同一个问题采样多条不同的推理路径（通过非零温度），然后对最终答案进行多数投票，选择出现次数最多的答案。](figures/chapter-21/original/fig4-self-consistency.png)</span>{#fig-self-consistency width=80%}</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a>*Source: Wang et al. (2022) "Self-Consistency Improves Chain of Thought Reasoning in Language Models", Figure 1*</span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>这个思想来自一个简单的观察：**正确答案通常可以通过多种不同的路径到达，而错误答案则各有各的错法**。考虑一个数学题，正确答案是42。三条推理路径可能分别通过"先算A再算B"、"先算B再算A"、"用代数方法"得到42，而一条错误路径可能因为算错了一步得到38。多数投票自然会选出42。</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Self-Consistency（Wang et al., 2022）</span></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="in">输入：语言模型 LM，CoT prompt P，查询问题 x_query，</span></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="in">      采样次数 N，采样温度 T</span></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a><span class="in">输出：最终答案 ŷ</span></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a><span class="in">1. 构造完整 prompt：</span></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a><span class="in">     full_prompt ← P + x_query</span></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a><span class="in">2. 采样多条推理路径：</span></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a><span class="in">     for i = 1 to N:</span></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a><span class="in">         (cᵢ, yᵢ) ← LM.sample(full_prompt, temperature=T)</span></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="in">         # cᵢ 是推理链，yᵢ 是从推理链中提取的答案</span></span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a><span class="in">3. 多数投票：</span></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a><span class="in">     ŷ ← mode({y₁, y₂, ..., yₙ})  # 选择出现次数最多的答案</span></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a><span class="in">4. return ŷ</span></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>*Source: Wang et al. (2022) "Self-Consistency Improves Chain of Thought Reasoning in Language Models", Section 2. [arXiv:2203.11171](https://arxiv.org/abs/2203.11171)*</span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：Self-Consistency如何工作</span></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a>**设定**：使用CoT prompt对同一道数学题采样5条推理路径。</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a>**问题**："一家书店有120本书。第一周卖了总量的1/3，第二周卖了剩余的1/4。还剩多少本？"</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a>**正确推理**：第一周卖 120×(1/3)=40本，剩80本。第二周卖 80×(1/4)=20本，剩60本。</span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a>**采样5条路径**（temperature=0.7）：</span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 路径 <span class="pp">|</span> 推理过程 <span class="pp">|</span> 最终答案 <span class="pp">|</span></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|---------|</span></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 路径1 <span class="pp">|</span> 第一周：120/3=40，剩80；第二周：80/4=20，剩60 <span class="pp">|</span> **60** ✓ <span class="pp">|</span></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 路径2 <span class="pp">|</span> 先算两周共卖：120/3+120/4=40+30=70，剩120-70=50 <span class="pp">|</span> 50 ✗ <span class="pp">|</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 路径3 <span class="pp">|</span> 第一周卖40本剩80，第二周卖80÷4=20，80-20=60 <span class="pp">|</span> **60** ✓ <span class="pp">|</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 路径4 <span class="pp">|</span> 120×(1/3)=40剩80，80×(1/4)=20，剩下80-20=60 <span class="pp">|</span> **60** ✓ <span class="pp">|</span></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 路径5 <span class="pp">|</span> 第一周：120/3=40；第二周：120/4=30，120-40-30=50 <span class="pp">|</span> 50 ✗ <span class="pp">|</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>**多数投票**：答案60出现3次，答案50出现2次。选择60。✓</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>**解读**：路径2和路径5犯了同样的错误——它们把"剩余的1/4"错误理解为"总量的1/4"。但另外三条路径正确理解了"剩余"的含义，各自独立地得到了60。多数投票成功过滤了错误。</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 实验结果</span></span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a>Self-Consistency在各个推理benchmark上持续改善CoT的效果：</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Benchmark <span class="pp">|</span> CoT (贪心) <span class="pp">|</span> Self-Consistency (40路径) <span class="pp">|</span> 提升 <span class="pp">|</span></span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-----------|-------------------------|------|</span></span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GSM8K (PaLM 540B) <span class="pp">|</span> 56.9% <span class="pp">|</span> **74.4%** <span class="pp">|</span> +17.5 <span class="pp">|</span></span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SVAMP (PaLM 540B) <span class="pp">|</span> 79.0% <span class="pp">|</span> **86.6%** <span class="pp">|</span> +7.6 <span class="pp">|</span></span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> AQuA (PaLM 540B) <span class="pp">|</span> 35.8% <span class="pp">|</span> **48.0%** <span class="pp">|</span> +12.2 <span class="pp">|</span></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>在GSM8K上，Self-Consistency将PaLM 540B的CoT准确率从56.9%提升到74.4%——提升了17.5个百分点，相当于在CoT基础上又减少了约40%的错误。</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tree of Thoughts (Yao et al., 2023)</span></span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a>CoT和Self-Consistency都假设推理是一个**线性**过程：从问题出发，一步一步走到答案。Self-Consistency虽然采样了多条路径，但每条路径内部仍然是线性的，而且不同路径之间互不交流。</span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>但有些问题的求解过程更像是**探索一棵搜索树**：你在某一步做出选择，如果发现走不通就需要回溯（backtrack），尝试另一个分支。这正是经典人工智能中搜索算法（BFS、DFS）擅长的事情。</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a>Yao et al. (2023) 提出的 **Tree of Thoughts (ToT)** 将CoT从线性推理扩展到了树状搜索。核心思想是：</span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>将推理过程分解为多个"思维步骤"（thought steps）</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>在每一步，生成多个候选思维</span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>用LLM自身来评估每个候选思维的"前景"（是否有希望到达正确答案）</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>使用BFS或DFS来系统地探索思维树</span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tree of Thoughts 框架（Yao et al., 2023）</span></span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a><span class="in">输入：语言模型 LM，问题 x，思维生成器 G，</span></span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a><span class="in">      状态评估器 V，搜索算法 (BFS 或 DFS)</span></span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a><span class="in">输出：最终解 ŷ</span></span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a><span class="in">定义：</span></span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a><span class="in">  s = [x] 为初始状态（只包含问题描述）</span></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a><span class="in">  T 为思维步骤的最大深度</span></span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a><span class="in">BFS 搜索过程：</span></span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a><span class="in">  S₀ ← {s}  # 初始候选集</span></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a><span class="in">  for t = 1 to T:</span></span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a><span class="in">      # 1. 生成候选思维</span></span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a><span class="in">      candidates ← {}</span></span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a><span class="in">      for s in Sₜ₋₁:</span></span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a><span class="in">          thoughts ← G(LM, s, k)  # 生成 k 个候选思维</span></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a><span class="in">          candidates ← candidates ∪ {[s, z] : z ∈ thoughts}</span></span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a><span class="in">      # 2. 评估候选状态</span></span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a><span class="in">      for s' in candidates:</span></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a><span class="in">          V(s') ← LM.evaluate(s')  # 用 LM 评估前景</span></span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a><span class="in">      # 3. 选择最优的 b 个状态（beam width = b）</span></span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a><span class="in">      Sₜ ← top_b(candidates, key=V)</span></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="in">  return best solution from Sₜ</span></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a>*Source: Yao et al. (2023) "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", Section 2-3. [arXiv:2305.10601](https://arxiv.org/abs/2305.10601)*</span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 四种方法的对比</span></span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a><span class="al">![四种prompting方法的推理结构对比。从左到右：标准IO（一步到位）、CoT（线性链）、CoT-SC（多条线性链+投票）、ToT（树状搜索+评估+回溯）。](figures/chapter-21/original/fig5-tree-of-thoughts.png)</span>{#fig-tot-comparison width=90%}</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a>*Source: Yao et al. (2023) "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", Figure 1*</span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a>从标准ICL到Tree of Thoughts，推理结构经历了一个清晰的演进：</span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 推理结构 <span class="pp">|</span> 路径数 <span class="pp">|</span> 是否有评估 <span class="pp">|</span> 是否回溯 <span class="pp">|</span></span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|--------|-----------|---------|</span></span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **标准ICL (IO)** <span class="pp">|</span> 无推理 <span class="pp">|</span> 1 <span class="pp">|</span> ✗ <span class="pp">|</span> ✗ <span class="pp">|</span></span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **CoT** <span class="pp">|</span> 线性链 <span class="pp">|</span> 1 <span class="pp">|</span> ✗ <span class="pp">|</span> ✗ <span class="pp">|</span></span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **CoT + Self-Consistency** <span class="pp">|</span> 多条线性链 <span class="pp">|</span> N <span class="pp">|</span> ✗（仅投票） <span class="pp">|</span> ✗ <span class="pp">|</span></span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Tree of Thoughts** <span class="pp">|</span> 树状搜索 <span class="pp">|</span> 动态 <span class="pp">|</span> ✓（LLM评估） <span class="pp">|</span> ✓ <span class="pp">|</span></span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a>标准ICL是"一步到位"，CoT是"一条直线"，Self-Consistency是"多条直线取众数"，ToT是"一棵搜索树"。</span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ToT的标志性实验：24点游戏</span></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a>Yao et al. 选择了一个巧妙的评测任务来展示ToT的优势：**24点游戏**。给定4个数字（1-13），使用加减乘除（每个数字恰好用一次），使结果等于24。例如，给定 {1, 2, 3, 4}，一个解是 $1 \times 2 \times 3 \times 4 = 24$。</span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a>这个任务非常适合树搜索，因为：(a) 它需要探索不同的运算组合；(b) 某些中间结果一看就走不通（比如中间算出了一个很大的数）；(c) 有明确的成功标准（等于24）。</span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 24点成功率 <span class="pp">|</span></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|</span></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 标准ICL (IO) <span class="pp">|</span> 7.3% <span class="pp">|</span></span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CoT <span class="pp">|</span> 4.0% <span class="pp">|</span></span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CoT + Self-Consistency (100路径) <span class="pp">|</span> 9.0% <span class="pp">|</span></span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **ToT (BFS, b=5)** | **74.0%** <span class="pp">|</span></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a>结果令人震惊：ToT（74%）比其他所有方法高出一个数量级。有趣的是，CoT（4%）甚至比标准ICL（7.3%）更差——这是因为CoT生成的线性推理链容易在早期犯错并将错误传递到后续步骤，而标准ICL至少还有"蒙对"的可能。ToT通过评估和回溯避免了这个问题。</span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a>然而，需要注意的是，ToT的计算成本远高于CoT——每个问题需要多次LLM调用（生成候选+评估），在24点游戏中大约需要数十次API调用。这使得它在大规模应用中成本高昂。</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实现CoT推理</span></span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cot_solve(question: <span class="bu">str</span>, cot_examples: <span class="bu">list</span>[<span class="bu">dict</span>], model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-4"</span>):</span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a><span class="co">    使用 Chain-of-Thought 解决推理问题。</span></span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a><span class="co">        question: 待解决的问题</span></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a><span class="co">        cot_examples: CoT示例列表，每个元素包含 'question', 'reasoning', 'answer'</span></span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a><span class="co">        model: 模型名称</span></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a><span class="co">        (reasoning, answer) 元组</span></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构造 CoT prompt</span></span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="st">"请一步一步地解决以下数学问题。</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ex <span class="kw">in</span> cot_examples:</span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>ex[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"A: </span><span class="sc">{</span>ex[<span class="st">'reasoning'</span>]<span class="sc">}</span><span class="ss"> 答案是</span><span class="sc">{</span>ex[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">。</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="st">"A: "</span></span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 调用模型（贪心解码）</span></span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,   <span class="co"># 贪心解码</span></span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 提取答案（假设格式为"答案是X"）</span></span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"答案是"</span> <span class="kw">in</span> output:</span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a>        answer <span class="op">=</span> output.split(<span class="st">"答案是"</span>)[<span class="op">-</span><span class="dv">1</span>].strip().rstrip(<span class="st">"。"</span>)</span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, answer</span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a>cot_examples <span class="op">=</span> [</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: <span class="st">"Shawn有5个玩具。圣诞节从妈妈和爸爸各收到2个。现在有几个？"</span>,</span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reasoning"</span>: <span class="st">"Shawn原有5个。妈妈给了2个，爸爸给了2个，共收到2+2=4个。"</span></span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>                     <span class="st">"5+4=9。"</span>,</span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: <span class="st">"9"</span></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: <span class="st">"花园有36朵玫瑰。园丁又种了18朵，然后摘了9朵送人。现在有多少？"</span>,</span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reasoning"</span>: <span class="st">"原来36朵，种了18朵变成36+18=54朵。摘了9朵后剩54-9=45朵。"</span>,</span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: <span class="st">"45"</span></span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>reasoning, answer <span class="op">=</span> cot_solve(</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a>    <span class="st">"一家书店有120本书。第一周卖了总量的1/3，第二周卖了剩余的1/4。还剩多少？"</span>,</span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a>    cot_examples</span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"推理过程: </span><span class="sc">{</span>reasoning<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"最终答案: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实现Self-Consistency</span></span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_consistency_solve(</span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a>    question: <span class="bu">str</span>,</span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a>    cot_examples: <span class="bu">list</span>[<span class="bu">dict</span>],</span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a>    n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a>    temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a>    model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-4"</span></span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a><span class="co">    使用 Self-Consistency 提升 CoT 推理的可靠性。</span></span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a><span class="co">    核心思想：采样多条推理路径，对最终答案进行多数投票。</span></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构造 CoT prompt（与上面相同）</span></span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="st">"请一步一步地解决以下数学问题。</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ex <span class="kw">in</span> cot_examples:</span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>ex[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"A: </span><span class="sc">{</span>ex[<span class="st">'reasoning'</span>]<span class="sc">}</span><span class="ss"> 答案是</span><span class="sc">{</span>ex[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">。</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">A: "</span></span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 采样多条推理路径</span></span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a>    answers <span class="op">=</span> []</span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>    reasonings <span class="op">=</span> []</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span>model,</span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}],</span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span>temperature,  <span class="co"># 非零温度以获得多样性</span></span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>        reasonings.append(output)</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 提取答案</span></span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"答案是"</span> <span class="kw">in</span> output:</span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a>            ans <span class="op">=</span> output.split(<span class="st">"答案是"</span>)[<span class="op">-</span><span class="dv">1</span>].strip().rstrip(<span class="st">"。"</span>)</span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a>            answers.append(ans)</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 多数投票</span></span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> answers:</span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span>, []</span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a>    vote_counts <span class="op">=</span> Counter(answers)</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a>    final_answer <span class="op">=</span> vote_counts.most_common(<span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a>    confidence <span class="op">=</span> vote_counts[final_answer] <span class="op">/</span> <span class="bu">len</span>(answers)</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_answer, confidence, <span class="bu">list</span>(<span class="bu">zip</span>(reasonings, answers))</span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a>answer, confidence, paths <span class="op">=</span> self_consistency_solve(</span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a>    <span class="st">"一家书店有120本书。第一周卖了总量的1/3，第二周卖了剩余的1/4。还剩多少？"</span>,</span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a>    cot_examples,</span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.7</span></span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"最终答案: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss"> (置信度: </span><span class="sc">{</span>confidence<span class="sc">:.0%}</span><span class="ss">)"</span>)</span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"各路径答案分布: </span><span class="sc">{</span>Counter([a <span class="cf">for</span> _, a <span class="kw">in</span> paths])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实现Zero-shot CoT</span></span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> zero_shot_cot_solve(question: <span class="bu">str</span>, model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-4"</span>):</span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="co">    Zero-shot CoT：只需要一句"Let's think step by step"。</span></span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a><span class="co">    分两阶段：先提取推理，再提取答案。</span></span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 阶段1：推理提取</span></span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a>    prompt_stage1 <span class="op">=</span> <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">A: Let's think step by step."</span></span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a>    response1 <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt_stage1}],</span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a>    reasoning <span class="op">=</span> response1.choices[<span class="dv">0</span>].message.content</span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 阶段2：答案提取</span></span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a>    prompt_stage2 <span class="op">=</span> (</span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Q: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"A: Let's think step by step. </span><span class="sc">{</span>reasoning<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Therefore, the answer (arabic numerals) is"</span></span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a>    response2 <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt_stage2}],</span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> response2.choices[<span class="dv">0</span>].message.content.strip()</span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> reasoning, answer</span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a><span class="fu">### 工程注意事项</span></span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a>**温度设置的策略**：CoT推理有两种使用场景，它们需要不同的温度设置。当使用贪心CoT（单路径）时，应设置 $\text{temperature} = 0$ 以获得最确定性的推理链。当使用Self-Consistency（多路径投票）时，需要设置 $\text{temperature} \in <span class="co">[</span><span class="ot">0.5, 1.0</span><span class="co">]</span>$ 以获得推理路径的多样性——如果温度为0，所有路径都会相同，多数投票就失去了意义。</span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a>**答案提取的鲁棒性**：从模型输出中提取最终答案是CoT工程中最容易出问题的环节。模型可能以各种格式输出答案："答案是42"、"所以结果为42"、"42。"、"$42$"。建议使用正则表达式或二次LLM调用来鲁棒地提取答案。</span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a>**成本考量**：Self-Consistency需要多次模型调用，成本正比于采样次数。Wang et al. 的实验显示，大约10-40条路径就能获得接近最优的效果——更多路径的边际收益递减。在实践中，10条路径（10倍成本）通常是性价比最高的选择。</span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-705"><a href="#cb14-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-706"><a href="#cb14-706" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨涌现能力的本质、CoT的机制解释、以及LLM推理能力的根本争论。</span></span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a><span class="fu">### 涌现能力：规模带来相变？ (Wei et al., 2022b)</span></span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a>Chain-of-Thought的规模依赖性只是一个更大现象的缩影。2022年6月，Jason Wei等人发表了一篇影响深远的论文——"Emergent Abilities of Large Language Models"——系统性地研究了一类特殊现象：某些能力在小模型中完全不存在，但当模型规模超过某个临界点时突然出现。</span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a>他们将**涌现能力**（emergent ability）定义为："一种在小模型中不存在、但在大模型中出现的能力。"更精确地说，如果我们画出能力（如某benchmark的准确率）随模型规模变化的曲线，涌现能力呈现出一种**不可预测的跳跃**：在某个规模之前，性能接近随机猜测或零；在某个规模之后，性能突然跃升到远高于随机的水平。这与物理学中的相变（phase transition）——如水在0°C突然从液态变为固态——有惊人的相似性。</span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a><span class="al">![涌现能力的经典图示。横轴是模型规模（FLOPs或参数量），纵轴是任务准确率。在某个规模阈值之前，性能接近随机水平；超过阈值后，性能急剧跃升——呈现出类似物理学中"相变"的特征。](figures/chapter-21/original/fig3-emergent-abilities.png)</span>{#fig-emergent-abilities width=85%}</span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-718"><a href="#cb14-718" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb14-719"><a href="#cb14-719" aria-hidden="true" tabindex="-1"></a>*Source: Wei et al. (2022) "Emergent Abilities of Large Language Models", Figure 2*</span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a>Wei et al. (2022b) 从两个角度收集了涌现能力的证据。</span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a>**BIG-Bench**上的涌现：BIG-Bench是一个包含200+语言任务的大型评测集。在许多任务上，小模型的表现接近随机，但当模型规模增加到某个阈值后，准确率突然飙升。例如，在三步算术推理任务上，模型从10B到100B几乎没有进展（约等于随机水平），但从100B到540B时准确率从近0%跳到超过40%。</span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a>**多模型系列的涌现**：Wei et al. 还跨越多个模型系列（GPT-3、LaMDA、PaLM、Chinchilla）展示了涌现现象。在不同的任务上，涌现发生的规模阈值不同，但模式一致：先是一段长长的"沉默期"（接近随机性能），然后是一个急剧的跳跃。</span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a>这个发现的影响是深远的。如果涌现是真实的，它意味着**我们无法通过研究小模型来预测大模型的能力**——因为质变不是量变的简单延伸。这对整个AI安全和对齐领域都有重大影响：如果我们不知道更大的模型会涌现出什么能力（包括可能有害的能力），那么"先训练再评估"的策略就存在根本性的风险。</span>
<span id="cb14-729"><a href="#cb14-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-730"><a href="#cb14-730" aria-hidden="true" tabindex="-1"></a><span class="fu">### 涌现是真实的还是"海市蜃楼"？ (Schaeffer et al., 2023)</span></span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a>涌现能力的叙事在2023年受到了一次强有力的挑战。Schaeffer et al. 在NeurIPS 2023最佳论文"Are Emergent Abilities of Large Language Models a Mirage?"中论证：**涌现可能不是模型行为的内在属性，而是我们选择的度量方式（metric）制造的假象**。</span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a>他们的核心论点可以用一个精彩的实验来说明。考虑一个分类任务，模型的"真实"准确率随规模平滑增长：</span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a>\text{真实准确率}(N) = 0.1 + 0.5 \times \log_{10}(N / 10^8)</span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a>其中 $N$ 是参数量。当 $N$ 从 $10^8$ 增长到 $10^{12}$ 时，真实准确率从0.1平滑增长到0.9——没有任何相变。</span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-742"><a href="#cb14-742" aria-hidden="true" tabindex="-1"></a>但是，如果我们用**精确匹配**（Exact Match, EM）作为度量指标，情况就完全不同了。EM是一个非线性度量：如果模型的输出与标准答案完全一致就得1分，否则0分。对于需要多步输出的任务，即使模型的"接近正确"的能力在平滑增长，只要它还不够"完全正确"，EM就会给出0。这意味着用EM衡量时，性能曲线可能呈现出阶跃状的"涌现"——但这只是EM度量的非线性造成的，不是模型能力的真实突变。</span>
<span id="cb14-743"><a href="#cb14-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a><span class="al">![涌现的"海市蜃楼"。同一组模型在同一任务上的表现，使用非线性度量（如精确匹配，左图）时呈现出"涌现"的阶跃，但使用连续度量（如token级准确率，右图）时变为平滑增长——暗示涌现可能是度量方式的产物。](figures/chapter-21/original/fig6-emergence-mirage.png)</span>{#fig-emergence-mirage width=85%}</span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a>*Source: Schaeffer et al. (2023) "Are Emergent Abilities of Large Language Models a Mirage?", Figure 1. NeurIPS 2023 Best Paper*</span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a>Schaeffer et al. 提供了以下关键证据：</span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a>**证据1：更换度量，涌现消失**。在多个被Wei et al. 标记为"涌现"的任务上，如果将度量从EM换成连续度量（如token级准确率、Brier Score、log-likelihood），相变就消失了——取而代之的是平滑、可预测的性能曲线。</span>
<span id="cb14-753"><a href="#cb14-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-754"><a href="#cb14-754" aria-hidden="true" tabindex="-1"></a>**证据2：InverseBench实验**。他们构造了一个人工任务，让模型输出一个数字序列。用EM衡量，小模型的EM接近0（因为没有完全匹配），大模型的EM突然跳到高值——看起来像涌现。但用部分匹配度量，性能增长是完全平滑的。</span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a>**证据3：统计理论分析**。他们从理论上证明：对于非线性度量函数 $f$（如EM的阶跃函数），即使底层的概率 $p(N)$ 是 $N$ 的连续函数，$f(p(N))$ 也可能呈现出阶跃状行为。涌现本质上可能是度量函数的非线性与有限样本效应的共同作用。</span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a><span class="fu">### 围绕涌现的持续争论</span></span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a>Schaeffer et al. 的工作并没有完全终结涌现争论。支持涌现真实性的研究者提出了几个反驳。</span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a>第一，**并非所有涌现都可以通过更换度量来消除**。在某些任务上（如BIG-Bench中的部分推理任务），即使用连续度量，性能曲线仍然呈现出非线性的加速增长，虽然不是完美的阶跃，但也远非线性增长。</span>
<span id="cb14-763"><a href="#cb14-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-764"><a href="#cb14-764" aria-hidden="true" tabindex="-1"></a>第二，**度量的选择本身是有意义的**。在实际应用中，我们关心的往往就是"模型能否给出完全正确的答案"，而不是"模型有多接近正确答案"。如果一个数学模型的每一步都有99%的概率正确，但一个10步问题的端到端正确率只有 $0.99^{10} \approx 90\%$，而一个20步问题只有 $0.99^{20} \approx 82\%$——从用户视角看，模型的"可靠性"确实呈现出非线性的下降。在这个意义上，涌现可能不是"假象"，而是"复合效应"的合理体现。</span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a>第三，**CoT的规模依赖性不容易用度量论解释**。CoT的涌现不仅是"从不正确到正确"，而是"从无法生成连贯推理链到能够生成连贯推理链"——这是一种质的变化，不只是精度的量变。小模型生成的"推理链"通常是语无伦次的，而大模型生成的推理链在逻辑上是连贯的，即使最终答案偶尔出错。</span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a>**目前的共识**（如果有的话）大致是：涌现能力的观测是真实的（在特定度量下，性能确实呈现出非线性跳跃），但其底层机制可能是平滑的（模型的"原始能力"在连续增长），非线性主要来自度量选择和任务的组合性。这并不意味着涌现"不重要"——它仍然意味着我们很难从小模型的表现外推大模型的行为，这对AI安全和能力预测都有重大意义。</span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a><span class="fu">### LLM真的在"推理"吗？</span></span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a>CoT的成功引发了一个更根本的哲学和科学问题：当大语言模型生成了一个看起来合理的推理链并得出正确答案时，它真的在"推理"吗？</span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-774"><a href="#cb14-774" aria-hidden="true" tabindex="-1"></a>**支持"推理"的证据**：</span>
<span id="cb14-775"><a href="#cb14-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a>首先，CoT生成的推理链在很多情况下是**正确且有逻辑的**——不仅最终答案对了，中间步骤也是对的。如果模型只是在做"高级模式匹配"，为什么它能在从未见过的新问题上产生正确的多步推理？</span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a>其次，CoT的**泛化能力**表明模型学到了某种"规则"而非仅仅记忆。Wei et al. 在OOD（out-of-distribution）设置中——测试问题比训练示例更长——发现CoT仍然有效。如果模型只是在复制训练分布中的模式，OOD性能应该大幅下降。</span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a>第三，Self-Consistency的有效性（不同推理路径指向同一答案）暗示模型能够从多个角度"验证"一个结论，这是推理的一个重要特征。</span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a>**反对"推理"的证据**：</span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a>然而，也有大量证据表明LLM的"推理"与人类推理有本质区别。</span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-786"><a href="#cb14-786" aria-hidden="true" tabindex="-1"></a>最关键的反面证据是**CoT的不忠实性**（unfaithfulness）。多项研究（如Turpin et al., 2024; Lanham et al., 2023）发现，模型生成的推理链可能**不是它真正使用的推理过程**。在某些实验中，如果在prompt中加入一个错误的"提示"（如"Professor Smith说答案是X"），模型会改变自己的推理过程来"论证"X是对的——推理链变成了事后合理化（post-hoc rationalization），而非真正的逐步推导。</span>
<span id="cb14-787"><a href="#cb14-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a>另一个关键发现是：LLM在推理中对**问题的表述方式**极度敏感。同一个逻辑问题，如果用不同的词语描述，或者改变无关的上下文信息，模型的答案可能截然不同。人类的逻辑推理通常对问题的表面描述是鲁棒的（至少在受过训练后），但LLM的"推理"似乎更多地依赖于文本的表面模式而非深层逻辑结构。</span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a>此外，LLM在需要**系统性规则遵循**的任务上（如棋盘游戏、精确计算大数、逻辑谜题的所有推理步骤都必须正确）表现仍然不可靠，这暗示它们缺乏人类式的"规则应用"机制。</span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a>**一个可能的折衷立场**是：LLM所做的既不是人类意义上的"推理"，也不是纯粹的"模式匹配"，而是一种介于两者之间的新型信息处理。它能够利用预训练中学到的统计规律来近似推理步骤，在很多情况下这种近似足够准确，但它缺乏人类推理的系统性和可靠性。</span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a>如果你要在这个方向写一篇论文，以下几个问题仍然值得深入探索。</span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a>第一个方向是**推理的忠实性**。如何确保模型生成的推理链确实是它得出答案的原因，而非事后合理化？如何设计实验来区分"真正的推理"和"可信的推理模拟"？这对AI可信度和安全性有直接影响。</span>
<span id="cb14-799"><a href="#cb14-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-800"><a href="#cb14-800" aria-hidden="true" tabindex="-1"></a>第二个方向是**推理能力的可扩展性**。CoT在小学数学上效果很好，但在更复杂的推理任务（竞赛数学、定理证明、多步策略规划）上表现有限。推理能力是否有理论上限？通过增加模型规模或改进推理策略，能否不断提升？</span>
<span id="cb14-801"><a href="#cb14-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-802"><a href="#cb14-802" aria-hidden="true" tabindex="-1"></a>第三个方向是**涌现的预测**。如果涌现是真实的（或至少在某种度量下是真实的），我们能否预测下一代模型会涌现出什么新能力？这对AI安全和政策制定至关重要——如果我们训练了一个10倍于GPT-4的模型，它会突然获得什么我们没预料到的能力？</span>
<span id="cb14-803"><a href="#cb14-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-804"><a href="#cb14-804" aria-hidden="true" tabindex="-1"></a>第四个方向是**推理时间计算**（test-time compute）。CoT、Self-Consistency、ToT都是在推理阶段投入更多计算来提升性能的策略。推理时间计算与训练时间计算之间是否存在可预测的换算关系？能否为推理时间计算建立类似Scaling Laws的定量理论？</span>
<span id="cb14-805"><a href="#cb14-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-806"><a href="#cb14-806" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-807"><a href="#cb14-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-808"><a href="#cb14-808" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb14-809"><a href="#cb14-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-810"><a href="#cb14-810" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本方法的局限</span></span>
<span id="cb14-811"><a href="#cb14-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-812"><a href="#cb14-812" aria-hidden="true" tabindex="-1"></a>Chain-of-Thought及其变体虽然在推理任务上取得了突破性进展，但它们面临着几个根本性的局限。</span>
<span id="cb14-813"><a href="#cb14-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-814"><a href="#cb14-814" aria-hidden="true" tabindex="-1"></a>第一个局限是**小模型无法受益**。CoT只在≥100B参数的模型上才显著有效。对于资源受限的场景（如在手机上运行的模型），CoT带来的提升非常有限。虽然后续的工作（如知识蒸馏、CoT微调）试图将大模型的推理能力迁移到小模型，但效果仍有差距。</span>
<span id="cb14-815"><a href="#cb14-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-816"><a href="#cb14-816" aria-hidden="true" tabindex="-1"></a>第二个局限是**推理链的不忠实性**。如前所述，模型生成的推理链可能不是它真正的"思考过程"。这意味着我们不能简单地通过检查推理链来验证模型的推理是否正确——一个看起来完美的推理链可能对应一个错误的内部过程。</span>
<span id="cb14-817"><a href="#cb14-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-818"><a href="#cb14-818" aria-hidden="true" tabindex="-1"></a>第三个局限是**计算成本**。CoT增加了输出长度（从几个token到几百个token），Self-Consistency将成本乘以采样次数（通常10-40倍），ToT更是需要数十次LLM调用。在大规模应用中，这些成本可能是不可接受的。</span>
<span id="cb14-819"><a href="#cb14-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-820"><a href="#cb14-820" aria-hidden="true" tabindex="-1"></a>第四个局限是**对prompt的敏感性**。CoT示例的质量、数量、措辞都显著影响最终效果。手工编写高质量的CoT示例需要领域知识，而自动生成的推理链质量参差不齐。</span>
<span id="cb14-821"><a href="#cb14-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-822"><a href="#cb14-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb14-823"><a href="#cb14-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-824"><a href="#cb14-824" aria-hidden="true" tabindex="-1"></a>CoT和涌现能力的讨论自然引出了一个更宏观的问题：**我们如何可靠地评估和衡量大语言模型的能力？**</span>
<span id="cb14-825"><a href="#cb14-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-826"><a href="#cb14-826" aria-hidden="true" tabindex="-1"></a>Schaeffer et al. 的工作已经表明，度量的选择可以根本性地改变我们对模型能力的判断。如果连"涌现"这个核心概念的真实性都取决于度量方式，那么我们对LLM能力的所有评估都需要更加审慎。</span>
<span id="cb14-827"><a href="#cb14-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-828"><a href="#cb14-828" aria-hidden="true" tabindex="-1"></a>这正是下一章——**评测方法论**——将要系统讨论的问题。如何设计不被"刷分"的benchmark？如何区分"真正的能力提升"和"度量假象"？如何在模型能力快速迭代的时代建立可靠的评测体系？这些看似"元问题"（关于方法的方法、关于评估的评估），实际上是确保AI研究科学性的基石。</span>
<span id="cb14-829"><a href="#cb14-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-830"><a href="#cb14-830" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第22章将聚焦**评测方法论**。从GLUE到SuperGLUE到MMLU到LLM-as-Judge，评测基准经历了怎样的演进？静态benchmark的数据泄漏问题如何解决？LLM-as-Judge这种"用模型评模型"的方法有什么偏差？如何在"涌现"和"假象"之间建立可靠的科学判断？</span></span>
<span id="cb14-831"><a href="#cb14-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-832"><a href="#cb14-832" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-833"><a href="#cb14-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-834"><a href="#cb14-834" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb14-835"><a href="#cb14-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-836"><a href="#cb14-836" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb14-837"><a href="#cb14-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-838"><a href="#cb14-838" aria-hidden="true" tabindex="-1"></a>本章围绕两个相互交织的主题——**思维链推理**和**涌现能力**——讲述了大语言模型在推理能力上的突破与争论。</span>
<span id="cb14-839"><a href="#cb14-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-840"><a href="#cb14-840" aria-hidden="true" tabindex="-1"></a>**Chain-of-Thought Prompting**（Wei et al., 2022）通过一个极其简单的改动——在few-shot示例中展示推理过程——让大语言模型在数学推理上获得了飞跃式提升（GSM8K：17.9% → 56.9%）。CoT的核心洞察是：通过引导模型生成中间推理步骤，将一个困难的端到端映射分解成多个简单的步骤，同时利用模型自己的生成文本作为"工作记忆"。</span>
<span id="cb14-841"><a href="#cb14-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-842"><a href="#cb14-842" aria-hidden="true" tabindex="-1"></a>**Zero-shot CoT**（Kojima et al., 2022）发现仅在prompt末尾添加"Let's think step by step"就能触发推理模式，在MultiArith上将准确率从17.7%提升到78.7%，证明了推理能力已经"内嵌"在大模型中，只需要正确的触发方式。</span>
<span id="cb14-843"><a href="#cb14-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-844"><a href="#cb14-844" aria-hidden="true" tabindex="-1"></a>**Self-Consistency**（Wang et al., 2022）通过采样多条推理路径并进行多数投票，进一步将CoT的GSM8K准确率从56.9%提升到74.4%，利用了"正确答案可以通过多种路径到达"的统计规律。</span>
<span id="cb14-845"><a href="#cb14-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-846"><a href="#cb14-846" aria-hidden="true" tabindex="-1"></a>**Tree of Thoughts**（Yao et al., 2023）将推理从线性链扩展到树状搜索，在24点游戏等需要探索和回溯的任务上，从7.3%（标准ICL）提升到74.0%，展示了结构化推理策略的潜力。</span>
<span id="cb14-847"><a href="#cb14-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-848"><a href="#cb14-848" aria-hidden="true" tabindex="-1"></a>**涌现能力**（Wei et al., 2022b）描述了一种令人不安的现象：某些能力在小模型中完全不存在，但在模型规模超过阈值后突然出现。Schaeffer et al.（2023）的反驳表明，涌现可能部分是度量方式制造的假象——在非线性度量（如精确匹配）下呈现的"阶跃"，在连续度量下可能是平滑增长。这场争论尚未完全解决，但它深刻地提醒我们：**评估方式本身可能扭曲我们对模型能力的认知**。</span>
<span id="cb14-849"><a href="#cb14-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-850"><a href="#cb14-850" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键对比速查</span></span>
<span id="cb14-851"><a href="#cb14-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-852"><a href="#cb14-852" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 推理结构 <span class="pp">|</span> 关键改进 <span class="pp">|</span> GSM8K (PaLM 540B) <span class="pp">|</span></span>
<span id="cb14-853"><a href="#cb14-853" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|---------|-------------------|</span></span>
<span id="cb14-854"><a href="#cb14-854" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 标准 ICL <span class="pp">|</span> 无中间步骤 <span class="pp">|</span> — <span class="pp">|</span> ~18% <span class="pp">|</span></span>
<span id="cb14-855"><a href="#cb14-855" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CoT (few-shot) <span class="pp">|</span> 线性链 × 1 <span class="pp">|</span> 展示推理过程 <span class="pp">|</span> 56.9% <span class="pp">|</span></span>
<span id="cb14-856"><a href="#cb14-856" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Zero-shot CoT <span class="pp">|</span> 线性链 × 1 <span class="pp">|</span> "Let's think step by step" <span class="pp">|</span> ~47%* <span class="pp">|</span></span>
<span id="cb14-857"><a href="#cb14-857" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Self-Consistency <span class="pp">|</span> 线性链 × N <span class="pp">|</span> 多数投票 <span class="pp">|</span> 74.4% <span class="pp">|</span></span>
<span id="cb14-858"><a href="#cb14-858" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Tree of Thoughts <span class="pp">|</span> 树状搜索 <span class="pp">|</span> LLM评估 + 回溯 <span class="pp">|</span> N/A（不同任务） <span class="pp">|</span></span>
<span id="cb14-859"><a href="#cb14-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-860"><a href="#cb14-860" aria-hidden="true" tabindex="-1"></a>*Zero-shot CoT在GSM8K上的具体数字因模型版本而异，此处为近似值。</span>
<span id="cb14-861"><a href="#cb14-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-862"><a href="#cb14-862" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb14-863"><a href="#cb14-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-864"><a href="#cb14-864" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么Chain-of-Thought只在大模型（≥100B参数）上有效，而小模型几乎不受益？试从"模型需要同时具备多种子能力"的角度给出解释，并讨论这是否与Schaeffer et al.关于涌现是"度量假象"的论点矛盾。</span>
<span id="cb14-865"><a href="#cb14-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-866"><a href="#cb14-866" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** Self-Consistency使用多数投票来选择最终答案。假设模型对每条推理路径独立地以概率 $p$ 给出正确答案（$p &gt; 0.5$），共采样 $N$ 条路径。(a) 推导多数投票给出正确答案的概率表达式（提示：二项分布）。(b) 当 $p = 0.6$，$N = 10$ 时，多数投票的正确率是多少？与单条路径（$p = 0.6$）相比提升了多少？(c) 当 $p &lt; 0.5$ 时会发生什么？</span>
<span id="cb14-867"><a href="#cb14-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-868"><a href="#cb14-868" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用任一LLM API，在以下三种设置下解决10道数学推理题（可使用GSM8K样本）：(a) 标准few-shot（只给答案），(b) CoT few-shot（给推理过程），(c) Self-Consistency（CoT + 10次采样 + 多数投票）。比较三种方法的准确率和API成本。</span>
<span id="cb14-869"><a href="#cb14-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-870"><a href="#cb14-870" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** Schaeffer et al. (2023) 论证涌现可能是度量方式的假象。但考虑Chain-of-Thought的规模依赖性：小模型生成的"推理链"通常是语无伦次的，而大模型生成的推理链是连贯的。这种**质的变化**能否被"度量假象"论完全解释？设计一个实验来检验你的假设。</span>
<span id="cb14-871"><a href="#cb14-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-872"><a href="#cb14-872" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放思考]** 如果LLM的"推理"确实只是"高级模式匹配"（而非人类意义上的逻辑推理），这对AI的实际应用意味着什么？在哪些应用场景中"可靠的模式匹配"就够了？在哪些场景中我们需要"真正的推理"？如何判断一个任务属于哪类？</span>
<span id="cb14-873"><a href="#cb14-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-874"><a href="#cb14-874" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-875"><a href="#cb14-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-876"><a href="#cb14-876" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb14-877"><a href="#cb14-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-878"><a href="#cb14-878" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb14-879"><a href="#cb14-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-880"><a href="#cb14-880" aria-hidden="true" tabindex="-1"></a>**Wei, J. et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"**。CoT的原始论文，本章最核心的参考。重点阅读：Section 2（方法定义，只有半页但定义了一个时代）、Section 3（实验结果）。可快速浏览：Appendix中的完整推理链示例。<span class="co">[</span><span class="ot">arXiv:2201.11903</span><span class="co">](https://arxiv.org/abs/2201.11903)</span></span>
<span id="cb14-881"><a href="#cb14-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-882"><a href="#cb14-882" aria-hidden="true" tabindex="-1"></a>**Wei, J. et al. (2022). "Emergent Abilities of Large Language Models"**。涌现能力的系统性研究，引发了整个领域关于"规模vs能力"的讨论。重点阅读：Section 2（涌现的定义）、Figure 1-2（经典的涌现曲线）。<span class="co">[</span><span class="ot">arXiv:2206.07682</span><span class="co">](https://arxiv.org/abs/2206.07682)</span></span>
<span id="cb14-883"><a href="#cb14-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-884"><a href="#cb14-884" aria-hidden="true" tabindex="-1"></a>**Schaeffer, R. et al. (2023). "Are Emergent Abilities of Large Language Models a Mirage?"**。NeurIPS 2023最佳论文，对涌现叙事的强有力挑战。重点阅读：Section 2-3（度量论证）。这是一篇"改变你思考方式"的论文。<span class="co">[</span><span class="ot">arXiv:2304.15004</span><span class="co">](https://arxiv.org/abs/2304.15004)</span></span>
<span id="cb14-885"><a href="#cb14-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-886"><a href="#cb14-886" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法改进</span></span>
<span id="cb14-887"><a href="#cb14-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-888"><a href="#cb14-888" aria-hidden="true" tabindex="-1"></a>**Kojima, S. et al. (2022). "Large Language Models are Zero-Shot Reasoners"**。发现"Let's think step by step"的神奇效果。重点阅读：Section 3（方法）、Table 4（不同触发短语的对比）。<span class="co">[</span><span class="ot">arXiv:2205.11916</span><span class="co">](https://arxiv.org/abs/2205.11916)</span></span>
<span id="cb14-889"><a href="#cb14-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-890"><a href="#cb14-890" aria-hidden="true" tabindex="-1"></a>**Wang, X. et al. (2022). "Self-Consistency Improves Chain of Thought Reasoning"**。Self-Consistency方法。重点阅读：Section 2（方法，优雅简洁）。<span class="co">[</span><span class="ot">arXiv:2203.11171</span><span class="co">](https://arxiv.org/abs/2203.11171)</span></span>
<span id="cb14-891"><a href="#cb14-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-892"><a href="#cb14-892" aria-hidden="true" tabindex="-1"></a>**Yao, S. et al. (2023). "Tree of Thoughts"**。将推理从线性扩展到树状搜索。重点阅读：Section 2（框架定义）、Section 4（24点游戏实验）。<span class="co">[</span><span class="ot">arXiv:2305.10601</span><span class="co">](https://arxiv.org/abs/2305.10601)</span></span>
<span id="cb14-893"><a href="#cb14-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-894"><a href="#cb14-894" aria-hidden="true" tabindex="-1"></a>**Zhou, D. et al. (2022). "Least-to-Most Prompting Enables Complex Reasoning"**。一种将复杂问题分解为子问题、从简单到复杂逐步求解的策略。重点阅读：Section 2-3。<span class="co">[</span><span class="ot">arXiv:2205.10625</span><span class="co">](https://arxiv.org/abs/2205.10625)</span></span>
<span id="cb14-895"><a href="#cb14-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-896"><a href="#cb14-896" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论与分析</span></span>
<span id="cb14-897"><a href="#cb14-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-898"><a href="#cb14-898" aria-hidden="true" tabindex="-1"></a>**Turpin, M. et al. (2024). "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting"**。揭示CoT推理链可能不忠实于模型的真实"推理过程"。<span class="co">[</span><span class="ot">arXiv:2305.04388</span><span class="co">](https://arxiv.org/abs/2305.04388)</span></span>
<span id="cb14-899"><a href="#cb14-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-900"><a href="#cb14-900" aria-hidden="true" tabindex="-1"></a>**Suzgun, M. et al. (2022). "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"**。BIG-Bench Hard（BBH）：精选的23个CoT对其有显著帮助的困难任务。<span class="co">[</span><span class="ot">arXiv:2210.09261</span><span class="co">](https://arxiv.org/abs/2210.09261)</span></span>
<span id="cb14-901"><a href="#cb14-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-902"><a href="#cb14-902" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb14-903"><a href="#cb14-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-904"><a href="#cb14-904" aria-hidden="true" tabindex="-1"></a>**Qiao, S. et al. (2023). "Reasoning with Language Model Prompting: A Survey"**。推理提示方法的全面综述。<span class="co">[</span><span class="ot">arXiv:2212.09597</span><span class="co">](https://arxiv.org/abs/2212.09597)</span></span>
<span id="cb14-905"><a href="#cb14-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-906"><a href="#cb14-906" aria-hidden="true" tabindex="-1"></a>**Huang, J. &amp; Chang, K. (2023). "Towards Reasoning in Large Language Models: A Survey"**。LLM推理能力的全面综述。<span class="co">[</span><span class="ot">arXiv:2212.10403</span><span class="co">](https://arxiv.org/abs/2212.10403)</span></span>
<span id="cb14-907"><a href="#cb14-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-908"><a href="#cb14-908" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb14-909"><a href="#cb14-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-910"><a href="#cb14-910" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LangChain**：<span class="co">[</span><span class="ot">langchain.com</span><span class="co">](https://www.langchain.com/)</span>（内置CoT和推理链工具）</span>
<span id="cb14-911"><a href="#cb14-911" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tree of Thoughts实现**：<span class="co">[</span><span class="ot">github.com/princeton-nlp/tree-of-thought-llm</span><span class="co">](https://github.com/princeton-nlp/tree-of-thought-llm)</span></span>
<span id="cb14-912"><a href="#cb14-912" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GSM8K数据集**：<span class="co">[</span><span class="ot">github.com/openai/grade-school-math</span><span class="co">](https://github.com/openai/grade-school-math)</span></span>
<span id="cb14-913"><a href="#cb14-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-914"><a href="#cb14-914" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-915"><a href="#cb14-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-916"><a href="#cb14-916" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb14-917"><a href="#cb14-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-918"><a href="#cb14-918" aria-hidden="true" tabindex="-1"></a>Chain-of-Thought prompting的故事充满了戏剧性的巧合和意外发现。</span>
<span id="cb14-919"><a href="#cb14-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-920"><a href="#cb14-920" aria-hidden="true" tabindex="-1"></a>2021年底，Google Brain的Jason Wei在研究PaLM模型的few-shot能力时，偶然发现了一个有趣的现象：如果在few-shot示例中加入简单的解题步骤，模型的数学推理能力会大幅提升。这个发现最初并没有被当作一个重要的研究贡献——毕竟，它的方法实在太简单了，简单到让人怀疑"这能发论文吗？"Wei后来在采访中回忆，论文提交过程中遇到了不少质疑："你就是换了个prompt格式，这算什么贡献？"</span>
<span id="cb14-921"><a href="#cb14-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-922"><a href="#cb14-922" aria-hidden="true" tabindex="-1"></a>但Wei坚持认为这个发现的意义不在于方法的复杂度，而在于它揭示的现象：大语言模型具有通过中间步骤进行推理的潜力，只是需要正确的"激活方式"。事后看来，这篇论文确实改变了整个领域对LLM推理能力的认知，被引用超过5000次。</span>
<span id="cb14-923"><a href="#cb14-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-924"><a href="#cb14-924" aria-hidden="true" tabindex="-1"></a>Kojima et al. 的Zero-shot CoT故事同样引人入胜。"Let's think step by step"这六个英文单词成为了AI历史上最著名的"咒语"之一。Kojima等人测试了超过50个不同的触发短语，发现这句话的效果最好——但他们自己也无法完全解释为什么。这六个词的发现几乎可以说是"实验驱动"而非"理论驱动"的——先发现了现象，然后才开始思考解释。</span>
<span id="cb14-925"><a href="#cb14-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-926"><a href="#cb14-926" aria-hidden="true" tabindex="-1"></a>涌现能力的讨论则触及了AI研究中一个更深层的张力：乐观主义与怀疑主义。Wei et al.的涌现论文被乐观主义者视为"规模就是通往AGI的道路"的证据，被怀疑主义者视为"没有控制好度量的可疑发现"。Schaeffer et al.的反驳论文在NeurIPS 2023获得最佳论文奖，这本身就是一个信号：学术界认为对热门叙事的严谨质疑同样值得褒奖。</span>
<span id="cb14-927"><a href="#cb14-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-928"><a href="#cb14-928" aria-hidden="true" tabindex="-1"></a>一个历史的讽刺是：2022年是CoT和涌现论文发表的年份，也是ChatGPT发布的年份（2022年11月30日）。ChatGPT的成功很大程度上依赖于指令微调和RLHF，而不是CoT——但正是CoT等研究揭示的LLM推理潜力，为ChatGPT的应用场景（编程、分析、解题）提供了信心基础。从某种意义上说，CoT为LLM从"有趣的研究工具"变成"改变世界的产品"铺平了认知上的道路。</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>