<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="上一章关于涌现能力的讨论揭示了一个令人不安的事实：度量方式本身可能扭曲我们对模型能力的认知。如果连’涌现’这个核心概念的真实性都取决于评测度量的选择，那么我们对LLM能力的所有判断都需要更加审慎。本章系统讲述NLP评测方法论的演进：从BLEU/ROUGE等自动指标的局限，到GLUE/SuperGLUE/MMLU等静态benchmark的兴衰，再到LLM-as-Judge和Chatbot Arena等生成式评测的新范式。我们将讨论数据污染、Goodhart定律、评测者偏差等核心问题，试图回答一个根本性的问题：什么才是好的评测？">

<title>第22章：评测方法论——如何判断模型变好了？ – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#为什么评测如此困难" id="toc-为什么评测如此困难" class="nav-link" data-scroll-target="#为什么评测如此困难"><span class="header-section-number">2.1</span> 为什么评测如此困难？</a></li>
  <li><a href="#goodhart-定律评测领域的永恒诅咒" id="toc-goodhart-定律评测领域的永恒诅咒" class="nav-link" data-scroll-target="#goodhart-定律评测领域的永恒诅咒"><span class="header-section-number">2.2</span> Goodhart 定律：评测领域的永恒诅咒</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#评测方法论的三次范式转变" id="toc-评测方法论的三次范式转变" class="nav-link" data-scroll-target="#评测方法论的三次范式转变"><span class="header-section-number">3.1</span> 评测方法论的三次范式转变</a></li>
  <li><a href="#一个类比评测就像考试制度" id="toc-一个类比评测就像考试制度" class="nav-link" data-scroll-target="#一个类比评测就像考试制度"><span class="header-section-number">3.2</span> 一个类比：评测就像考试制度</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#第一代自动指标" id="toc-第一代自动指标" class="nav-link" data-scroll-target="#第一代自动指标"><span class="header-section-number">4.1</span> 第一代：自动指标</a></li>
  <li><a href="#第二代理解能力-benchmark" id="toc-第二代理解能力-benchmark" class="nav-link" data-scroll-target="#第二代理解能力-benchmark"><span class="header-section-number">4.2</span> 第二代：理解能力 benchmark</a></li>
  <li><a href="#第三代生成式评测的新范式" id="toc-第三代生成式评测的新范式" class="nav-link" data-scroll-target="#第三代生成式评测的新范式"><span class="header-section-number">4.3</span> 第三代：生成式评测的新范式</a></li>
  <li><a href="#数据污染与科学诚实" id="toc-数据污染与科学诚实" class="nav-link" data-scroll-target="#数据污染与科学诚实"><span class="header-section-number">4.4</span> 数据污染与科学诚实</a></li>
  <li><a href="#可靠性评测" id="toc-可靠性评测" class="nav-link" data-scroll-target="#可靠性评测"><span class="header-section-number">4.5</span> 可靠性评测</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#搭建评测-pipeline" id="toc-搭建评测-pipeline" class="nav-link" data-scroll-target="#搭建评测-pipeline"><span class="header-section-number">5.1</span> 搭建评测 Pipeline</a></li>
  <li><a href="#使用-lm-evaluation-harness" id="toc-使用-lm-evaluation-harness" class="nav-link" data-scroll-target="#使用-lm-evaluation-harness"><span class="header-section-number">5.2</span> 使用 lm-evaluation-harness</a></li>
  <li><a href="#实现-llm-as-judge" id="toc-实现-llm-as-judge" class="nav-link" data-scroll-target="#实现-llm-as-judge"><span class="header-section-number">5.3</span> 实现 LLM-as-Judge</a></li>
  <li><a href="#数据污染快速检测" id="toc-数据污染快速检测" class="nav-link" data-scroll-target="#数据污染快速检测"><span class="header-section-number">5.4</span> 数据污染快速检测</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么评测科学这么重要理论视角" id="toc-为什么评测科学这么重要理论视角" class="nav-link" data-scroll-target="#为什么评测科学这么重要理论视角"><span class="header-section-number">6.1</span> 为什么评测科学这么重要？——理论视角</a></li>
  <li><a href="#评测的隐含假设" id="toc-评测的隐含假设" class="nav-link" data-scroll-target="#评测的隐含假设"><span class="header-section-number">6.2</span> 评测的隐含假设</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.4</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#当前评测方法论的核心局限" id="toc-当前评测方法论的核心局限" class="nav-link" data-scroll-target="#当前评测方法论的核心局限"><span class="header-section-number">7.1</span> 当前评测方法论的核心局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键对比速查" id="toc-关键对比速查" class="nav-link" data-scroll-target="#关键对比速查"><span class="header-section-number">8.2</span> 关键对比速查</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.3</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.4</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#经典指标" id="toc-经典指标" class="nav-link" data-scroll-target="#经典指标"><span class="header-section-number">9.2</span> 经典指标</a></li>
  <li><a href="#数据污染与科学诚实-1" id="toc-数据污染与科学诚实-1" class="nav-link" data-scroll-target="#数据污染与科学诚实-1"><span class="header-section-number">9.3</span> 数据污染与科学诚实</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.4</span> 后续发展</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">9.5</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.6</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第22章：评测方法论——如何判断模型变好了？</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Evaluation Methodology: Benchmarks, Metrics, and the Science of Measuring AI</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">Evaluation</div>
    <div class="quarto-category">Benchmarks</div>
    <div class="quarto-category">Metrics</div>
  </div>
  </div>

<div>
  <div class="description">
    上一章关于涌现能力的讨论揭示了一个令人不安的事实：度量方式本身可能扭曲我们对模型能力的认知。如果连’涌现’这个核心概念的真实性都取决于评测度量的选择，那么我们对LLM能力的所有判断都需要更加审慎。本章系统讲述NLP评测方法论的演进：从BLEU/ROUGE等自动指标的局限，到GLUE/SuperGLUE/MMLU等静态benchmark的兴衰，再到LLM-as-Judge和Chatbot Arena等生成式评测的新范式。我们将讨论数据污染、Goodhart定律、评测者偏差等核心问题，试图回答一个根本性的问题：什么才是好的评测？
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：Benchmark 越刷越高，模型真的越来越强吗？我们如何设计不被”博弈”的评测体系？</p>
<p><strong>历史坐标</strong>：2018–2024 | GLUE → SuperGLUE → MMLU → HELM → Chatbot Arena | 从静态 benchmark 到动态人类偏好评测</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Wang et al.&nbsp;(2018)</strong> “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding” (arXiv:1804.07461) — 参考了 Section 2-3（任务设计与基线）、Table 1（9个任务定义）</li>
<li><strong>Wang et al.&nbsp;(2019)</strong> “SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems” (arXiv:1905.00537) — 参考了 Section 2（任务选择标准）、Table 1（8个任务）</li>
<li><strong>Hendrycks et al.&nbsp;(2020)</strong> “Measuring Massive Multitask Language Understanding” (arXiv:2009.03300) — 参考了 Section 2-3（57学科设计）、Figure 1（学科分布）</li>
<li><strong>Liang et al.&nbsp;(2022)</strong> “Holistic Evaluation of Language Models” (arXiv:2211.09110) — 参考了 Section 2（评测分类学：7个场景×7个度量维度）、Figure 1（HELM框架图）</li>
<li><strong>Srivastava et al.&nbsp;(2022)</strong> “Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models” (arXiv:2206.04615) — 参考了 Section 2（BIG-bench 200+任务设计）</li>
<li><strong>Zheng et al.&nbsp;(2023)</strong> “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena” (arXiv:2306.05685) — 参考了 Section 2-4（MT-Bench设计、Chatbot Arena Elo系统、LLM-as-Judge偏差分析）、Figure 1（评测框架图）；提取了论文原图</li>
<li><strong>Lin et al.&nbsp;(2021)</strong> “TruthfulQA: Measuring How Models Mimic Human Falsehoods” (arXiv:2109.07958) — 参考了 Section 2-3（事实性评测设计）</li>
<li><strong>Schaeffer et al.&nbsp;(2023)</strong> “Are Emergent Abilities of Large Language Models a Mirage?” (arXiv:2304.15004) — 参考了 Section 2-3（度量选择对涌现判断的影响），延续Ch21讨论；提取了 Figure 2（涌现消失的多面板对比图）</li>
<li><strong>Papineni et al.&nbsp;(2002)</strong> “BLEU: a Method for Automatic Evaluation of Machine Translation” (ACL 2002) — 参考了 Section 2（BLEU公式定义）</li>
<li><strong>Lin (2004)</strong> “ROUGE: A Package for Automatic Evaluation of Summaries” (ACL 2004 Workshop) — 参考了 ROUGE-N/L 定义</li>
<li><strong>Oren et al.&nbsp;(2023)</strong> “Proving Test Set Contamination in Black Box Language Models” (arXiv:2310.17623) — 参考了分区对比数据污染检测方法</li>
<li><strong>Deng et al.&nbsp;(2023)</strong> “Investigating Data Contamination in Modern Benchmarks for Large Language Models” (arXiv:2311.09783) — 参考了 TS-Guessing 方法（通过部分输入检测数据污染）</li>
<li><strong>Golchin &amp; Surdeanu (2023)</strong> “Time Travel in LLMs: Tracing Data Contamination in Large Language Models” (arXiv:2308.08493) — 参考了 Time Travel 方法（通过逐字复现检测数据污染）</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li>SLP3 Chapter 4 (Logistic Regression and Text Classification, Jan 2026 draft) — 参考了 precision/recall/F1/confusion matrix 的经典教学（Fig. 4.4）</li>
<li>SLP3 Chapter 7 (Large Language Models) — 参考了 MMLU benchmark 的教学呈现（Fig. 7.16：2-shot prompting 示例）、Rawlsian 公平性评测</li>
<li>SLP3 Chapter 12 (Machine Translation) — 参考了 BLEU、chrF、BERTScore、COMET/BLEURT 的完整评测指标演进，以及 paired bootstrap 统计显著性检验</li>
<li>D2L Section 10.7 (Sequence-to-Sequence Learning) — 参考了 BLEU 的数值演算示例（p₁=4/5, p₂=3/4, p₃=1/3）和可运行 Python 实现</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li>Stanford CS224N Lecture 11 (Spring 2024, Yann Dubois guest lecture) — 专题 “Benchmarking and Evaluation”，系统讲解了从 GLUE/MMLU 到 LLM-as-Judge 的评测演进、MMLU 错误率问题、评测不一致性（如 Gemini 非标准 MMLU prompting）</li>
<li>CMU 11-711 ANLP Lecture 17 (Fall 2024, Graham Neubig) — “Evaluation and Multimodal”，覆盖 GLUE、MMLU、HELM、Chatbot Arena、MT-Bench 等评测体系</li>
<li>Princeton COS 597R Lecture 5 (Fall 2024, Sanjeev Arora) — “Emergent Behavior”，讨论 Schaeffer et al.&nbsp;的涌现幻觉论文及评测度量选择问题</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们讲述了 Chain-of-Thought 推理和涌现能力的故事。CoT 通过在 few-shot 示例中展示推理过程，让大语言模型在数学推理上获得了飞跃式提升，而这种能力呈现出令人惊叹的”涌现”特征——小模型完全不受益，大模型突然爆发。</p>
<p>然而，上一章结尾埋下的伏笔令人不安。Schaeffer et al.（2023）的 NeurIPS 最佳论文表明，所谓的”涌现”可能很大程度上是<strong>度量方式制造的假象</strong>：同一份实验数据，使用精确匹配（Exact Match）度量时呈现出震撼的”阶跃”曲线，换成连续度量后却变成了平滑增长。一个被整个领域热烈讨论的”相变现象”，居然可能只是我们选择了错误的温度计。</p>
<div id="fig-emergence-mirage" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emergence-mirage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-22/original/fig4-emergence-mirage-schaeffer2023.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emergence-mirage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Schaeffer et al.&nbsp;(2023) 的核心发现：同一组模型在同一批任务上的表现，左侧使用非线性度量（如 Exact Match）时呈现”涌现”式阶跃，右侧换成连续度量（如 Token Edit Distance、Brier Score）后变为平滑增长。“涌现”消失了。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Schaeffer, R., Miranda, B., &amp; Koyejo, S. (2023) “Are Emergent Abilities of Large Language Models a Mirage?”, Figure 2. NeurIPS 2023 Outstanding Paper Award. <a href="https://arxiv.org/abs/2304.15004">arXiv:2304.15004</a></em></p>
</div>
<p>这个发现的冲击远超涌现本身。它触及了一个更深层的问题：<strong>如果评测方式可以根本性地改变我们对模型能力的判断，那么我们对 LLM 的所有评估——从 GLUE 排行榜到 ChatGPT “超越人类”的宣传——有多少是真实的，有多少是度量假象？</strong></p>
<p>这个问题不是学术象牙塔里的自娱自乐。当企业根据 benchmark 分数决定部署哪个模型，当政府根据评测结果制定 AI 监管政策，当研究者根据排行榜排名规划研究方向——评测的科学性直接关系到数十亿美元的决策和整个领域的研究方向。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：评测不是”选个 benchmark 跑个分”那么简单。它是一门方法论，受 Goodhart 定律笼罩——“当一个度量成为目标时，它就不再是好的度量”。从 BLEU 到 GLUE 到 Chatbot Arena，NLP 评测的每一次演进都是对前一代评测失效模式的回应。理解这段演进史，是成为严谨研究者的必修课。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="为什么评测如此困难" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="为什么评测如此困难"><span class="header-section-number">2.1</span> 为什么评测如此困难？</h3>
<p>评测看似是一个简单的工程问题：设计一组测试题，让模型回答，计算分数。但实际上，NLP 评测面临一系列根本性的困难，每一个都足以让整个评测体系崩塌。</p>
<p>第一个困难是<strong>能力的多维性</strong>。一个语言模型需要同时具备语法理解、常识推理、事实知识、数学计算、代码生成、创意写作等数十种能力。任何单一 benchmark 都只能覆盖其中一小部分。用 MMLU 分数代表”模型智能”，就像用数学考试成绩代表”学生能力”一样片面。</p>
<p>第二个困难是<strong>Goodhart 定律</strong>（Goodhart’s Law）。英国经济学家 Charles Goodhart 在 1975 年观察到：“当一个度量成为目标时，它就不再是好的度量。”这在 NLP 领域体现得淋漓尽致：一旦某个 benchmark 成为社区追逐的目标，模型就会被专门优化来”刷分”，而这种优化往往不反映真正的能力提升。GLUE 排行榜在两年内被”刷满”（超过人类基线），但没有人真的相信 2020 年的模型已经”理解了语言”。</p>
<p>第三个困难是<strong>评测对象的根本变化</strong>。传统 NLP 评测假设模型是某个特定任务的求解器（分类器、翻译器、问答系统）。但 GPT-3 以来的 LLM 是通用的语言生成器——它们的输出是开放式的自然语言文本。如何评估一段自由文本的”质量”？这几乎是一个哲学问题。两个人类专家对同一段文本的评价都可能大相径庭。</p>
<p>第四个困难是<strong>数据污染</strong>（Data Contamination）。现代 LLM 的训练数据规模达到数万亿 token，覆盖了互联网上几乎所有公开文本。这意味着 benchmark 的测试题很可能已经出现在训练数据中——模型可能是”背出了答案”而非”理解了问题”。更棘手的是，对于闭源模型，外部研究者甚至无法验证是否存在数据污染。</p>
</section>
<section id="goodhart-定律评测领域的永恒诅咒" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="goodhart-定律评测领域的永恒诅咒"><span class="header-section-number">2.2</span> Goodhart 定律：评测领域的永恒诅咒</h3>
<p>让我们更深入地理解 Goodhart 定律如何具体影响 NLP 评测。这个定律有一个更精确的表述，由人类学家 Marilyn Strathern 给出：“当一个度量成为目标时，它就不再是好的度量。”在 NLP 领域，这个定律以惊人的规律性反复上演：</p>
<p><span class="math display">\[
\text{Benchmark 发布} \xrightarrow{\text{社区竞争}} \text{针对性优化} \xrightarrow{\text{分数饱和}} \text{Benchmark 失效} \xrightarrow{\text{需要新 Benchmark}}
\]</span></p>
<div id="fig-goodhart-cycle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-goodhart-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-22/original/fig2-goodhart-cycle.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-goodhart-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Goodhart 循环：NLP benchmark 从发布到失效的必经之路。GLUE 在不到两年内走完了全部四个阶段。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>作者绘制。概念基于 Goodhart’s Law: “When a measure becomes a target, it ceases to be a good measure.”</em></p>
</div>
<p>GLUE 的故事就是一个教科书级的案例。2018 年 GLUE 发布时，最好的系统在综合分上只有约 69 分（人类基线 87 分）。BERT 在几个月内将分数提升到 80 以上。到 2019 年底，多个模型超过了人类基线。到 2020 年，排行榜前列的模型之间差距微乎其微——benchmark 丧失了区分能力。</p>
<p>SuperGLUE 于 2019 年发布，专门针对 GLUE 的不足设计了更难的任务。但它的寿命同样不长——T5 等模型在 2020 年就逼近了人类基线。MMLU（2020）试图通过覆盖 57 个学科来延长 benchmark 的寿命。BIG-bench（2022）干脆众包了 200+ 个任务。但每一个 benchmark 都面临同样的命运：<strong>发布、追逐、饱和、失效</strong>。</p>
<p>一个自然的问题是：我们能否设计一个”不会饱和”的 benchmark？答案很可能是否定的，至少对于静态 benchmark 而言。只要测试题是固定的，只要训练数据足够大，模型终将覆盖所有题目。这就是为什么评测方法论需要持续演进——从静态 benchmark 到动态评测，从自动指标到人类判断，从固定题目到对抗性构造。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="评测方法论的三次范式转变" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="评测方法论的三次范式转变"><span class="header-section-number">3.1</span> 评测方法论的三次范式转变</h3>
<p>如果我们将 NLP 评测的历史画成一张演进图（见 <a href="#fig-eval-timeline" class="quarto-xref">Figure&nbsp;3</a>），会清晰地看到三次范式转变，每一次都是对前一代评测失效模式的回应。</p>
<div id="fig-eval-timeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eval-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-22/original/fig1-evaluation-timeline.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-eval-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: NLP 评测方法论的演进：从 BLEU（2002）到 Chatbot Arena（2023），评测经历了自动指标、静态 benchmark、生成式评测三个时代。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>作者绘制。时间线数据来源：各论文原始发表年份（BLEU: Papineni et al.&nbsp;2002; GLUE: Wang et al.&nbsp;2018; MMLU: Hendrycks et al.&nbsp;2020; Chatbot Arena: Zheng et al.&nbsp;2023 等）。</em></p>
</div>
<p><strong>第一代：自动指标时代（2002–2017）</strong>。以 BLEU（机器翻译）和 ROUGE（文本摘要）为代表。核心思想是将文本质量量化为 n-gram 匹配的分数——一个可以全自动计算、不需要人类参与的数字。这让大规模评测成为可能，但代价是指标与真实文本质量之间的相关性有限。一个模型可以获得高 BLEU 分数，输出却读起来像机器生成的拼凑文本。</p>
<p><strong>第二代：理解能力 benchmark 时代（2018–2022）</strong>。以 GLUE、SuperGLUE、MMLU、HELM 为代表。核心思想是设计一组精心挑选的任务，每个任务测试特定的语言理解能力（推理、蕴含、常识等），用准确率等清晰的度量衡量模型表现。这种方式比 BLEU 精确得多，但面临静态 benchmark 的固有困境——数据泄漏、过拟合、饱和。</p>
<p><strong>第三代：生成式评测时代（2023–）</strong>。以 LLM-as-Judge（MT-Bench）和 Chatbot Arena 为代表。核心思想是用另一个 LLM（或众包人类）来评判模型的开放式输出。这解决了”如何评估自由文本”的难题，但引入了新的问题——评判者自身的偏差（LLM 评判者倾向于偏好更长、更华丽的回答）。</p>
<p>每一代评测方法都修复了上一代的某些痛点，但也引入了新的局限性。理解这种”修复→新问题→再修复”的循环，是理解评测方法论的关键。</p>
</section>
<section id="一个类比评测就像考试制度" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="一个类比评测就像考试制度"><span class="header-section-number">3.2</span> 一个类比：评测就像考试制度</h3>
<p>把 NLP 评测比作教育中的考试制度，可以帮助建立直觉。</p>
<p>BLEU 就像用”学生作文中有多少个正确单词”来评分——简单粗暴，但完全忽略了语义和连贯性。GLUE 像是一场标准化考试（如 SAT）——精心设计、有明确的评分标准，但”应试教育”的模型可以通过刷题获得高分而不真正理解知识。Chatbot Arena 则像是让两个学生当面辩论，由观众投票决定谁更厉害——更接近”真实能力”的评估，但观众可能被口才而非逻辑说服。</p>
<p>没有完美的考试制度，也没有完美的评测方法。好的评测方法论不是追求”终极 benchmark”，而是理解每种评测方式的优势、局限和适用场景。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="第一代自动指标" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="第一代自动指标"><span class="header-section-number">4.1</span> 第一代：自动指标</h3>
<section id="bleun-gram-精度与短句惩罚" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="bleun-gram-精度与短句惩罚"><span class="header-section-number">4.1.1</span> BLEU：n-gram 精度与短句惩罚</h4>
<p>BLEU（Bilingual Evaluation Understudy，Papineni et al., 2002）是机器翻译领域最经典的自动评测指标。它的核心思想出奇地简单：比较机器翻译输出和参考译文之间的 n-gram 重合度。</p>
<p>BLEU 的计算分为两部分。第一部分是<strong>修正的 n-gram 精度</strong>（Modified n-gram Precision）。对于每个 n-gram，统计它在候选翻译中出现的次数，但用参考译文中出现的最大次数进行裁剪（clipping），避免一个词被重复计数：</p>
<p><span class="math display">\[
p_n = \frac{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}_{\text{clip}}(\text{n-gram})}{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}(\text{n-gram})}
\]</span></p>
<p>第二部分是<strong>短句惩罚</strong>（Brevity Penalty, BP）。如果没有这个惩罚，模型可以通过只输出最有把握的几个词来获得很高的精度。BP 的设计是：当候选翻译的长度 <span class="math inline">\(c\)</span> 短于参考翻译的有效长度 <span class="math inline">\(r\)</span> 时，施加指数惩罚：</p>
<p><span class="math display">\[
\text{BP} = \begin{cases} 1 &amp; \text{if } c &gt; r \\ e^{1 - r/c} &amp; \text{if } c \leq r \end{cases}
\]</span></p>
<p>最终的 BLEU 分数将两者结合：</p>
<p><span class="math display">\[
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\]</span></p>
<p>其中 <span class="math inline">\(w_n\)</span> 通常取均匀权重 <span class="math inline">\(1/N\)</span>，<span class="math inline">\(N\)</span> 通常取 4（即 BLEU-4，考虑 1-gram 到 4-gram）。</p>
</section>
<section id="完整数值示例bleu-计算" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="完整数值示例bleu-计算"><span class="header-section-number">4.1.2</span> 完整数值示例：BLEU 计算</h4>
<p><strong>设定</strong>：候选翻译（Candidate）和参考翻译（Reference）如下：</p>
<ul>
<li><strong>Reference</strong>: “the cat sat on the mat”</li>
<li><strong>Candidate</strong>: “the the the the”</li>
</ul>
<p><strong>Step 1: 1-gram 精度（不裁剪）</strong></p>
<p>Candidate 中 “the” 出现 4 次，Reference 中也有 “the”。如果不裁剪，<span class="math inline">\(p_1 = 4/4 = 1.0\)</span>——满分！但这显然是荒谬的。</p>
<p><strong>Step 2: 1-gram 精度（裁剪后）</strong></p>
<p>“the” 在 Reference 中最多出现 2 次，所以裁剪后计数为 <span class="math inline">\(\min(4, 2) = 2\)</span>：</p>
<p><span class="math display">\[
p_1 = \frac{2}{4} = 0.5
\]</span></p>
<p><strong>Step 3: 更正常的例子</strong></p>
<p>换一个更合理的候选翻译：</p>
<ul>
<li><strong>Reference</strong>: “the cat sat on the mat”（6 个词）</li>
<li><strong>Candidate</strong>: “the cat on the mat”（5 个词）</li>
</ul>
<p>1-gram 精度：5 个词中，“the”（clip 到 2）、“cat”（1）、“on”（1）、“mat”（1），共 5 个匹配。<span class="math inline">\(p_1 = 5/5 = 1.0\)</span>。</p>
<p>2-gram 精度：Candidate 的 2-gram 有 “the cat”、“cat on”、“on the”、“the mat”。Reference 的 2-gram 有 “the cat”、“cat sat”、“sat on”、“on the”、“the mat”。匹配 3 个（“the cat”、“on the”、“the mat”），共 4 个。<span class="math inline">\(p_2 = 3/4 = 0.75\)</span>。</p>
<p><strong>Step 4: 短句惩罚</strong></p>
<p><span class="math inline">\(c = 5\)</span>，<span class="math inline">\(r = 6\)</span>，<span class="math inline">\(c &lt; r\)</span>，所以：</p>
<p><span class="math display">\[
\text{BP} = e^{1 - 6/5} = e^{-0.2} \approx 0.819
\]</span></p>
<p><strong>Step 5: 最终 BLEU-2 分数</strong>（只用 1-gram 和 2-gram）</p>
<p><span class="math display">\[
\text{BLEU-2} = 0.819 \times \exp\left(\frac{1}{2} \ln 1.0 + \frac{1}{2} \ln 0.75\right) = 0.819 \times \exp(-0.144) = 0.819 \times 0.866 \approx 0.709
\]</span></p>
<p><strong>解读</strong>：候选翻译遗漏了 “sat”，2-gram 中 “cat on” 不匹配，短句惩罚进一步扣分，最终得到 0.709。这个分数合理地反映了翻译的质量。</p>
</section>
<section id="bleu-的局限性" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="bleu-的局限性"><span class="header-section-number">4.1.3</span> BLEU 的局限性</h4>
<p>BLEU 的问题在本质上是 n-gram 匹配的局限。它无法识别同义词替换（“purchase” 和 “buy” 被视为完全不同），对语序的敏感性有限（只在高阶 n-gram 中间接体现），完全忽略了语义连贯性和流畅度。更深层的问题是，BLEU 假设存在”标准答案”——但翻译是一个一对多的问题，一个源句可能有多种同样好的译法。</p>
<p>Callahan et al.&nbsp;(2006) 的研究表明，BLEU 与人类翻译质量评判的相关性在系统级别（比较不同翻译系统）尚可接受，但在句子级别极不可靠。这意味着你可以用 BLEU 大致比较两个翻译系统的整体水平，但不能用它判断某个具体句子翻译得好不好。</p>
</section>
<section id="rouge召回导向的摘要评测" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="rouge召回导向的摘要评测"><span class="header-section-number">4.1.4</span> ROUGE：召回导向的摘要评测</h4>
<p>ROUGE（Recall-Oriented Understudy for Gisting Evaluation，Lin, 2004）是文本摘要领域的标准指标。与 BLEU 侧重精度（候选文本中有多少 n-gram 出现在参考中）不同，ROUGE 侧重<strong>召回率</strong>（参考文本中有多少 n-gram 被候选文本覆盖）。这反映了摘要任务的特点：好的摘要应该覆盖原文的关键信息。</p>
<p>ROUGE 有多个变体。ROUGE-N 计算 n-gram 的召回率：</p>
<p><span class="math display">\[
\text{ROUGE-N} = \frac{\sum_{S \in \text{Ref}} \sum_{\text{n-gram} \in S} \text{Count}_{\text{match}}(\text{n-gram})}{\sum_{S \in \text{Ref}} \sum_{\text{n-gram} \in S} \text{Count}(\text{n-gram})}
\]</span></p>
<p>ROUGE-L 使用<strong>最长公共子序列</strong>（Longest Common Subsequence, LCS）来衡量句子级的结构相似性，不要求 n-gram 连续出现：</p>
<p><span class="math display">\[
\text{ROUGE-L} = F_{\text{LCS}} = \frac{(1 + \beta^2) \cdot R_{\text{LCS}} \cdot P_{\text{LCS}}}{R_{\text{LCS}} + \beta^2 \cdot P_{\text{LCS}}}
\]</span></p>
<p>其中 <span class="math inline">\(R_{\text{LCS}} = \text{LCS}(X, Y) / m\)</span>，<span class="math inline">\(P_{\text{LCS}} = \text{LCS}(X, Y) / n\)</span>，<span class="math inline">\(m\)</span> 和 <span class="math inline">\(n\)</span> 分别是参考和候选的长度。</p>
<p>ROUGE 面临与 BLEU 类似的根本局限：n-gram 匹配无法捕捉语义。一个改写了措辞但保留了核心意思的摘要，可能获得很低的 ROUGE 分数。</p>
</section>
<section id="超越-n-gram语义评测指标" class="level4" data-number="4.1.5">
<h4 data-number="4.1.5" class="anchored" data-anchor-id="超越-n-gram语义评测指标"><span class="header-section-number">4.1.5</span> 超越 n-gram：语义评测指标</h4>
<p>BLEU 和 ROUGE 的局限催生了一系列基于语义的评测指标。<strong>BERTScore</strong>（Zhang et al., 2020）使用 BERT 的上下文嵌入来计算候选文本和参考文本之间的相似度，对同义词和释义更加鲁棒。<strong>BLEURT</strong>（Sellam et al., 2020）在 BERT 基础上进一步微调，学习预测人类质量评分。<strong>COMET</strong>（Rei et al., 2020）在机器翻译评测中引入源句信息，训练一个跨语言模型来预测翻译质量。</p>
<p>这些指标在与人类判断的相关性上显著优于 BLEU/ROUGE，但它们也引入了新的依赖：评测指标本身依赖于一个预训练模型。当被评测的模型和评测指标使用相同的预训练基础时，可能产生系统性偏差。</p>
</section>
</section>
<section id="第二代理解能力-benchmark" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="第二代理解能力-benchmark"><span class="header-section-number">4.2</span> 第二代：理解能力 benchmark</h3>
<section id="glue-与-superglue多任务理解评测" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="glue-与-superglue多任务理解评测"><span class="header-section-number">4.2.1</span> GLUE 与 SuperGLUE：多任务理解评测</h4>
<p>GLUE（General Language Understanding Evaluation，Wang et al., 2018）的设计理念是：一个真正”理解语言”的模型应该能够同时处理多种自然语言理解任务。GLUE 包含 9 个任务，涵盖了语言理解的多个维度：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>任务</th>
<th>类型</th>
<th>描述</th>
<th>度量</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>MNLI</strong></td>
<td>蕴含</td>
<td>三分类（蕴含/矛盾/中性）</td>
<td>Accuracy</td>
<td>“猫在垫子上” ⇒ “有动物在家具上”?</td>
</tr>
<tr class="even">
<td><strong>QQP</strong></td>
<td>释义</td>
<td>两个问题是否语义相同</td>
<td>F1/Acc</td>
<td>“How old are you” ≈ “What is your age”?</td>
</tr>
<tr class="odd">
<td><strong>QNLI</strong></td>
<td>问答/蕴含</td>
<td>句子是否包含问题的答案</td>
<td>Accuracy</td>
<td>Q: “When did X happen?” + 候选句</td>
</tr>
<tr class="even">
<td><strong>SST-2</strong></td>
<td>情感</td>
<td>电影评论正/负情感</td>
<td>Accuracy</td>
<td>“This movie was wonderful” → 正面</td>
</tr>
<tr class="odd">
<td><strong>CoLA</strong></td>
<td>语法</td>
<td>句子是否语法正确</td>
<td>Matthews Corr</td>
<td>“The boy the cat bit ran” → ?</td>
</tr>
<tr class="even">
<td><strong>STS-B</strong></td>
<td>相似度</td>
<td>两句话的语义相似度（1-5分）</td>
<td>Pearson/Spearman</td>
<td>回归任务</td>
</tr>
<tr class="odd">
<td><strong>MRPC</strong></td>
<td>释义</td>
<td>两个句子是否语义等价</td>
<td>F1/Acc</td>
<td>新闻句子对</td>
</tr>
<tr class="even">
<td><strong>RTE</strong></td>
<td>蕴含</td>
<td>二分类蕴含</td>
<td>Accuracy</td>
<td>前提→假设</td>
</tr>
<tr class="odd">
<td><strong>WNLI</strong></td>
<td>共指</td>
<td>Winograd 共指消解</td>
<td>Accuracy</td>
<td>代词指代谁？</td>
</tr>
</tbody>
</table>
<p>GLUE 的关键设计决策值得深入分析。首先，<strong>任务多样性</strong>：9 个任务覆盖了从句法（CoLA）到语义（STS-B）到推理（MNLI）的多个层次。其次，<strong>单一分数</strong>：所有任务的分数被平均为一个综合分数，方便排行榜比较。第三，<strong>人类基线</strong>：GLUE 公布了人类在每个任务上的表现，作为”天花板”参考。</p>
<p>然而，GLUE 的”寿命”出乎意料地短。BERT（2018年10月）发布后仅几个月，GLUE 排行榜就被”攻陷”——多个模型在综合分上接近甚至超过人类基线。这迫使设计者在 2019 年推出了 SuperGLUE，包含 8 个更具挑战性的任务：BoolQ（布尔问答）、CB（文本蕴含）、COPA（因果推理）、MultiRC（多句阅读理解）、ReCoRD（常识阅读理解）、RTE（增强版）、WiC（上下文词义）、WSC（Winograd 模式）。</p>
<p>SuperGLUE 的命运与 GLUE 类似——它在 2020 年也基本被”刷满”。这个”两年寿命”的模式揭示了静态 benchmark 的一个根本困境：<strong>benchmark 的难度是固定的，但模型的能力在持续增长</strong>。</p>
</section>
<section id="mmlu大规模多任务理解" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="mmlu大规模多任务理解"><span class="header-section-number">4.2.2</span> MMLU：大规模多任务理解</h4>
<p>MMLU（Massive Multitask Language Understanding，Hendrycks et al., 2020）试图通过<strong>规模</strong>来延长 benchmark 的寿命。它包含 57 个学科的 15,908 道选择题，覆盖从高中数学到专业法律、从天文学到计算机科学的广泛领域。每道题是四选一的选择题，如：</p>
<blockquote class="blockquote">
<strong>Abstract Algebra</strong>: Let p = (1, 2, 5, 4)(2, 3) in S_5. Find the index of
<p>
</p><p>in S_5. (A) 8 (B) 2 (C) 24 (D) 120</p>
</blockquote>
<p>MMLU 的设计有几个值得注意的特点。首先，题目来源于真实的考试和教材，而非人工构造——这提高了”生态有效性”。其次，57 个学科的细粒度分类允许分析模型在不同知识领域的强弱。第三，四选一的格式使得评测完全自动化（只需检查模型选择了哪个选项）。</p>
<p>MMLU 很快成为 LLM 评测的”黄金标准”。GPT-4 的技术报告以 MMLU 分数作为核心能力指标之一（GPT-4: 86.4% vs GPT-3.5: 70.0%）。然而，MMLU 也面临自己的问题。选择题格式限制了评测的深度——模型可能通过排除法而非真正理解来答题。随着 LLM 训练数据的膨胀，MMLU 题目被”记住”的风险越来越高。</p>
<p>更令人不安的是，2024 年的一项质量审计揭示了 MMLU 本身的严重质量问题：<strong>整体错误率约为 6.5%</strong>，包括错误的标注答案、模糊的题目表述和过时的知识。某些学科的错误率远高于平均值——Virology（病毒学）子集的错误率高达 <strong>57%</strong>。这意味着当我们用 MMLU 分数来比较模型时，部分”错误”可能只是模型给出了比 benchmark 答案更正确的回答。这个发现从另一个角度动摇了 benchmark 评测的可信度：不仅模型可能”作弊”（数据污染），benchmark 本身也可能出错。</p>
</section>
<section id="helm多维度整体评测" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="helm多维度整体评测"><span class="header-section-number">4.2.3</span> HELM：多维度整体评测</h4>
<p>HELM（Holistic Evaluation of Language Models，Liang et al., 2022）是评测方法论上的一次重要尝试。它不再追求”更难的题目”，而是追求<strong>更全面的评测维度</strong>。HELM 定义了 7 个核心评测场景（Core Scenarios）和 7 个度量维度（Metrics）：</p>
<p><strong>评测场景</strong>：问答、信息检索、文本摘要、情感分析、毒性检测、版权与公平性、对话。</p>
<p><strong>度量维度</strong>：准确性（Accuracy）、校准性（Calibration）、鲁棒性（Robustness）、公平性（Fairness）、偏差（Bias）、毒性（Toxicity）、效率（Efficiency）。</p>
<p>HELM 的关键洞察是：一个模型不仅要”准确”，还要”校准良好”（知道自己什么时候不确定）、“鲁棒”（对输入扰动不敏感）、“公平”（对不同人群同等对待）。一个 MMLU 分数 90% 但输出大量有毒内容的模型，显然不应该被视为”好模型”。</p>
<p>这种多维度评测理念影响深远，后续的评测框架（如 OpenCompass、LM Evaluation Harness）都借鉴了 HELM 的设计哲学。</p>
</section>
<section id="big-bench众包任务的极端尝试" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="big-bench众包任务的极端尝试"><span class="header-section-number">4.2.4</span> BIG-bench：众包任务的极端尝试</h4>
<p>BIG-bench（Beyond the Imitation Game benchmark，Srivastava et al., 2022）走了另一条路：通过<strong>众包</strong>来构建超大规模 benchmark。444 位研究者贡献了 204 个任务，涵盖语言学、数学、常识推理、社会偏见等领域。BIG-bench Hard（BBH）是其中精选的 23 个特别困难的任务——这些任务上 Chain-of-Thought 才有显著效果。</p>
<p>BIG-bench 的贡献不仅是任务数量，更是评测方法论：它系统地分析了任务难度与模型规模的关系，为”涌现能力”的讨论提供了大量实证数据（也为 Schaeffer et al.&nbsp;的反驳提供了分析素材）。</p>
</section>
</section>
<section id="第三代生成式评测的新范式" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="第三代生成式评测的新范式"><span class="header-section-number">4.3</span> 第三代：生成式评测的新范式</h3>
<section id="为什么需要新范式" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="为什么需要新范式"><span class="header-section-number">4.3.1</span> 为什么需要新范式？</h4>
<p>第二代 benchmark 的核心假设是：模型的能力可以通过一组<strong>有标准答案</strong>的任务来衡量。但 ChatGPT 之后的 LLM 主要被用于<strong>开放式生成</strong>——写邮件、改代码、头脑风暴、角色扮演——这些任务没有唯一的”正确答案”。</p>
<p>想象你让两个 LLM 分别写一封求职信。模型 A 写了一封简洁有力的短信，模型 B 写了一封详尽周到的长信。哪个更好？这取决于求职者的风格偏好、目标公司的文化、甚至招聘经理的个人品味。传统 benchmark 无法处理这种主观性。</p>
</section>
<section id="llm-as-judge用模型评模型" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="llm-as-judge用模型评模型"><span class="header-section-number">4.3.2</span> LLM-as-Judge：用模型评模型</h4>
<p>Zheng et al.（2023）提出了一个大胆的方案：用一个强大的 LLM（如 GPT-4）作为评判者来评估其他模型的输出。<a href="#fig-llm-as-judge" class="quarto-xref">Figure&nbsp;4</a> 展示了一个具体的评判过程：GPT-4 作为裁判，比较两个助手对同一个多轮问题的回答，并给出详细的评判理由。</p>
<div id="fig-llm-as-judge" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llm-as-judge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-22/original/fig3-llm-as-judge-zheng2023.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llm-as-judge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: LLM-as-Judge 的工作示例：GPT-4 评判两个 AI 助手对”联储在二级市场买债”问题的多轮回答。注意评判理由中对”重复性”和”具体性”的分析——这种细粒度反馈是传统 benchmark 无法提供的。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Zheng, L. et al.&nbsp;(2023) “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena”, Figure 1. <a href="https://arxiv.org/abs/2306.05685">arXiv:2306.05685</a></em></p>
</div>
<p>Zheng et al.&nbsp;设计了两个互补的评测系统。</p>
<p><strong>MT-Bench</strong> 是一个包含 80 个多轮对话问题的测试集，覆盖写作、推理、数学、编程、知识、角色扮演等 8 个类别。每个问题都设计为需要两轮对话——第二轮的问题依赖于第一轮的回答，测试模型的多轮对话能力。评判方式有两种：</p>
<p><strong>单一评分</strong>（Single Rating）：GPT-4 对每个回答打 1-10 分，并给出理由。</p>
<p><strong>成对比较</strong>（Pairwise Comparison）：GPT-4 看到两个模型对同一问题的回答，判断哪个更好。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: LLM-as-Judge Pairwise Comparison（改编自 Zheng et al., 2023）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Input: question Q, answer_A from Model A, answer_B from Model B, judge LLM J
Output: winner ∈ {A, B, Tie}

Step 1: Construct prompt for judge
    prompt = f"""
    [System] Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question.

    [Question] {Q}
    [Assistant A] {answer_A}
    [Assistant B] {answer_B}

    Please evaluate which assistant provides a better response.
    Output your verdict as: [[A]], [[B]], or [[Tie]].
    Provide your reasoning before the verdict.
    """

Step 2: Get judgment
    judgment = J(prompt)

Step 3: Swap positions and re-judge (to mitigate position bias)
    prompt_swapped = swap(answer_A, answer_B) in prompt
    judgment_swapped = J(prompt_swapped)

Step 4: Final verdict
    if both judgments agree: return agreed winner
    else: return Tie (inconsistent)</code></pre>
<p><em>改编自 Zheng et al.&nbsp;(2023) “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena”, Section 3. <a href="https://arxiv.org/abs/2306.05685">arXiv:2306.05685</a></em></p>
</div>
</div>
<p>注意 Step 3 的位置交换——这是一个关键设计。Zheng et al.&nbsp;发现 GPT-4 作为评判者时存在显著的<strong>位置偏差</strong>：它倾向于偏好放在第一个位置的回答。通过交换位置并比较两次判断，可以部分缓解这一偏差。</p>
</section>
<section id="llm-as-judge-的偏差分析" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="llm-as-judge-的偏差分析"><span class="header-section-number">4.3.3</span> LLM-as-Judge 的偏差分析</h4>
<p>Zheng et al.&nbsp;系统地研究了 LLM 评判者的多种偏差，这对理解生成式评测的局限性至关重要：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>偏差类型</th>
<th>描述</th>
<th>严重程度</th>
<th>缓解方法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>位置偏差</strong></td>
<td>倾向于偏好第一个/最后一个位置的回答</td>
<td>高</td>
<td>交换位置，取一致结果</td>
</tr>
<tr class="even">
<td><strong>冗长偏差</strong></td>
<td>倾向于偏好更长的回答</td>
<td>中</td>
<td>在 prompt 中明确要求关注质量而非长度</td>
</tr>
<tr class="odd">
<td><strong>自我偏好</strong></td>
<td>模型倾向于偏好自己（或相似风格）的输出</td>
<td>中</td>
<td>使用与被评模型不同的 judge</td>
</tr>
<tr class="even">
<td><strong>格式偏差</strong></td>
<td>偏好带有列表、标题等格式化的回答</td>
<td>低-中</td>
<td>控制格式变量</td>
</tr>
</tbody>
</table>
<p>尽管存在这些偏差，Zheng et al.&nbsp;发现 GPT-4 作为评判者与人类评判者的一致率超过 80%——高于人类评判者之间的一致率（约 81%）。这个结果既令人鼓舞（LLM-as-Judge 可用），也令人不安（如果 GPT-4 是评判标准，那它自己的问题就永远无法被发现）。</p>
</section>
<section id="chatbot-arena众包人类偏好" class="level4" data-number="4.3.4">
<h4 data-number="4.3.4" class="anchored" data-anchor-id="chatbot-arena众包人类偏好"><span class="header-section-number">4.3.4</span> Chatbot Arena：众包人类偏好</h4>
<p>Chatbot Arena（LMSYS, 2023-）采用了完全不同的策略：不用 LLM 评判，而是<strong>众包真实用户的偏好</strong>。用户在网站上输入一个问题，两个匿名模型分别回答，用户选择哪个回答更好。模型的身份在用户投票后才揭示。</p>
<p>这种”盲品测试”的设计有几个关键优势。首先，用户可以自由提出任何问题——测试分布完全由真实使用场景决定，不受预设题目的限制。其次，匿名设计消除了品牌偏好（用户不知道自己在评价 GPT-4 还是 Claude）。第三，大规模众包（截至 2025 年已有数十万次投票）提供了统计上稳健的排名。</p>
<p>模型的排名使用 <strong>Elo 评分系统</strong>——与国际象棋的评分系统相同。每次对决后，赢家获得评分，输家失去评分，变化量取决于双方的评分差距：</p>
<p><span class="math display">\[
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}
\]</span></p>
<p><span class="math display">\[
R_A^{\text{new}} = R_A + K \cdot (S_A - E_A)
\]</span></p>
<p>其中 <span class="math inline">\(E_A\)</span> 是模型 A 的预期胜率，<span class="math inline">\(R_A, R_B\)</span> 是当前 Elo 评分，<span class="math inline">\(S_A\)</span> 是实际结果（赢=1，输=0，平=0.5），<span class="math inline">\(K\)</span> 是更新系数。</p>
</section>
<section id="完整数值示例elo-评分更新" class="level4" data-number="4.3.5">
<h4 data-number="4.3.5" class="anchored" data-anchor-id="完整数值示例elo-评分更新"><span class="header-section-number">4.3.5</span> 完整数值示例：Elo 评分更新</h4>
<p><strong>设定</strong>：模型 A 当前 Elo = 1200，模型 B 当前 Elo = 1000，<span class="math inline">\(K = 32\)</span>。</p>
<p><strong>Step 1: 计算预期胜率</strong></p>
<p><span class="math display">\[
E_A = \frac{1}{1 + 10^{(1000 - 1200)/400}} = \frac{1}{1 + 10^{-0.5}} = \frac{1}{1 + 0.316} = 0.760
\]</span></p>
<p><span class="math display">\[
E_B = 1 - E_A = 0.240
\]</span></p>
<p><strong>解读</strong>：根据当前评分，系统预期模型 A 有 76% 的胜率。</p>
<p><strong>Step 2: 模型 A 获胜（符合预期）</strong></p>
<p><span class="math display">\[
R_A^{\text{new}} = 1200 + 32 \times (1 - 0.760) = 1200 + 7.7 = 1207.7
\]</span></p>
<p><span class="math display">\[
R_B^{\text{new}} = 1000 + 32 \times (0 - 0.240) = 1000 - 7.7 = 992.3
\]</span></p>
<p><strong>解读</strong>：强者战胜弱者，评分变化不大（+7.7）。</p>
<p><strong>Step 3: 如果模型 B 爆冷获胜呢？</strong></p>
<p><span class="math display">\[
R_A^{\text{new}} = 1200 + 32 \times (0 - 0.760) = 1200 - 24.3 = 1175.7
\]</span></p>
<p><span class="math display">\[
R_B^{\text{new}} = 1000 + 32 \times (1 - 0.240) = 1000 + 24.3 = 1024.3
\]</span></p>
<p><strong>解读</strong>：弱者战胜强者，评分变化大得多（±24.3）。这就是 Elo 系统的精妙之处——“爆冷”传递了更多信息，因此带来更大的评分更新。</p>
</section>
<section id="bradley-terry-模型成对比较的概率基础" class="level4" data-number="4.3.6">
<h4 data-number="4.3.6" class="anchored" data-anchor-id="bradley-terry-模型成对比较的概率基础"><span class="header-section-number">4.3.6</span> Bradley-Terry 模型：成对比较的概率基础</h4>
<p>Chatbot Arena 的排名本质上基于 <strong>Bradley-Terry 模型</strong>（Bradley &amp; Terry, 1952），这是成对比较（paired comparison）领域最经典的概率模型。</p>
<p>给定 <span class="math inline">\(N\)</span> 个模型，每个模型 <span class="math inline">\(i\)</span> 有一个”能力参数” <span class="math inline">\(\theta_i &gt; 0\)</span>。模型 <span class="math inline">\(i\)</span> 在与模型 <span class="math inline">\(j\)</span> 的对决中获胜的概率为：</p>
<p><span class="math display">\[
P(i \succ j) = \frac{\theta_i}{\theta_i + \theta_j}
\]</span></p>
<p>取对数后，令 <span class="math inline">\(\beta_i = \log \theta_i\)</span>：</p>
<p><span class="math display">\[
P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}} = \sigma(\beta_i - \beta_j)
\]</span></p>
<p>其中 <span class="math inline">\(\sigma\)</span> 是 sigmoid 函数。这正是一个逻辑回归模型！Elo 评分系统可以看作 Bradley-Terry 模型的在线更新版本（用 <span class="math inline">\(R_i / 400\)</span> 近似 <span class="math inline">\(\beta_i\)</span>）。</p>
<p>给定一组成对比较数据 <span class="math inline">\(\{(i_k, j_k, y_k)\}_{k=1}^{M}\)</span>（<span class="math inline">\(y_k = 1\)</span> 表示 <span class="math inline">\(i_k\)</span> 获胜），Bradley-Terry 模型的参数通过最大似然估计求解：</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} \sum_{k=1}^{M} \left[ y_k \log \sigma(\beta_{i_k} - \beta_{j_k}) + (1 - y_k) \log \sigma(\beta_{j_k} - \beta_{i_k}) \right]
\]</span></p>
<p>这个优化问题是凸的，可以用标准的梯度方法高效求解。</p>
</section>
</section>
<section id="数据污染与科学诚实" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="数据污染与科学诚实"><span class="header-section-number">4.4</span> 数据污染与科学诚实</h3>
<section id="数据污染的本质" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="数据污染的本质"><span class="header-section-number">4.4.1</span> 数据污染的本质</h4>
<p>数据污染（Data Contamination）是 LLM 评测面临的最严重威胁之一。当模型的训练数据包含了 benchmark 的测试题时，模型的高分可能反映的是”记忆”而非”能力”。这就像一个学生考前偷看了试卷——他的高分并不代表他真正掌握了知识。</p>
<p>数据污染的严重性在于其<strong>隐蔽性</strong>和<strong>不可避免性</strong>。现代 LLM 的训练数据通常来自互联网爬取，规模达到数万亿 token。MMLU 的题目来自公开的考试和教材，这些内容很可能出现在训练数据中。对于闭源模型（如 GPT-4、Claude），外部研究者甚至无法检查训练数据中是否包含特定 benchmark。</p>
</section>
<section id="污染检测方法" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="污染检测方法"><span class="header-section-number">4.4.2</span> 污染检测方法</h4>
<p>研究者开发了多种方法来检测数据污染。最直接的方法是 <strong>n-gram 重叠检测</strong>：检查 benchmark 测试题的 n-gram 是否出现在训练数据中。GPT-3 和 GPT-4 的技术报告都进行了这类分析。但这种方法有明显的局限——改写（paraphrase）后的题目不会被检测到，而且它要求能够访问训练数据，对闭源模型不适用。</p>
<p>面对黑盒模型，研究者发展出了更巧妙的检测策略。Oren et al.（2023）提出了<strong>分区对比法</strong>：通过分析模型在 benchmark 不同分割（partition）上的表现差异来推断污染——如果模型在某个子集上表现异常地好，这可能暗示该子集被”泄漏”到了训练数据中。Deng et al.（2023）提出了 <strong>TS-Guessing</strong> 方法：给模型展示 benchmark 题目的部分内容（如只给选项不给题干），检测模型是否仍能以远超随机的概率选出正确答案。如果模型”见过”这些题目，即使只看到部分信息也能猜出答案。Golchin &amp; Surdeanu（2023）则提出了更直接的 <strong>Time Travel</strong> 方法：通过精心设计的 prompt，测试模型是否能逐字复现 benchmark 的原始文本——如果模型能几乎完美地重建某个 benchmark 实例的精确措辞，这就是数据污染的强证据。</p>
<p>另一种方法是<strong>金丝雀检测</strong>（Canary Detection）：在 benchmark 中故意嵌入一些独特的标记（canary tokens），如果模型能够复现这些标记，就证明存在污染。这种方法的优势在于它可以在 benchmark 设计阶段就内置防污染机制。</p>
</section>
<section id="著名的数据污染案例" class="level4" data-number="4.4.3">
<h4 data-number="4.4.3" class="anchored" data-anchor-id="著名的数据污染案例"><span class="header-section-number">4.4.3</span> 著名的数据污染案例</h4>
<p>2023 年，多项研究揭示了大规模数据污染的存在。研究者发现某些模型在 MMLU 特定学科上的表现远超预期，而这些学科的题目恰好在互联网上大量流传。更令人担忧的是，一些模型在 benchmark 发布前后的性能提升，与新增训练数据中包含 benchmark 相关内容的时间高度吻合。</p>
<p>这些案例提醒我们：<strong>benchmark 分数的提升不一定意味着能力的提升</strong>。在解读任何评测结果时，都需要考虑数据污染的可能性。</p>
</section>
</section>
<section id="可靠性评测" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="可靠性评测"><span class="header-section-number">4.5</span> 可靠性评测</h3>
<section id="事实性如何衡量幻觉" class="level4" data-number="4.5.1">
<h4 data-number="4.5.1" class="anchored" data-anchor-id="事实性如何衡量幻觉"><span class="header-section-number">4.5.1</span> 事实性：如何衡量幻觉？</h4>
<p>LLM 的”幻觉”（Hallucination）——生成看似合理但实际错误的内容——是部署中最严重的问题之一。TruthfulQA（Lin et al., 2021）是最早系统评测 LLM 事实性的 benchmark。它包含 817 个问题，这些问题被设计为容易诱导模型生成错误答案——因为人类对这些问题也经常犯错（如常见的误解、都市传说等）。</p>
<p>TruthfulQA 的一个反直觉的发现是：<strong>更大的模型并不总是更诚实</strong>。在没有经过对齐训练的情况下，更大的模型由于更好地学习了人类文本中的常见错误，反而更容易生成”流畅但错误”的回答。这与模型在其他 benchmark 上”越大越好”的趋势形成了鲜明对比。</p>
</section>
<section id="一致性同一问题不同问法" class="level4" data-number="4.5.2">
<h4 data-number="4.5.2" class="anchored" data-anchor-id="一致性同一问题不同问法"><span class="header-section-number">4.5.2</span> 一致性：同一问题不同问法</h4>
<p>一个可靠的模型应该对同一问题的不同措辞给出一致的回答。但实践中，LLM 对 prompt 的措辞极其敏感——仅仅改变问题的语序或添加一个无关的前缀，都可能导致完全不同的回答。</p>
<p>这种不一致性对评测的影响是深远的：<strong>一个模型在 MMLU 上 86% 的准确率，可能变成 82% 如果你稍微改写题目的措辞</strong>。这意味着 benchmark 分数的精度远低于它表面上的精确度。</p>
</section>
<section id="拒答质量与越狱鲁棒性" class="level4" data-number="4.5.3">
<h4 data-number="4.5.3" class="anchored" data-anchor-id="拒答质量与越狱鲁棒性"><span class="header-section-number">4.5.3</span> 拒答质量与越狱鲁棒性</h4>
<p>对齐后的模型被训练在面对有害请求时拒绝回答。但”拒答”本身也是一个评测维度：模型是否能够在<strong>应该拒绝时拒绝</strong>（安全性），同时在<strong>不应该拒绝时不拒绝</strong>（可用性）？过度拒答（对无害问题也拒绝）和拒答不足（对有害问题回答）都是失败模式。</p>
<p>越狱（Jailbreak）测试评估模型的安全护栏是否能抵抗对抗性攻击。研究者不断发现新的方法绕过模型的安全限制——如角色扮演、编码变换、多步引导等。这是一个”攻防博弈”的领域，评测本身需要持续更新以跟上攻击技术的演进。</p>
<hr>
</section>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="搭建评测-pipeline" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="搭建评测-pipeline"><span class="header-section-number">5.1</span> 搭建评测 Pipeline</h3>
<p>一个完整的 LLM 评测 pipeline 通常包含以下组件：数据加载、推理执行、指标计算、结果聚合。下面是一个使用 Python 实现基础评测的示例：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">最小化的 LLM 评测 Pipeline 示例</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">评测模型在 MMLU 风格选择题上的表现</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 数据加载 ==========</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_mmlu_style_data(filepath):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""加载 MMLU 风格的选择题数据</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    格式: {"question": str, "choices": [str], "answer": int, "subject": str}</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filepath) <span class="im">as</span> f:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [json.loads(line) <span class="cf">for</span> line <span class="kw">in</span> f]</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 推理模板 ==========</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_mmlu_prompt(question, choices, few_shot_examples<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""构建 MMLU 评测 prompt</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">    关键细节：prompt 格式对结果有显著影响！</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="st">""</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> few_shot_examples:</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ex <span class="kw">in</span> few_shot_examples:</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">+=</span> <span class="ss">f"Question: </span><span class="sc">{</span>ex[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(ex[<span class="st">'choices'</span>]):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                prompt <span class="op">+=</span> <span class="ss">f"(</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">65</span><span class="op">+</span>i)<span class="sc">}</span><span class="ss">) </span><span class="sc">{</span>c<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">+=</span> <span class="ss">f"Answer: (</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">65</span><span class="op">+</span>ex[<span class="st">'answer'</span>])<span class="sc">}</span><span class="ss">)</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(choices):</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"(</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">65</span><span class="op">+</span>i)<span class="sc">}</span><span class="ss">) </span><span class="sc">{</span>c<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="st">"Answer: ("</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prompt</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 指标计算 ==========</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(predictions, references):</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""计算多维度评测指标"""</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="bu">sum</span>(p <span class="op">==</span> r <span class="cf">for</span> p, r <span class="kw">in</span> <span class="bu">zip</span>(predictions, references))</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">len</span>(references)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> correct <span class="op">/</span> total</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 按学科分组计算</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    subject_scores <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: {<span class="st">"correct"</span>: <span class="dv">0</span>, <span class="st">"total"</span>: <span class="dv">0</span>})</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (省略分组逻辑)</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">"accuracy"</span>: accuracy,</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"correct"</span>: correct,</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"total"</span>: total,</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        <span class="st">"95_ci"</span>: <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(accuracy <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> accuracy) <span class="op">/</span> total)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 结果报告 ==========</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report_results(metrics):</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""生成评测报告"""</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Overall Accuracy: </span><span class="sc">{</span>metrics[<span class="st">'accuracy'</span>]<span class="sc">:.1%}</span><span class="ss"> "</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"(± </span><span class="sc">{</span>metrics[<span class="st">'95_ci'</span>]<span class="sc">:.1%}</span><span class="ss">)"</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Correct: </span><span class="sc">{</span>metrics[<span class="st">'correct'</span>]<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>metrics[<span class="st">'total'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 主流程 ==========</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="co"># data = load_mmlu_style_data("mmlu_test.jsonl")</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions = [call_model(format_mmlu_prompt(d)) for d in data]</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co"># metrics = compute_metrics(predictions, [d["answer"] for d in data])</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a><span class="co"># report_results(metrics)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="使用-lm-evaluation-harness" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="使用-lm-evaluation-harness"><span class="header-section-number">5.2</span> 使用 lm-evaluation-harness</h3>
<p>在实践中，评测通常使用成熟的框架。<strong>lm-evaluation-harness</strong>（EleutherAI）是最广泛使用的开源评测框架，支持数百个 benchmark：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install lm-eval</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 在 MMLU 上评测 Hugging Face 模型</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">lm_eval</span> <span class="at">--model</span> hf <span class="dt">\</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model_args</span> pretrained=meta-llama/Llama-2-7b-hf <span class="dt">\</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--tasks</span> mmlu <span class="dt">\</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">--batch_size</span> 8 <span class="dt">\</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">--output_path</span> results/</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 评测多个 benchmark</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="ex">lm_eval</span> <span class="at">--model</span> hf <span class="dt">\</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model_args</span> pretrained=meta-llama/Llama-2-7b-hf <span class="dt">\</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">--tasks</span> mmlu,hellaswag,arc_challenge,truthfulqa_mc <span class="dt">\</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">--batch_size</span> 8</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="实现-llm-as-judge" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="实现-llm-as-judge"><span class="header-section-number">5.3</span> 实现 LLM-as-Judge</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">简化版 LLM-as-Judge 实现</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">使用成对比较 + 位置交换来缓解位置偏差</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>JUDGE_PROMPT <span class="op">=</span> <span class="st">"""Please act as an impartial judge and evaluate the quality</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="st">of the responses provided by two AI assistants to the user question below.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="st">[User Question]</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="sc">{question}</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="st">[Assistant A's Answer]</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_a}</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="st">[Assistant B's Answer]</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_b}</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="st">Evaluate which assistant provides a better answer. Consider helpfulness,</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="st">relevance, accuracy, depth, and clarity. Avoid position bias.</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="st">Output your verdict EXACTLY as: [[A]], [[B]], or [[Tie]].</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="st">Provide brief reasoning before your verdict."""</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> judge_pair(question, answer_a, answer_b, judge_model):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co">    对一对回答进行评判，带位置交换</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns: "A", "B", or "Tie"</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 第一次评判：A在前</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    prompt_1 <span class="op">=</span> JUDGE_PROMPT.<span class="bu">format</span>(</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        question<span class="op">=</span>question, answer_a<span class="op">=</span>answer_a, answer_b<span class="op">=</span>answer_b</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    result_1 <span class="op">=</span> judge_model(prompt_1)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    verdict_1 <span class="op">=</span> extract_verdict(result_1)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 第二次评判：B在前（交换位置）</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    prompt_2 <span class="op">=</span> JUDGE_PROMPT.<span class="bu">format</span>(</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        question<span class="op">=</span>question, answer_a<span class="op">=</span>answer_b, answer_b<span class="op">=</span>answer_a</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    result_2 <span class="op">=</span> judge_model(prompt_2)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    verdict_2 <span class="op">=</span> extract_verdict(result_2)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 注意：需要反转 verdict_2 的结果</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    verdict_2_flipped <span class="op">=</span> flip_verdict(verdict_2)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 两次一致才算数</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verdict_1 <span class="op">==</span> verdict_2_flipped:</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> verdict_1</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"Tie"</span>  <span class="co"># 不一致 → 视为平局</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_verdict(text):</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""从 judge 输出中提取判定"""</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    match <span class="op">=</span> re.search(<span class="vs">r'</span><span class="ch">\[\[</span><span class="kw">(</span><span class="vs">A</span><span class="cf">|</span><span class="vs">B</span><span class="cf">|</span><span class="vs">Tie</span><span class="kw">)</span><span class="ch">\]\]</span><span class="vs">'</span>, text)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> match.group(<span class="dv">1</span>) <span class="cf">if</span> match <span class="cf">else</span> <span class="st">"Tie"</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flip_verdict(verdict):</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""位置交换后，A和B的含义互换"""</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verdict <span class="op">==</span> <span class="st">"A"</span>: <span class="cf">return</span> <span class="st">"B"</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verdict <span class="op">==</span> <span class="st">"B"</span>: <span class="cf">return</span> <span class="st">"A"</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"Tie"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="数据污染快速检测" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="数据污染快速检测"><span class="header-section-number">5.4</span> 数据污染快速检测</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">简易数据污染检测：检查模型是否能"背出" benchmark 题目</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">思路：给出题目的前半部分，看模型能否续写出后半部分</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contamination_check(model, benchmark_items, n_samples<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">    通过 prefix completion 检测潜在的数据污染</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">    如果模型能高精度地续写 benchmark 题目，</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">    说明这些题目很可能出现在训练数据中。</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> random</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> random.sample(benchmark_items, <span class="bu">min</span>(n_samples, <span class="bu">len</span>(benchmark_items)))</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    exact_matches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    high_overlap <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> samples:</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> item[<span class="st">"question"</span>]</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 取前 50% 作为 prefix</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        split_point <span class="op">=</span> <span class="bu">len</span>(text) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        prefix <span class="op">=</span> text[:split_point]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        suffix <span class="op">=</span> text[split_point:]</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 让模型续写</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        completion <span class="op">=</span> model.generate(prefix, max_tokens<span class="op">=</span><span class="bu">len</span>(suffix.split()) <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算重叠度</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        overlap <span class="op">=</span> compute_rouge_l(completion, suffix)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> overlap <span class="op">&gt;</span> <span class="fl">0.9</span>:</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            exact_matches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> overlap <span class="op">&gt;</span> <span class="fl">0.7</span>:</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>            high_overlap <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">"exact_match_rate"</span>: exact_matches <span class="op">/</span> <span class="bu">len</span>(samples),</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">"high_overlap_rate"</span>: high_overlap <span class="op">/</span> <span class="bu">len</span>(samples),</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">"verdict"</span>: <span class="st">"LIKELY CONTAMINATED"</span> <span class="cf">if</span> exact_matches <span class="op">/</span> <span class="bu">len</span>(samples) <span class="op">&gt;</span> <span class="fl">0.1</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">else</span> <span class="st">"PROBABLY CLEAN"</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    }</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨评测方法论的理论基础、边界条件和开放问题</p>
</blockquote>
<section id="为什么评测科学这么重要理论视角" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么评测科学这么重要理论视角"><span class="header-section-number">6.1</span> 为什么评测科学这么重要？——理论视角</h3>
<p>从科学哲学的角度看，评测问题其实是<strong>操作性定义</strong>（operational definition）的问题。当我们说一个模型”理解语言”或”具备推理能力”时，这些概念需要通过具体的评测任务来操作化。正如智商测试（IQ test）不等于”智能”，MMLU 分数也不等于”理解”——但在没有更好替代的情况下，它是我们能得到的最好近似。</p>
<p>问题在于，不同的操作性定义可能给出截然不同的结论。Schaeffer et al.（2023）的工作正是这一点的极端体现：同一份数据、同一个能力，用精确匹配度量时”涌现”了，用连续度量时却”一直都在平滑增长”。这告诉我们：<strong>任何评测结论都不能脱离其度量方式来理解</strong>。</p>
<p>从信息论的角度，评测本质上是通过有限的观测（测试题目和模型回答）来推断模型的潜在能力（一个无限维的函数空间中的一个点）。这是一个严重<strong>欠定</strong>（underdetermined）的问题——有限的测试永远无法完全确定模型的能力边界。更糟糕的是，由于对抗性优化的存在，模型可能在测试分布上表现出色，但在测试分布之外完全崩溃。</p>
</section>
<section id="评测的隐含假设" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="评测的隐含假设"><span class="header-section-number">6.2</span> 评测的隐含假设</h3>
<p>每种评测方式都隐含了一系列假设，理解这些假设是评判评测结论的前提。</p>
<p><strong>假设 1：独立同分布</strong>。大多数 benchmark 假设测试样本是从某个目标分布中独立采样的。但实际上，benchmark 题目的选择往往有强烈的偏见——MMLU 偏向西方教育体系的知识，GLUE 偏向英语互联网文本的分布。模型在 benchmark 上的高分不意味着它在真实使用场景中同样表现良好。</p>
<p><strong>假设 2：任务可分解性</strong>。GLUE 和 HELM 假设语言理解可以分解为若干独立的子任务（情感分析、蕴含判断等），且在这些子任务上的表现可以综合反映整体能力。但语言理解可能是一个整体性的能力，无法简单分解。</p>
<p><strong>假设 3：人类基线的可比性</strong>。几乎所有 benchmark 都以人类表现作为参考。但人类和 LLM 的错误模式截然不同——人类可能因粗心犯简单错误，而 LLM 可能犯人类不可能犯的”荒谬”错误。直接比较人类和模型的分数可能具有误导性。</p>
<p><strong>假设 4：评测与应用的一致性</strong>。benchmark 分数应该能够预测模型在实际应用中的表现。但由于分布偏移、prompt 敏感性和数据污染，这种预测关系往往很弱。许多用户反馈”benchmark 排名第一的模型在我的场景中表现不如排名第三的”。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</h3>
<p><strong>自动指标的边界</strong>：BLEU/ROUGE 在系统级比较时尚可接受（特别是在差距较大时），但在句子级评估和相近系统的比较中几乎不可靠。语义指标（BERTScore、COMET）有所改善，但在跨领域、跨语言场景中可能退化。</p>
<p><strong>静态 benchmark 的边界</strong>：GLUE/MMLU 等 benchmark 在模型能力远低于人类时是有效的区分工具，但一旦模型接近或超过人类基线，benchmark 就失去了区分能力（“天花板效应”）。此外，benchmark 的有效寿命受到数据污染和针对性优化的限制——通常只有 1-3 年。</p>
<p><strong>LLM-as-Judge 的边界</strong>：当被评模型的能力远低于评判模型时，LLM-as-Judge 工作良好。但当被评模型接近或超过评判模型时，评判结果的可靠性急剧下降。这创造了一个”玻璃天花板”：无法评测比评判者更强的模型。</p>
<p><strong>Chatbot Arena 的边界</strong>：需要大量真实用户参与，对小众模型或特定领域模型不友好。用户群体的偏见（如偏好英语、偏好编程任务）会影响排名的公平性。此外，Elo 评分假设模型能力是一维的——但现实中模型可能在不同维度（创意 vs 精度 vs 速度）上各有优劣。</p>
</section>
<section id="开放研究问题" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.4</span> 开放研究问题</h3>
<p><strong>问题 1：动态评测</strong>。如何设计一个能够持续生成新题目、不会被”刷满”的评测系统？一种思路是<strong>对抗性评测</strong>（adversarial evaluation）——自动生成能够揭示模型弱点的测试用例。但如何保证生成的题目既有意义又有正确答案？</p>
<p><strong>问题 2：多维评测的聚合</strong>。HELM 提出了 7 个评测维度，但如何将这 7 个维度聚合为一个可比较的分数？不同用户对不同维度的关注度不同（医疗领域更重视准确性，创意领域更重视多样性）。是否存在一种”帕累托最优”的聚合方式？</p>
<p><strong>问题 3：评测 Scalable Oversight</strong>。当模型的能力超过人类评判者时，如何评测？这是 AI 安全领域的一个核心问题。如果连人类专家都无法判断模型的回答是否正确（如高级数学定理的证明），我们如何建立可靠的评测？</p>
<p><strong>问题 4：评测公平性</strong>。现有 benchmark 严重偏向英语和西方知识体系。如何设计真正多语言、多文化公平的评测？MMLU 中”Abstract Algebra”的题目对一个从未接触过英语高等教育的人毫无意义，但这不代表他缺乏”智能”。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="当前评测方法论的核心局限" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="当前评测方法论的核心局限"><span class="header-section-number">7.1</span> 当前评测方法论的核心局限</h3>
<p>本章讨论的评测方法虽然代表了当前最先进的实践，但每一种都有根本性的局限。</p>
<p>静态 benchmark 受 Goodhart 定律支配，寿命有限。LLM-as-Judge 受限于评判模型自身的能力天花板和系统性偏差。Chatbot Arena 依赖大规模用户参与，且排名只反映”普通用户的平均偏好”而非特定任务的专业表现。所有方法都无法完全解决数据污染问题——特别是对闭源模型。</p>
<p>更根本的问题是，我们甚至缺乏对”模型能力”的清晰定义。当我们说一个模型”理解语言”或”具备推理能力”时，这些概念的精确含义是什么？没有清晰的定义，就不可能有完美的评测。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</h3>
<p>评测方法论的核心洞察——<strong>度量本身会影响我们对能力的判断</strong>——直接引出了下一章的主题：<strong>指令微调</strong>。</p>
<p>如果我们连”模型好不好”都难以可靠地衡量，那么”训练模型变好”就更加困难。指令微调（Instruction Tuning）的目标是让模型”听话”——能够理解并遵循人类的指令。但”听话”本身就是一个评测问题：什么样的回答算”好的回答”？谁来判断？这些问题将在 Ch23（指令微调）和 Ch24（RLHF）中得到进一步讨论。</p>
<p>从更宏观的视角看，评测问题是对齐（Alignment）问题的缩影。对齐的核心困难——如何定义和衡量”符合人类意图”——本质上就是一个评测问题。本章讨论的方法论（Bradley-Terry 模型、人类偏好收集、LLM-as-Judge）将在对齐章节中反复出现。</p>
<blockquote class="blockquote">
<p>下一章预告：第 23 章将讨论<strong>指令微调</strong>——如何通过在指令-回答数据上训练，让模型从”能力强但不好用”变成”既有能力又听话”。从 FLAN 到 Self-Instruct 到 Alpaca，指令数据的构建方法经历了怎样的演进？而评测这些”听话”模型的表现，正需要本章讨论的方法论。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>本章系统讲述了 NLP 评测方法论的演进，核心线索是<strong>评测与模型之间的”军备竞赛”</strong>——每一代评测方法都在修复上一代的缺陷，同时引入新的挑战。</p>
<p><strong>第一代自动指标</strong>（BLEU、ROUGE）解决了”如何自动评测”的问题，通过 n-gram 匹配量化文本质量。BLEU 的核心公式是修正 n-gram 精度加短句惩罚，ROUGE 则侧重召回率。但 n-gram 匹配无法捕捉语义——同义替换、释义重写都会导致低分。</p>
<p><strong>第二代理解能力 benchmark</strong>（GLUE、SuperGLUE、MMLU、HELM、BIG-bench）解决了”如何评测理解能力”的问题，通过精心设计的任务集测试模型的多种语言理解能力。但静态 benchmark 受 Goodhart 定律支配——发布、追逐、饱和、失效的循环通常只需 1-3 年。数据污染更是从根本上威胁了评测结果的可信度。</p>
<p><strong>第三代生成式评测</strong>（LLM-as-Judge、Chatbot Arena）解决了”如何评测开放式生成”的问题。LLM-as-Judge 通过让 GPT-4 等强模型担任评判者，实现了大规模自动评测。Chatbot Arena 通过众包真实用户的盲品偏好，建立了 Elo 排名系统。但 LLM 评判者存在位置偏差、冗长偏差和自我偏好，而 Chatbot Arena 的用户群体偏见也不可忽视。</p>
<p>贯穿全章的核心教训是 <strong>Goodhart 定律</strong>：“当一个度量成为目标时，它就不再是好的度量。”这不仅适用于 NLP 评测，也是理解 AI 对齐问题的关键视角。</p>
</section>
<section id="关键对比速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键对比速查"><span class="header-section-number">8.2</span> 关键对比速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>评测方法</th>
<th>时代</th>
<th>代表</th>
<th>优势</th>
<th>核心局限</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>自动指标</strong></td>
<td>2002–</td>
<td>BLEU, ROUGE</td>
<td>全自动、可复现</td>
<td>与人类判断相关性低</td>
</tr>
<tr class="even">
<td><strong>语义指标</strong></td>
<td>2020–</td>
<td>BERTScore, COMET</td>
<td>捕捉语义相似度</td>
<td>依赖预训练模型</td>
</tr>
<tr class="odd">
<td><strong>静态 Benchmark</strong></td>
<td>2018–</td>
<td>GLUE, MMLU</td>
<td>标准化、可比较</td>
<td>Goodhart 定律、数据污染</td>
</tr>
<tr class="even">
<td><strong>多维评测</strong></td>
<td>2022–</td>
<td>HELM</td>
<td>全面性</td>
<td>维度聚合困难</td>
</tr>
<tr class="odd">
<td><strong>LLM-as-Judge</strong></td>
<td>2023–</td>
<td>MT-Bench</td>
<td>评测开放生成</td>
<td>评判者偏差、能力天花板</td>
</tr>
<tr class="even">
<td><strong>人类偏好</strong></td>
<td>2023–</td>
<td>Chatbot Arena</td>
<td>最接近真实使用</td>
<td>用户偏见、规模需求</td>
</tr>
</tbody>
</table>
</section>
<section id="关键公式速查" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.3</span> 关键公式速查</h3>
<ul>
<li><strong>BLEU</strong>：<span class="math inline">\(\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)\)</span></li>
<li><strong>短句惩罚</strong>：<span class="math inline">\(\text{BP} = e^{\min(0, 1 - r/c)}\)</span></li>
<li><strong>Elo 预期胜率</strong>：<span class="math inline">\(E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}\)</span></li>
<li><strong>Bradley-Terry</strong>：<span class="math inline">\(P(i \succ j) = \sigma(\beta_i - \beta_j)\)</span></li>
</ul>
</section>
<section id="思考题" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.4</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 解释 Goodhart 定律在 NLP 评测中的表现。为什么 GLUE 在发布仅两年后就”失效”了？如果你是 benchmark 设计者，你会采取什么策略来延长 benchmark 的有效寿命？</p></li>
<li><p><strong>[数学推导]</strong> 在 Elo 评分系统中，假设模型 A 的 Elo 为 1400，模型 B 的 Elo 为 1000。(a) 计算 A 对 B 的预期胜率。(b) 如果 A 获胜，双方的 Elo 如何更新（<span class="math inline">\(K=32\)</span>）？(c) 如果连续 5 场 B 都爆冷获胜，A 和 B 的 Elo 会变成多少？(d) 讨论 Elo 系统的一个局限：为什么一维评分可能不足以描述 LLM 的能力？</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用 <code>lm-evaluation-harness</code> 框架，在 MMLU 的一个子集（如 abstract_algebra 或 high_school_mathematics）上评测一个开源模型。报告：(a) 5-shot 准确率及 95% 置信区间，(b) 将同样的题目改写（paraphrase），重新评测，观察分数变化，(c) 讨论这种不稳定性对评测结论的影响。</p></li>
<li><p><strong>[研究思考]</strong> 当被评模型的能力超过评判模型时，LLM-as-Judge 会面临什么问题？设计一个实验来检验这种”评判天花板”效应。如果未来的模型在所有维度上都超过 GPT-4，我们应该如何评测它们？</p></li>
<li><p><strong>[开放思考]</strong> 假设你要设计一个”不会过时”的 LLM 评测系统。你会采用什么架构？考虑：动态题目生成、多维能力评测、防数据污染机制、人机混合评判。讨论每个设计选择的 trade-off。</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Wang, A. et al.&nbsp;(2018). “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”</strong>。理解能力评测的里程碑。重点阅读：Section 2（任务设计原则）。即使 GLUE 已”过时”，它的设计思想仍是后续 benchmark 的基础。<a href="https://arxiv.org/abs/1804.07461">arXiv:1804.07461</a></p>
<p><strong>Zheng, L. et al.&nbsp;(2023). “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena”</strong>。生成式评测新范式的代表作。重点阅读：Section 3（LLM-as-Judge 方法论）、Section 4（偏差分析）。<a href="https://arxiv.org/abs/2306.05685">arXiv:2306.05685</a></p>
<p><strong>Liang, P. et al.&nbsp;(2022). “Holistic Evaluation of Language Models”</strong>。最全面的评测框架设计。重点阅读：Section 2（评测分类学）。可快速浏览：具体模型的结果（过时较快）。<a href="https://arxiv.org/abs/2211.09110">arXiv:2211.09110</a></p>
</section>
<section id="经典指标" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="经典指标"><span class="header-section-number">9.2</span> 经典指标</h3>
<p><strong>Papineni, K. et al.&nbsp;(2002). “BLEU: a Method for Automatic Evaluation of Machine Translation”</strong>。自动评测的开山之作。重点阅读：Section 2（BLEU 公式定义）。20 多年后仍被广泛使用。<a href="https://aclanthology.org/P02-1040/">ACL 2002</a></p>
<p><strong>Zhang, T. et al.&nbsp;(2020). “BERTScore: Evaluating Text Generation with BERT”</strong>。语义评测指标的代表。<a href="https://arxiv.org/abs/1904.09675">arXiv:1904.09675</a></p>
</section>
<section id="数据污染与科学诚实-1" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="数据污染与科学诚实-1"><span class="header-section-number">9.3</span> 数据污染与科学诚实</h3>
<p><strong>Oren, Y. et al.&nbsp;(2023). “Proving Test Set Contamination in Black Box Language Models”</strong>。黑盒模型的污染检测——无需访问训练数据即可推断污染。<a href="https://arxiv.org/abs/2310.17623">arXiv:2310.17623</a></p>
<p><strong>Deng, C. et al.&nbsp;(2023). “Investigating Data Contamination in Modern Benchmarks for Large Language Models”</strong>。TS-Guessing 方法——通过给模型部分输入来检测是否”见过”benchmark 题目。<a href="https://arxiv.org/abs/2311.09783">arXiv:2311.09783</a></p>
<p><strong>Golchin, S. &amp; Surdeanu, M. (2023). “Time Travel in LLMs: Tracing Data Contamination in Large Language Models”</strong>。Time Travel 方法——测试模型是否能逐字复现 benchmark 原文。<a href="https://arxiv.org/abs/2308.08493">arXiv:2308.08493</a></p>
<p><strong>Schaeffer, R. et al.&nbsp;(2023). “Are Emergent Abilities of Large Language Models a Mirage?”</strong>。度量选择对能力判断的影响。NeurIPS 2023 最佳论文。<a href="https://arxiv.org/abs/2304.15004">arXiv:2304.15004</a></p>
</section>
<section id="后续发展" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.4</span> 后续发展</h3>
<p><strong>Chiang, W. et al.&nbsp;(2024). “Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference”</strong>。Chatbot Arena 的完整技术报告。<a href="https://arxiv.org/abs/2403.04132">arXiv:2403.04132</a></p>
<p><strong>Hendrycks, D. et al.&nbsp;(2020). “Measuring Massive Multitask Language Understanding”</strong>。MMLU benchmark。<a href="https://arxiv.org/abs/2009.03300">arXiv:2009.03300</a></p>
</section>
<section id="综述与教程" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">9.5</span> 综述与教程</h3>
<p><strong>Chang, Y. et al.&nbsp;(2024). “A Survey on Evaluation of Large Language Models”</strong>。LLM 评测方法的全面综述。<a href="https://arxiv.org/abs/2307.03109">arXiv:2307.03109</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.6">
<h3 data-number="9.6" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.6</span> 代码资源</h3>
<ul>
<li><strong>lm-evaluation-harness</strong>：<a href="https://github.com/EleutherAI/lm-evaluation-harness">github.com/EleutherAI/lm-evaluation-harness</a>（最广泛使用的评测框架）</li>
<li><strong>OpenCompass</strong>：<a href="https://github.com/open-compass/opencompass">github.com/open-compass/opencompass</a>（支持中文评测）</li>
<li><strong>Chatbot Arena</strong>：<a href="https://chat.lmsys.org/">chat.lmsys.org</a>（在线参与评测）</li>
<li><strong>HELM</strong>：<a href="https://crfm.stanford.edu/helm/">crfm.stanford.edu/helm</a>（Stanford 评测平台）</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>NLP 评测的历史充满了意料之外的转折。</p>
<p>BLEU 的发明者 Kishore Papineni 最初是在 IBM 研究院工作时提出这个指标的。他的动机非常实际：机器翻译研究者需要一个自动化的评测方法来快速迭代系统，而人类评测太慢太贵。BLEU 论文发表于 2002 年 ACL，至今已被引用超过 20,000 次，是 NLP 领域被引用最多的论文之一。然而 Papineni 本人后来在多个场合表示，BLEU 被过度使用了——它从来不是为了替代人类判断，而只是一个快速的近似。</p>
<p>GLUE 的故事则带有一丝戏剧性。当 Sam Wang 等人在 2018 年设计 GLUE 时，他们认为这个 benchmark 至少能管用好几年。没想到仅仅几个月后，BERT 就让 GLUE 的排行榜变成了”卷王争霸赛”。Wang 等人不得不紧急设计 SuperGLUE——这可能是 NLP 历史上最快被自己的成功逼迫”升级”的 benchmark。</p>
<p>Chatbot Arena 的诞生同样充满偶然。LMSYS（Large Model Systems Organization）最初是 UC Berkeley 一群研究系统优化的学生组建的，他们的初衷是研究 LLM 推理效率。但当他们搭建了一个供公众体验不同模型的网站后，用户自发开始比较不同模型的回答质量。这个”副产品”——用户自发的模型比较——最终演变成了整个 LLM 领域最被信赖的排名系统。</p>
<p>评测方法论的演进还在继续。2024-2025 年，社区开始探讨”活性 benchmark”（living benchmarks）的概念——benchmark 不再是一次性发布的静态数据集，而是持续更新、动态生成、自动防污染的活系统。这或许是应对 Goodhart 定律的下一步尝试，但历史告诉我们，每一次尝试最终都会遇到新的挑战。正如一位评审人幽默地总结的：“评测方法论的唯一不变定律是，没有评测方法是永恒的。”</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第22章：评测方法论——如何判断模型变好了？"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Evaluation Methodology: Benchmarks, Metrics, and the Science of Measuring AI"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, Evaluation, Benchmarks, Metrics]</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [GLUE, SuperGLUE, MMLU, HELM, BigBench, Chatbot Arena, LLM-as-Judge, BLEU, ROUGE, Data Contamination, Goodhart, Evaluation Methodology]</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "上一章关于涌现能力的讨论揭示了一个令人不安的事实：度量方式本身可能扭曲我们对模型能力的认知。如果连'涌现'这个核心概念的真实性都取决于评测度量的选择，那么我们对LLM能力的所有判断都需要更加审慎。本章系统讲述NLP评测方法论的演进：从BLEU/ROUGE等自动指标的局限，到GLUE/SuperGLUE/MMLU等静态benchmark的兴衰，再到LLM-as-Judge和Chatbot Arena等生成式评测的新范式。我们将讨论数据污染、Goodhart定律、评测者偏差等核心问题，试图回答一个根本性的问题：什么才是好的评测？"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-22/original/fig1-evaluation-timeline.png"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：Benchmark 越刷越高，模型真的越来越强吗？我们如何设计不被"博弈"的评测体系？</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2018–2024 </span><span class="pp">|</span><span class="at"> GLUE → SuperGLUE → MMLU → HELM → Chatbot Arena </span><span class="pp">|</span><span class="at"> 从静态 benchmark 到动态人类偏好评测</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wang et al. (2018)** "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding" (arXiv:1804.07461) — 参考了 Section 2-3（任务设计与基线）、Table 1（9个任务定义）</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wang et al. (2019)** "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems" (arXiv:1905.00537) — 参考了 Section 2（任务选择标准）、Table 1（8个任务）</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hendrycks et al. (2020)** "Measuring Massive Multitask Language Understanding" (arXiv:2009.03300) — 参考了 Section 2-3（57学科设计）、Figure 1（学科分布）</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liang et al. (2022)** "Holistic Evaluation of Language Models" (arXiv:2211.09110) — 参考了 Section 2（评测分类学：7个场景×7个度量维度）、Figure 1（HELM框架图）</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Srivastava et al. (2022)** "Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models" (arXiv:2206.04615) — 参考了 Section 2（BIG-bench 200+任务设计）</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zheng et al. (2023)** "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena" (arXiv:2306.05685) — 参考了 Section 2-4（MT-Bench设计、Chatbot Arena Elo系统、LLM-as-Judge偏差分析）、Figure 1（评测框架图）；提取了论文原图</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lin et al. (2021)** "TruthfulQA: Measuring How Models Mimic Human Falsehoods" (arXiv:2109.07958) — 参考了 Section 2-3（事实性评测设计）</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Schaeffer et al. (2023)** "Are Emergent Abilities of Large Language Models a Mirage?" (arXiv:2304.15004) — 参考了 Section 2-3（度量选择对涌现判断的影响），延续Ch21讨论；提取了 Figure 2（涌现消失的多面板对比图）</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Papineni et al. (2002)** "BLEU: a Method for Automatic Evaluation of Machine Translation" (ACL 2002) — 参考了 Section 2（BLEU公式定义）</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lin (2004)** "ROUGE: A Package for Automatic Evaluation of Summaries" (ACL 2004 Workshop) — 参考了 ROUGE-N/L 定义</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Oren et al. (2023)** "Proving Test Set Contamination in Black Box Language Models" (arXiv:2310.17623) — 参考了分区对比数据污染检测方法</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deng et al. (2023)** "Investigating Data Contamination in Modern Benchmarks for Large Language Models" (arXiv:2311.09783) — 参考了 TS-Guessing 方法（通过部分输入检测数据污染）</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Golchin &amp; Surdeanu (2023)** "Time Travel in LLMs: Tracing Data Contamination in Large Language Models" (arXiv:2308.08493) — 参考了 Time Travel 方法（通过逐字复现检测数据污染）</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SLP3 Chapter 4 (Logistic Regression and Text Classification, Jan 2026 draft) — 参考了 precision/recall/F1/confusion matrix 的经典教学（Fig. 4.4）</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SLP3 Chapter 7 (Large Language Models) — 参考了 MMLU benchmark 的教学呈现（Fig. 7.16：2-shot prompting 示例）、Rawlsian 公平性评测</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SLP3 Chapter 12 (Machine Translation) — 参考了 BLEU、chrF、BERTScore、COMET/BLEURT 的完整评测指标演进，以及 paired bootstrap 统计显著性检验</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>D2L Section 10.7 (Sequence-to-Sequence Learning) — 参考了 BLEU 的数值演算示例（p₁=4/5, p₂=3/4, p₃=1/3）和可运行 Python 实现</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stanford CS224N Lecture 11 (Spring 2024, Yann Dubois guest lecture) — 专题 "Benchmarking and Evaluation"，系统讲解了从 GLUE/MMLU 到 LLM-as-Judge 的评测演进、MMLU 错误率问题、评测不一致性（如 Gemini 非标准 MMLU prompting）</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CMU 11-711 ANLP Lecture 17 (Fall 2024, Graham Neubig) — "Evaluation and Multimodal"，覆盖 GLUE、MMLU、HELM、Chatbot Arena、MT-Bench 等评测体系</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Princeton COS 597R Lecture 5 (Fall 2024, Sanjeev Arora) — "Emergent Behavior"，讨论 Schaeffer et al. 的涌现幻觉论文及评测度量选择问题</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>上一章我们讲述了 Chain-of-Thought 推理和涌现能力的故事。CoT 通过在 few-shot 示例中展示推理过程，让大语言模型在数学推理上获得了飞跃式提升，而这种能力呈现出令人惊叹的"涌现"特征——小模型完全不受益，大模型突然爆发。</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>然而，上一章结尾埋下的伏笔令人不安。Schaeffer et al.（2023）的 NeurIPS 最佳论文表明，所谓的"涌现"可能很大程度上是**度量方式制造的假象**：同一份实验数据，使用精确匹配（Exact Match）度量时呈现出震撼的"阶跃"曲线，换成连续度量后却变成了平滑增长。一个被整个领域热烈讨论的"相变现象"，居然可能只是我们选择了错误的温度计。</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="al">![Schaeffer et al. (2023) 的核心发现：同一组模型在同一批任务上的表现，左侧使用非线性度量（如 Exact Match）时呈现"涌现"式阶跃，右侧换成连续度量（如 Token Edit Distance、Brier Score）后变为平滑增长。"涌现"消失了。](figures/chapter-22/original/fig4-emergence-mirage-schaeffer2023.png)</span>{#fig-emergence-mirage width=85%}</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>*Source: Schaeffer, R., Miranda, B., &amp; Koyejo, S. (2023) "Are Emergent Abilities of Large Language Models a Mirage?", Figure 2. NeurIPS 2023 Outstanding Paper Award. [arXiv:2304.15004](https://arxiv.org/abs/2304.15004)*</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>这个发现的冲击远超涌现本身。它触及了一个更深层的问题：**如果评测方式可以根本性地改变我们对模型能力的判断，那么我们对 LLM 的所有评估——从 GLUE 排行榜到 ChatGPT "超越人类"的宣传——有多少是真实的，有多少是度量假象？**</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>这个问题不是学术象牙塔里的自娱自乐。当企业根据 benchmark 分数决定部署哪个模型，当政府根据评测结果制定 AI 监管政策，当研究者根据排行榜排名规划研究方向——评测的科学性直接关系到数十亿美元的决策和整个领域的研究方向。</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：评测不是"选个 benchmark 跑个分"那么简单。它是一门方法论，受 Goodhart 定律笼罩——"当一个度量成为目标时，它就不再是好的度量"。从 BLEU 到 GLUE 到 Chatbot Arena，NLP 评测的每一次演进都是对前一代评测失效模式的回应。理解这段演进史，是成为严谨研究者的必修课。</span></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么评测如此困难？</span></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>评测看似是一个简单的工程问题：设计一组测试题，让模型回答，计算分数。但实际上，NLP 评测面临一系列根本性的困难，每一个都足以让整个评测体系崩塌。</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>第一个困难是**能力的多维性**。一个语言模型需要同时具备语法理解、常识推理、事实知识、数学计算、代码生成、创意写作等数十种能力。任何单一 benchmark 都只能覆盖其中一小部分。用 MMLU 分数代表"模型智能"，就像用数学考试成绩代表"学生能力"一样片面。</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>第二个困难是**Goodhart 定律**（Goodhart's Law）。英国经济学家 Charles Goodhart 在 1975 年观察到："当一个度量成为目标时，它就不再是好的度量。"这在 NLP 领域体现得淋漓尽致：一旦某个 benchmark 成为社区追逐的目标，模型就会被专门优化来"刷分"，而这种优化往往不反映真正的能力提升。GLUE 排行榜在两年内被"刷满"（超过人类基线），但没有人真的相信 2020 年的模型已经"理解了语言"。</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>第三个困难是**评测对象的根本变化**。传统 NLP 评测假设模型是某个特定任务的求解器（分类器、翻译器、问答系统）。但 GPT-3 以来的 LLM 是通用的语言生成器——它们的输出是开放式的自然语言文本。如何评估一段自由文本的"质量"？这几乎是一个哲学问题。两个人类专家对同一段文本的评价都可能大相径庭。</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>第四个困难是**数据污染**（Data Contamination）。现代 LLM 的训练数据规模达到数万亿 token，覆盖了互联网上几乎所有公开文本。这意味着 benchmark 的测试题很可能已经出现在训练数据中——模型可能是"背出了答案"而非"理解了问题"。更棘手的是，对于闭源模型，外部研究者甚至无法验证是否存在数据污染。</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a><span class="fu">### Goodhart 定律：评测领域的永恒诅咒</span></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>让我们更深入地理解 Goodhart 定律如何具体影响 NLP 评测。这个定律有一个更精确的表述，由人类学家 Marilyn Strathern 给出："当一个度量成为目标时，它就不再是好的度量。"在 NLP 领域，这个定律以惊人的规律性反复上演：</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>\text{Benchmark 发布} \xrightarrow{\text{社区竞争}} \text{针对性优化} \xrightarrow{\text{分数饱和}} \text{Benchmark 失效} \xrightarrow{\text{需要新 Benchmark}}</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a><span class="al">![Goodhart 循环：NLP benchmark 从发布到失效的必经之路。GLUE 在不到两年内走完了全部四个阶段。](figures/chapter-22/original/fig2-goodhart-cycle.png)</span>{#fig-goodhart-cycle width=70%}</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>*作者绘制。概念基于 Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."*</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>GLUE 的故事就是一个教科书级的案例。2018 年 GLUE 发布时，最好的系统在综合分上只有约 69 分（人类基线 87 分）。BERT 在几个月内将分数提升到 80 以上。到 2019 年底，多个模型超过了人类基线。到 2020 年，排行榜前列的模型之间差距微乎其微——benchmark 丧失了区分能力。</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>SuperGLUE 于 2019 年发布，专门针对 GLUE 的不足设计了更难的任务。但它的寿命同样不长——T5 等模型在 2020 年就逼近了人类基线。MMLU（2020）试图通过覆盖 57 个学科来延长 benchmark 的寿命。BIG-bench（2022）干脆众包了 200+ 个任务。但每一个 benchmark 都面临同样的命运：**发布、追逐、饱和、失效**。</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>一个自然的问题是：我们能否设计一个"不会饱和"的 benchmark？答案很可能是否定的，至少对于静态 benchmark 而言。只要测试题是固定的，只要训练数据足够大，模型终将覆盖所有题目。这就是为什么评测方法论需要持续演进——从静态 benchmark 到动态评测，从自动指标到人类判断，从固定题目到对抗性构造。</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a><span class="fu">### 评测方法论的三次范式转变</span></span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>如果我们将 NLP 评测的历史画成一张演进图（见 @fig-eval-timeline），会清晰地看到三次范式转变，每一次都是对前一代评测失效模式的回应。</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a><span class="al">![NLP 评测方法论的演进：从 BLEU（2002）到 Chatbot Arena（2023），评测经历了自动指标、静态 benchmark、生成式评测三个时代。](figures/chapter-22/original/fig1-evaluation-timeline.png)</span>{#fig-eval-timeline width=95%}</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>*作者绘制。时间线数据来源：各论文原始发表年份（BLEU: Papineni et al. 2002; GLUE: Wang et al. 2018; MMLU: Hendrycks et al. 2020; Chatbot Arena: Zheng et al. 2023 等）。*</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>**第一代：自动指标时代（2002–2017）**。以 BLEU（机器翻译）和 ROUGE（文本摘要）为代表。核心思想是将文本质量量化为 n-gram 匹配的分数——一个可以全自动计算、不需要人类参与的数字。这让大规模评测成为可能，但代价是指标与真实文本质量之间的相关性有限。一个模型可以获得高 BLEU 分数，输出却读起来像机器生成的拼凑文本。</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a>**第二代：理解能力 benchmark 时代（2018–2022）**。以 GLUE、SuperGLUE、MMLU、HELM 为代表。核心思想是设计一组精心挑选的任务，每个任务测试特定的语言理解能力（推理、蕴含、常识等），用准确率等清晰的度量衡量模型表现。这种方式比 BLEU 精确得多，但面临静态 benchmark 的固有困境——数据泄漏、过拟合、饱和。</span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>**第三代：生成式评测时代（2023–）**。以 LLM-as-Judge（MT-Bench）和 Chatbot Arena 为代表。核心思想是用另一个 LLM（或众包人类）来评判模型的开放式输出。这解决了"如何评估自由文本"的难题，但引入了新的问题——评判者自身的偏差（LLM 评判者倾向于偏好更长、更华丽的回答）。</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>每一代评测方法都修复了上一代的某些痛点，但也引入了新的局限性。理解这种"修复→新问题→再修复"的循环，是理解评测方法论的关键。</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个类比：评测就像考试制度</span></span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a>把 NLP 评测比作教育中的考试制度，可以帮助建立直觉。</span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>BLEU 就像用"学生作文中有多少个正确单词"来评分——简单粗暴，但完全忽略了语义和连贯性。GLUE 像是一场标准化考试（如 SAT）——精心设计、有明确的评分标准，但"应试教育"的模型可以通过刷题获得高分而不真正理解知识。Chatbot Arena 则像是让两个学生当面辩论，由观众投票决定谁更厉害——更接近"真实能力"的评估，但观众可能被口才而非逻辑说服。</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>没有完美的考试制度，也没有完美的评测方法。好的评测方法论不是追求"终极 benchmark"，而是理解每种评测方式的优势、局限和适用场景。</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a><span class="fu">### 第一代：自动指标</span></span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BLEU：n-gram 精度与短句惩罚</span></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a>BLEU（Bilingual Evaluation Understudy，Papineni et al., 2002）是机器翻译领域最经典的自动评测指标。它的核心思想出奇地简单：比较机器翻译输出和参考译文之间的 n-gram 重合度。</span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a>BLEU 的计算分为两部分。第一部分是**修正的 n-gram 精度**（Modified n-gram Precision）。对于每个 n-gram，统计它在候选翻译中出现的次数，但用参考译文中出现的最大次数进行裁剪（clipping），避免一个词被重复计数：</span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>p_n = \frac{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}_{\text{clip}}(\text{n-gram})}{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}(\text{n-gram})}</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a>第二部分是**短句惩罚**（Brevity Penalty, BP）。如果没有这个惩罚，模型可以通过只输出最有把握的几个词来获得很高的精度。BP 的设计是：当候选翻译的长度 $c$ 短于参考翻译的有效长度 $r$ 时，施加指数惩罚：</span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>\text{BP} = \begin{cases} 1 &amp; \text{if } c &gt; r <span class="sc">\\</span> e^{1 - r/c} &amp; \text{if } c \leq r \end{cases}</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a>最终的 BLEU 分数将两者结合：</span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a>其中 $w_n$ 通常取均匀权重 $1/N$，$N$ 通常取 4（即 BLEU-4，考虑 1-gram 到 4-gram）。</span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：BLEU 计算</span></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a>**设定**：候选翻译（Candidate）和参考翻译（Reference）如下：</span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Reference**: "the cat sat on the mat"</span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Candidate**: "the the the the"</span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a>**Step 1: 1-gram 精度（不裁剪）**</span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a>Candidate 中 "the" 出现 4 次，Reference 中也有 "the"。如果不裁剪，$p_1 = 4/4 = 1.0$——满分！但这显然是荒谬的。</span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>**Step 2: 1-gram 精度（裁剪后）**</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a>"the" 在 Reference 中最多出现 2 次，所以裁剪后计数为 $\min(4, 2) = 2$：</span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a>p_1 = \frac{2}{4} = 0.5</span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a>**Step 3: 更正常的例子**</span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>换一个更合理的候选翻译：</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Reference**: "the cat sat on the mat"（6 个词）</span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Candidate**: "the cat on the mat"（5 个词）</span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a>1-gram 精度：5 个词中，"the"（clip 到 2）、"cat"（1）、"on"（1）、"mat"（1），共 5 个匹配。$p_1 = 5/5 = 1.0$。</span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a>2-gram 精度：Candidate 的 2-gram 有 "the cat"、"cat on"、"on the"、"the mat"。Reference 的 2-gram 有 "the cat"、"cat sat"、"sat on"、"on the"、"the mat"。匹配 3 个（"the cat"、"on the"、"the mat"），共 4 个。$p_2 = 3/4 = 0.75$。</span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a>**Step 4: 短句惩罚**</span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a>$c = 5$，$r = 6$，$c &lt; r$，所以：</span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a>\text{BP} = e^{1 - 6/5} = e^{-0.2} \approx 0.819</span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a>**Step 5: 最终 BLEU-2 分数**（只用 1-gram 和 2-gram）</span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a>\text{BLEU-2} = 0.819 \times \exp\left(\frac{1}{2} \ln 1.0 + \frac{1}{2} \ln 0.75\right) = 0.819 \times \exp(-0.144) = 0.819 \times 0.866 \approx 0.709</span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a>**解读**：候选翻译遗漏了 "sat"，2-gram 中 "cat on" 不匹配，短句惩罚进一步扣分，最终得到 0.709。这个分数合理地反映了翻译的质量。</span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BLEU 的局限性</span></span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a>BLEU 的问题在本质上是 n-gram 匹配的局限。它无法识别同义词替换（"purchase" 和 "buy" 被视为完全不同），对语序的敏感性有限（只在高阶 n-gram 中间接体现），完全忽略了语义连贯性和流畅度。更深层的问题是，BLEU 假设存在"标准答案"——但翻译是一个一对多的问题，一个源句可能有多种同样好的译法。</span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a>Callahan et al. (2006) 的研究表明，BLEU 与人类翻译质量评判的相关性在系统级别（比较不同翻译系统）尚可接受，但在句子级别极不可靠。这意味着你可以用 BLEU 大致比较两个翻译系统的整体水平，但不能用它判断某个具体句子翻译得好不好。</span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ROUGE：召回导向的摘要评测</span></span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a>ROUGE（Recall-Oriented Understudy for Gisting Evaluation，Lin, 2004）是文本摘要领域的标准指标。与 BLEU 侧重精度（候选文本中有多少 n-gram 出现在参考中）不同，ROUGE 侧重**召回率**（参考文本中有多少 n-gram 被候选文本覆盖）。这反映了摘要任务的特点：好的摘要应该覆盖原文的关键信息。</span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a>ROUGE 有多个变体。ROUGE-N 计算 n-gram 的召回率：</span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a>\text{ROUGE-N} = \frac{\sum_{S \in \text{Ref}} \sum_{\text{n-gram} \in S} \text{Count}_{\text{match}}(\text{n-gram})}{\sum_{S \in \text{Ref}} \sum_{\text{n-gram} \in S} \text{Count}(\text{n-gram})}</span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a>ROUGE-L 使用**最长公共子序列**（Longest Common Subsequence, LCS）来衡量句子级的结构相似性，不要求 n-gram 连续出现：</span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a>\text{ROUGE-L} = F_{\text{LCS}} = \frac{(1 + \beta^2) \cdot R_{\text{LCS}} \cdot P_{\text{LCS}}}{R_{\text{LCS}} + \beta^2 \cdot P_{\text{LCS}}}</span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a>其中 $R_{\text{LCS}} = \text{LCS}(X, Y) / m$，$P_{\text{LCS}} = \text{LCS}(X, Y) / n$，$m$ 和 $n$ 分别是参考和候选的长度。</span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a>ROUGE 面临与 BLEU 类似的根本局限：n-gram 匹配无法捕捉语义。一个改写了措辞但保留了核心意思的摘要，可能获得很低的 ROUGE 分数。</span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 超越 n-gram：语义评测指标</span></span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a>BLEU 和 ROUGE 的局限催生了一系列基于语义的评测指标。**BERTScore**（Zhang et al., 2020）使用 BERT 的上下文嵌入来计算候选文本和参考文本之间的相似度，对同义词和释义更加鲁棒。**BLEURT**（Sellam et al., 2020）在 BERT 基础上进一步微调，学习预测人类质量评分。**COMET**（Rei et al., 2020）在机器翻译评测中引入源句信息，训练一个跨语言模型来预测翻译质量。</span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a>这些指标在与人类判断的相关性上显著优于 BLEU/ROUGE，但它们也引入了新的依赖：评测指标本身依赖于一个预训练模型。当被评测的模型和评测指标使用相同的预训练基础时，可能产生系统性偏差。</span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a><span class="fu">### 第二代：理解能力 benchmark</span></span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GLUE 与 SuperGLUE：多任务理解评测</span></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a>GLUE（General Language Understanding Evaluation，Wang et al., 2018）的设计理念是：一个真正"理解语言"的模型应该能够同时处理多种自然语言理解任务。GLUE 包含 9 个任务，涵盖了语言理解的多个维度：</span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> 类型 <span class="pp">|</span> 描述 <span class="pp">|</span> 度量 <span class="pp">|</span> 示例 <span class="pp">|</span></span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|------|------|------|</span></span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **MNLI** <span class="pp">|</span> 蕴含 <span class="pp">|</span> 三分类（蕴含/矛盾/中性） <span class="pp">|</span> Accuracy <span class="pp">|</span> "猫在垫子上" ⇒ "有动物在家具上"? <span class="pp">|</span></span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **QQP** <span class="pp">|</span> 释义 <span class="pp">|</span> 两个问题是否语义相同 <span class="pp">|</span> F1/Acc <span class="pp">|</span> "How old are you" ≈ "What is your age"? <span class="pp">|</span></span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **QNLI** <span class="pp">|</span> 问答/蕴含 <span class="pp">|</span> 句子是否包含问题的答案 <span class="pp">|</span> Accuracy <span class="pp">|</span> Q: "When did X happen?" + 候选句 <span class="pp">|</span></span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SST-2** <span class="pp">|</span> 情感 <span class="pp">|</span> 电影评论正/负情感 <span class="pp">|</span> Accuracy <span class="pp">|</span> "This movie was wonderful" → 正面 <span class="pp">|</span></span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **CoLA** <span class="pp">|</span> 语法 <span class="pp">|</span> 句子是否语法正确 <span class="pp">|</span> Matthews Corr <span class="pp">|</span> "The boy the cat bit ran" → ? <span class="pp">|</span></span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **STS-B** <span class="pp">|</span> 相似度 <span class="pp">|</span> 两句话的语义相似度（1-5分） <span class="pp">|</span> Pearson/Spearman <span class="pp">|</span> 回归任务 <span class="pp">|</span></span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **MRPC** <span class="pp">|</span> 释义 <span class="pp">|</span> 两个句子是否语义等价 <span class="pp">|</span> F1/Acc <span class="pp">|</span> 新闻句子对 <span class="pp">|</span></span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **RTE** <span class="pp">|</span> 蕴含 <span class="pp">|</span> 二分类蕴含 <span class="pp">|</span> Accuracy <span class="pp">|</span> 前提→假设 <span class="pp">|</span></span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **WNLI** <span class="pp">|</span> 共指 <span class="pp">|</span> Winograd 共指消解 <span class="pp">|</span> Accuracy <span class="pp">|</span> 代词指代谁？ <span class="pp">|</span></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a>GLUE 的关键设计决策值得深入分析。首先，**任务多样性**：9 个任务覆盖了从句法（CoLA）到语义（STS-B）到推理（MNLI）的多个层次。其次，**单一分数**：所有任务的分数被平均为一个综合分数，方便排行榜比较。第三，**人类基线**：GLUE 公布了人类在每个任务上的表现，作为"天花板"参考。</span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a>然而，GLUE 的"寿命"出乎意料地短。BERT（2018年10月）发布后仅几个月，GLUE 排行榜就被"攻陷"——多个模型在综合分上接近甚至超过人类基线。这迫使设计者在 2019 年推出了 SuperGLUE，包含 8 个更具挑战性的任务：BoolQ（布尔问答）、CB（文本蕴含）、COPA（因果推理）、MultiRC（多句阅读理解）、ReCoRD（常识阅读理解）、RTE（增强版）、WiC（上下文词义）、WSC（Winograd 模式）。</span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a>SuperGLUE 的命运与 GLUE 类似——它在 2020 年也基本被"刷满"。这个"两年寿命"的模式揭示了静态 benchmark 的一个根本困境：**benchmark 的难度是固定的，但模型的能力在持续增长**。</span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a><span class="fu">#### MMLU：大规模多任务理解</span></span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a>MMLU（Massive Multitask Language Understanding，Hendrycks et al., 2020）试图通过**规模**来延长 benchmark 的寿命。它包含 57 个学科的 15,908 道选择题，覆盖从高中数学到专业法律、从天文学到计算机科学的广泛领域。每道题是四选一的选择题，如：</span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Abstract Algebra**: Let p = (1, 2, 5, 4)(2, 3) in S_5. Find the index of </span><span class="dt">&lt;</span><span class="kw">p</span><span class="dt">&gt;</span><span class="at"> in S_5.</span></span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; (A) 8  (B) 2  (C) 24  (D) 120</span></span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a>MMLU 的设计有几个值得注意的特点。首先，题目来源于真实的考试和教材，而非人工构造——这提高了"生态有效性"。其次，57 个学科的细粒度分类允许分析模型在不同知识领域的强弱。第三，四选一的格式使得评测完全自动化（只需检查模型选择了哪个选项）。</span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a>MMLU 很快成为 LLM 评测的"黄金标准"。GPT-4 的技术报告以 MMLU 分数作为核心能力指标之一（GPT-4: 86.4% vs GPT-3.5: 70.0%）。然而，MMLU 也面临自己的问题。选择题格式限制了评测的深度——模型可能通过排除法而非真正理解来答题。随着 LLM 训练数据的膨胀，MMLU 题目被"记住"的风险越来越高。</span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a>更令人不安的是，2024 年的一项质量审计揭示了 MMLU 本身的严重质量问题：**整体错误率约为 6.5%**，包括错误的标注答案、模糊的题目表述和过时的知识。某些学科的错误率远高于平均值——Virology（病毒学）子集的错误率高达 **57%**。这意味着当我们用 MMLU 分数来比较模型时，部分"错误"可能只是模型给出了比 benchmark 答案更正确的回答。这个发现从另一个角度动摇了 benchmark 评测的可信度：不仅模型可能"作弊"（数据污染），benchmark 本身也可能出错。</span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a><span class="fu">#### HELM：多维度整体评测</span></span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a>HELM（Holistic Evaluation of Language Models，Liang et al., 2022）是评测方法论上的一次重要尝试。它不再追求"更难的题目"，而是追求**更全面的评测维度**。HELM 定义了 7 个核心评测场景（Core Scenarios）和 7 个度量维度（Metrics）：</span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a>**评测场景**：问答、信息检索、文本摘要、情感分析、毒性检测、版权与公平性、对话。</span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a>**度量维度**：准确性（Accuracy）、校准性（Calibration）、鲁棒性（Robustness）、公平性（Fairness）、偏差（Bias）、毒性（Toxicity）、效率（Efficiency）。</span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a>HELM 的关键洞察是：一个模型不仅要"准确"，还要"校准良好"（知道自己什么时候不确定）、"鲁棒"（对输入扰动不敏感）、"公平"（对不同人群同等对待）。一个 MMLU 分数 90% 但输出大量有毒内容的模型，显然不应该被视为"好模型"。</span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a>这种多维度评测理念影响深远，后续的评测框架（如 OpenCompass、LM Evaluation Harness）都借鉴了 HELM 的设计哲学。</span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BIG-bench：众包任务的极端尝试</span></span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a>BIG-bench（Beyond the Imitation Game benchmark，Srivastava et al., 2022）走了另一条路：通过**众包**来构建超大规模 benchmark。444 位研究者贡献了 204 个任务，涵盖语言学、数学、常识推理、社会偏见等领域。BIG-bench Hard（BBH）是其中精选的 23 个特别困难的任务——这些任务上 Chain-of-Thought 才有显著效果。</span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a>BIG-bench 的贡献不仅是任务数量，更是评测方法论：它系统地分析了任务难度与模型规模的关系，为"涌现能力"的讨论提供了大量实证数据（也为 Schaeffer et al. 的反驳提供了分析素材）。</span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a><span class="fu">### 第三代：生成式评测的新范式</span></span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 为什么需要新范式？</span></span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a>第二代 benchmark 的核心假设是：模型的能力可以通过一组**有标准答案**的任务来衡量。但 ChatGPT 之后的 LLM 主要被用于**开放式生成**——写邮件、改代码、头脑风暴、角色扮演——这些任务没有唯一的"正确答案"。</span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a>想象你让两个 LLM 分别写一封求职信。模型 A 写了一封简洁有力的短信，模型 B 写了一封详尽周到的长信。哪个更好？这取决于求职者的风格偏好、目标公司的文化、甚至招聘经理的个人品味。传统 benchmark 无法处理这种主观性。</span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LLM-as-Judge：用模型评模型</span></span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a>Zheng et al.（2023）提出了一个大胆的方案：用一个强大的 LLM（如 GPT-4）作为评判者来评估其他模型的输出。@fig-llm-as-judge 展示了一个具体的评判过程：GPT-4 作为裁判，比较两个助手对同一个多轮问题的回答，并给出详细的评判理由。</span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a><span class="al">![LLM-as-Judge 的工作示例：GPT-4 评判两个 AI 助手对"联储在二级市场买债"问题的多轮回答。注意评判理由中对"重复性"和"具体性"的分析——这种细粒度反馈是传统 benchmark 无法提供的。](figures/chapter-22/original/fig3-llm-as-judge-zheng2023.png)</span>{#fig-llm-as-judge width=80%}</span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a>*Source: Zheng, L. et al. (2023) "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", Figure 1. [arXiv:2306.05685](https://arxiv.org/abs/2306.05685)*</span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a>Zheng et al. 设计了两个互补的评测系统。</span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a>**MT-Bench** 是一个包含 80 个多轮对话问题的测试集，覆盖写作、推理、数学、编程、知识、角色扮演等 8 个类别。每个问题都设计为需要两轮对话——第二轮的问题依赖于第一轮的回答，测试模型的多轮对话能力。评判方式有两种：</span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a>**单一评分**（Single Rating）：GPT-4 对每个回答打 1-10 分，并给出理由。</span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a>**成对比较**（Pairwise Comparison）：GPT-4 看到两个模型对同一问题的回答，判断哪个更好。</span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: LLM-as-Judge Pairwise Comparison（改编自 Zheng et al., 2023）</span></span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a><span class="in">Input: question Q, answer_A from Model A, answer_B from Model B, judge LLM J</span></span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a><span class="in">Output: winner ∈ {A, B, Tie}</span></span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a><span class="in">Step 1: Construct prompt for judge</span></span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a><span class="in">    prompt = f"""</span></span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a><span class="in">    [System] Please act as an impartial judge and evaluate the quality</span></span>
<span id="cb6-340"><a href="#cb6-340" aria-hidden="true" tabindex="-1"></a><span class="in">    of the responses provided by two AI assistants to the user question.</span></span>
<span id="cb6-341"><a href="#cb6-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a><span class="in">    [Question] {Q}</span></span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a><span class="in">    [Assistant A] {answer_A}</span></span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a><span class="in">    [Assistant B] {answer_B}</span></span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a><span class="in">    Please evaluate which assistant provides a better response.</span></span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a><span class="in">    Output your verdict as: [[A]], [[B]], or [[Tie]].</span></span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a><span class="in">    Provide your reasoning before the verdict.</span></span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a><span class="in">    """</span></span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a><span class="in">Step 2: Get judgment</span></span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a><span class="in">    judgment = J(prompt)</span></span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a><span class="in">Step 3: Swap positions and re-judge (to mitigate position bias)</span></span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a><span class="in">    prompt_swapped = swap(answer_A, answer_B) in prompt</span></span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a><span class="in">    judgment_swapped = J(prompt_swapped)</span></span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a><span class="in">Step 4: Final verdict</span></span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a><span class="in">    if both judgments agree: return agreed winner</span></span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a><span class="in">    else: return Tie (inconsistent)</span></span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a>*改编自 Zheng et al. (2023) "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", Section 3. [arXiv:2306.05685](https://arxiv.org/abs/2306.05685)*</span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-366"><a href="#cb6-366" aria-hidden="true" tabindex="-1"></a>注意 Step 3 的位置交换——这是一个关键设计。Zheng et al. 发现 GPT-4 作为评判者时存在显著的**位置偏差**：它倾向于偏好放在第一个位置的回答。通过交换位置并比较两次判断，可以部分缓解这一偏差。</span>
<span id="cb6-367"><a href="#cb6-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LLM-as-Judge 的偏差分析</span></span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a>Zheng et al. 系统地研究了 LLM 评判者的多种偏差，这对理解生成式评测的局限性至关重要：</span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 偏差类型 <span class="pp">|</span> 描述 <span class="pp">|</span> 严重程度 <span class="pp">|</span> 缓解方法 <span class="pp">|</span></span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|------|----------|----------|</span></span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **位置偏差** <span class="pp">|</span> 倾向于偏好第一个/最后一个位置的回答 <span class="pp">|</span> 高 <span class="pp">|</span> 交换位置，取一致结果 <span class="pp">|</span></span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **冗长偏差** <span class="pp">|</span> 倾向于偏好更长的回答 <span class="pp">|</span> 中 <span class="pp">|</span> 在 prompt 中明确要求关注质量而非长度 <span class="pp">|</span></span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **自我偏好** <span class="pp">|</span> 模型倾向于偏好自己（或相似风格）的输出 <span class="pp">|</span> 中 <span class="pp">|</span> 使用与被评模型不同的 judge <span class="pp">|</span></span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **格式偏差** <span class="pp">|</span> 偏好带有列表、标题等格式化的回答 <span class="pp">|</span> 低-中 <span class="pp">|</span> 控制格式变量 <span class="pp">|</span></span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a>尽管存在这些偏差，Zheng et al. 发现 GPT-4 作为评判者与人类评判者的一致率超过 80%——高于人类评判者之间的一致率（约 81%）。这个结果既令人鼓舞（LLM-as-Judge 可用），也令人不安（如果 GPT-4 是评判标准，那它自己的问题就永远无法被发现）。</span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Chatbot Arena：众包人类偏好</span></span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a>Chatbot Arena（LMSYS, 2023-）采用了完全不同的策略：不用 LLM 评判，而是**众包真实用户的偏好**。用户在网站上输入一个问题，两个匿名模型分别回答，用户选择哪个回答更好。模型的身份在用户投票后才揭示。</span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a>这种"盲品测试"的设计有几个关键优势。首先，用户可以自由提出任何问题——测试分布完全由真实使用场景决定，不受预设题目的限制。其次，匿名设计消除了品牌偏好（用户不知道自己在评价 GPT-4 还是 Claude）。第三，大规模众包（截至 2025 年已有数十万次投票）提供了统计上稳健的排名。</span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a>模型的排名使用 **Elo 评分系统**——与国际象棋的评分系统相同。每次对决后，赢家获得评分，输家失去评分，变化量取决于双方的评分差距：</span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a>E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}</span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a>R_A^{\text{new}} = R_A + K \cdot (S_A - E_A)</span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a>其中 $E_A$ 是模型 A 的预期胜率，$R_A, R_B$ 是当前 Elo 评分，$S_A$ 是实际结果（赢=1，输=0，平=0.5），$K$ 是更新系数。</span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：Elo 评分更新</span></span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a>**设定**：模型 A 当前 Elo = 1200，模型 B 当前 Elo = 1000，$K = 32$。</span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a>**Step 1: 计算预期胜率**</span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a>E_A = \frac{1}{1 + 10^{(1000 - 1200)/400}} = \frac{1}{1 + 10^{-0.5}} = \frac{1}{1 + 0.316} = 0.760</span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a>E_B = 1 - E_A = 0.240</span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a>**解读**：根据当前评分，系统预期模型 A 有 76% 的胜率。</span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a>**Step 2: 模型 A 获胜（符合预期）**</span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a>R_A^{\text{new}} = 1200 + 32 \times (1 - 0.760) = 1200 + 7.7 = 1207.7</span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a>R_B^{\text{new}} = 1000 + 32 \times (0 - 0.240) = 1000 - 7.7 = 992.3</span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a>**解读**：强者战胜弱者，评分变化不大（+7.7）。</span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a>**Step 3: 如果模型 B 爆冷获胜呢？**</span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a>R_A^{\text{new}} = 1200 + 32 \times (0 - 0.760) = 1200 - 24.3 = 1175.7</span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-434"><a href="#cb6-434" aria-hidden="true" tabindex="-1"></a>R_B^{\text{new}} = 1000 + 32 \times (1 - 0.240) = 1000 + 24.3 = 1024.3</span>
<span id="cb6-435"><a href="#cb6-435" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a>**解读**：弱者战胜强者，评分变化大得多（±24.3）。这就是 Elo 系统的精妙之处——"爆冷"传递了更多信息，因此带来更大的评分更新。</span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bradley-Terry 模型：成对比较的概率基础</span></span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a>Chatbot Arena 的排名本质上基于 **Bradley-Terry 模型**（Bradley &amp; Terry, 1952），这是成对比较（paired comparison）领域最经典的概率模型。</span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a>给定 $N$ 个模型，每个模型 $i$ 有一个"能力参数" $\theta_i &gt; 0$。模型 $i$ 在与模型 $j$ 的对决中获胜的概率为：</span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a>P(i \succ j) = \frac{\theta_i}{\theta_i + \theta_j}</span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a>取对数后，令 $\beta_i = \log \theta_i$：</span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a>P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}} = \sigma(\beta_i - \beta_j)</span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a>其中 $\sigma$ 是 sigmoid 函数。这正是一个逻辑回归模型！Elo 评分系统可以看作 Bradley-Terry 模型的在线更新版本（用 $R_i / 400$ 近似 $\beta_i$）。</span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a>给定一组成对比较数据 $<span class="sc">\{</span>(i_k, j_k, y_k)<span class="sc">\}</span>_{k=1}^{M}$（$y_k = 1$ 表示 $i_k$ 获胜），Bradley-Terry 模型的参数通过最大似然估计求解：</span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a>\hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} \sum_{k=1}^{M} \left<span class="co">[</span><span class="ot"> y_k \log \sigma(\beta_{i_k} - \beta_{j_k}) + (1 - y_k) \log \sigma(\beta_{j_k} - \beta_{i_k}) \right</span><span class="co">]</span></span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a>这个优化问题是凸的，可以用标准的梯度方法高效求解。</span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据污染与科学诚实</span></span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数据污染的本质</span></span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a>数据污染（Data Contamination）是 LLM 评测面临的最严重威胁之一。当模型的训练数据包含了 benchmark 的测试题时，模型的高分可能反映的是"记忆"而非"能力"。这就像一个学生考前偷看了试卷——他的高分并不代表他真正掌握了知识。</span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a>数据污染的严重性在于其**隐蔽性**和**不可避免性**。现代 LLM 的训练数据通常来自互联网爬取，规模达到数万亿 token。MMLU 的题目来自公开的考试和教材，这些内容很可能出现在训练数据中。对于闭源模型（如 GPT-4、Claude），外部研究者甚至无法检查训练数据中是否包含特定 benchmark。</span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 污染检测方法</span></span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a>研究者开发了多种方法来检测数据污染。最直接的方法是 **n-gram 重叠检测**：检查 benchmark 测试题的 n-gram 是否出现在训练数据中。GPT-3 和 GPT-4 的技术报告都进行了这类分析。但这种方法有明显的局限——改写（paraphrase）后的题目不会被检测到，而且它要求能够访问训练数据，对闭源模型不适用。</span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a>面对黑盒模型，研究者发展出了更巧妙的检测策略。Oren et al.（2023）提出了**分区对比法**：通过分析模型在 benchmark 不同分割（partition）上的表现差异来推断污染——如果模型在某个子集上表现异常地好，这可能暗示该子集被"泄漏"到了训练数据中。Deng et al.（2023）提出了 **TS-Guessing** 方法：给模型展示 benchmark 题目的部分内容（如只给选项不给题干），检测模型是否仍能以远超随机的概率选出正确答案。如果模型"见过"这些题目，即使只看到部分信息也能猜出答案。Golchin &amp; Surdeanu（2023）则提出了更直接的 **Time Travel** 方法：通过精心设计的 prompt，测试模型是否能逐字复现 benchmark 的原始文本——如果模型能几乎完美地重建某个 benchmark 实例的精确措辞，这就是数据污染的强证据。</span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a>另一种方法是**金丝雀检测**（Canary Detection）：在 benchmark 中故意嵌入一些独特的标记（canary tokens），如果模型能够复现这些标记，就证明存在污染。这种方法的优势在于它可以在 benchmark 设计阶段就内置防污染机制。</span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 著名的数据污染案例</span></span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a>2023 年，多项研究揭示了大规模数据污染的存在。研究者发现某些模型在 MMLU 特定学科上的表现远超预期，而这些学科的题目恰好在互联网上大量流传。更令人担忧的是，一些模型在 benchmark 发布前后的性能提升，与新增训练数据中包含 benchmark 相关内容的时间高度吻合。</span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a>这些案例提醒我们：**benchmark 分数的提升不一定意味着能力的提升**。在解读任何评测结果时，都需要考虑数据污染的可能性。</span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a><span class="fu">### 可靠性评测</span></span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 事实性：如何衡量幻觉？</span></span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a>LLM 的"幻觉"（Hallucination）——生成看似合理但实际错误的内容——是部署中最严重的问题之一。TruthfulQA（Lin et al., 2021）是最早系统评测 LLM 事实性的 benchmark。它包含 817 个问题，这些问题被设计为容易诱导模型生成错误答案——因为人类对这些问题也经常犯错（如常见的误解、都市传说等）。</span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a>TruthfulQA 的一个反直觉的发现是：**更大的模型并不总是更诚实**。在没有经过对齐训练的情况下，更大的模型由于更好地学习了人类文本中的常见错误，反而更容易生成"流畅但错误"的回答。这与模型在其他 benchmark 上"越大越好"的趋势形成了鲜明对比。</span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 一致性：同一问题不同问法</span></span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a>一个可靠的模型应该对同一问题的不同措辞给出一致的回答。但实践中，LLM 对 prompt 的措辞极其敏感——仅仅改变问题的语序或添加一个无关的前缀，都可能导致完全不同的回答。</span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a>这种不一致性对评测的影响是深远的：**一个模型在 MMLU 上 86% 的准确率，可能变成 82% 如果你稍微改写题目的措辞**。这意味着 benchmark 分数的精度远低于它表面上的精确度。</span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 拒答质量与越狱鲁棒性</span></span>
<span id="cb6-502"><a href="#cb6-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-503"><a href="#cb6-503" aria-hidden="true" tabindex="-1"></a>对齐后的模型被训练在面对有害请求时拒绝回答。但"拒答"本身也是一个评测维度：模型是否能够在**应该拒绝时拒绝**（安全性），同时在**不应该拒绝时不拒绝**（可用性）？过度拒答（对无害问题也拒绝）和拒答不足（对有害问题回答）都是失败模式。</span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a>越狱（Jailbreak）测试评估模型的安全护栏是否能抵抗对抗性攻击。研究者不断发现新的方法绕过模型的安全限制——如角色扮演、编码变换、多步引导等。这是一个"攻防博弈"的领域，评测本身需要持续更新以跟上攻击技术的演进。</span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a><span class="fu">### 搭建评测 Pipeline</span></span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a>一个完整的 LLM 评测 pipeline 通常包含以下组件：数据加载、推理执行、指标计算、结果聚合。下面是一个使用 Python 实现基础评测的示例：</span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a><span class="co">最小化的 LLM 评测 Pipeline 示例</span></span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a><span class="co">评测模型在 MMLU 风格选择题上的表现</span></span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 数据加载 ==========</span></span>
<span id="cb6-525"><a href="#cb6-525" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_mmlu_style_data(filepath):</span>
<span id="cb6-526"><a href="#cb6-526" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""加载 MMLU 风格的选择题数据</span></span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a><span class="co">    格式: {"question": str, "choices": [str], "answer": int, "subject": str}</span></span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filepath) <span class="im">as</span> f:</span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [json.loads(line) <span class="cf">for</span> line <span class="kw">in</span> f]</span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 推理模板 ==========</span></span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_mmlu_prompt(question, choices, few_shot_examples<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""构建 MMLU 评测 prompt</span></span>
<span id="cb6-535"><a href="#cb6-535" aria-hidden="true" tabindex="-1"></a><span class="co">    关键细节：prompt 格式对结果有显著影响！</span></span>
<span id="cb6-536"><a href="#cb6-536" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="st">""</span></span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> few_shot_examples:</span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ex <span class="kw">in</span> few_shot_examples:</span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">+=</span> <span class="ss">f"Question: </span><span class="sc">{</span>ex[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(ex[<span class="st">'choices'</span>]):</span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a>                prompt <span class="op">+=</span> <span class="ss">f"(</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">65</span><span class="op">+</span>i)<span class="sc">}</span><span class="ss">) </span><span class="sc">{</span>c<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">+=</span> <span class="ss">f"Answer: (</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">65</span><span class="op">+</span>ex[<span class="st">'answer'</span>])<span class="sc">}</span><span class="ss">)</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(choices):</span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"(</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">65</span><span class="op">+</span>i)<span class="sc">}</span><span class="ss">) </span><span class="sc">{</span>c<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="st">"Answer: ("</span></span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prompt</span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 指标计算 ==========</span></span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(predictions, references):</span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""计算多维度评测指标"""</span></span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="bu">sum</span>(p <span class="op">==</span> r <span class="cf">for</span> p, r <span class="kw">in</span> <span class="bu">zip</span>(predictions, references))</span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">len</span>(references)</span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> correct <span class="op">/</span> total</span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-558"><a href="#cb6-558" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 按学科分组计算</span></span>
<span id="cb6-559"><a href="#cb6-559" aria-hidden="true" tabindex="-1"></a>    subject_scores <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: {<span class="st">"correct"</span>: <span class="dv">0</span>, <span class="st">"total"</span>: <span class="dv">0</span>})</span>
<span id="cb6-560"><a href="#cb6-560" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (省略分组逻辑)</span></span>
<span id="cb6-561"><a href="#cb6-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-562"><a href="#cb6-562" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb6-563"><a href="#cb6-563" aria-hidden="true" tabindex="-1"></a>        <span class="st">"accuracy"</span>: accuracy,</span>
<span id="cb6-564"><a href="#cb6-564" aria-hidden="true" tabindex="-1"></a>        <span class="st">"correct"</span>: correct,</span>
<span id="cb6-565"><a href="#cb6-565" aria-hidden="true" tabindex="-1"></a>        <span class="st">"total"</span>: total,</span>
<span id="cb6-566"><a href="#cb6-566" aria-hidden="true" tabindex="-1"></a>        <span class="st">"95_ci"</span>: <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(accuracy <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> accuracy) <span class="op">/</span> total)</span>
<span id="cb6-567"><a href="#cb6-567" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb6-568"><a href="#cb6-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-569"><a href="#cb6-569" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 结果报告 ==========</span></span>
<span id="cb6-570"><a href="#cb6-570" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report_results(metrics):</span>
<span id="cb6-571"><a href="#cb6-571" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""生成评测报告"""</span></span>
<span id="cb6-572"><a href="#cb6-572" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Overall Accuracy: </span><span class="sc">{</span>metrics[<span class="st">'accuracy'</span>]<span class="sc">:.1%}</span><span class="ss"> "</span></span>
<span id="cb6-573"><a href="#cb6-573" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"(± </span><span class="sc">{</span>metrics[<span class="st">'95_ci'</span>]<span class="sc">:.1%}</span><span class="ss">)"</span>)</span>
<span id="cb6-574"><a href="#cb6-574" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Correct: </span><span class="sc">{</span>metrics[<span class="st">'correct'</span>]<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>metrics[<span class="st">'total'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-575"><a href="#cb6-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-576"><a href="#cb6-576" aria-hidden="true" tabindex="-1"></a><span class="co"># ========== 主流程 ==========</span></span>
<span id="cb6-577"><a href="#cb6-577" aria-hidden="true" tabindex="-1"></a><span class="co"># data = load_mmlu_style_data("mmlu_test.jsonl")</span></span>
<span id="cb6-578"><a href="#cb6-578" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions = [call_model(format_mmlu_prompt(d)) for d in data]</span></span>
<span id="cb6-579"><a href="#cb6-579" aria-hidden="true" tabindex="-1"></a><span class="co"># metrics = compute_metrics(predictions, [d["answer"] for d in data])</span></span>
<span id="cb6-580"><a href="#cb6-580" aria-hidden="true" tabindex="-1"></a><span class="co"># report_results(metrics)</span></span>
<span id="cb6-581"><a href="#cb6-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-582"><a href="#cb6-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-583"><a href="#cb6-583" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用 lm-evaluation-harness</span></span>
<span id="cb6-584"><a href="#cb6-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-585"><a href="#cb6-585" aria-hidden="true" tabindex="-1"></a>在实践中，评测通常使用成熟的框架。**lm-evaluation-harness**（EleutherAI）是最广泛使用的开源评测框架，支持数百个 benchmark：</span>
<span id="cb6-586"><a href="#cb6-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-587"><a href="#cb6-587" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb6-588"><a href="#cb6-588" aria-hidden="true" tabindex="-1"></a><span class="co"># 安装</span></span>
<span id="cb6-589"><a href="#cb6-589" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install lm-eval</span>
<span id="cb6-590"><a href="#cb6-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-591"><a href="#cb6-591" aria-hidden="true" tabindex="-1"></a><span class="co"># 在 MMLU 上评测 Hugging Face 模型</span></span>
<span id="cb6-592"><a href="#cb6-592" aria-hidden="true" tabindex="-1"></a><span class="ex">lm_eval</span> <span class="at">--model</span> hf <span class="dt">\</span></span>
<span id="cb6-593"><a href="#cb6-593" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model_args</span> pretrained=meta-llama/Llama-2-7b-hf <span class="dt">\</span></span>
<span id="cb6-594"><a href="#cb6-594" aria-hidden="true" tabindex="-1"></a>    <span class="at">--tasks</span> mmlu <span class="dt">\</span></span>
<span id="cb6-595"><a href="#cb6-595" aria-hidden="true" tabindex="-1"></a>    <span class="at">--batch_size</span> 8 <span class="dt">\</span></span>
<span id="cb6-596"><a href="#cb6-596" aria-hidden="true" tabindex="-1"></a>    <span class="at">--output_path</span> results/</span>
<span id="cb6-597"><a href="#cb6-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-598"><a href="#cb6-598" aria-hidden="true" tabindex="-1"></a><span class="co"># 评测多个 benchmark</span></span>
<span id="cb6-599"><a href="#cb6-599" aria-hidden="true" tabindex="-1"></a><span class="ex">lm_eval</span> <span class="at">--model</span> hf <span class="dt">\</span></span>
<span id="cb6-600"><a href="#cb6-600" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model_args</span> pretrained=meta-llama/Llama-2-7b-hf <span class="dt">\</span></span>
<span id="cb6-601"><a href="#cb6-601" aria-hidden="true" tabindex="-1"></a>    <span class="at">--tasks</span> mmlu,hellaswag,arc_challenge,truthfulqa_mc <span class="dt">\</span></span>
<span id="cb6-602"><a href="#cb6-602" aria-hidden="true" tabindex="-1"></a>    <span class="at">--batch_size</span> 8</span>
<span id="cb6-603"><a href="#cb6-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-604"><a href="#cb6-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-605"><a href="#cb6-605" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实现 LLM-as-Judge</span></span>
<span id="cb6-606"><a href="#cb6-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-607"><a href="#cb6-607" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-608"><a href="#cb6-608" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-609"><a href="#cb6-609" aria-hidden="true" tabindex="-1"></a><span class="co">简化版 LLM-as-Judge 实现</span></span>
<span id="cb6-610"><a href="#cb6-610" aria-hidden="true" tabindex="-1"></a><span class="co">使用成对比较 + 位置交换来缓解位置偏差</span></span>
<span id="cb6-611"><a href="#cb6-611" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-612"><a href="#cb6-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-613"><a href="#cb6-613" aria-hidden="true" tabindex="-1"></a>JUDGE_PROMPT <span class="op">=</span> <span class="st">"""Please act as an impartial judge and evaluate the quality</span></span>
<span id="cb6-614"><a href="#cb6-614" aria-hidden="true" tabindex="-1"></a><span class="st">of the responses provided by two AI assistants to the user question below.</span></span>
<span id="cb6-615"><a href="#cb6-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-616"><a href="#cb6-616" aria-hidden="true" tabindex="-1"></a><span class="st">[User Question]</span></span>
<span id="cb6-617"><a href="#cb6-617" aria-hidden="true" tabindex="-1"></a><span class="sc">{question}</span></span>
<span id="cb6-618"><a href="#cb6-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-619"><a href="#cb6-619" aria-hidden="true" tabindex="-1"></a><span class="st">[Assistant A's Answer]</span></span>
<span id="cb6-620"><a href="#cb6-620" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_a}</span></span>
<span id="cb6-621"><a href="#cb6-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-622"><a href="#cb6-622" aria-hidden="true" tabindex="-1"></a><span class="st">[Assistant B's Answer]</span></span>
<span id="cb6-623"><a href="#cb6-623" aria-hidden="true" tabindex="-1"></a><span class="sc">{answer_b}</span></span>
<span id="cb6-624"><a href="#cb6-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-625"><a href="#cb6-625" aria-hidden="true" tabindex="-1"></a><span class="st">Evaluate which assistant provides a better answer. Consider helpfulness,</span></span>
<span id="cb6-626"><a href="#cb6-626" aria-hidden="true" tabindex="-1"></a><span class="st">relevance, accuracy, depth, and clarity. Avoid position bias.</span></span>
<span id="cb6-627"><a href="#cb6-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-628"><a href="#cb6-628" aria-hidden="true" tabindex="-1"></a><span class="st">Output your verdict EXACTLY as: [[A]], [[B]], or [[Tie]].</span></span>
<span id="cb6-629"><a href="#cb6-629" aria-hidden="true" tabindex="-1"></a><span class="st">Provide brief reasoning before your verdict."""</span></span>
<span id="cb6-630"><a href="#cb6-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-631"><a href="#cb6-631" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb6-632"><a href="#cb6-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-633"><a href="#cb6-633" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> judge_pair(question, answer_a, answer_b, judge_model):</span>
<span id="cb6-634"><a href="#cb6-634" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-635"><a href="#cb6-635" aria-hidden="true" tabindex="-1"></a><span class="co">    对一对回答进行评判，带位置交换</span></span>
<span id="cb6-636"><a href="#cb6-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-637"><a href="#cb6-637" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns: "A", "B", or "Tie"</span></span>
<span id="cb6-638"><a href="#cb6-638" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-639"><a href="#cb6-639" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 第一次评判：A在前</span></span>
<span id="cb6-640"><a href="#cb6-640" aria-hidden="true" tabindex="-1"></a>    prompt_1 <span class="op">=</span> JUDGE_PROMPT.<span class="bu">format</span>(</span>
<span id="cb6-641"><a href="#cb6-641" aria-hidden="true" tabindex="-1"></a>        question<span class="op">=</span>question, answer_a<span class="op">=</span>answer_a, answer_b<span class="op">=</span>answer_b</span>
<span id="cb6-642"><a href="#cb6-642" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-643"><a href="#cb6-643" aria-hidden="true" tabindex="-1"></a>    result_1 <span class="op">=</span> judge_model(prompt_1)</span>
<span id="cb6-644"><a href="#cb6-644" aria-hidden="true" tabindex="-1"></a>    verdict_1 <span class="op">=</span> extract_verdict(result_1)</span>
<span id="cb6-645"><a href="#cb6-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-646"><a href="#cb6-646" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 第二次评判：B在前（交换位置）</span></span>
<span id="cb6-647"><a href="#cb6-647" aria-hidden="true" tabindex="-1"></a>    prompt_2 <span class="op">=</span> JUDGE_PROMPT.<span class="bu">format</span>(</span>
<span id="cb6-648"><a href="#cb6-648" aria-hidden="true" tabindex="-1"></a>        question<span class="op">=</span>question, answer_a<span class="op">=</span>answer_b, answer_b<span class="op">=</span>answer_a</span>
<span id="cb6-649"><a href="#cb6-649" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-650"><a href="#cb6-650" aria-hidden="true" tabindex="-1"></a>    result_2 <span class="op">=</span> judge_model(prompt_2)</span>
<span id="cb6-651"><a href="#cb6-651" aria-hidden="true" tabindex="-1"></a>    verdict_2 <span class="op">=</span> extract_verdict(result_2)</span>
<span id="cb6-652"><a href="#cb6-652" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 注意：需要反转 verdict_2 的结果</span></span>
<span id="cb6-653"><a href="#cb6-653" aria-hidden="true" tabindex="-1"></a>    verdict_2_flipped <span class="op">=</span> flip_verdict(verdict_2)</span>
<span id="cb6-654"><a href="#cb6-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-655"><a href="#cb6-655" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 两次一致才算数</span></span>
<span id="cb6-656"><a href="#cb6-656" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verdict_1 <span class="op">==</span> verdict_2_flipped:</span>
<span id="cb6-657"><a href="#cb6-657" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> verdict_1</span>
<span id="cb6-658"><a href="#cb6-658" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-659"><a href="#cb6-659" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"Tie"</span>  <span class="co"># 不一致 → 视为平局</span></span>
<span id="cb6-660"><a href="#cb6-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-661"><a href="#cb6-661" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extract_verdict(text):</span>
<span id="cb6-662"><a href="#cb6-662" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""从 judge 输出中提取判定"""</span></span>
<span id="cb6-663"><a href="#cb6-663" aria-hidden="true" tabindex="-1"></a>    match <span class="op">=</span> re.search(<span class="vs">r'</span><span class="ch">\[\[</span><span class="kw">(</span><span class="vs">A</span><span class="cf">|</span><span class="vs">B</span><span class="cf">|</span><span class="vs">Tie</span><span class="kw">)</span><span class="ch">\]\]</span><span class="vs">'</span>, text)</span>
<span id="cb6-664"><a href="#cb6-664" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> match.group(<span class="dv">1</span>) <span class="cf">if</span> match <span class="cf">else</span> <span class="st">"Tie"</span></span>
<span id="cb6-665"><a href="#cb6-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-666"><a href="#cb6-666" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flip_verdict(verdict):</span>
<span id="cb6-667"><a href="#cb6-667" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""位置交换后，A和B的含义互换"""</span></span>
<span id="cb6-668"><a href="#cb6-668" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verdict <span class="op">==</span> <span class="st">"A"</span>: <span class="cf">return</span> <span class="st">"B"</span></span>
<span id="cb6-669"><a href="#cb6-669" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verdict <span class="op">==</span> <span class="st">"B"</span>: <span class="cf">return</span> <span class="st">"A"</span></span>
<span id="cb6-670"><a href="#cb6-670" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"Tie"</span></span>
<span id="cb6-671"><a href="#cb6-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-672"><a href="#cb6-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-673"><a href="#cb6-673" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据污染快速检测</span></span>
<span id="cb6-674"><a href="#cb6-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-675"><a href="#cb6-675" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-676"><a href="#cb6-676" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-677"><a href="#cb6-677" aria-hidden="true" tabindex="-1"></a><span class="co">简易数据污染检测：检查模型是否能"背出" benchmark 题目</span></span>
<span id="cb6-678"><a href="#cb6-678" aria-hidden="true" tabindex="-1"></a><span class="co">思路：给出题目的前半部分，看模型能否续写出后半部分</span></span>
<span id="cb6-679"><a href="#cb6-679" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-680"><a href="#cb6-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-681"><a href="#cb6-681" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contamination_check(model, benchmark_items, n_samples<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-682"><a href="#cb6-682" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-683"><a href="#cb6-683" aria-hidden="true" tabindex="-1"></a><span class="co">    通过 prefix completion 检测潜在的数据污染</span></span>
<span id="cb6-684"><a href="#cb6-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-685"><a href="#cb6-685" aria-hidden="true" tabindex="-1"></a><span class="co">    如果模型能高精度地续写 benchmark 题目，</span></span>
<span id="cb6-686"><a href="#cb6-686" aria-hidden="true" tabindex="-1"></a><span class="co">    说明这些题目很可能出现在训练数据中。</span></span>
<span id="cb6-687"><a href="#cb6-687" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-688"><a href="#cb6-688" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> random</span>
<span id="cb6-689"><a href="#cb6-689" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> random.sample(benchmark_items, <span class="bu">min</span>(n_samples, <span class="bu">len</span>(benchmark_items)))</span>
<span id="cb6-690"><a href="#cb6-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-691"><a href="#cb6-691" aria-hidden="true" tabindex="-1"></a>    exact_matches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-692"><a href="#cb6-692" aria-hidden="true" tabindex="-1"></a>    high_overlap <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-693"><a href="#cb6-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-694"><a href="#cb6-694" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> samples:</span>
<span id="cb6-695"><a href="#cb6-695" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> item[<span class="st">"question"</span>]</span>
<span id="cb6-696"><a href="#cb6-696" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 取前 50% 作为 prefix</span></span>
<span id="cb6-697"><a href="#cb6-697" aria-hidden="true" tabindex="-1"></a>        split_point <span class="op">=</span> <span class="bu">len</span>(text) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb6-698"><a href="#cb6-698" aria-hidden="true" tabindex="-1"></a>        prefix <span class="op">=</span> text[:split_point]</span>
<span id="cb6-699"><a href="#cb6-699" aria-hidden="true" tabindex="-1"></a>        suffix <span class="op">=</span> text[split_point:]</span>
<span id="cb6-700"><a href="#cb6-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-701"><a href="#cb6-701" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 让模型续写</span></span>
<span id="cb6-702"><a href="#cb6-702" aria-hidden="true" tabindex="-1"></a>        completion <span class="op">=</span> model.generate(prefix, max_tokens<span class="op">=</span><span class="bu">len</span>(suffix.split()) <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb6-703"><a href="#cb6-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-704"><a href="#cb6-704" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算重叠度</span></span>
<span id="cb6-705"><a href="#cb6-705" aria-hidden="true" tabindex="-1"></a>        overlap <span class="op">=</span> compute_rouge_l(completion, suffix)</span>
<span id="cb6-706"><a href="#cb6-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-707"><a href="#cb6-707" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> overlap <span class="op">&gt;</span> <span class="fl">0.9</span>:</span>
<span id="cb6-708"><a href="#cb6-708" aria-hidden="true" tabindex="-1"></a>            exact_matches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-709"><a href="#cb6-709" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> overlap <span class="op">&gt;</span> <span class="fl">0.7</span>:</span>
<span id="cb6-710"><a href="#cb6-710" aria-hidden="true" tabindex="-1"></a>            high_overlap <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-711"><a href="#cb6-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-712"><a href="#cb6-712" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb6-713"><a href="#cb6-713" aria-hidden="true" tabindex="-1"></a>        <span class="st">"exact_match_rate"</span>: exact_matches <span class="op">/</span> <span class="bu">len</span>(samples),</span>
<span id="cb6-714"><a href="#cb6-714" aria-hidden="true" tabindex="-1"></a>        <span class="st">"high_overlap_rate"</span>: high_overlap <span class="op">/</span> <span class="bu">len</span>(samples),</span>
<span id="cb6-715"><a href="#cb6-715" aria-hidden="true" tabindex="-1"></a>        <span class="st">"verdict"</span>: <span class="st">"LIKELY CONTAMINATED"</span> <span class="cf">if</span> exact_matches <span class="op">/</span> <span class="bu">len</span>(samples) <span class="op">&gt;</span> <span class="fl">0.1</span></span>
<span id="cb6-716"><a href="#cb6-716" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">else</span> <span class="st">"PROBABLY CLEAN"</span></span>
<span id="cb6-717"><a href="#cb6-717" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb6-718"><a href="#cb6-718" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-719"><a href="#cb6-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-720"><a href="#cb6-720" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-721"><a href="#cb6-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-722"><a href="#cb6-722" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb6-723"><a href="#cb6-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-724"><a href="#cb6-724" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨评测方法论的理论基础、边界条件和开放问题</span></span>
<span id="cb6-725"><a href="#cb6-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-726"><a href="#cb6-726" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么评测科学这么重要？——理论视角</span></span>
<span id="cb6-727"><a href="#cb6-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-728"><a href="#cb6-728" aria-hidden="true" tabindex="-1"></a>从科学哲学的角度看，评测问题其实是**操作性定义**（operational definition）的问题。当我们说一个模型"理解语言"或"具备推理能力"时，这些概念需要通过具体的评测任务来操作化。正如智商测试（IQ test）不等于"智能"，MMLU 分数也不等于"理解"——但在没有更好替代的情况下，它是我们能得到的最好近似。</span>
<span id="cb6-729"><a href="#cb6-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-730"><a href="#cb6-730" aria-hidden="true" tabindex="-1"></a>问题在于，不同的操作性定义可能给出截然不同的结论。Schaeffer et al.（2023）的工作正是这一点的极端体现：同一份数据、同一个能力，用精确匹配度量时"涌现"了，用连续度量时却"一直都在平滑增长"。这告诉我们：**任何评测结论都不能脱离其度量方式来理解**。</span>
<span id="cb6-731"><a href="#cb6-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-732"><a href="#cb6-732" aria-hidden="true" tabindex="-1"></a>从信息论的角度，评测本质上是通过有限的观测（测试题目和模型回答）来推断模型的潜在能力（一个无限维的函数空间中的一个点）。这是一个严重**欠定**（underdetermined）的问题——有限的测试永远无法完全确定模型的能力边界。更糟糕的是，由于对抗性优化的存在，模型可能在测试分布上表现出色，但在测试分布之外完全崩溃。</span>
<span id="cb6-733"><a href="#cb6-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-734"><a href="#cb6-734" aria-hidden="true" tabindex="-1"></a><span class="fu">### 评测的隐含假设</span></span>
<span id="cb6-735"><a href="#cb6-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-736"><a href="#cb6-736" aria-hidden="true" tabindex="-1"></a>每种评测方式都隐含了一系列假设，理解这些假设是评判评测结论的前提。</span>
<span id="cb6-737"><a href="#cb6-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-738"><a href="#cb6-738" aria-hidden="true" tabindex="-1"></a>**假设 1：独立同分布**。大多数 benchmark 假设测试样本是从某个目标分布中独立采样的。但实际上，benchmark 题目的选择往往有强烈的偏见——MMLU 偏向西方教育体系的知识，GLUE 偏向英语互联网文本的分布。模型在 benchmark 上的高分不意味着它在真实使用场景中同样表现良好。</span>
<span id="cb6-739"><a href="#cb6-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-740"><a href="#cb6-740" aria-hidden="true" tabindex="-1"></a>**假设 2：任务可分解性**。GLUE 和 HELM 假设语言理解可以分解为若干独立的子任务（情感分析、蕴含判断等），且在这些子任务上的表现可以综合反映整体能力。但语言理解可能是一个整体性的能力，无法简单分解。</span>
<span id="cb6-741"><a href="#cb6-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-742"><a href="#cb6-742" aria-hidden="true" tabindex="-1"></a>**假设 3：人类基线的可比性**。几乎所有 benchmark 都以人类表现作为参考。但人类和 LLM 的错误模式截然不同——人类可能因粗心犯简单错误，而 LLM 可能犯人类不可能犯的"荒谬"错误。直接比较人类和模型的分数可能具有误导性。</span>
<span id="cb6-743"><a href="#cb6-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-744"><a href="#cb6-744" aria-hidden="true" tabindex="-1"></a>**假设 4：评测与应用的一致性**。benchmark 分数应该能够预测模型在实际应用中的表现。但由于分布偏移、prompt 敏感性和数据污染，这种预测关系往往很弱。许多用户反馈"benchmark 排名第一的模型在我的场景中表现不如排名第三的"。</span>
<span id="cb6-745"><a href="#cb6-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-746"><a href="#cb6-746" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb6-747"><a href="#cb6-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-748"><a href="#cb6-748" aria-hidden="true" tabindex="-1"></a>**自动指标的边界**：BLEU/ROUGE 在系统级比较时尚可接受（特别是在差距较大时），但在句子级评估和相近系统的比较中几乎不可靠。语义指标（BERTScore、COMET）有所改善，但在跨领域、跨语言场景中可能退化。</span>
<span id="cb6-749"><a href="#cb6-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-750"><a href="#cb6-750" aria-hidden="true" tabindex="-1"></a>**静态 benchmark 的边界**：GLUE/MMLU 等 benchmark 在模型能力远低于人类时是有效的区分工具，但一旦模型接近或超过人类基线，benchmark 就失去了区分能力（"天花板效应"）。此外，benchmark 的有效寿命受到数据污染和针对性优化的限制——通常只有 1-3 年。</span>
<span id="cb6-751"><a href="#cb6-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-752"><a href="#cb6-752" aria-hidden="true" tabindex="-1"></a>**LLM-as-Judge 的边界**：当被评模型的能力远低于评判模型时，LLM-as-Judge 工作良好。但当被评模型接近或超过评判模型时，评判结果的可靠性急剧下降。这创造了一个"玻璃天花板"：无法评测比评判者更强的模型。</span>
<span id="cb6-753"><a href="#cb6-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-754"><a href="#cb6-754" aria-hidden="true" tabindex="-1"></a>**Chatbot Arena 的边界**：需要大量真实用户参与，对小众模型或特定领域模型不友好。用户群体的偏见（如偏好英语、偏好编程任务）会影响排名的公平性。此外，Elo 评分假设模型能力是一维的——但现实中模型可能在不同维度（创意 vs 精度 vs 速度）上各有优劣。</span>
<span id="cb6-755"><a href="#cb6-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-756"><a href="#cb6-756" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb6-757"><a href="#cb6-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-758"><a href="#cb6-758" aria-hidden="true" tabindex="-1"></a>**问题 1：动态评测**。如何设计一个能够持续生成新题目、不会被"刷满"的评测系统？一种思路是**对抗性评测**（adversarial evaluation）——自动生成能够揭示模型弱点的测试用例。但如何保证生成的题目既有意义又有正确答案？</span>
<span id="cb6-759"><a href="#cb6-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-760"><a href="#cb6-760" aria-hidden="true" tabindex="-1"></a>**问题 2：多维评测的聚合**。HELM 提出了 7 个评测维度，但如何将这 7 个维度聚合为一个可比较的分数？不同用户对不同维度的关注度不同（医疗领域更重视准确性，创意领域更重视多样性）。是否存在一种"帕累托最优"的聚合方式？</span>
<span id="cb6-761"><a href="#cb6-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-762"><a href="#cb6-762" aria-hidden="true" tabindex="-1"></a>**问题 3：评测 Scalable Oversight**。当模型的能力超过人类评判者时，如何评测？这是 AI 安全领域的一个核心问题。如果连人类专家都无法判断模型的回答是否正确（如高级数学定理的证明），我们如何建立可靠的评测？</span>
<span id="cb6-763"><a href="#cb6-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-764"><a href="#cb6-764" aria-hidden="true" tabindex="-1"></a>**问题 4：评测公平性**。现有 benchmark 严重偏向英语和西方知识体系。如何设计真正多语言、多文化公平的评测？MMLU 中"Abstract Algebra"的题目对一个从未接触过英语高等教育的人毫无意义，但这不代表他缺乏"智能"。</span>
<span id="cb6-765"><a href="#cb6-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-766"><a href="#cb6-766" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-767"><a href="#cb6-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-768"><a href="#cb6-768" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb6-769"><a href="#cb6-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-770"><a href="#cb6-770" aria-hidden="true" tabindex="-1"></a><span class="fu">### 当前评测方法论的核心局限</span></span>
<span id="cb6-771"><a href="#cb6-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-772"><a href="#cb6-772" aria-hidden="true" tabindex="-1"></a>本章讨论的评测方法虽然代表了当前最先进的实践，但每一种都有根本性的局限。</span>
<span id="cb6-773"><a href="#cb6-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-774"><a href="#cb6-774" aria-hidden="true" tabindex="-1"></a>静态 benchmark 受 Goodhart 定律支配，寿命有限。LLM-as-Judge 受限于评判模型自身的能力天花板和系统性偏差。Chatbot Arena 依赖大规模用户参与，且排名只反映"普通用户的平均偏好"而非特定任务的专业表现。所有方法都无法完全解决数据污染问题——特别是对闭源模型。</span>
<span id="cb6-775"><a href="#cb6-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-776"><a href="#cb6-776" aria-hidden="true" tabindex="-1"></a>更根本的问题是，我们甚至缺乏对"模型能力"的清晰定义。当我们说一个模型"理解语言"或"具备推理能力"时，这些概念的精确含义是什么？没有清晰的定义，就不可能有完美的评测。</span>
<span id="cb6-777"><a href="#cb6-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-778"><a href="#cb6-778" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a>评测方法论的核心洞察——**度量本身会影响我们对能力的判断**——直接引出了下一章的主题：**指令微调**。</span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a>如果我们连"模型好不好"都难以可靠地衡量，那么"训练模型变好"就更加困难。指令微调（Instruction Tuning）的目标是让模型"听话"——能够理解并遵循人类的指令。但"听话"本身就是一个评测问题：什么样的回答算"好的回答"？谁来判断？这些问题将在 Ch23（指令微调）和 Ch24（RLHF）中得到进一步讨论。</span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a>从更宏观的视角看，评测问题是对齐（Alignment）问题的缩影。对齐的核心困难——如何定义和衡量"符合人类意图"——本质上就是一个评测问题。本章讨论的方法论（Bradley-Terry 模型、人类偏好收集、LLM-as-Judge）将在对齐章节中反复出现。</span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第 23 章将讨论**指令微调**——如何通过在指令-回答数据上训练，让模型从"能力强但不好用"变成"既有能力又听话"。从 FLAN 到 Self-Instruct 到 Alpaca，指令数据的构建方法经历了怎样的演进？而评测这些"听话"模型的表现，正需要本章讨论的方法论。</span></span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a>本章系统讲述了 NLP 评测方法论的演进，核心线索是**评测与模型之间的"军备竞赛"**——每一代评测方法都在修复上一代的缺陷，同时引入新的挑战。</span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a>**第一代自动指标**（BLEU、ROUGE）解决了"如何自动评测"的问题，通过 n-gram 匹配量化文本质量。BLEU 的核心公式是修正 n-gram 精度加短句惩罚，ROUGE 则侧重召回率。但 n-gram 匹配无法捕捉语义——同义替换、释义重写都会导致低分。</span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a>**第二代理解能力 benchmark**（GLUE、SuperGLUE、MMLU、HELM、BIG-bench）解决了"如何评测理解能力"的问题，通过精心设计的任务集测试模型的多种语言理解能力。但静态 benchmark 受 Goodhart 定律支配——发布、追逐、饱和、失效的循环通常只需 1-3 年。数据污染更是从根本上威胁了评测结果的可信度。</span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a>**第三代生成式评测**（LLM-as-Judge、Chatbot Arena）解决了"如何评测开放式生成"的问题。LLM-as-Judge 通过让 GPT-4 等强模型担任评判者，实现了大规模自动评测。Chatbot Arena 通过众包真实用户的盲品偏好，建立了 Elo 排名系统。但 LLM 评判者存在位置偏差、冗长偏差和自我偏好，而 Chatbot Arena 的用户群体偏见也不可忽视。</span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a>贯穿全章的核心教训是 **Goodhart 定律**："当一个度量成为目标时，它就不再是好的度量。"这不仅适用于 NLP 评测，也是理解 AI 对齐问题的关键视角。</span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键对比速查</span></span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-806"><a href="#cb6-806" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 评测方法 <span class="pp">|</span> 时代 <span class="pp">|</span> 代表 <span class="pp">|</span> 优势 <span class="pp">|</span> 核心局限 <span class="pp">|</span></span>
<span id="cb6-807"><a href="#cb6-807" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|------|------|------|----------|</span></span>
<span id="cb6-808"><a href="#cb6-808" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **自动指标** <span class="pp">|</span> 2002– <span class="pp">|</span> BLEU, ROUGE <span class="pp">|</span> 全自动、可复现 <span class="pp">|</span> 与人类判断相关性低 <span class="pp">|</span></span>
<span id="cb6-809"><a href="#cb6-809" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **语义指标** <span class="pp">|</span> 2020– <span class="pp">|</span> BERTScore, COMET <span class="pp">|</span> 捕捉语义相似度 <span class="pp">|</span> 依赖预训练模型 <span class="pp">|</span></span>
<span id="cb6-810"><a href="#cb6-810" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **静态 Benchmark** <span class="pp">|</span> 2018– <span class="pp">|</span> GLUE, MMLU <span class="pp">|</span> 标准化、可比较 <span class="pp">|</span> Goodhart 定律、数据污染 <span class="pp">|</span></span>
<span id="cb6-811"><a href="#cb6-811" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **多维评测** <span class="pp">|</span> 2022– <span class="pp">|</span> HELM <span class="pp">|</span> 全面性 <span class="pp">|</span> 维度聚合困难 <span class="pp">|</span></span>
<span id="cb6-812"><a href="#cb6-812" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **LLM-as-Judge** <span class="pp">|</span> 2023– <span class="pp">|</span> MT-Bench <span class="pp">|</span> 评测开放生成 <span class="pp">|</span> 评判者偏差、能力天花板 <span class="pp">|</span></span>
<span id="cb6-813"><a href="#cb6-813" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **人类偏好** <span class="pp">|</span> 2023– <span class="pp">|</span> Chatbot Arena <span class="pp">|</span> 最接近真实使用 <span class="pp">|</span> 用户偏见、规模需求 <span class="pp">|</span></span>
<span id="cb6-814"><a href="#cb6-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-815"><a href="#cb6-815" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb6-816"><a href="#cb6-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-817"><a href="#cb6-817" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BLEU**：$\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$</span>
<span id="cb6-818"><a href="#cb6-818" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**短句惩罚**：$\text{BP} = e^{\min(0, 1 - r/c)}$</span>
<span id="cb6-819"><a href="#cb6-819" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Elo 预期胜率**：$E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}$</span>
<span id="cb6-820"><a href="#cb6-820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bradley-Terry**：$P(i \succ j) = \sigma(\beta_i - \beta_j)$</span>
<span id="cb6-821"><a href="#cb6-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-822"><a href="#cb6-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb6-823"><a href="#cb6-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-824"><a href="#cb6-824" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 解释 Goodhart 定律在 NLP 评测中的表现。为什么 GLUE 在发布仅两年后就"失效"了？如果你是 benchmark 设计者，你会采取什么策略来延长 benchmark 的有效寿命？</span>
<span id="cb6-825"><a href="#cb6-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-826"><a href="#cb6-826" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 在 Elo 评分系统中，假设模型 A 的 Elo 为 1400，模型 B 的 Elo 为 1000。(a) 计算 A 对 B 的预期胜率。(b) 如果 A 获胜，双方的 Elo 如何更新（$K=32$）？(c) 如果连续 5 场 B 都爆冷获胜，A 和 B 的 Elo 会变成多少？(d) 讨论 Elo 系统的一个局限：为什么一维评分可能不足以描述 LLM 的能力？</span>
<span id="cb6-827"><a href="#cb6-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-828"><a href="#cb6-828" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用 <span class="in">`lm-evaluation-harness`</span> 框架，在 MMLU 的一个子集（如 abstract_algebra 或 high_school_mathematics）上评测一个开源模型。报告：(a) 5-shot 准确率及 95% 置信区间，(b) 将同样的题目改写（paraphrase），重新评测，观察分数变化，(c) 讨论这种不稳定性对评测结论的影响。</span>
<span id="cb6-829"><a href="#cb6-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-830"><a href="#cb6-830" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** 当被评模型的能力超过评判模型时，LLM-as-Judge 会面临什么问题？设计一个实验来检验这种"评判天花板"效应。如果未来的模型在所有维度上都超过 GPT-4，我们应该如何评测它们？</span>
<span id="cb6-831"><a href="#cb6-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-832"><a href="#cb6-832" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放思考]** 假设你要设计一个"不会过时"的 LLM 评测系统。你会采用什么架构？考虑：动态题目生成、多维能力评测、防数据污染机制、人机混合评判。讨论每个设计选择的 trade-off。</span>
<span id="cb6-833"><a href="#cb6-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-834"><a href="#cb6-834" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-835"><a href="#cb6-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-836"><a href="#cb6-836" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb6-837"><a href="#cb6-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-838"><a href="#cb6-838" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb6-839"><a href="#cb6-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-840"><a href="#cb6-840" aria-hidden="true" tabindex="-1"></a>**Wang, A. et al. (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"**。理解能力评测的里程碑。重点阅读：Section 2（任务设计原则）。即使 GLUE 已"过时"，它的设计思想仍是后续 benchmark 的基础。<span class="co">[</span><span class="ot">arXiv:1804.07461</span><span class="co">](https://arxiv.org/abs/1804.07461)</span></span>
<span id="cb6-841"><a href="#cb6-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-842"><a href="#cb6-842" aria-hidden="true" tabindex="-1"></a>**Zheng, L. et al. (2023). "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"**。生成式评测新范式的代表作。重点阅读：Section 3（LLM-as-Judge 方法论）、Section 4（偏差分析）。<span class="co">[</span><span class="ot">arXiv:2306.05685</span><span class="co">](https://arxiv.org/abs/2306.05685)</span></span>
<span id="cb6-843"><a href="#cb6-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-844"><a href="#cb6-844" aria-hidden="true" tabindex="-1"></a>**Liang, P. et al. (2022). "Holistic Evaluation of Language Models"**。最全面的评测框架设计。重点阅读：Section 2（评测分类学）。可快速浏览：具体模型的结果（过时较快）。<span class="co">[</span><span class="ot">arXiv:2211.09110</span><span class="co">](https://arxiv.org/abs/2211.09110)</span></span>
<span id="cb6-845"><a href="#cb6-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-846"><a href="#cb6-846" aria-hidden="true" tabindex="-1"></a><span class="fu">### 经典指标</span></span>
<span id="cb6-847"><a href="#cb6-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-848"><a href="#cb6-848" aria-hidden="true" tabindex="-1"></a>**Papineni, K. et al. (2002). "BLEU: a Method for Automatic Evaluation of Machine Translation"**。自动评测的开山之作。重点阅读：Section 2（BLEU 公式定义）。20 多年后仍被广泛使用。<span class="co">[</span><span class="ot">ACL 2002</span><span class="co">](https://aclanthology.org/P02-1040/)</span></span>
<span id="cb6-849"><a href="#cb6-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-850"><a href="#cb6-850" aria-hidden="true" tabindex="-1"></a>**Zhang, T. et al. (2020). "BERTScore: Evaluating Text Generation with BERT"**。语义评测指标的代表。<span class="co">[</span><span class="ot">arXiv:1904.09675</span><span class="co">](https://arxiv.org/abs/1904.09675)</span></span>
<span id="cb6-851"><a href="#cb6-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-852"><a href="#cb6-852" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数据污染与科学诚实</span></span>
<span id="cb6-853"><a href="#cb6-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-854"><a href="#cb6-854" aria-hidden="true" tabindex="-1"></a>**Oren, Y. et al. (2023). "Proving Test Set Contamination in Black Box Language Models"**。黑盒模型的污染检测——无需访问训练数据即可推断污染。<span class="co">[</span><span class="ot">arXiv:2310.17623</span><span class="co">](https://arxiv.org/abs/2310.17623)</span></span>
<span id="cb6-855"><a href="#cb6-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-856"><a href="#cb6-856" aria-hidden="true" tabindex="-1"></a>**Deng, C. et al. (2023). "Investigating Data Contamination in Modern Benchmarks for Large Language Models"**。TS-Guessing 方法——通过给模型部分输入来检测是否"见过"benchmark 题目。<span class="co">[</span><span class="ot">arXiv:2311.09783</span><span class="co">](https://arxiv.org/abs/2311.09783)</span></span>
<span id="cb6-857"><a href="#cb6-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-858"><a href="#cb6-858" aria-hidden="true" tabindex="-1"></a>**Golchin, S. &amp; Surdeanu, M. (2023). "Time Travel in LLMs: Tracing Data Contamination in Large Language Models"**。Time Travel 方法——测试模型是否能逐字复现 benchmark 原文。<span class="co">[</span><span class="ot">arXiv:2308.08493</span><span class="co">](https://arxiv.org/abs/2308.08493)</span></span>
<span id="cb6-859"><a href="#cb6-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-860"><a href="#cb6-860" aria-hidden="true" tabindex="-1"></a>**Schaeffer, R. et al. (2023). "Are Emergent Abilities of Large Language Models a Mirage?"**。度量选择对能力判断的影响。NeurIPS 2023 最佳论文。<span class="co">[</span><span class="ot">arXiv:2304.15004</span><span class="co">](https://arxiv.org/abs/2304.15004)</span></span>
<span id="cb6-861"><a href="#cb6-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-862"><a href="#cb6-862" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb6-863"><a href="#cb6-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-864"><a href="#cb6-864" aria-hidden="true" tabindex="-1"></a>**Chiang, W. et al. (2024). "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"**。Chatbot Arena 的完整技术报告。<span class="co">[</span><span class="ot">arXiv:2403.04132</span><span class="co">](https://arxiv.org/abs/2403.04132)</span></span>
<span id="cb6-865"><a href="#cb6-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-866"><a href="#cb6-866" aria-hidden="true" tabindex="-1"></a>**Hendrycks, D. et al. (2020). "Measuring Massive Multitask Language Understanding"**。MMLU benchmark。<span class="co">[</span><span class="ot">arXiv:2009.03300</span><span class="co">](https://arxiv.org/abs/2009.03300)</span></span>
<span id="cb6-867"><a href="#cb6-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-868"><a href="#cb6-868" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb6-869"><a href="#cb6-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-870"><a href="#cb6-870" aria-hidden="true" tabindex="-1"></a>**Chang, Y. et al. (2024). "A Survey on Evaluation of Large Language Models"**。LLM 评测方法的全面综述。<span class="co">[</span><span class="ot">arXiv:2307.03109</span><span class="co">](https://arxiv.org/abs/2307.03109)</span></span>
<span id="cb6-871"><a href="#cb6-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-872"><a href="#cb6-872" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb6-873"><a href="#cb6-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-874"><a href="#cb6-874" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**lm-evaluation-harness**：<span class="co">[</span><span class="ot">github.com/EleutherAI/lm-evaluation-harness</span><span class="co">](https://github.com/EleutherAI/lm-evaluation-harness)</span>（最广泛使用的评测框架）</span>
<span id="cb6-875"><a href="#cb6-875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**OpenCompass**：<span class="co">[</span><span class="ot">github.com/open-compass/opencompass</span><span class="co">](https://github.com/open-compass/opencompass)</span>（支持中文评测）</span>
<span id="cb6-876"><a href="#cb6-876" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chatbot Arena**：<span class="co">[</span><span class="ot">chat.lmsys.org</span><span class="co">](https://chat.lmsys.org/)</span>（在线参与评测）</span>
<span id="cb6-877"><a href="#cb6-877" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**HELM**：<span class="co">[</span><span class="ot">crfm.stanford.edu/helm</span><span class="co">](https://crfm.stanford.edu/helm/)</span>（Stanford 评测平台）</span>
<span id="cb6-878"><a href="#cb6-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-879"><a href="#cb6-879" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-880"><a href="#cb6-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-881"><a href="#cb6-881" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb6-882"><a href="#cb6-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-883"><a href="#cb6-883" aria-hidden="true" tabindex="-1"></a>NLP 评测的历史充满了意料之外的转折。</span>
<span id="cb6-884"><a href="#cb6-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-885"><a href="#cb6-885" aria-hidden="true" tabindex="-1"></a>BLEU 的发明者 Kishore Papineni 最初是在 IBM 研究院工作时提出这个指标的。他的动机非常实际：机器翻译研究者需要一个自动化的评测方法来快速迭代系统，而人类评测太慢太贵。BLEU 论文发表于 2002 年 ACL，至今已被引用超过 20,000 次，是 NLP 领域被引用最多的论文之一。然而 Papineni 本人后来在多个场合表示，BLEU 被过度使用了——它从来不是为了替代人类判断，而只是一个快速的近似。</span>
<span id="cb6-886"><a href="#cb6-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-887"><a href="#cb6-887" aria-hidden="true" tabindex="-1"></a>GLUE 的故事则带有一丝戏剧性。当 Sam Wang 等人在 2018 年设计 GLUE 时，他们认为这个 benchmark 至少能管用好几年。没想到仅仅几个月后，BERT 就让 GLUE 的排行榜变成了"卷王争霸赛"。Wang 等人不得不紧急设计 SuperGLUE——这可能是 NLP 历史上最快被自己的成功逼迫"升级"的 benchmark。</span>
<span id="cb6-888"><a href="#cb6-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-889"><a href="#cb6-889" aria-hidden="true" tabindex="-1"></a>Chatbot Arena 的诞生同样充满偶然。LMSYS（Large Model Systems Organization）最初是 UC Berkeley 一群研究系统优化的学生组建的，他们的初衷是研究 LLM 推理效率。但当他们搭建了一个供公众体验不同模型的网站后，用户自发开始比较不同模型的回答质量。这个"副产品"——用户自发的模型比较——最终演变成了整个 LLM 领域最被信赖的排名系统。</span>
<span id="cb6-890"><a href="#cb6-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-891"><a href="#cb6-891" aria-hidden="true" tabindex="-1"></a>评测方法论的演进还在继续。2024-2025 年，社区开始探讨"活性 benchmark"（living benchmarks）的概念——benchmark 不再是一次性发布的静态数据集，而是持续更新、动态生成、自动防污染的活系统。这或许是应对 Goodhart 定律的下一步尝试，但历史告诉我们，每一次尝试最终都会遇到新的挑战。正如一位评审人幽默地总结的："评测方法论的唯一不变定律是，没有评测方法是永恒的。"</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>