<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-26">
<meta name="description" content="当Transformer遇上长序列：Sparse Attention、Linear Attention、Low-Rank三条路线如何挑战O(n²)的魔咒，以及为什么实践给出了出人意料的答案。">

<title>第9章：高效注意力——复杂度优化 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#on2-到底意味着什么" id="toc-on2-到底意味着什么" class="nav-link" data-scroll-target="#on2-到底意味着什么"><span class="header-section-number">2.1</span> <span class="math inline">\(O(n^2)\)</span> 到底意味着什么？</a></li>
  <li><a href="#但注意力矩阵真的满了吗" id="toc-但注意力矩阵真的满了吗" class="nav-link" data-scroll-target="#但注意力矩阵真的满了吗"><span class="header-section-number">2.2</span> 但注意力矩阵真的”满”了吗？</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#两条路线的直觉" id="toc-两条路线的直觉" class="nav-link" data-scroll-target="#两条路线的直觉"><span class="header-section-number">3.1</span> 两条路线的直觉</a></li>
  <li><a href="#从注意力公式看优化空间" id="toc-从注意力公式看优化空间" class="nav-link" data-scroll-target="#从注意力公式看优化空间"><span class="header-section-number">3.2</span> 从注意力公式看优化空间</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#sparse-attention-家族" id="toc-sparse-attention-家族" class="nav-link" data-scroll-target="#sparse-attention-家族"><span class="header-section-number">4.1</span> Sparse Attention 家族</a></li>
  <li><a href="#low-rank-方法linformer" id="toc-low-rank-方法linformer" class="nav-link" data-scroll-target="#low-rank-方法linformer"><span class="header-section-number">4.2</span> Low-Rank 方法：Linformer</a></li>
  <li><a href="#linear-attention-家族" id="toc-linear-attention-家族" class="nav-link" data-scroll-target="#linear-attention-家族"><span class="header-section-number">4.3</span> Linear Attention 家族</a></li>
  <li><a href="#复杂度全景对比" id="toc-复杂度全景对比" class="nav-link" data-scroll-target="#复杂度全景对比"><span class="header-section-number">4.4</span> 复杂度全景对比</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#从零实现-sliding-window-attention" id="toc-从零实现-sliding-window-attention" class="nav-link" data-scroll-target="#从零实现-sliding-window-attention"><span class="header-section-number">5.1</span> 从零实现 Sliding Window Attention</a></li>
  <li><a href="#从零实现-linear-attention" id="toc-从零实现-linear-attention" class="nav-link" data-scroll-target="#从零实现-linear-attention"><span class="header-section-number">5.2</span> 从零实现 Linear Attention</a></li>
  <li><a href="#使用-hugging-face-的高效注意力模型" id="toc-使用-hugging-face-的高效注意力模型" class="nav-link" data-scroll-target="#使用-hugging-face-的高效注意力模型"><span class="header-section-number">5.3</span> 使用 Hugging Face 的高效注意力模型</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么线性注意力有性能损失" id="toc-为什么线性注意力有性能损失" class="nav-link" data-scroll-target="#为什么线性注意力有性能损失"><span class="header-section-number">6.1</span> 为什么线性注意力有性能损失？</a></li>
  <li><a href="#稀疏注意力的理论基础" id="toc-稀疏注意力的理论基础" class="nav-link" data-scroll-target="#稀疏注意力的理论基础"><span class="header-section-number">6.2</span> 稀疏注意力的理论基础</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#实践给出的出人意料的答案" id="toc-实践给出的出人意料的答案" class="nav-link" data-scroll-target="#实践给出的出人意料的答案"><span class="header-section-number">7.1</span> 实践给出的出人意料的答案</a></li>
  <li><a href="#高效注意力的遗产" id="toc-高效注意力的遗产" class="nav-link" data-scroll-target="#高效注意力的遗产"><span class="header-section-number">7.2</span> 高效注意力的遗产</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.3</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">9.2</span> 理论基础</a></li>
  <li><a href="#综述" id="toc-综述" class="nav-link" data-scroll-target="#综述"><span class="header-section-number">9.3</span> 综述</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.4</span> 后续发展</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第9章：高效注意力——复杂度优化</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">O(n²)的第一次反击</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Efficient Attention</div>
    <div class="quarto-category">Transformer</div>
  </div>
  </div>

<div>
  <div class="description">
    当Transformer遇上长序列：Sparse Attention、Linear Attention、Low-Rank三条路线如何挑战O(n²)的魔咒，以及为什么实践给出了出人意料的答案。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Tay et al.&nbsp;(2022)</strong> “Efficient Transformers: A Survey” — 参考了七大类分类体系（Figure 2）、各方法的统一比较框架</li>
<li><strong>Beltagy et al.&nbsp;(2020)</strong> “Longformer: The Long-Document Transformer” — 参考了滑动窗口+全局注意力设计（Section 3）、注意力模式可视化（Figure 2）</li>
<li><strong>Zaheer et al.&nbsp;(2020)</strong> “Big Bird: Transformers for Longer Sequences” — 参考了稀疏注意力的图灵完备性证明（Section 2）、三组件设计（Section 4）</li>
<li><strong>Choromanski et al.&nbsp;(2020)</strong> “Rethinking Attention with Performers” — 参考了FAVOR+算法推导（Section 3-4）、正交随机特征</li>
<li><strong>Katharopoulos et al.&nbsp;(2020)</strong> “Transformers are RNNs” — 参考了Transformer-RNN对偶性推导（Section 3.3）</li>
<li><strong>Wang et al.&nbsp;(2020)</strong> “Linformer: Self-Attention with Linear Complexity” — 参考了低秩近似理论（Section 3）</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Section 11.6 (Self-Attention and Positional Encoding) — 参考了注意力复杂度分析的教学组织方式</li>
<li><strong>SLP3</strong> Chapter 9-10 — 参考了Transformer架构和注意力机制的基础讲解框架</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 9 (2025) “Pretraining” — 参考了高效注意力方法的概述与对比</li>
<li><strong>CMU 11-711</strong> ANLP (2024) — 参考了长序列建模的工程挑战分析</li>
</ul>
</section>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何在保持 Transformer 表达能力的同时，打破 Self-Attention 的 <span class="math inline">\(O(n^2)\)</span> 计算瓶颈？</p>
<p><strong>历史坐标</strong>：2020年 | Longformer, BigBird, Performer, Linformer | “X-former”百花齐放的一年</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了 Transformer 的诞生。这个完全基于注意力的架构彻底抛弃了循环结构，用 Scaled Dot-Product Attention、Multi-Head、残差连接和 Layer Normalization 构建了一个并行高效、表达能力强大的序列建模框架。Transformer 在机器翻译上的表现令人震惊，训练速度比 RNN 快了一个数量级，很快成为整个 NLP 领域的基础架构。</p>
<p>然而，我们在上一章结尾也指出了 Transformer 最大的隐患——Self-Attention 的二次复杂度。每个 token 需要与序列中所有其他 token 计算注意力分数，这意味着计算量和内存消耗都随序列长度 <span class="math inline">\(n\)</span> 呈 <span class="math inline">\(O(n^2)\)</span> 增长。对于一篇几百词的句子，这不是问题；但当我们想处理一本10万词的书、一份完整的法律合同、或一段长达数小时的对话时，<span class="math inline">\(O(n^2)\)</span> 就变成了一道不可逾越的障碍。</p>
<p>2020年，NLP 社区对这个问题发起了集体冲锋。Longformer、BigBird、Performer、Linformer……一时间，各种 “X-former” 如雨后春笋般涌现，每一个都宣称找到了打破 <span class="math inline">\(O(n^2)\)</span> 的方法。Yi Tay 等人在一篇 survey 中将这些方法归纳为七大类——固定模式、可学习模式、低秩方法、核方法、记忆机制、下采样和循环——形成了高效 Transformer 的完整图谱。</p>
<div id="fig-tay-taxonomy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tay-taxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-9/original/tay-survey-fig2-taxonomy.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tay-taxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 高效 Transformer 方法分类体系。不同颜色的圈代表不同类别的方法，许多模型同时属于多个类别。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Tay et al.&nbsp;(2022) “Efficient Transformers: A Survey”, Figure 2. <a href="https://arxiv.org/abs/2009.06732">arXiv:2009.06732</a></em></p>
</div>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：打破 <span class="math inline">\(O(n^2)\)</span> 有两条主要思路。第一条是<strong>稀疏注意力</strong>——既然大多数注意力权重接近零，何不只计算”重要的”那些？第二条是<strong>线性注意力</strong>——能否用数学技巧重新组织计算，避免显式构建 <span class="math inline">\(n \times n\)</span> 的注意力矩阵？两条路线各有优劣，而实践最终给出了一个出人意料的答案。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="on2-到底意味着什么" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="on2-到底意味着什么"><span class="header-section-number">2.1</span> <span class="math inline">\(O(n^2)\)</span> 到底意味着什么？</h3>
<p>让我们用具体数字感受一下二次复杂度的严重性。标准 Self-Attention 的核心计算是 <span class="math inline">\(\text{softmax}(QK^\top / \sqrt{d_k}) \cdot V\)</span>，其中 <span class="math inline">\(Q, K, V \in \mathbb{R}^{n \times d}\)</span>。矩阵乘法 <span class="math inline">\(QK^\top\)</span> 产生一个 <span class="math inline">\(n \times n\)</span> 的注意力矩阵——这就是二次复杂度的来源。</p>
<p><span class="math display">\[
\text{时间复杂度} = O(n^2 d) \quad \text{（计算注意力矩阵）}
\]</span></p>
<p><span class="math display">\[
\text{空间复杂度} = O(n^2) \quad \text{（存储注意力矩阵）}
\]</span></p>
<p>把这个公式代入具体场景：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>序列长度 <span class="math inline">\(n\)</span></th>
<th>注意力矩阵大小</th>
<th>FP16 显存</th>
<th>典型场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>512</td>
<td>262K</td>
<td>0.5 MB</td>
<td>BERT 默认</td>
</tr>
<tr class="even">
<td>2,048</td>
<td>4.2M</td>
<td>8 MB</td>
<td>GPT-2</td>
</tr>
<tr class="odd">
<td>8,192</td>
<td>67M</td>
<td>128 MB</td>
<td>中等文档</td>
</tr>
<tr class="even">
<td>32,768</td>
<td>1.07B</td>
<td>2 GB</td>
<td>长文档</td>
</tr>
<tr class="odd">
<td>100,000</td>
<td>10B</td>
<td>20 GB</td>
<td>一本书</td>
</tr>
<tr class="even">
<td>1,000,000</td>
<td><span class="math inline">\(10^{12}\)</span></td>
<td>2 TB</td>
<td>长代码库</td>
</tr>
</tbody>
</table>
<p>这个表格揭示了一个残酷的现实：序列长度每翻一倍，计算和内存需求翻四倍。当序列长度达到10万时，仅存储单层的注意力矩阵就需要20GB显存——这还没算多头、多层的叠加。</p>
<div id="fig-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-9/fig-complexity-comparison.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: 各类注意力方法的计算量随序列长度的增长曲线（双对数坐标）。全注意力（红色）的二次增长与线性方法（蓝、橙、绿）的差距在长序列上极为显著。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>作者绘制。数据基于各方法论文报告的理论复杂度计算。</em></p>
</div>
</section>
<section id="但注意力矩阵真的满了吗" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="但注意力矩阵真的满了吗"><span class="header-section-number">2.2</span> 但注意力矩阵真的”满”了吗？</h3>
<p>问题的关键在于：<span class="math inline">\(O(n^2)\)</span> 的计算真的都是必要的吗？让我们想象处理一篇长文档。当模型在理解第500段中的某个词时，它真的需要精确关注第1段里的每一个词吗？</p>
<p>实证研究给出了否定的答案。多项研究发现，训练好的 Transformer 中，绝大多数注意力权重接近于零。注意力矩阵往往呈现出明显的<strong>稀疏模式</strong>：</p>
<ul>
<li><strong>局部模式</strong>：大部分注意力集中在当前位置附近的几个词上</li>
<li><strong>全局模式</strong>：某些特殊 token（如 <code>[CLS]</code>、句号、频繁词）会被全局关注</li>
<li><strong>条纹模式</strong>：某些层呈现规律性的跨步注意力</li>
</ul>
<p>Linformer 的作者进一步发现，注意力矩阵通常是<strong>低秩的</strong>——也就是说，一个 <span class="math inline">\(n \times n\)</span> 的矩阵可以被远小于 <span class="math inline">\(n\)</span> 维的信息近似表示。这些经验观察打开了优化的大门：如果我们知道大多数注意力权重不重要，为什么还要花费算力去计算它们？</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>理想的高效注意力机制应该满足以下条件：</p>
<p>首先，<strong>亚二次复杂度</strong>——最好是 <span class="math inline">\(O(n)\)</span> 或 <span class="math inline">\(O(n \log n)\)</span>，让序列长度的增长不再令人恐惧。其次，<strong>性能损失可控</strong>——如果提速10倍但效果下降5个点，那就得不偿失了。第三，<strong>通用性</strong>——不能只在特定任务或特定长度上有效。最后，<strong>工程可行性</strong>——能在现有硬件上高效实现，而非只是理论上漂亮。</p>
<p>这四个条件构成了评判所有高效注意力方法的标尺。接下来我们将看到，不同的方法在这四个维度上做出了不同的取舍。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="两条路线的直觉" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="两条路线的直觉"><span class="header-section-number">3.1</span> 两条路线的直觉</h3>
<p>打破 <span class="math inline">\(O(n^2)\)</span> 的方法可以用一个生活类比来理解。</p>
<p>想象你是一个公司的 CEO，需要了解公司里每个员工的工作状态。最原始的方式是<strong>全员一对一面谈</strong>——这就是标准 Self-Attention 的做法，每个人和每个人都交流一次，复杂度是 <span class="math inline">\(O(n^2)\)</span>。</p>
<p><strong>稀疏注意力</strong>的思路是：不需要每个人都和每个人谈。你只需要和自己部门的同事（<strong>局部窗口</strong>）交流，加上几个了解全局的高管（<strong>全局 token</strong>），偶尔随机抽查几个人（<strong>随机注意力</strong>），就能掌握大致情况。这就是 Longformer 和 BigBird 的策略——显式地决定”谁和谁交流”，跳过不重要的组合。</p>
<p><strong>线性注意力</strong>的思路完全不同。它不是减少交流的次数，而是<strong>换一种交流方式</strong>。想象每个员工先把自己的状态压缩成一份简短的报告（特征映射），然后公司汇总所有报告得到一份全局摘要，最后每个人根据自己的需求从全局摘要中提取信息。这种”先汇总再查询”的方式，避免了所有人之间的两两交流，复杂度降到了 <span class="math inline">\(O(n)\)</span>。这就是 Performer 和 Linear Transformer 的核心思想。</p>
</section>
<section id="从注意力公式看优化空间" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="从注意力公式看优化空间"><span class="header-section-number">3.2</span> 从注意力公式看优化空间</h3>
<p>让我们从数学角度看这两条路线的本质。标准注意力可以写成：</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]</span></p>
<p>展开对第 <span class="math inline">\(i\)</span> 个位置的输出：</p>
<p><span class="math display">\[
\text{output}_i = \frac{\sum_{j=1}^{n} \exp(q_i^\top k_j / \sqrt{d_k}) \cdot v_j}{\sum_{j=1}^{n} \exp(q_i^\top k_j / \sqrt{d_k})}
\]</span></p>
<p><strong>稀疏注意力</strong>的做法是限制求和范围——不再对所有 <span class="math inline">\(j \in \{1, \ldots, n\}\)</span> 求和，而只对 <span class="math inline">\(j \in \mathcal{S}_i\)</span>（一个远小于 <span class="math inline">\(n\)</span> 的子集）求和：</p>
<p><span class="math display">\[
\text{output}_i = \frac{\sum_{j \in \mathcal{S}_i} \exp(q_i^\top k_j / \sqrt{d_k}) \cdot v_j}{\sum_{j \in \mathcal{S}_i} \exp(q_i^\top k_j / \sqrt{d_k})}
\]</span></p>
<p>不同的稀疏注意力方法，本质上就是在设计不同的集合 <span class="math inline">\(\mathcal{S}_i\)</span>。</p>
<p><strong>线性注意力</strong>的做法更巧妙。它用一个特征映射 <span class="math inline">\(\phi\)</span> 来近似 softmax 核：</p>
<p><span class="math display">\[
\exp(q_i^\top k_j / \sqrt{d_k}) \approx \phi(q_i)^\top \phi(k_j)
\]</span></p>
<p>这样注意力公式变成：</p>
<p><span class="math display">\[
\text{output}_i = \frac{\phi(q_i)^\top \sum_{j=1}^{n} \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_{j=1}^{n} \phi(k_j)}
\]</span></p>
<p>关键观察：<span class="math inline">\(\sum_{j=1}^{n} \phi(k_j) v_j^\top\)</span> 和 <span class="math inline">\(\sum_{j=1}^{n} \phi(k_j)\)</span> 可以<strong>先计算一次并复用</strong>。这就把 <span class="math inline">\(O(n^2)\)</span> 变成了 <span class="math inline">\(O(n)\)</span>！</p>
<div id="fig-linear-attention-order" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linear-attention-order-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-9/fig-linear-attention-order.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linear-attention-order-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 标准注意力 vs 线性注意力的计算顺序对比。左：标准注意力先计算 <span class="math inline">\(QK^\top\)</span> 产生 <span class="math inline">\(n \times n\)</span> 的瓶颈矩阵；右：线性注意力先计算 <span class="math inline">\(\phi(K)^\top V\)</span> 产生 <span class="math inline">\(d \times d\)</span> 的小矩阵，从而避免了二次复杂度。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>作者绘制。基于 Katharopoulos et al.&nbsp;(2020) “Transformers are RNNs” 的矩阵乘法结合律分析。</em></p>
</div>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="sparse-attention-家族" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="sparse-attention-家族"><span class="header-section-number">4.1</span> Sparse Attention 家族</h3>
<section id="longformer滑动窗口-全局-token" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="longformer滑动窗口-全局-token"><span class="header-section-number">4.1.1</span> Longformer：滑动窗口 + 全局 token</h4>
<p>Longformer（Beltagy et al., 2020）的设计非常直观。它的注意力模式由两部分组成：</p>
<p><strong>局部滑动窗口（Local Sliding Window）</strong>。每个 token 只关注自身周围固定大小 <span class="math inline">\(w\)</span> 的窗口。对于位置 <span class="math inline">\(i\)</span>，注意力范围是 <span class="math inline">\(\{i - w/2, \ldots, i + w/2\}\)</span>。单层的感受野是 <span class="math inline">\(w\)</span>，但通过堆叠 <span class="math inline">\(L\)</span> 层，顶层的感受野扩展到 <span class="math inline">\(L \times w\)</span>——这与 CNN 的感受野增长方式完全一致。</p>
<p><strong>全局注意力（Global Attention）</strong>。某些特殊位置被赋予”全局”权限，可以关注序列中的所有 token，同时也被所有 token 关注。这些位置通常是任务相关的特殊 token，例如分类任务中的 <code>[CLS]</code>。全局 token 的数量远小于 <span class="math inline">\(n\)</span>（通常只有几个），因此不影响整体复杂度。</p>
<p>把两者组合起来，Longformer 的复杂度是 <span class="math inline">\(O(n \times w)\)</span>。由于窗口大小 <span class="math inline">\(w\)</span> 是常数（通常为 256 或 512），复杂度是<strong>关于序列长度线性的</strong>。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>直觉：为什么滑动窗口 + 全局 token 足够？
</div>
</div>
<div class="callout-body-container callout-body">
<p>想象阅读一篇论文。你在理解某个段落时，主要参考的是<strong>上下文几段的内容</strong>（局部窗口），偶尔需要回顾<strong>摘要或引言中的核心论点</strong>（全局 token）。你不需要在读每一句话时都精确回顾论文的每一个细节——局部上下文加上少量全局锚点，已经足够理解大部分内容。</p>
</div>
</div>
<div id="fig-attention-patterns" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-9/fig-attention-patterns.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: 四种注意力模式对比。(a) 全注意力：每个 token 关注所有位置（100%）；(b) 滑动窗口：只关注相邻位置（38%）；(c) 扩张滑动窗口：以固定间隔扩大感受野（44%）；(d) 全局+滑动窗口：特殊 token 获得全局视野（61%）。底部百分比表示活跃注意力对占全部可能的比例。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Beltagy et al.&nbsp;(2020) “Longformer: The Long-Document Transformer”, Figure 2. <a href="https://arxiv.org/abs/2004.05150">arXiv:2004.05150</a></em></p>
</div>
</section>
<section id="bigbird随机-局部-全局" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="bigbird随机-局部-全局"><span class="header-section-number">4.1.2</span> BigBird：随机 + 局部 + 全局</h4>
<p>BigBird（Zaheer et al., 2020）在 Longformer 的基础上增加了一个额外的组件——<strong>随机注意力</strong>。每个 token 除了关注局部窗口和全局 token 之外，还会随机关注 <span class="math inline">\(r\)</span> 个其他位置。</p>
<p>BigBird 的注意力由三部分构成：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>组件</th>
<th>模式</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>局部窗口</strong></td>
<td>每个 token 关注相邻 <span class="math inline">\(w\)</span> 个 token</td>
<td>捕获局部上下文</td>
</tr>
<tr class="even">
<td><strong>全局 token</strong></td>
<td><span class="math inline">\(g\)</span> 个 token 关注/被关注所有位置</td>
<td>提供全局信息通道</td>
</tr>
<tr class="odd">
<td><strong>随机连接</strong></td>
<td>每个 token 随机关注 <span class="math inline">\(r\)</span> 个位置</td>
<td>缩短任意两点间的信息传播路径</td>
</tr>
</tbody>
</table>
<div id="fig-bigbird-combined" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bigbird-combined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-9/original/bigbird-fig1d-combined.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bigbird-combined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: BigBird 的注意力矩阵：绿色为全局 token（前两行/列）、蓝色为局部滑动窗口、橙色为随机连接。三者组合实现了稀疏但”连通”的注意力模式。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Zaheer et al.&nbsp;(2020) “Big Bird: Transformers for Longer Sequences”, Figure 1. <a href="https://arxiv.org/abs/2007.14062">arXiv:2007.14062</a></em></p>
</div>
<p>为什么需要随机注意力？BigBird 的作者从图论的角度给出了解释。将注意力模式看作一张图：每个 token 是节点，有注意力连接的 token 之间有边。纯局部窗口构成的图，两个远距离节点之间的最短路径可能很长（需要经过多层才能通信）。加入随机边之后，根据 Watts-Strogatz 小世界网络理论，图的直径会急剧缩短——这就是”六度分隔”理论在注意力机制中的应用。</p>
<p>更重要的是，BigBird 的作者从理论上证明了一个关键结论：<strong>BigBird 的稀疏注意力是图灵完备的</strong>，并且是序列到序列函数的通用逼近器。这意味着 BigBird 在理论上不会因为稀疏化而损失表达能力——当然，前提是网络足够深、全局 token 足够多。</p>
</section>
<section id="完整数值示例稀疏注意力-vs-全注意力" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="完整数值示例稀疏注意力-vs-全注意力"><span class="header-section-number">4.1.3</span> 完整数值示例：稀疏注意力 vs 全注意力</h4>
<p>让我们用一个小例子直观地感受稀疏注意力的效果。</p>
<p><strong>设定</strong>：4 个 token，<span class="math inline">\(d_k = 2\)</span>，窗口大小 <span class="math inline">\(w = 2\)</span>（每个 token 只看左右各1个邻居）。</p>
<p><strong>Step 1: Q, K, V 矩阵</strong></p>
<p><span class="math display">\[
Q = \begin{bmatrix} 1.0 &amp; 0.5 \\ 0.8 &amp; 0.2 \\ 0.3 &amp; 0.9 \\ 0.6 &amp; 0.7 \end{bmatrix}, \quad
K = \begin{bmatrix} 0.9 &amp; 0.4 \\ 0.7 &amp; 0.6 \\ 0.5 &amp; 0.8 \\ 0.2 &amp; 1.0 \end{bmatrix}, \quad
V = \begin{bmatrix} 1.0 &amp; 0.0 \\ 0.0 &amp; 1.0 \\ 0.5 &amp; 0.5 \\ 1.0 &amp; 1.0 \end{bmatrix}
\]</span></p>
<p><strong>Step 2: 全注意力——计算完整的 <span class="math inline">\(QK^\top / \sqrt{d_k}\)</span></strong></p>
<p><span class="math display">\[
QK^\top = \begin{bmatrix}
1.10 &amp; 1.00 &amp; 0.90 &amp; 0.70 \\
0.80 &amp; 0.68 &amp; 0.56 &amp; 0.36 \\
0.63 &amp; 0.75 &amp; 0.87 &amp; 0.96 \\
0.82 &amp; 0.84 &amp; 0.86 &amp; 0.82
\end{bmatrix}
\]</span></p>
<p>除以 <span class="math inline">\(\sqrt{d_k} = \sqrt{2} \approx 1.414\)</span>：</p>
<p><span class="math display">\[
\frac{QK^\top}{\sqrt{d_k}} = \begin{bmatrix}
0.78 &amp; 0.71 &amp; 0.64 &amp; 0.50 \\
0.57 &amp; 0.48 &amp; 0.40 &amp; 0.25 \\
0.45 &amp; 0.53 &amp; 0.62 &amp; 0.68 \\
0.58 &amp; 0.59 &amp; 0.61 &amp; 0.58
\end{bmatrix}
\]</span></p>
<p><strong>Step 3: 全注意力——Softmax（逐行）</strong></p>
<p>对第一行：<span class="math inline">\(\text{softmax}([0.78, 0.71, 0.64, 0.50])\)</span></p>
<p><span class="math display">\[
A_{\text{full}} \approx \begin{bmatrix}
0.279 &amp; 0.261 &amp; 0.243 &amp; 0.212 \\
0.272 &amp; 0.249 &amp; 0.230 &amp; 0.198 \\
0.229 &amp; 0.249 &amp; 0.272 &amp; 0.289 \\
0.254 &amp; 0.256 &amp; 0.262 &amp; 0.253
\end{bmatrix}
\]</span></p>
<p><strong>Step 4: 稀疏注意力——只保留窗口内的权重</strong></p>
<p>窗口大小 <span class="math inline">\(w = 2\)</span>（左右各1），每个 token 的关注范围如下：</p>
<ul>
<li>Token 0：可关注 {0, 1}</li>
<li>Token 1：可关注 {0, 1, 2}</li>
<li>Token 2：可关注 {1, 2, 3}</li>
<li>Token 3：可关注 {2, 3}</li>
</ul>
<p>稀疏掩码：</p>
<p><span class="math display">\[
M_{\text{sparse}} = \begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>将被掩码的位置设为 <span class="math inline">\(-\infty\)</span>（softmax 后变为 0），重新计算 softmax：</p>
<p><span class="math display">\[
A_{\text{sparse}} \approx \begin{bmatrix}
0.517 &amp; 0.483 &amp; 0 &amp; 0 \\
0.371 &amp; 0.339 &amp; 0.290 &amp; 0 \\
0 &amp; 0.294 &amp; 0.336 &amp; 0.370 \\
0 &amp; 0 &amp; 0.508 &amp; 0.492
\end{bmatrix}
\]</span></p>
<p><strong>Step 5: 对比输出</strong></p>
<p><span class="math display">\[
\text{Output}_{\text{full}} = A_{\text{full}} \cdot V \approx \begin{bmatrix}
0.609 &amp; 0.607 \\
0.594 &amp; 0.576 \\
0.654 &amp; 0.680 \\
0.637 &amp; 0.640
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\text{Output}_{\text{sparse}} = A_{\text{sparse}} \cdot V \approx \begin{bmatrix}
0.517 &amp; 0.483 \\
0.516 &amp; 0.484 \\
0.662 &amp; 0.685 \\
0.746 &amp; 0.746
\end{bmatrix}
\]</span></p>
<p><strong>解读</strong>：中间位置（Token 1, 2）的稀疏输出与全注意力输出比较接近，因为这些位置的窗口覆盖了大部分重要邻居。而边缘位置（Token 0, 3）的差异较大——它们失去了对远处 token 的关注能力。这正是稀疏注意力的核心权衡：<strong>局部信息保持完好，远距离信息需要通过多层传播来弥补</strong>。</p>
<p>在实际应用中，通过堆叠多层（<span class="math inline">\(L\)</span> 层的感受野为 <span class="math inline">\(L \times w\)</span>）以及加入全局 token，这种信息损失可以被有效缓解。</p>
</section>
</section>
<section id="low-rank-方法linformer" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="low-rank-方法linformer"><span class="header-section-number">4.2</span> Low-Rank 方法：Linformer</h3>
<p>Linformer（Wang et al., 2020）从一个不同的角度切入：既然注意力矩阵是低秩的，何不直接用低秩近似来降低复杂度？</p>
<p>核心思想是在计算 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span> 时，先用一个投影矩阵 <span class="math inline">\(E \in \mathbb{R}^{k \times n}\)</span>（其中 <span class="math inline">\(k \ll n\)</span>）将序列长度从 <span class="math inline">\(n\)</span> 压缩到 <span class="math inline">\(k\)</span>：</p>
<p><span class="math display">\[
\bar{K} = E \cdot K \in \mathbb{R}^{k \times d}, \quad \bar{V} = E \cdot V \in \mathbb{R}^{k \times d}
\]</span></p>
<p>然后用压缩后的 <span class="math inline">\(\bar{K}\)</span>、<span class="math inline">\(\bar{V}\)</span> 计算注意力：</p>
<p><span class="math display">\[
\text{Attention}(Q, \bar{K}, \bar{V}) = \text{softmax}\left(\frac{Q\bar{K}^\top}{\sqrt{d_k}}\right) \bar{V}
\]</span></p>
<p>此时注意力矩阵变成了 <span class="math inline">\(n \times k\)</span>（而非 <span class="math inline">\(n \times n\)</span>），时间和空间复杂度都降为 <span class="math inline">\(O(nk)\)</span>。由于 <span class="math inline">\(k\)</span> 是固定常数，复杂度对序列长度是线性的。</p>
<p>Linformer 的理论贡献是证明了一个关键性质：对于任意 <span class="math inline">\(\epsilon &gt; 0\)</span>，存在一个足够大的投影维度 <span class="math inline">\(k = O(d / \epsilon^2)\)</span>（与 <span class="math inline">\(n\)</span> 无关），使得低秩近似的误差不超过 <span class="math inline">\(\epsilon\)</span>。换句话说，<strong>注意力矩阵的有效信息维度与序列长度无关</strong>——这为低秩近似提供了坚实的理论基础。</p>
<p>不过，Linformer 有一个实际上的限制：投影矩阵 <span class="math inline">\(E\)</span> 的大小是 <span class="math inline">\(k \times n\)</span>，这意味着模型被绑定到固定的序列长度 <span class="math inline">\(n\)</span>。推理时如果输入长度不同，需要重新训练或做额外处理。</p>
</section>
<section id="linear-attention-家族" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="linear-attention-家族"><span class="header-section-number">4.3</span> Linear Attention 家族</h3>
<section id="核心洞察核技巧kernel-trick" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="核心洞察核技巧kernel-trick"><span class="header-section-number">4.3.1</span> 核心洞察：核技巧（Kernel Trick）</h4>
<p>线性注意力的理论基础来自一个优雅的数学观察。让我们重新审视 softmax 注意力的公式。对于第 <span class="math inline">\(i\)</span> 个位置的输出：</p>
<p><span class="math display">\[
\text{output}_i = \frac{\sum_{j=1}^{n} \kappa(q_i, k_j) \cdot v_j}{\sum_{j=1}^{n} \kappa(q_i, k_j)}
\]</span></p>
<p>其中核函数 <span class="math inline">\(\kappa(q, k) = \exp(q^\top k / \sqrt{d_k})\)</span>。如果我们能找到一个特征映射 <span class="math inline">\(\phi\)</span> 使得：</p>
<p><span class="math display">\[
\kappa(q, k) = \phi(q)^\top \phi(k)
\]</span></p>
<p>那么分子可以重写为：</p>
<p><span class="math display">\[
\sum_{j=1}^{n} \phi(q_i)^\top \phi(k_j) \cdot v_j = \phi(q_i)^\top \underbrace{\sum_{j=1}^{n} \phi(k_j) \cdot v_j^\top}_{S \in \mathbb{R}^{m \times d}}
\]</span></p>
<p>关键在于：矩阵 <span class="math inline">\(S = \sum_{j=1}^{n} \phi(k_j) v_j^\top\)</span> 只需要计算一次，所有位置的查询都可以复用。如果特征映射 <span class="math inline">\(\phi\)</span> 的维度是 <span class="math inline">\(m\)</span>，那么：</p>
<ul>
<li>计算 <span class="math inline">\(S\)</span>：<span class="math inline">\(O(nmd)\)</span></li>
<li>对每个查询提取输出：<span class="math inline">\(O(md)\)</span></li>
<li>总复杂度：<span class="math inline">\(O(nmd)\)</span></li>
</ul>
<p>当 <span class="math inline">\(m\)</span> 是常数时，总复杂度是 <span class="math inline">\(O(nd)\)</span>——<strong>真正的线性复杂度</strong>！</p>
</section>
<section id="performerfavor-算法" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="performerfavor-算法"><span class="header-section-number">4.3.2</span> Performer：FAVOR+ 算法</h4>
<p>Performer（Choromanski et al., 2020）的挑战是找到一个好的特征映射 <span class="math inline">\(\phi\)</span> 来近似 softmax 核。这并不简单，因为 <span class="math inline">\(\exp(q^\top k)\)</span> 是一个非平凡的核函数。</p>
<p>Performer 提出了 <strong>FAVOR+</strong>（Fast Attention Via positive Orthogonal Random features）算法。核心思想是利用随机特征来近似 softmax 核：</p>
<p><span class="math display">\[
\exp(q^\top k) = \mathbb{E}_{\omega \sim \mathcal{N}(0, I)} \left[ \exp(\omega^\top q - \|q\|^2/2) \cdot \exp(\omega^\top k - \|k\|^2/2) \right]
\]</span></p>
<p>这个等式来自高斯核的随机特征分解。基于此，Performer 定义特征映射：</p>
<p><span class="math display">\[
\phi(x) = \frac{\exp(-\|x\|^2/2)}{\sqrt{m}} \begin{bmatrix} \exp(\omega_1^\top x) \\ \exp(\omega_2^\top x) \\ \vdots \\ \exp(\omega_m^\top x) \end{bmatrix}
\]</span></p>
<p>其中 <span class="math inline">\(\omega_1, \ldots, \omega_m\)</span> 是从 <span class="math inline">\(\mathcal{N}(0, I)\)</span> 中采样的随机向量。FAVOR+ 的”+“代表两个改进：使用<strong>正随机特征</strong>（保证注意力权重非负）和<strong>正交随机特征</strong>（降低近似方差）。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>直觉：为什么随机特征能近似 softmax？
</div>
</div>
<div class="callout-body-container callout-body">
<p>这背后的数学原理是 Bochner 定理：任何平移不变的正定核都可以表示为某个概率分布的傅里叶变换。对于高斯核（softmax 核的基础），这个分布恰好也是高斯分布。随机特征方法本质上是用蒙特卡洛采样来近似这个积分——采样点越多（<span class="math inline">\(m\)</span> 越大），近似越精确。</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>算法框：Performer FAVOR+ 前向计算
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>：查询 <span class="math inline">\(Q \in \mathbb{R}^{n \times d}\)</span>，键 <span class="math inline">\(K \in \mathbb{R}^{n \times d}\)</span>，值 <span class="math inline">\(V \in \mathbb{R}^{n \times d}\)</span>，随机特征维度 <span class="math inline">\(m\)</span></p>
<p><strong>Step 1 — 采样正交随机矩阵</strong>：从 <span class="math inline">\(\mathcal{N}(0, I_d)\)</span> 采样 <span class="math inline">\(m\)</span> 个向量 <span class="math inline">\(\omega_1, \ldots, \omega_m\)</span>，经 Gram-Schmidt 正交化得到 <span class="math inline">\(\Omega \in \mathbb{R}^{m \times d}\)</span></p>
<p><strong>Step 2 — 正随机特征映射</strong>：对 <span class="math inline">\(Q\)</span> 和 <span class="math inline">\(K\)</span> 的每一行 <span class="math inline">\(x \in \mathbb{R}^d\)</span>，计算 <span class="math display">\[\phi(x) = \frac{\exp(-\|x\|^2/2)}{\sqrt{m}} \begin{bmatrix} \exp(\omega_1^\top x) \\ \exp(\omega_2^\top x) \\ \vdots \\ \exp(\omega_m^\top x) \end{bmatrix} \in \mathbb{R}^m\]</span> 得到 <span class="math inline">\(\hat{Q} = \phi(Q) \in \mathbb{R}^{n \times m}\)</span>，<span class="math inline">\(\hat{K} = \phi(K) \in \mathbb{R}^{n \times m}\)</span></p>
<p><strong>Step 3 — 先聚合再查询</strong>（避免 <span class="math inline">\(n \times n\)</span> 矩阵）：</p>
<ul>
<li>计算 KV 聚合矩阵：<span class="math inline">\(S = \hat{K}^\top V \in \mathbb{R}^{m \times d}\)</span> — 复杂度 <span class="math inline">\(O(nmd)\)</span></li>
<li>计算归一化向量：<span class="math inline">\(z = \hat{K}^\top \mathbf{1}_n \in \mathbb{R}^m\)</span> — 复杂度 <span class="math inline">\(O(nm)\)</span></li>
</ul>
<p><strong>Step 4 — 输出</strong>：对每个位置 <span class="math inline">\(i\)</span>，<span class="math inline">\(\text{output}_i = \frac{\hat{q}_i^\top S}{\hat{q}_i^\top z} \in \mathbb{R}^d\)</span> — 复杂度 <span class="math inline">\(O(md)\)</span></p>
<p><strong>总复杂度</strong>：<span class="math inline">\(O(nmd)\)</span> 时间，<span class="math inline">\(O(nm + md + nd)\)</span> 空间。当 <span class="math inline">\(m \ll n\)</span> 时，相比标准注意力的 <span class="math inline">\(O(n^2 d)\)</span> 获得显著加速。</p>
<p><strong>“+” 的含义</strong>：(1) <strong>Positive</strong> — 使用 <span class="math inline">\(\exp(\omega^\top x)\)</span> 而非 <span class="math inline">\(\cos/\sin\)</span> 保证注意力权重非负；(2) <strong>Orthogonal</strong> — <span class="math inline">\(\omega_i\)</span> 正交化降低蒙特卡洛方差，减少所需采样数 <span class="math inline">\(m\)</span>。</p>
</div>
</div>
</section>
<section id="linear-transformertransformer-与-rnn-的对偶性" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="linear-transformertransformer-与-rnn-的对偶性"><span class="header-section-number">4.3.3</span> Linear Transformer：Transformer 与 RNN 的对偶性</h4>
<p>Katharopoulos et al.（2020）在 “Transformers are RNNs” 中揭示了一个令人惊叹的联系。他们选择了一个简单的特征映射：</p>
<p><span class="math display">\[
\phi(x) = \text{elu}(x) + 1
\]</span></p>
<p>其中 <span class="math inline">\(\text{elu}\)</span> 是指数线性单元。这个选择虽然简单，但保证了映射后的值始终为正（核函数必须非负）。</p>
<p>更深刻的洞察在于：当线性注意力用于<strong>因果（自回归）</strong>场景时，它可以被改写为一个 RNN！</p>
<p>在因果设定下，第 <span class="math inline">\(i\)</span> 个位置只能关注 <span class="math inline">\(j \leq i\)</span> 的位置。线性注意力变成：</p>
<p><span class="math display">\[
\text{output}_i = \frac{\phi(q_i)^\top S_i}{\phi(q_i)^\top z_i}
\]</span></p>
<p>其中 <span class="math inline">\(S_i = \sum_{j=1}^{i} \phi(k_j) v_j^\top\)</span> 和 <span class="math inline">\(z_i = \sum_{j=1}^{i} \phi(k_j)\)</span> 可以<strong>递归更新</strong>：</p>
<p><span class="math display">\[
S_i = S_{i-1} + \phi(k_i) v_i^\top, \quad z_i = z_{i-1} + \phi(k_i)
\]</span></p>
<p>这恰好就是一个 RNN 的隐状态更新！<span class="math inline">\(S_i\)</span> 相当于 RNN 的隐藏状态，<span class="math inline">\(\phi(k_i) v_i^\top\)</span> 是每步的新信息。Transformer 和 RNN，看似截然不同的两种架构，在线性注意力的桥梁下竟然是同一枚硬币的两面。</p>
<p>这个对偶性带来了实际好处：训练时可以用并行的”Transformer 模式”（矩阵乘法），推理时可以切换到序列的”RNN 模式”（逐步递归），两者在数学上完全等价。</p>
</section>
</section>
<section id="复杂度全景对比" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="复杂度全景对比"><span class="header-section-number">4.4</span> 复杂度全景对比</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 19%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>时间复杂度</th>
<th>空间复杂度</th>
<th>理论保证</th>
<th>实际表现</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>全注意力</strong></td>
<td><span class="math inline">\(O(n^2 d)\)</span></td>
<td><span class="math inline">\(O(n^2 + nd)\)</span></td>
<td>最优表达能力</td>
<td>基准</td>
</tr>
<tr class="even">
<td><strong>Longformer</strong></td>
<td><span class="math inline">\(O(nw)\)</span></td>
<td><span class="math inline">\(O(nw)\)</span></td>
<td>感受野 <span class="math inline">\(L \times w\)</span></td>
<td>长文档任务优秀</td>
</tr>
<tr class="odd">
<td><strong>BigBird</strong></td>
<td><span class="math inline">\(O(n(w+r+g))\)</span></td>
<td><span class="math inline">\(O(n(w+r+g))\)</span></td>
<td>图灵完备</td>
<td>长文档 + QA</td>
</tr>
<tr class="even">
<td><strong>Linformer</strong></td>
<td><span class="math inline">\(O(nk)\)</span></td>
<td><span class="math inline">\(O(nk)\)</span></td>
<td>低秩近似有界</td>
<td>固定长度任务</td>
</tr>
<tr class="odd">
<td><strong>Performer</strong></td>
<td><span class="math inline">\(O(nmd)\)</span></td>
<td><span class="math inline">\(O(nm + md + nd)\)</span></td>
<td>无偏近似</td>
<td>语言建模稍弱</td>
</tr>
<tr class="even">
<td><strong>Linear Transformer</strong></td>
<td><span class="math inline">\(O(nd^2)\)</span></td>
<td><span class="math inline">\(O(nd + d^2)\)</span></td>
<td>RNN 对偶</td>
<td>自回归推理快</td>
</tr>
</tbody>
</table>
<p>这里有一个微妙但重要的细节：<span class="math inline">\(O(n)\)</span> 复杂度中的常数因子差异很大。Longformer 的 <span class="math inline">\(w\)</span> 通常是256-512，Performer 的 <span class="math inline">\(m\)</span> 通常需要和 <span class="math inline">\(d\)</span> 同阶甚至更大才能保证近似质量。所以<strong>渐近复杂度只是故事的一部分</strong>。</p>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="从零实现-sliding-window-attention" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="从零实现-sliding-window-attention"><span class="header-section-number">5.1</span> 从零实现 Sliding Window Attention</h3>
<p>下面用 PyTorch 实现 Longformer 风格的滑动窗口注意力，帮助理解其核心逻辑：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sliding_window_attention(Q, K, V, window_size, global_indices<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Longformer 风格的滑动窗口注意力</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Q, K, V: [batch, seq_len, d_k]</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        window_size: 单侧窗口大小（总窗口 = 2 * window_size + 1）</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        global_indices: 需要全局注意力的位置索引列表</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        output: [batch, seq_len, d_k]</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    batch, seq_len, d_k <span class="op">=</span> Q.shape</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> math.sqrt(d_k)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 构建滑动窗口掩码</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mask[i,j] = True 表示位置 i 可以关注位置 j</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.zeros(seq_len, seq_len, dtype<span class="op">=</span>torch.<span class="bu">bool</span>, device<span class="op">=</span>Q.device)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        left <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> <span class="bu">min</span>(seq_len, i <span class="op">+</span> window_size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        mask[i, left:right] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 添加全局注意力</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> global_indices <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> g <span class="kw">in</span> global_indices:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            mask[g, :] <span class="op">=</span> <span class="va">True</span>   <span class="co"># 全局 token 关注所有位置</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            mask[:, g] <span class="op">=</span> <span class="va">True</span>   <span class="co"># 所有位置关注全局 token</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 计算注意力分数</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> scale  <span class="co"># [batch, n, n]</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: 应用掩码（被掩码的位置设为 -inf）</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores.masked_fill(<span class="op">~</span>mask.unsqueeze(<span class="dv">0</span>), <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Softmax + 加权求和</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> attn_weights.masked_fill(torch.isnan(attn_weights), <span class="fl">0.0</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.matmul(attn_weights, V)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, attn_weights</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例用法</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>batch, seq_len, d_k <span class="op">=</span> <span class="dv">1</span>, <span class="dv">16</span>, <span class="dv">64</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.randn(batch, seq_len, d_k)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.randn(batch, seq_len, d_k)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.randn(batch, seq_len, d_k)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>output, weights <span class="op">=</span> sliding_window_attention(</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    Q, K, V,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span><span class="dv">3</span>,          <span class="co"># 每侧看3个 token</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    global_indices<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">15</span>]  <span class="co"># 首尾 token 为全局</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"输出形状: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)      <span class="co"># [1, 16, 64]</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"注意力矩阵形状: </span><span class="sc">{</span>weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [1, 16, 16]</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"每个 token 平均关注 </span><span class="sc">{</span>(weights[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">0</span>)<span class="sc">.</span><span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)<span class="sc">.</span><span class="bu">float</span>()<span class="sc">.</span>mean()<span class="sc">:.1f}</span><span class="ss"> 个位置"</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># 窗口7 + 2个全局 ≈ 9个位置（远少于16个全位置）</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>关于效率的重要说明
</div>
</div>
<div class="callout-body-container callout-body">
<p>上面的实现是为了<strong>教学目的</strong>——它仍然构建了完整的 <span class="math inline">\(n \times n\)</span> 注意力矩阵再掩码。真正高效的实现需要避免构建完整矩阵，例如使用块稀疏矩阵或自定义 CUDA kernel。Longformer 的官方实现和 Hugging Face 的 <code>transformers</code> 库提供了优化版本。</p>
</div>
</div>
</section>
<section id="从零实现-linear-attention" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="从零实现-linear-attention"><span class="header-section-number">5.2</span> 从零实现 Linear Attention</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_attention(Q, K, V, feature_map<span class="op">=</span><span class="st">'elu'</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    线性注意力（Linear Transformer 风格）</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    将 softmax(QK^T)V 替换为 φ(Q)(φ(K)^T V)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    利用矩阵乘法结合律避免构建 n×n 矩阵</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    batch, seq_len, d_k <span class="op">=</span> Q.shape</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 特征映射 φ(x) = elu(x) + 1（保证非负）</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature_map <span class="op">==</span> <span class="st">'elu'</span>:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        phi_Q <span class="op">=</span> F.elu(Q) <span class="op">+</span> <span class="dv">1</span>   <span class="co"># [batch, n, d_k]</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        phi_K <span class="op">=</span> F.elu(K) <span class="op">+</span> <span class="dv">1</span>   <span class="co"># [batch, n, d_k]</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 先计算 KV = φ(K)^T V → [batch, d_k, d_k]</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 这是关键：先做 K^T V 而非 Q K^T</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    KV <span class="op">=</span> torch.matmul(phi_K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), V)  <span class="co"># [batch, d_k, d_k]</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 计算归一化因子 Z = φ(K)^T 1 → [batch, d_k]</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> phi_K.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">2</span>)  <span class="co"># [batch, d_k]</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: 输出 = φ(Q) · KV / (φ(Q) · Z)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span> torch.matmul(phi_Q, KV)           <span class="co"># [batch, n, d_k]</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span> torch.matmul(phi_Q, Z.unsqueeze(<span class="op">-</span><span class="dv">1</span>))  <span class="co"># [batch, n, 1]</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> numerator <span class="op">/</span> (denominator <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 对比标准注意力和线性注意力</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq_len <span class="kw">in</span> [<span class="dv">512</span>, <span class="dv">1024</span>, <span class="dv">2048</span>, <span class="dv">4096</span>]:</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> torch.randn(<span class="dv">1</span>, seq_len, <span class="dv">64</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> torch.randn(<span class="dv">1</span>, seq_len, <span class="dv">64</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> torch.randn(<span class="dv">1</span>, seq_len, <span class="dv">64</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 标准注意力</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> <span class="fl">8.0</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    out_standard <span class="op">=</span> torch.matmul(attn, V)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    t_standard <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 线性注意力</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    out_linear <span class="op">=</span> linear_attention(Q, K, V)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    t_linear <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>seq_len<span class="sc">:5d}</span><span class="ss"> | 标准: </span><span class="sc">{</span>t_standard<span class="op">*</span><span class="dv">1000</span><span class="sc">:.1f}</span><span class="ss">ms | "</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"线性: </span><span class="sc">{</span>t_linear<span class="op">*</span><span class="dv">1000</span><span class="sc">:.1f}</span><span class="ss">ms | 加速: </span><span class="sc">{</span>t_standard<span class="op">/</span>t_linear<span class="sc">:.1f}</span><span class="ss">x"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="使用-hugging-face-的高效注意力模型" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="使用-hugging-face-的高效注意力模型"><span class="header-section-number">5.3</span> 使用 Hugging Face 的高效注意力模型</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> LongformerModel, LongformerTokenizer</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BigBirdModel, BigBirdTokenizer</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Longformer：处理最长 4096 token</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> LongformerTokenizer.from_pretrained(<span class="st">'allenai/longformer-base-4096'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LongformerModel.from_pretrained(<span class="st">'allenai/longformer-base-4096'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"这是一篇非常长的文档..."</span> <span class="op">*</span> <span class="dv">200</span>  <span class="co"># 模拟长文本</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span><span class="dv">4096</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Longformer 需要指定全局注意力掩码</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>global_attention_mask <span class="op">=</span> torch.zeros_like(inputs[<span class="st">'input_ids'</span>])</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>global_attention_mask[:, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># [CLS] 获得全局注意力</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs, global_attention_mask<span class="op">=</span>global_attention_mask)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs.last_hidden_state: [batch, seq_len, hidden_size]</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># BigBird：同样支持长序列</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>tokenizer_bb <span class="op">=</span> BigBirdTokenizer.from_pretrained(<span class="st">'google/bigbird-roberta-base'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>model_bb <span class="op">=</span> BigBirdModel.from_pretrained(<span class="st">'google/bigbird-roberta-base'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<section id="为什么线性注意力有性能损失" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么线性注意力有性能损失"><span class="header-section-number">6.1</span> 为什么线性注意力有性能损失？</h3>
<p>线性注意力在理论上很美，实践中却常常不如全注意力。这个差距值得深入探讨。</p>
<p><strong>Softmax 的”选择性”不可替代</strong>。标准 softmax 注意力有一个独特的性质：它是<strong>尖锐的</strong>（sharp）。当某个 key 与 query 特别匹配时，softmax 可以把绝大部分权重集中在这个 key 上，几乎实现”精确检索”。这种”赢者通吃”的行为对于许多 NLP 任务（如指代消解、事实提取）至关重要。</p>
<p>而线性注意力的特征映射 <span class="math inline">\(\phi\)</span> 天然产生更”平滑”的注意力分布——它很难实现这种极端的集中。用信息论的语言来说，softmax 注意力可以实现接近零熵的分布，而线性注意力的熵有一个隐式的下界。</p>
<p><strong>近似误差的累积效应</strong>。在单层中，线性注意力对 softmax 的近似误差可能很小。但在深度网络中，这些误差会逐层累积。每一层的输出是下一层的输入，小的近似偏差可能在多层传播后被放大为显著的性能差距。Performer 的作者也承认，在语言建模任务上，FAVOR+ 需要较多的随机特征（<span class="math inline">\(m\)</span> 较大）才能达到接近全注意力的效果，这部分抵消了线性复杂度的优势。</p>
</section>
<section id="稀疏注意力的理论基础" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="稀疏注意力的理论基础"><span class="header-section-number">6.2</span> 稀疏注意力的理论基础</h3>
<p>BigBird 的理论分析为稀疏注意力提供了坚实的数学基础。其核心结论包括：</p>
<p><strong>通用逼近性</strong>。BigBird 证明了：对于任何连续的序列到序列函数 <span class="math inline">\(f\)</span>，存在一个使用 BigBird 稀疏注意力的 Transformer 网络可以以任意精度逼近 <span class="math inline">\(f\)</span>。这个证明的关键是全局 token 充当了”信息中转站”，保证了任意两个位置之间的信息传播路径存在。</p>
<p><strong>图灵完备性</strong>。BigBird 还证明了稀疏注意力的图灵完备性——它可以模拟任何图灵机的计算。这个证明利用了全局 token 来模拟图灵机的磁带头。这些理论结果很重要：它们表明稀疏注意力在表达能力上<strong>不输于全注意力</strong>，前提是网络足够大、全局 token 的数量和位置设计得当。</p>
</section>
<section id="开放研究问题" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.3</span> 开放研究问题</h3>
<p><strong>最优注意力模式是什么？</strong> Longformer 和 BigBird 使用的是人工设计的固定模式。这些模式是最优的吗？是否存在可以根据输入内容动态调整的注意力模式？Routing Transformer 和 Reformer 尝试了可学习的模式，但引入了额外的计算开销。如何在适应性和效率之间找到最佳平衡，仍是一个开放问题。</p>
<p><strong>线性注意力的表达能力边界在哪里？</strong> 虽然我们知道线性注意力在某些任务上不如 softmax 注意力，但这种差距是否可以通过更好的特征映射或更深的网络来弥补？还是说 softmax 的”选择性”是某些任务的硬性需求？</p>
<p><strong>长序列中信息的自然结构是什么？</strong> 所有高效注意力方法都隐含了一个假设——注意力矩阵存在可利用的结构（稀疏性、低秩性）。但这些结构在不同任务和不同文本类型中有多大差异？处理代码和处理小说，最优的注意力模式可能完全不同。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="实践给出的出人意料的答案" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="实践给出的出人意料的答案"><span class="header-section-number">7.1</span> 实践给出的出人意料的答案</h3>
<p>本章介绍的高效注意力方法在理论上非常优雅，但实践给出了一个令人意外的转折。在 2020-2021 年”X-former”百花齐放之后，NLP 领域的主流模型——GPT-4、Claude、LLaMA、Gemini——几乎无一例外地选择了<strong>标准的全注意力</strong>。</p>
<p>为什么？答案在于 Flash Attention（Dao et al., 2022）。Flash Attention 不是从算法层面改变注意力的复杂度，而是从<strong>硬件层面</strong>优化了标准注意力的实现。它的核心发现是：标准注意力的实际瓶颈不是计算量（FLOPs），而是<strong>内存读写</strong>（IO）。通过精心设计的分块计算和内存访问模式，Flash Attention 将标准全注意力的实际速度提升了 2-4 倍，使得在主流硬件上处理数千甚至数万 token 变得可行。</p>
<p>这给了我们一个深刻的教训：<strong>算法复杂度不等于实际速度</strong>。GPU 的算力增长速度远快于内存带宽，这意味着很多计算场景实际上是”IO瓶颈”而非”计算瓶颈”。线性注意力在 FLOPs 上赢了，但在实际运行时间上未必赢，因为它的内存访问模式可能对 GPU 不友好。</p>
</section>
<section id="高效注意力的遗产" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="高效注意力的遗产"><span class="header-section-number">7.2</span> 高效注意力的遗产</h3>
<p>这是否意味着本章的内容”没用”了？恰恰相反。这些方法留下了宝贵的思想遗产：</p>
<p>稀疏注意力的思想被融入了现代架构。Mistral 模型使用了滑动窗口注意力作为默认机制，在推理效率和性能之间取得了优秀的平衡。Mixture of Experts（MoE）架构中的条件计算思想与稀疏注意力一脉相承。</p>
<p>线性注意力的 Transformer-RNN 对偶性启发了一系列新架构。Mamba（Gu &amp; Dao, 2023）等状态空间模型可以看作线性注意力思想的自然延伸——用线性递归替代注意力，在长序列建模上取得了突破性进展。</p>
<p>低秩近似的观察启发了 LoRA 等参数高效微调方法——如果注意力矩阵是低秩的，那么模型的权重矩阵也许同样可以用低秩方式更新。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.3</span> 这些局限导向了什么？</h3>
<p>高效注意力的探索告诉我们：序列建模的核心挑战不仅在于注意力的复杂度，更在于如何让模型在大规模数据上学到有用的表示。这引出了一个更深层的范式转变——<strong>预训练</strong>。无论用什么注意力机制，如何训练一个好的基础模型、如何将一个大模型的知识迁移到具体任务，才是推动 NLP 前进的真正引擎。</p>
<blockquote class="blockquote">
<p>下一章预告：第10章将回到一个更根本的问题——预训练思想的起源。从 Word2Vec 作为”预训练的雏形”，到计算机视觉领域 ImageNet 预训练的启示，我们将追溯”先训练再迁移”这一范式是如何一步步成形的。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>这一章我们探讨了打破 Transformer <span class="math inline">\(O(n^2)\)</span> 复杂度的三条技术路线。核心问题是如何在保持表达能力的同时降低 Self-Attention 的计算代价。</p>
<p>稀疏注意力（Longformer、BigBird）的策略是”只计算重要的注意力对”——通过局部窗口、全局 token 和随机连接的组合，把复杂度降到 <span class="math inline">\(O(n)\)</span>，同时在理论上保持通用逼近性和图灵完备性。</p>
<p>线性注意力（Performer、Linear Transformer）的策略是”改变计算顺序”——用核方法近似 softmax，利用矩阵乘法的结合律避免构建 <span class="math inline">\(n \times n\)</span> 矩阵，揭示了 Transformer 与 RNN 的深层对偶性。</p>
<p>低秩方法（Linformer）的策略是”压缩 Key 和 Value”——基于注意力矩阵的低秩性质，用投影降低维度。</p>
<p>这些方法的意义不仅在于效率优化本身，更在于它们揭示的理论洞察——注意力的稀疏性、低秩性、以及 Transformer-RNN 对偶性——深刻影响了后续的架构设计和研究方向。</p>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 62%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>核心公式</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>标准注意力</td>
<td><span class="math inline">\(\text{Attn} = \text{softmax}(QK^\top / \sqrt{d_k}) V\)</span></td>
</tr>
<tr class="even">
<td>稀疏注意力</td>
<td><span class="math inline">\(\text{Attn}_i = \text{softmax}_{j \in \mathcal{S}_i}(q_i^\top k_j / \sqrt{d_k}) \cdot v_j\)</span></td>
</tr>
<tr class="odd">
<td>线性注意力</td>
<td><span class="math inline">\(\text{Attn}_i = \frac{\phi(q_i)^\top \sum_j \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_j \phi(k_j)}\)</span></td>
</tr>
<tr class="even">
<td>Linformer</td>
<td><span class="math inline">\(\text{Attn} = \text{softmax}(Q(EK)^\top / \sqrt{d_k}) \cdot EV\)</span></td>
</tr>
<tr class="odd">
<td>因果线性注意力</td>
<td><span class="math inline">\(S_i = S_{i-1} + \phi(k_i) v_i^\top\)</span> （RNN递归形式）</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么稀疏注意力需要”全局 token”？如果只用滑动窗口而没有全局 token，模型在处理什么类型的任务时会遇到困难？</p></li>
<li><p><strong>[数学推导]</strong> 推导线性注意力的复杂度。证明：当使用特征映射 <span class="math inline">\(\phi: \mathbb{R}^d \to \mathbb{R}^m\)</span> 时，通过改变矩阵乘法的结合顺序，可以将注意力的复杂度从 <span class="math inline">\(O(n^2 d)\)</span> 降至 <span class="math inline">\(O(nmd)\)</span>。</p></li>
<li><p><strong>[数学推导-进阶]</strong> 证明 Katharopoulos et al.&nbsp;的核心结论：因果线性注意力等价于一个隐状态维度为 <span class="math inline">\(d_k \times d_v\)</span> 的 RNN。写出这个 RNN 的状态转移方程和输出方程。</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 在同一数据集（如 WikiText-2）上对比标准 Transformer 和 Longformer 的困惑度（perplexity），分别在序列长度 512、2048、8192 下测试。你观察到了什么趋势？</p></li>
<li><p><strong>[研究思考]</strong> Flash Attention 通过优化内存访问模式加速了标准全注意力，使得大多数高效注意力方法在实际速度上失去了优势。这是否意味着算法层面的注意力优化不再重要？考虑以下场景：(a) 序列长度超过100万（基因组学）；(b) 边缘设备上的推理；(c) 在线/流式处理任务。</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Beltagy et al.&nbsp;(2020). “Longformer: The Long-Document Transformer”</strong>（<a href="https://arxiv.org/abs/2004.05150">arXiv:2004.05150</a>）。重点阅读 Section 3 的注意力模式设计和 Section 5 的长文档实验。图1的注意力模式可视化非常直观。</p>
<p><strong>Zaheer et al.&nbsp;(2020). “Big Bird: Transformers for Longer Sequences”</strong>（<a href="https://arxiv.org/abs/2007.14062">arXiv:2007.14062</a>）。重点阅读 Section 2 的理论分析（图灵完备性证明）和 Section 4 的三种注意力组件设计。理论部分比较硬核但很有价值。</p>
</section>
<section id="理论基础" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">9.2</span> 理论基础</h3>
<p><strong>Choromanski et al.&nbsp;(2020). “Rethinking Attention with Performers”</strong>（<a href="https://arxiv.org/abs/2009.14794">arXiv:2009.14794</a>）。FAVOR+ 算法的详细推导。重点阅读 Section 3 的核方法分析和 Section 4 的正交随机特征。发表于 ICLR 2021。</p>
<p><strong>Katharopoulos et al.&nbsp;(2020). “Transformers are RNNs”</strong>（<a href="https://arxiv.org/abs/2006.16236">arXiv:2006.16236</a>）。Transformer-RNN 对偶性的发现。重点阅读 Section 3.3 的因果线性注意力推导。发表于 ICML 2020。</p>
<p><strong>Wang et al.&nbsp;(2020). “Linformer: Self-Attention with Linear Complexity”</strong>（<a href="https://arxiv.org/abs/2006.04768">arXiv:2006.04768</a>）。低秩近似理论的关键分析。重点阅读 Section 3 的 Johnson-Lindenstrauss 引理应用。</p>
</section>
<section id="综述" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="综述"><span class="header-section-number">9.3</span> 综述</h3>
<p><strong>Tay et al.&nbsp;(2022). “Efficient Transformers: A Survey”</strong>（<a href="https://arxiv.org/abs/2009.06732">arXiv:2009.06732</a>）。高效 Transformer 领域最全面的综述，提出了七大类分类体系。如果只读一篇综述，读这篇。发表于 ACM Computing Surveys。</p>
</section>
<section id="后续发展" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.4</span> 后续发展</h3>
<p><strong>Dao et al.&nbsp;(2022). “FlashAttention: Fast and Memory-Efficient Exact Attention”</strong>。硬件感知的注意力优化，将在第26章详细讨论。</p>
<p><strong>Gu &amp; Dao (2023). “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”</strong>。线性注意力思想的自然延伸，用选择性状态空间模型替代注意力。</p>
</section>
<section id="代码资源" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.5</span> 代码资源</h3>
<ul>
<li>Longformer 官方实现：<a href="https://github.com/allenai/longformer">github.com/allenai/longformer</a></li>
<li>BigBird 官方实现：<a href="https://github.com/google-research/bigbird">github.com/google-research/bigbird</a></li>
<li>Linear Transformer 实现：<a href="https://linear-transformers.com/">linear-transformers.com</a></li>
<li>Hugging Face 的高效注意力模型集合：<code>transformers</code> 库中的 <code>LongformerModel</code>、<code>BigBirdModel</code></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>2020年可以被称为”X-former 之年”。在短短几个月内，Reformer（1月）、Longformer（4月）、Linformer（6月）、Linear Transformer（6月）、BigBird（7月）、Performer（9月）接连发布。每篇论文都声称找到了打破 <span class="math inline">\(O(n^2)\)</span> 的方法，一时间让人眼花缭乱。Yi Tay 等人及时推出的 survey “Efficient Transformers: A Survey” 帮助社区理清了这片混战的局面。</p>
<p>然而，历史的走向出乎所有人意料。这些精妙的算法层面优化，最终被一个硬件层面的优化——Flash Attention——大幅抢了风头。Tri Dao（Flash Attention 的作者）日后加入了 Princeton，与 Danqi Chen 一起教授 NLP 课程，这大概是学术界对”系统优化也是科学”的最好注脚。</p>
<p>不过，稀疏注意力的思想并未消亡。2023年发布的 Mistral 7B 模型在其架构中采用了滑动窗口注意力，证明了 Longformer 的核心思想在精简形式下仍然具有实用价值。而线性注意力的思想更是通过 Mamba 等状态空间模型获得了”第二次生命”，在长序列建模上展现出巨大的潜力。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第9章：高效注意力——复杂度优化"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "O(n²)的第一次反击"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-26"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, Efficient Attention, Transformer]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [Sparse Attention, Longformer, BigBird, Performer, Linformer, Linear Attention, Kernel Methods]</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "当Transformer遇上长序列：Sparse Attention、Linear Attention、Low-Rank三条路线如何挑战O(n²)的魔咒，以及为什么实践给出了出人意料的答案。"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/efficient-attention-banner.png"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tay et al. (2022)** "Efficient Transformers: A Survey" — 参考了七大类分类体系（Figure 2）、各方法的统一比较框架</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Beltagy et al. (2020)** "Longformer: The Long-Document Transformer" — 参考了滑动窗口+全局注意力设计（Section 3）、注意力模式可视化（Figure 2）</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zaheer et al. (2020)** "Big Bird: Transformers for Longer Sequences" — 参考了稀疏注意力的图灵完备性证明（Section 2）、三组件设计（Section 4）</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Choromanski et al. (2020)** "Rethinking Attention with Performers" — 参考了FAVOR+算法推导（Section 3-4）、正交随机特征</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Katharopoulos et al. (2020)** "Transformers are RNNs" — 参考了Transformer-RNN对偶性推导（Section 3.3）</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wang et al. (2020)** "Linformer: Self-Attention with Linear Complexity" — 参考了低秩近似理论（Section 3）</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Section 11.6 (Self-Attention and Positional Encoding) — 参考了注意力复杂度分析的教学组织方式</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SLP3** Chapter 9-10 — 参考了Transformer架构和注意力机制的基础讲解框架</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 9 (2025) "Pretraining" — 参考了高效注意力方法的概述与对比</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CMU 11-711** ANLP (2024) — 参考了长序列建模的工程挑战分析</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何在保持 Transformer 表达能力的同时，打破 Self-Attention 的 $O(n^2)$ 计算瓶颈？</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2020年 </span><span class="pp">|</span><span class="at"> Longformer, BigBird, Performer, Linformer </span><span class="pp">|</span><span class="at"> "X-former"百花齐放的一年</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>上一章我们见证了 Transformer 的诞生。这个完全基于注意力的架构彻底抛弃了循环结构，用 Scaled Dot-Product Attention、Multi-Head、残差连接和 Layer Normalization 构建了一个并行高效、表达能力强大的序列建模框架。Transformer 在机器翻译上的表现令人震惊，训练速度比 RNN 快了一个数量级，很快成为整个 NLP 领域的基础架构。</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>然而，我们在上一章结尾也指出了 Transformer 最大的隐患——Self-Attention 的二次复杂度。每个 token 需要与序列中所有其他 token 计算注意力分数，这意味着计算量和内存消耗都随序列长度 $n$ 呈 $O(n^2)$ 增长。对于一篇几百词的句子，这不是问题；但当我们想处理一本10万词的书、一份完整的法律合同、或一段长达数小时的对话时，$O(n^2)$ 就变成了一道不可逾越的障碍。</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>2020年，NLP 社区对这个问题发起了集体冲锋。Longformer、BigBird、Performer、Linformer……一时间，各种 "X-former" 如雨后春笋般涌现，每一个都宣称找到了打破 $O(n^2)$ 的方法。Yi Tay 等人在一篇 survey 中将这些方法归纳为七大类——固定模式、可学习模式、低秩方法、核方法、记忆机制、下采样和循环——形成了高效 Transformer 的完整图谱。</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="al">![高效 Transformer 方法分类体系。不同颜色的圈代表不同类别的方法，许多模型同时属于多个类别。](figures/chapter-9/original/tay-survey-fig2-taxonomy.png)</span>{#fig-tay-taxonomy width=75%}</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>*Source: Tay et al. (2022) "Efficient Transformers: A Survey", Figure 2. [arXiv:2009.06732](https://arxiv.org/abs/2009.06732)*</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：打破 $O(n^2)$ 有两条主要思路。第一条是**稀疏注意力**——既然大多数注意力权重接近零，何不只计算"重要的"那些？第二条是**线性注意力**——能否用数学技巧重新组织计算，避免显式构建 $n \times n$ 的注意力矩阵？两条路线各有优劣，而实践最终给出了一个出人意料的答案。</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### $O(n^2)$ 到底意味着什么？</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>让我们用具体数字感受一下二次复杂度的严重性。标准 Self-Attention 的核心计算是 $\text{softmax}(QK^\top / \sqrt{d_k}) \cdot V$，其中 $Q, K, V \in \mathbb{R}^{n \times d}$。矩阵乘法 $QK^\top$ 产生一个 $n \times n$ 的注意力矩阵——这就是二次复杂度的来源。</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>\text{时间复杂度} = O(n^2 d) \quad \text{（计算注意力矩阵）}</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>\text{空间复杂度} = O(n^2) \quad \text{（存储注意力矩阵）}</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>把这个公式代入具体场景：</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 序列长度 $n$ <span class="pp">|</span> 注意力矩阵大小 <span class="pp">|</span> FP16 显存 <span class="pp">|</span> 典型场景 <span class="pp">|</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------------|---------------|-----------|---------|</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 512 <span class="pp">|</span> 262K <span class="pp">|</span> 0.5 MB <span class="pp">|</span> BERT 默认 <span class="pp">|</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2,048 <span class="pp">|</span> 4.2M <span class="pp">|</span> 8 MB <span class="pp">|</span> GPT-2 <span class="pp">|</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 8,192 <span class="pp">|</span> 67M <span class="pp">|</span> 128 MB <span class="pp">|</span> 中等文档 <span class="pp">|</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 32,768 <span class="pp">|</span> 1.07B <span class="pp">|</span> 2 GB <span class="pp">|</span> 长文档 <span class="pp">|</span></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 100,000 <span class="pp">|</span> 10B <span class="pp">|</span> 20 GB <span class="pp">|</span> 一本书 <span class="pp">|</span></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1,000,000 <span class="pp">|</span> $10^{12}$ <span class="pp">|</span> 2 TB <span class="pp">|</span> 长代码库 <span class="pp">|</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>这个表格揭示了一个残酷的现实：序列长度每翻一倍，计算和内存需求翻四倍。当序列长度达到10万时，仅存储单层的注意力矩阵就需要20GB显存——这还没算多头、多层的叠加。</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="al">![各类注意力方法的计算量随序列长度的增长曲线（双对数坐标）。全注意力（红色）的二次增长与线性方法（蓝、橙、绿）的差距在长序列上极为显著。](figures/chapter-9/fig-complexity-comparison.png)</span>{#fig-complexity width=80%}</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>*作者绘制。数据基于各方法论文报告的理论复杂度计算。*</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### 但注意力矩阵真的"满"了吗？</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>问题的关键在于：$O(n^2)$ 的计算真的都是必要的吗？让我们想象处理一篇长文档。当模型在理解第500段中的某个词时，它真的需要精确关注第1段里的每一个词吗？</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>实证研究给出了否定的答案。多项研究发现，训练好的 Transformer 中，绝大多数注意力权重接近于零。注意力矩阵往往呈现出明显的**稀疏模式**：</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**局部模式**：大部分注意力集中在当前位置附近的几个词上</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**全局模式**：某些特殊 token（如 <span class="in">`[CLS]`</span>、句号、频繁词）会被全局关注</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**条纹模式**：某些层呈现规律性的跨步注意力</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>Linformer 的作者进一步发现，注意力矩阵通常是**低秩的**——也就是说，一个 $n \times n$ 的矩阵可以被远小于 $n$ 维的信息近似表示。这些经验观察打开了优化的大门：如果我们知道大多数注意力权重不重要，为什么还要花费算力去计算它们？</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>理想的高效注意力机制应该满足以下条件：</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>首先，**亚二次复杂度**——最好是 $O(n)$ 或 $O(n \log n)$，让序列长度的增长不再令人恐惧。其次，**性能损失可控**——如果提速10倍但效果下降5个点，那就得不偿失了。第三，**通用性**——不能只在特定任务或特定长度上有效。最后，**工程可行性**——能在现有硬件上高效实现，而非只是理论上漂亮。</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>这四个条件构成了评判所有高效注意力方法的标尺。接下来我们将看到，不同的方法在这四个维度上做出了不同的取舍。</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### 两条路线的直觉</span></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>打破 $O(n^2)$ 的方法可以用一个生活类比来理解。</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>想象你是一个公司的 CEO，需要了解公司里每个员工的工作状态。最原始的方式是**全员一对一面谈**——这就是标准 Self-Attention 的做法，每个人和每个人都交流一次，复杂度是 $O(n^2)$。</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>**稀疏注意力**的思路是：不需要每个人都和每个人谈。你只需要和自己部门的同事（**局部窗口**）交流，加上几个了解全局的高管（**全局 token**），偶尔随机抽查几个人（**随机注意力**），就能掌握大致情况。这就是 Longformer 和 BigBird 的策略——显式地决定"谁和谁交流"，跳过不重要的组合。</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>**线性注意力**的思路完全不同。它不是减少交流的次数，而是**换一种交流方式**。想象每个员工先把自己的状态压缩成一份简短的报告（特征映射），然后公司汇总所有报告得到一份全局摘要，最后每个人根据自己的需求从全局摘要中提取信息。这种"先汇总再查询"的方式，避免了所有人之间的两两交流，复杂度降到了 $O(n)$。这就是 Performer 和 Linear Transformer 的核心思想。</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从注意力公式看优化空间</span></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>让我们从数学角度看这两条路线的本质。标准注意力可以写成：</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>展开对第 $i$ 个位置的输出：</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>\text{output}_i = \frac{\sum_{j=1}^{n} \exp(q_i^\top k_j / \sqrt{d_k}) \cdot v_j}{\sum_{j=1}^{n} \exp(q_i^\top k_j / \sqrt{d_k})}</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>**稀疏注意力**的做法是限制求和范围——不再对所有 $j \in <span class="sc">\{</span>1, \ldots, n<span class="sc">\}</span>$ 求和，而只对 $j \in \mathcal{S}_i$（一个远小于 $n$ 的子集）求和：</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>\text{output}_i = \frac{\sum_{j \in \mathcal{S}_i} \exp(q_i^\top k_j / \sqrt{d_k}) \cdot v_j}{\sum_{j \in \mathcal{S}_i} \exp(q_i^\top k_j / \sqrt{d_k})}</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>不同的稀疏注意力方法，本质上就是在设计不同的集合 $\mathcal{S}_i$。</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>**线性注意力**的做法更巧妙。它用一个特征映射 $\phi$ 来近似 softmax 核：</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>\exp(q_i^\top k_j / \sqrt{d_k}) \approx \phi(q_i)^\top \phi(k_j)</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>这样注意力公式变成：</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>\text{output}_i = \frac{\phi(q_i)^\top \sum_{j=1}^{n} \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_{j=1}^{n} \phi(k_j)}</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>关键观察：$\sum_{j=1}^{n} \phi(k_j) v_j^\top$ 和 $\sum_{j=1}^{n} \phi(k_j)$ 可以**先计算一次并复用**。这就把 $O(n^2)$ 变成了 $O(n)$！</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="al">![标准注意力 vs 线性注意力的计算顺序对比。左：标准注意力先计算 $QK^\top$ 产生 $n \times n$ 的瓶颈矩阵；右：线性注意力先计算 $\phi(K)^\top V$ 产生 $d \times d$ 的小矩阵，从而避免了二次复杂度。](figures/chapter-9/fig-linear-attention-order.png)</span>{#fig-linear-attention-order width=90%}</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>*作者绘制。基于 Katharopoulos et al. (2020) "Transformers are RNNs" 的矩阵乘法结合律分析。*</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sparse Attention 家族</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Longformer：滑动窗口 + 全局 token</span></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>Longformer（Beltagy et al., 2020）的设计非常直观。它的注意力模式由两部分组成：</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>**局部滑动窗口（Local Sliding Window）**。每个 token 只关注自身周围固定大小 $w$ 的窗口。对于位置 $i$，注意力范围是 $<span class="sc">\{</span>i - w/2, \ldots, i + w/2<span class="sc">\}</span>$。单层的感受野是 $w$，但通过堆叠 $L$ 层，顶层的感受野扩展到 $L \times w$——这与 CNN 的感受野增长方式完全一致。</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>**全局注意力（Global Attention）**。某些特殊位置被赋予"全局"权限，可以关注序列中的所有 token，同时也被所有 token 关注。这些位置通常是任务相关的特殊 token，例如分类任务中的 <span class="in">`[CLS]`</span>。全局 token 的数量远小于 $n$（通常只有几个），因此不影响整体复杂度。</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>把两者组合起来，Longformer 的复杂度是 $O(n \times w)$。由于窗口大小 $w$ 是常数（通常为 256 或 512），复杂度是**关于序列长度线性的**。</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a><span class="fu">## 直觉：为什么滑动窗口 + 全局 token 足够？</span></span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>想象阅读一篇论文。你在理解某个段落时，主要参考的是**上下文几段的内容**（局部窗口），偶尔需要回顾**摘要或引言中的核心论点**（全局 token）。你不需要在读每一句话时都精确回顾论文的每一个细节——局部上下文加上少量全局锚点，已经足够理解大部分内容。</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a><span class="al">![四种注意力模式对比。(a) 全注意力：每个 token 关注所有位置（100%）；(b) 滑动窗口：只关注相邻位置（38%）；(c) 扩张滑动窗口：以固定间隔扩大感受野（44%）；(d) 全局+滑动窗口：特殊 token 获得全局视野（61%）。底部百分比表示活跃注意力对占全部可能的比例。](figures/chapter-9/fig-attention-patterns.png)</span>{#fig-attention-patterns width=95%}</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>*Source: Beltagy et al. (2020) "Longformer: The Long-Document Transformer", Figure 2. [arXiv:2004.05150](https://arxiv.org/abs/2004.05150)*</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a><span class="fu">#### BigBird：随机 + 局部 + 全局</span></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>BigBird（Zaheer et al., 2020）在 Longformer 的基础上增加了一个额外的组件——**随机注意力**。每个 token 除了关注局部窗口和全局 token 之外，还会随机关注 $r$ 个其他位置。</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>BigBird 的注意力由三部分构成：</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 组件 <span class="pp">|</span> 模式 <span class="pp">|</span> 作用 <span class="pp">|</span></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|------|</span></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **局部窗口** <span class="pp">|</span> 每个 token 关注相邻 $w$ 个 token <span class="pp">|</span> 捕获局部上下文 <span class="pp">|</span></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **全局 token** <span class="pp">|</span> $g$ 个 token 关注/被关注所有位置 <span class="pp">|</span> 提供全局信息通道 <span class="pp">|</span></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **随机连接** <span class="pp">|</span> 每个 token 随机关注 $r$ 个位置 <span class="pp">|</span> 缩短任意两点间的信息传播路径 <span class="pp">|</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a><span class="al">![BigBird 的注意力矩阵：绿色为全局 token（前两行/列）、蓝色为局部滑动窗口、橙色为随机连接。三者组合实现了稀疏但"连通"的注意力模式。](figures/chapter-9/original/bigbird-fig1d-combined.png)</span>{#fig-bigbird-combined width=50%}</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>*Source: Zaheer et al. (2020) "Big Bird: Transformers for Longer Sequences", Figure 1. [arXiv:2007.14062](https://arxiv.org/abs/2007.14062)*</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>为什么需要随机注意力？BigBird 的作者从图论的角度给出了解释。将注意力模式看作一张图：每个 token 是节点，有注意力连接的 token 之间有边。纯局部窗口构成的图，两个远距离节点之间的最短路径可能很长（需要经过多层才能通信）。加入随机边之后，根据 Watts-Strogatz 小世界网络理论，图的直径会急剧缩短——这就是"六度分隔"理论在注意力机制中的应用。</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>更重要的是，BigBird 的作者从理论上证明了一个关键结论：**BigBird 的稀疏注意力是图灵完备的**，并且是序列到序列函数的通用逼近器。这意味着 BigBird 在理论上不会因为稀疏化而损失表达能力——当然，前提是网络足够深、全局 token 足够多。</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：稀疏注意力 vs 全注意力</span></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>让我们用一个小例子直观地感受稀疏注意力的效果。</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>**设定**：4 个 token，$d_k = 2$，窗口大小 $w = 2$（每个 token 只看左右各1个邻居）。</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>**Step 1: Q, K, V 矩阵**</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>Q = \begin{bmatrix} 1.0 &amp; 0.5 <span class="sc">\\</span> 0.8 &amp; 0.2 <span class="sc">\\</span> 0.3 &amp; 0.9 <span class="sc">\\</span> 0.6 &amp; 0.7 \end{bmatrix}, \quad</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>K = \begin{bmatrix} 0.9 &amp; 0.4 <span class="sc">\\</span> 0.7 &amp; 0.6 <span class="sc">\\</span> 0.5 &amp; 0.8 <span class="sc">\\</span> 0.2 &amp; 1.0 \end{bmatrix}, \quad</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>V = \begin{bmatrix} 1.0 &amp; 0.0 <span class="sc">\\</span> 0.0 &amp; 1.0 <span class="sc">\\</span> 0.5 &amp; 0.5 <span class="sc">\\</span> 1.0 &amp; 1.0 \end{bmatrix}</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>**Step 2: 全注意力——计算完整的 $QK^\top / \sqrt{d_k}$**</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>QK^\top = \begin{bmatrix}</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>1.10 &amp; 1.00 &amp; 0.90 &amp; 0.70 <span class="sc">\\</span></span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>0.80 &amp; 0.68 &amp; 0.56 &amp; 0.36 <span class="sc">\\</span></span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>0.63 &amp; 0.75 &amp; 0.87 &amp; 0.96 <span class="sc">\\</span></span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>0.82 &amp; 0.84 &amp; 0.86 &amp; 0.82</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>除以 $\sqrt{d_k} = \sqrt{2} \approx 1.414$：</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>\frac{QK^\top}{\sqrt{d_k}} = \begin{bmatrix}</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>0.78 &amp; 0.71 &amp; 0.64 &amp; 0.50 <span class="sc">\\</span></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>0.57 &amp; 0.48 &amp; 0.40 &amp; 0.25 <span class="sc">\\</span></span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>0.45 &amp; 0.53 &amp; 0.62 &amp; 0.68 <span class="sc">\\</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>0.58 &amp; 0.59 &amp; 0.61 &amp; 0.58</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>**Step 3: 全注意力——Softmax（逐行）**</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>对第一行：$\text{softmax}(<span class="co">[</span><span class="ot">0.78, 0.71, 0.64, 0.50</span><span class="co">]</span>)$</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>A_{\text{full}} \approx \begin{bmatrix}</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>0.279 &amp; 0.261 &amp; 0.243 &amp; 0.212 <span class="sc">\\</span></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>0.272 &amp; 0.249 &amp; 0.230 &amp; 0.198 <span class="sc">\\</span></span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>0.229 &amp; 0.249 &amp; 0.272 &amp; 0.289 <span class="sc">\\</span></span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>0.254 &amp; 0.256 &amp; 0.262 &amp; 0.253</span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>**Step 4: 稀疏注意力——只保留窗口内的权重**</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>窗口大小 $w = 2$（左右各1），每个 token 的关注范围如下：</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Token 0：可关注 {0, 1}</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Token 1：可关注 {0, 1, 2}</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Token 2：可关注 {1, 2, 3}</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Token 3：可关注 {2, 3}</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>稀疏掩码：</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>M_{\text{sparse}} = \begin{bmatrix}</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1 &amp; 1</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>将被掩码的位置设为 $-\infty$（softmax 后变为 0），重新计算 softmax：</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>A_{\text{sparse}} \approx \begin{bmatrix}</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>0.517 &amp; 0.483 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>0.371 &amp; 0.339 &amp; 0.290 &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>0 &amp; 0.294 &amp; 0.336 &amp; 0.370 <span class="sc">\\</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0.508 &amp; 0.492</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>**Step 5: 对比输出**</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>\text{Output}_{\text{full}} = A_{\text{full}} \cdot V \approx \begin{bmatrix}</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a>0.609 &amp; 0.607 <span class="sc">\\</span></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>0.594 &amp; 0.576 <span class="sc">\\</span></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>0.654 &amp; 0.680 <span class="sc">\\</span></span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>0.637 &amp; 0.640</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>\text{Output}_{\text{sparse}} = A_{\text{sparse}} \cdot V \approx \begin{bmatrix}</span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>0.517 &amp; 0.483 <span class="sc">\\</span></span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a>0.516 &amp; 0.484 <span class="sc">\\</span></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>0.662 &amp; 0.685 <span class="sc">\\</span></span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>0.746 &amp; 0.746</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>**解读**：中间位置（Token 1, 2）的稀疏输出与全注意力输出比较接近，因为这些位置的窗口覆盖了大部分重要邻居。而边缘位置（Token 0, 3）的差异较大——它们失去了对远处 token 的关注能力。这正是稀疏注意力的核心权衡：**局部信息保持完好，远距离信息需要通过多层传播来弥补**。</span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a>在实际应用中，通过堆叠多层（$L$ 层的感受野为 $L \times w$）以及加入全局 token，这种信息损失可以被有效缓解。</span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### Low-Rank 方法：Linformer</span></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a>Linformer（Wang et al., 2020）从一个不同的角度切入：既然注意力矩阵是低秩的，何不直接用低秩近似来降低复杂度？</span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a>核心思想是在计算 $K$ 和 $V$ 时，先用一个投影矩阵 $E \in \mathbb{R}^{k \times n}$（其中 $k \ll n$）将序列长度从 $n$ 压缩到 $k$：</span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>\bar{K} = E \cdot K \in \mathbb{R}^{k \times d}, \quad \bar{V} = E \cdot V \in \mathbb{R}^{k \times d}</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a>然后用压缩后的 $\bar{K}$、$\bar{V}$ 计算注意力：</span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, \bar{K}, \bar{V}) = \text{softmax}\left(\frac{Q\bar{K}^\top}{\sqrt{d_k}}\right) \bar{V}</span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>此时注意力矩阵变成了 $n \times k$（而非 $n \times n$），时间和空间复杂度都降为 $O(nk)$。由于 $k$ 是固定常数，复杂度对序列长度是线性的。</span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>Linformer 的理论贡献是证明了一个关键性质：对于任意 $\epsilon &gt; 0$，存在一个足够大的投影维度 $k = O(d / \epsilon^2)$（与 $n$ 无关），使得低秩近似的误差不超过 $\epsilon$。换句话说，**注意力矩阵的有效信息维度与序列长度无关**——这为低秩近似提供了坚实的理论基础。</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>不过，Linformer 有一个实际上的限制：投影矩阵 $E$ 的大小是 $k \times n$，这意味着模型被绑定到固定的序列长度 $n$。推理时如果输入长度不同，需要重新训练或做额外处理。</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear Attention 家族</span></span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 核心洞察：核技巧（Kernel Trick）</span></span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>线性注意力的理论基础来自一个优雅的数学观察。让我们重新审视 softmax 注意力的公式。对于第 $i$ 个位置的输出：</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a>\text{output}_i = \frac{\sum_{j=1}^{n} \kappa(q_i, k_j) \cdot v_j}{\sum_{j=1}^{n} \kappa(q_i, k_j)}</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a>其中核函数 $\kappa(q, k) = \exp(q^\top k / \sqrt{d_k})$。如果我们能找到一个特征映射 $\phi$ 使得：</span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a>\kappa(q, k) = \phi(q)^\top \phi(k)</span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a>那么分子可以重写为：</span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a>\sum_{j=1}^{n} \phi(q_i)^\top \phi(k_j) \cdot v_j = \phi(q_i)^\top \underbrace{\sum_{j=1}^{n} \phi(k_j) \cdot v_j^\top}_{S \in \mathbb{R}^{m \times d}}</span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a>关键在于：矩阵 $S = \sum_{j=1}^{n} \phi(k_j) v_j^\top$ 只需要计算一次，所有位置的查询都可以复用。如果特征映射 $\phi$ 的维度是 $m$，那么：</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算 $S$：$O(nmd)$</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>对每个查询提取输出：$O(md)$</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>总复杂度：$O(nmd)$</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>当 $m$ 是常数时，总复杂度是 $O(nd)$——**真正的线性复杂度**！</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Performer：FAVOR+ 算法</span></span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>Performer（Choromanski et al., 2020）的挑战是找到一个好的特征映射 $\phi$ 来近似 softmax 核。这并不简单，因为 $\exp(q^\top k)$ 是一个非平凡的核函数。</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>Performer 提出了 **FAVOR+**（Fast Attention Via positive Orthogonal Random features）算法。核心思想是利用随机特征来近似 softmax 核：</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a>\exp(q^\top k) = \mathbb{E}_{\omega \sim \mathcal{N}(0, I)} \left<span class="co">[</span><span class="ot"> \exp(\omega^\top q - \|q\|^2/2) \cdot \exp(\omega^\top k - \|k\|^2/2) \right</span><span class="co">]</span></span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>这个等式来自高斯核的随机特征分解。基于此，Performer 定义特征映射：</span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>\phi(x) = \frac{\exp(-<span class="sc">\|</span>x<span class="sc">\|</span>^2/2)}{\sqrt{m}} \begin{bmatrix} \exp(\omega_1^\top x) <span class="sc">\\</span> \exp(\omega_2^\top x) <span class="sc">\\</span> \vdots <span class="sc">\\</span> \exp(\omega_m^\top x) \end{bmatrix}</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>其中 $\omega_1, \ldots, \omega_m$ 是从 $\mathcal{N}(0, I)$ 中采样的随机向量。FAVOR+ 的"+"代表两个改进：使用**正随机特征**（保证注意力权重非负）和**正交随机特征**（降低近似方差）。</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a><span class="fu">## 直觉：为什么随机特征能近似 softmax？</span></span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a>这背后的数学原理是 Bochner 定理：任何平移不变的正定核都可以表示为某个概率分布的傅里叶变换。对于高斯核（softmax 核的基础），这个分布恰好也是高斯分布。随机特征方法本质上是用蒙特卡洛采样来近似这个积分——采样点越多（$m$ 越大），近似越精确。</span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a><span class="fu">## 算法框：Performer FAVOR+ 前向计算</span></span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>**输入**：查询 $Q \in \mathbb{R}^{n \times d}$，键 $K \in \mathbb{R}^{n \times d}$，值 $V \in \mathbb{R}^{n \times d}$，随机特征维度 $m$</span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>**Step 1 — 采样正交随机矩阵**：从 $\mathcal{N}(0, I_d)$ 采样 $m$ 个向量 $\omega_1, \ldots, \omega_m$，经 Gram-Schmidt 正交化得到 $\Omega \in \mathbb{R}^{m \times d}$</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a>**Step 2 — 正随机特征映射**：对 $Q$ 和 $K$ 的每一行 $x \in \mathbb{R}^d$，计算</span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>$$\phi(x) = \frac{\exp(-<span class="sc">\|</span>x<span class="sc">\|</span>^2/2)}{\sqrt{m}} \begin{bmatrix} \exp(\omega_1^\top x) <span class="sc">\\</span> \exp(\omega_2^\top x) <span class="sc">\\</span> \vdots <span class="sc">\\</span> \exp(\omega_m^\top x) \end{bmatrix} \in \mathbb{R}^m$$</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a>得到 $\hat{Q} = \phi(Q) \in \mathbb{R}^{n \times m}$，$\hat{K} = \phi(K) \in \mathbb{R}^{n \times m}$</span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a>**Step 3 — 先聚合再查询**（避免 $n \times n$ 矩阵）：</span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算 KV 聚合矩阵：$S = \hat{K}^\top V \in \mathbb{R}^{m \times d}$ — 复杂度 $O(nmd)$</span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算归一化向量：$z = \hat{K}^\top \mathbf{1}_n \in \mathbb{R}^m$ — 复杂度 $O(nm)$</span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a>**Step 4 — 输出**：对每个位置 $i$，$\text{output}_i = \frac{\hat{q}_i^\top S}{\hat{q}_i^\top z} \in \mathbb{R}^d$ — 复杂度 $O(md)$</span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>**总复杂度**：$O(nmd)$ 时间，$O(nm + md + nd)$ 空间。当 $m \ll n$ 时，相比标准注意力的 $O(n^2 d)$ 获得显著加速。</span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>**"+" 的含义**：(1) **Positive** — 使用 $\exp(\omega^\top x)$ 而非 $\cos/\sin$ 保证注意力权重非负；(2) **Orthogonal** — $\omega_i$ 正交化降低蒙特卡洛方差，减少所需采样数 $m$。</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear Transformer：Transformer 与 RNN 的对偶性</span></span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>Katharopoulos et al.（2020）在 "Transformers are RNNs" 中揭示了一个令人惊叹的联系。他们选择了一个简单的特征映射：</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a>\phi(x) = \text{elu}(x) + 1</span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>其中 $\text{elu}$ 是指数线性单元。这个选择虽然简单，但保证了映射后的值始终为正（核函数必须非负）。</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>更深刻的洞察在于：当线性注意力用于**因果（自回归）**场景时，它可以被改写为一个 RNN！</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>在因果设定下，第 $i$ 个位置只能关注 $j \leq i$ 的位置。线性注意力变成：</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a>\text{output}_i = \frac{\phi(q_i)^\top S_i}{\phi(q_i)^\top z_i}</span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a>其中 $S_i = \sum_{j=1}^{i} \phi(k_j) v_j^\top$ 和 $z_i = \sum_{j=1}^{i} \phi(k_j)$ 可以**递归更新**：</span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a>S_i = S_{i-1} + \phi(k_i) v_i^\top, \quad z_i = z_{i-1} + \phi(k_i)</span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a>这恰好就是一个 RNN 的隐状态更新！$S_i$ 相当于 RNN 的隐藏状态，$\phi(k_i) v_i^\top$ 是每步的新信息。Transformer 和 RNN，看似截然不同的两种架构，在线性注意力的桥梁下竟然是同一枚硬币的两面。</span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a>这个对偶性带来了实际好处：训练时可以用并行的"Transformer 模式"（矩阵乘法），推理时可以切换到序列的"RNN 模式"（逐步递归），两者在数学上完全等价。</span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度全景对比</span></span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 时间复杂度 <span class="pp">|</span> 空间复杂度 <span class="pp">|</span> 理论保证 <span class="pp">|</span> 实际表现 <span class="pp">|</span></span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|-----------|---------|---------|</span></span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **全注意力** <span class="pp">|</span> $O(n^2 d)$ <span class="pp">|</span> $O(n^2 + nd)$ <span class="pp">|</span> 最优表达能力 <span class="pp">|</span> 基准 <span class="pp">|</span></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Longformer** <span class="pp">|</span> $O(nw)$ <span class="pp">|</span> $O(nw)$ <span class="pp">|</span> 感受野 $L \times w$ <span class="pp">|</span> 长文档任务优秀 <span class="pp">|</span></span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **BigBird** <span class="pp">|</span> $O(n(w+r+g))$ <span class="pp">|</span> $O(n(w+r+g))$ <span class="pp">|</span> 图灵完备 <span class="pp">|</span> 长文档 + QA <span class="pp">|</span></span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Linformer** <span class="pp">|</span> $O(nk)$ <span class="pp">|</span> $O(nk)$ <span class="pp">|</span> 低秩近似有界 <span class="pp">|</span> 固定长度任务 <span class="pp">|</span></span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Performer** <span class="pp">|</span> $O(nmd)$ <span class="pp">|</span> $O(nm + md + nd)$ <span class="pp">|</span> 无偏近似 <span class="pp">|</span> 语言建模稍弱 <span class="pp">|</span></span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Linear Transformer** <span class="pp">|</span> $O(nd^2)$ <span class="pp">|</span> $O(nd + d^2)$ <span class="pp">|</span> RNN 对偶 <span class="pp">|</span> 自回归推理快 <span class="pp">|</span></span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>这里有一个微妙但重要的细节：$O(n)$ 复杂度中的常数因子差异很大。Longformer 的 $w$ 通常是256-512，Performer 的 $m$ 通常需要和 $d$ 同阶甚至更大才能保证近似质量。所以**渐近复杂度只是故事的一部分**。</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现 Sliding Window Attention</span></span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a>下面用 PyTorch 实现 Longformer 风格的滑动窗口注意力，帮助理解其核心逻辑：</span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sliding_window_attention(Q, K, V, window_size, global_indices<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a><span class="co">    Longformer 风格的滑动窗口注意力</span></span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a><span class="co">        Q, K, V: [batch, seq_len, d_k]</span></span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a><span class="co">        window_size: 单侧窗口大小（总窗口 = 2 * window_size + 1）</span></span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a><span class="co">        global_indices: 需要全局注意力的位置索引列表</span></span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a><span class="co">        output: [batch, seq_len, d_k]</span></span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a>    batch, seq_len, d_k <span class="op">=</span> Q.shape</span>
<span id="cb4-495"><a href="#cb4-495" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> math.sqrt(d_k)</span>
<span id="cb4-496"><a href="#cb4-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 构建滑动窗口掩码</span></span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mask[i,j] = True 表示位置 i 可以关注位置 j</span></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.zeros(seq_len, seq_len, dtype<span class="op">=</span>torch.<span class="bu">bool</span>, device<span class="op">=</span>Q.device)</span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a>        left <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size)</span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> <span class="bu">min</span>(seq_len, i <span class="op">+</span> window_size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a>        mask[i, left:right] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 添加全局注意力</span></span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> global_indices <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-507"><a href="#cb4-507" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> g <span class="kw">in</span> global_indices:</span>
<span id="cb4-508"><a href="#cb4-508" aria-hidden="true" tabindex="-1"></a>            mask[g, :] <span class="op">=</span> <span class="va">True</span>   <span class="co"># 全局 token 关注所有位置</span></span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a>            mask[:, g] <span class="op">=</span> <span class="va">True</span>   <span class="co"># 所有位置关注全局 token</span></span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 计算注意力分数</span></span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> scale  <span class="co"># [batch, n, n]</span></span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: 应用掩码（被掩码的位置设为 -inf）</span></span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores.masked_fill(<span class="op">~</span>mask.unsqueeze(<span class="dv">0</span>), <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 5: Softmax + 加权求和</span></span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> attn_weights.masked_fill(torch.isnan(attn_weights), <span class="fl">0.0</span>)</span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.matmul(attn_weights, V)</span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, attn_weights</span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例用法</span></span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a>batch, seq_len, d_k <span class="op">=</span> <span class="dv">1</span>, <span class="dv">16</span>, <span class="dv">64</span></span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> torch.randn(batch, seq_len, d_k)</span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> torch.randn(batch, seq_len, d_k)</span>
<span id="cb4-528"><a href="#cb4-528" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> torch.randn(batch, seq_len, d_k)</span>
<span id="cb4-529"><a href="#cb4-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a>output, weights <span class="op">=</span> sliding_window_attention(</span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a>    Q, K, V,</span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span><span class="dv">3</span>,          <span class="co"># 每侧看3个 token</span></span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a>    global_indices<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">15</span>]  <span class="co"># 首尾 token 为全局</span></span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-535"><a href="#cb4-535" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"输出形状: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)      <span class="co"># [1, 16, 64]</span></span>
<span id="cb4-536"><a href="#cb4-536" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"注意力矩阵形状: </span><span class="sc">{</span>weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [1, 16, 16]</span></span>
<span id="cb4-537"><a href="#cb4-537" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"每个 token 平均关注 </span><span class="sc">{</span>(weights[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">0</span>)<span class="sc">.</span><span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)<span class="sc">.</span><span class="bu">float</span>()<span class="sc">.</span>mean()<span class="sc">:.1f}</span><span class="ss"> 个位置"</span>)</span>
<span id="cb4-538"><a href="#cb4-538" aria-hidden="true" tabindex="-1"></a><span class="co"># 窗口7 + 2个全局 ≈ 9个位置（远少于16个全位置）</span></span>
<span id="cb4-539"><a href="#cb4-539" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-540"><a href="#cb4-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-541"><a href="#cb4-541" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb4-542"><a href="#cb4-542" aria-hidden="true" tabindex="-1"></a><span class="fu">## 关于效率的重要说明</span></span>
<span id="cb4-543"><a href="#cb4-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-544"><a href="#cb4-544" aria-hidden="true" tabindex="-1"></a>上面的实现是为了**教学目的**——它仍然构建了完整的 $n \times n$ 注意力矩阵再掩码。真正高效的实现需要避免构建完整矩阵，例如使用块稀疏矩阵或自定义 CUDA kernel。Longformer 的官方实现和 Hugging Face 的 <span class="in">`transformers`</span> 库提供了优化版本。</span>
<span id="cb4-545"><a href="#cb4-545" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-546"><a href="#cb4-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-547"><a href="#cb4-547" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现 Linear Attention</span></span>
<span id="cb4-548"><a href="#cb4-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-549"><a href="#cb4-549" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-550"><a href="#cb4-550" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_attention(Q, K, V, feature_map<span class="op">=</span><span class="st">'elu'</span>):</span>
<span id="cb4-551"><a href="#cb4-551" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-552"><a href="#cb4-552" aria-hidden="true" tabindex="-1"></a><span class="co">    线性注意力（Linear Transformer 风格）</span></span>
<span id="cb4-553"><a href="#cb4-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-554"><a href="#cb4-554" aria-hidden="true" tabindex="-1"></a><span class="co">    将 softmax(QK^T)V 替换为 φ(Q)(φ(K)^T V)</span></span>
<span id="cb4-555"><a href="#cb4-555" aria-hidden="true" tabindex="-1"></a><span class="co">    利用矩阵乘法结合律避免构建 n×n 矩阵</span></span>
<span id="cb4-556"><a href="#cb4-556" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-557"><a href="#cb4-557" aria-hidden="true" tabindex="-1"></a>    batch, seq_len, d_k <span class="op">=</span> Q.shape</span>
<span id="cb4-558"><a href="#cb4-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-559"><a href="#cb4-559" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 特征映射 φ(x) = elu(x) + 1（保证非负）</span></span>
<span id="cb4-560"><a href="#cb4-560" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature_map <span class="op">==</span> <span class="st">'elu'</span>:</span>
<span id="cb4-561"><a href="#cb4-561" aria-hidden="true" tabindex="-1"></a>        phi_Q <span class="op">=</span> F.elu(Q) <span class="op">+</span> <span class="dv">1</span>   <span class="co"># [batch, n, d_k]</span></span>
<span id="cb4-562"><a href="#cb4-562" aria-hidden="true" tabindex="-1"></a>        phi_K <span class="op">=</span> F.elu(K) <span class="op">+</span> <span class="dv">1</span>   <span class="co"># [batch, n, d_k]</span></span>
<span id="cb4-563"><a href="#cb4-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-564"><a href="#cb4-564" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 先计算 KV = φ(K)^T V → [batch, d_k, d_k]</span></span>
<span id="cb4-565"><a href="#cb4-565" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 这是关键：先做 K^T V 而非 Q K^T</span></span>
<span id="cb4-566"><a href="#cb4-566" aria-hidden="true" tabindex="-1"></a>    KV <span class="op">=</span> torch.matmul(phi_K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), V)  <span class="co"># [batch, d_k, d_k]</span></span>
<span id="cb4-567"><a href="#cb4-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-568"><a href="#cb4-568" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 计算归一化因子 Z = φ(K)^T 1 → [batch, d_k]</span></span>
<span id="cb4-569"><a href="#cb4-569" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> phi_K.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">2</span>)  <span class="co"># [batch, d_k]</span></span>
<span id="cb4-570"><a href="#cb4-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-571"><a href="#cb4-571" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: 输出 = φ(Q) · KV / (φ(Q) · Z)</span></span>
<span id="cb4-572"><a href="#cb4-572" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span> torch.matmul(phi_Q, KV)           <span class="co"># [batch, n, d_k]</span></span>
<span id="cb4-573"><a href="#cb4-573" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span> torch.matmul(phi_Q, Z.unsqueeze(<span class="op">-</span><span class="dv">1</span>))  <span class="co"># [batch, n, 1]</span></span>
<span id="cb4-574"><a href="#cb4-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-575"><a href="#cb4-575" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> numerator <span class="op">/</span> (denominator <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb4-576"><a href="#cb4-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-577"><a href="#cb4-577" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb4-578"><a href="#cb4-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-579"><a href="#cb4-579" aria-hidden="true" tabindex="-1"></a><span class="co"># 对比标准注意力和线性注意力</span></span>
<span id="cb4-580"><a href="#cb4-580" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-581"><a href="#cb4-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-582"><a href="#cb4-582" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq_len <span class="kw">in</span> [<span class="dv">512</span>, <span class="dv">1024</span>, <span class="dv">2048</span>, <span class="dv">4096</span>]:</span>
<span id="cb4-583"><a href="#cb4-583" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> torch.randn(<span class="dv">1</span>, seq_len, <span class="dv">64</span>)</span>
<span id="cb4-584"><a href="#cb4-584" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> torch.randn(<span class="dv">1</span>, seq_len, <span class="dv">64</span>)</span>
<span id="cb4-585"><a href="#cb4-585" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> torch.randn(<span class="dv">1</span>, seq_len, <span class="dv">64</span>)</span>
<span id="cb4-586"><a href="#cb4-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-587"><a href="#cb4-587" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 标准注意力</span></span>
<span id="cb4-588"><a href="#cb4-588" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb4-589"><a href="#cb4-589" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> <span class="fl">8.0</span></span>
<span id="cb4-590"><a href="#cb4-590" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-591"><a href="#cb4-591" aria-hidden="true" tabindex="-1"></a>    out_standard <span class="op">=</span> torch.matmul(attn, V)</span>
<span id="cb4-592"><a href="#cb4-592" aria-hidden="true" tabindex="-1"></a>    t_standard <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb4-593"><a href="#cb4-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-594"><a href="#cb4-594" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 线性注意力</span></span>
<span id="cb4-595"><a href="#cb4-595" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb4-596"><a href="#cb4-596" aria-hidden="true" tabindex="-1"></a>    out_linear <span class="op">=</span> linear_attention(Q, K, V)</span>
<span id="cb4-597"><a href="#cb4-597" aria-hidden="true" tabindex="-1"></a>    t_linear <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb4-598"><a href="#cb4-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-599"><a href="#cb4-599" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>seq_len<span class="sc">:5d}</span><span class="ss"> | 标准: </span><span class="sc">{</span>t_standard<span class="op">*</span><span class="dv">1000</span><span class="sc">:.1f}</span><span class="ss">ms | "</span></span>
<span id="cb4-600"><a href="#cb4-600" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"线性: </span><span class="sc">{</span>t_linear<span class="op">*</span><span class="dv">1000</span><span class="sc">:.1f}</span><span class="ss">ms | 加速: </span><span class="sc">{</span>t_standard<span class="op">/</span>t_linear<span class="sc">:.1f}</span><span class="ss">x"</span>)</span>
<span id="cb4-601"><a href="#cb4-601" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-602"><a href="#cb4-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-603"><a href="#cb4-603" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用 Hugging Face 的高效注意力模型</span></span>
<span id="cb4-604"><a href="#cb4-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-605"><a href="#cb4-605" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb4-606"><a href="#cb4-606" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> LongformerModel, LongformerTokenizer</span>
<span id="cb4-607"><a href="#cb4-607" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BigBirdModel, BigBirdTokenizer</span>
<span id="cb4-608"><a href="#cb4-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-609"><a href="#cb4-609" aria-hidden="true" tabindex="-1"></a><span class="co"># Longformer：处理最长 4096 token</span></span>
<span id="cb4-610"><a href="#cb4-610" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> LongformerTokenizer.from_pretrained(<span class="st">'allenai/longformer-base-4096'</span>)</span>
<span id="cb4-611"><a href="#cb4-611" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LongformerModel.from_pretrained(<span class="st">'allenai/longformer-base-4096'</span>)</span>
<span id="cb4-612"><a href="#cb4-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-613"><a href="#cb4-613" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"这是一篇非常长的文档..."</span> <span class="op">*</span> <span class="dv">200</span>  <span class="co"># 模拟长文本</span></span>
<span id="cb4-614"><a href="#cb4-614" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span><span class="dv">4096</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-615"><a href="#cb4-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-616"><a href="#cb4-616" aria-hidden="true" tabindex="-1"></a><span class="co"># Longformer 需要指定全局注意力掩码</span></span>
<span id="cb4-617"><a href="#cb4-617" aria-hidden="true" tabindex="-1"></a>global_attention_mask <span class="op">=</span> torch.zeros_like(inputs[<span class="st">'input_ids'</span>])</span>
<span id="cb4-618"><a href="#cb4-618" aria-hidden="true" tabindex="-1"></a>global_attention_mask[:, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># [CLS] 获得全局注意力</span></span>
<span id="cb4-619"><a href="#cb4-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-620"><a href="#cb4-620" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs, global_attention_mask<span class="op">=</span>global_attention_mask)</span>
<span id="cb4-621"><a href="#cb4-621" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs.last_hidden_state: [batch, seq_len, hidden_size]</span></span>
<span id="cb4-622"><a href="#cb4-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-623"><a href="#cb4-623" aria-hidden="true" tabindex="-1"></a><span class="co"># BigBird：同样支持长序列</span></span>
<span id="cb4-624"><a href="#cb4-624" aria-hidden="true" tabindex="-1"></a>tokenizer_bb <span class="op">=</span> BigBirdTokenizer.from_pretrained(<span class="st">'google/bigbird-roberta-base'</span>)</span>
<span id="cb4-625"><a href="#cb4-625" aria-hidden="true" tabindex="-1"></a>model_bb <span class="op">=</span> BigBirdModel.from_pretrained(<span class="st">'google/bigbird-roberta-base'</span>)</span>
<span id="cb4-626"><a href="#cb4-626" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-627"><a href="#cb4-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-628"><a href="#cb4-628" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-629"><a href="#cb4-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-630"><a href="#cb4-630" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb4-631"><a href="#cb4-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-632"><a href="#cb4-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么线性注意力有性能损失？</span></span>
<span id="cb4-633"><a href="#cb4-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-634"><a href="#cb4-634" aria-hidden="true" tabindex="-1"></a>线性注意力在理论上很美，实践中却常常不如全注意力。这个差距值得深入探讨。</span>
<span id="cb4-635"><a href="#cb4-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-636"><a href="#cb4-636" aria-hidden="true" tabindex="-1"></a>**Softmax 的"选择性"不可替代**。标准 softmax 注意力有一个独特的性质：它是**尖锐的**（sharp）。当某个 key 与 query 特别匹配时，softmax 可以把绝大部分权重集中在这个 key 上，几乎实现"精确检索"。这种"赢者通吃"的行为对于许多 NLP 任务（如指代消解、事实提取）至关重要。</span>
<span id="cb4-637"><a href="#cb4-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-638"><a href="#cb4-638" aria-hidden="true" tabindex="-1"></a>而线性注意力的特征映射 $\phi$ 天然产生更"平滑"的注意力分布——它很难实现这种极端的集中。用信息论的语言来说，softmax 注意力可以实现接近零熵的分布，而线性注意力的熵有一个隐式的下界。</span>
<span id="cb4-639"><a href="#cb4-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-640"><a href="#cb4-640" aria-hidden="true" tabindex="-1"></a>**近似误差的累积效应**。在单层中，线性注意力对 softmax 的近似误差可能很小。但在深度网络中，这些误差会逐层累积。每一层的输出是下一层的输入，小的近似偏差可能在多层传播后被放大为显著的性能差距。Performer 的作者也承认，在语言建模任务上，FAVOR+ 需要较多的随机特征（$m$ 较大）才能达到接近全注意力的效果，这部分抵消了线性复杂度的优势。</span>
<span id="cb4-641"><a href="#cb4-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-642"><a href="#cb4-642" aria-hidden="true" tabindex="-1"></a><span class="fu">### 稀疏注意力的理论基础</span></span>
<span id="cb4-643"><a href="#cb4-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-644"><a href="#cb4-644" aria-hidden="true" tabindex="-1"></a>BigBird 的理论分析为稀疏注意力提供了坚实的数学基础。其核心结论包括：</span>
<span id="cb4-645"><a href="#cb4-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-646"><a href="#cb4-646" aria-hidden="true" tabindex="-1"></a>**通用逼近性**。BigBird 证明了：对于任何连续的序列到序列函数 $f$，存在一个使用 BigBird 稀疏注意力的 Transformer 网络可以以任意精度逼近 $f$。这个证明的关键是全局 token 充当了"信息中转站"，保证了任意两个位置之间的信息传播路径存在。</span>
<span id="cb4-647"><a href="#cb4-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-648"><a href="#cb4-648" aria-hidden="true" tabindex="-1"></a>**图灵完备性**。BigBird 还证明了稀疏注意力的图灵完备性——它可以模拟任何图灵机的计算。这个证明利用了全局 token 来模拟图灵机的磁带头。这些理论结果很重要：它们表明稀疏注意力在表达能力上**不输于全注意力**，前提是网络足够大、全局 token 的数量和位置设计得当。</span>
<span id="cb4-649"><a href="#cb4-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-650"><a href="#cb4-650" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb4-651"><a href="#cb4-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-652"><a href="#cb4-652" aria-hidden="true" tabindex="-1"></a>**最优注意力模式是什么？** Longformer 和 BigBird 使用的是人工设计的固定模式。这些模式是最优的吗？是否存在可以根据输入内容动态调整的注意力模式？Routing Transformer 和 Reformer 尝试了可学习的模式，但引入了额外的计算开销。如何在适应性和效率之间找到最佳平衡，仍是一个开放问题。</span>
<span id="cb4-653"><a href="#cb4-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-654"><a href="#cb4-654" aria-hidden="true" tabindex="-1"></a>**线性注意力的表达能力边界在哪里？** 虽然我们知道线性注意力在某些任务上不如 softmax 注意力，但这种差距是否可以通过更好的特征映射或更深的网络来弥补？还是说 softmax 的"选择性"是某些任务的硬性需求？</span>
<span id="cb4-655"><a href="#cb4-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-656"><a href="#cb4-656" aria-hidden="true" tabindex="-1"></a>**长序列中信息的自然结构是什么？** 所有高效注意力方法都隐含了一个假设——注意力矩阵存在可利用的结构（稀疏性、低秩性）。但这些结构在不同任务和不同文本类型中有多大差异？处理代码和处理小说，最优的注意力模式可能完全不同。</span>
<span id="cb4-657"><a href="#cb4-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-658"><a href="#cb4-658" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-659"><a href="#cb4-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-660"><a href="#cb4-660" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb4-661"><a href="#cb4-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-662"><a href="#cb4-662" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实践给出的出人意料的答案</span></span>
<span id="cb4-663"><a href="#cb4-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-664"><a href="#cb4-664" aria-hidden="true" tabindex="-1"></a>本章介绍的高效注意力方法在理论上非常优雅，但实践给出了一个令人意外的转折。在 2020-2021 年"X-former"百花齐放之后，NLP 领域的主流模型——GPT-4、Claude、LLaMA、Gemini——几乎无一例外地选择了**标准的全注意力**。</span>
<span id="cb4-665"><a href="#cb4-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-666"><a href="#cb4-666" aria-hidden="true" tabindex="-1"></a>为什么？答案在于 Flash Attention（Dao et al., 2022）。Flash Attention 不是从算法层面改变注意力的复杂度，而是从**硬件层面**优化了标准注意力的实现。它的核心发现是：标准注意力的实际瓶颈不是计算量（FLOPs），而是**内存读写**（IO）。通过精心设计的分块计算和内存访问模式，Flash Attention 将标准全注意力的实际速度提升了 2-4 倍，使得在主流硬件上处理数千甚至数万 token 变得可行。</span>
<span id="cb4-667"><a href="#cb4-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-668"><a href="#cb4-668" aria-hidden="true" tabindex="-1"></a>这给了我们一个深刻的教训：**算法复杂度不等于实际速度**。GPU 的算力增长速度远快于内存带宽，这意味着很多计算场景实际上是"IO瓶颈"而非"计算瓶颈"。线性注意力在 FLOPs 上赢了，但在实际运行时间上未必赢，因为它的内存访问模式可能对 GPU 不友好。</span>
<span id="cb4-669"><a href="#cb4-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-670"><a href="#cb4-670" aria-hidden="true" tabindex="-1"></a><span class="fu">### 高效注意力的遗产</span></span>
<span id="cb4-671"><a href="#cb4-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-672"><a href="#cb4-672" aria-hidden="true" tabindex="-1"></a>这是否意味着本章的内容"没用"了？恰恰相反。这些方法留下了宝贵的思想遗产：</span>
<span id="cb4-673"><a href="#cb4-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-674"><a href="#cb4-674" aria-hidden="true" tabindex="-1"></a>稀疏注意力的思想被融入了现代架构。Mistral 模型使用了滑动窗口注意力作为默认机制，在推理效率和性能之间取得了优秀的平衡。Mixture of Experts（MoE）架构中的条件计算思想与稀疏注意力一脉相承。</span>
<span id="cb4-675"><a href="#cb4-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-676"><a href="#cb4-676" aria-hidden="true" tabindex="-1"></a>线性注意力的 Transformer-RNN 对偶性启发了一系列新架构。Mamba（Gu &amp; Dao, 2023）等状态空间模型可以看作线性注意力思想的自然延伸——用线性递归替代注意力，在长序列建模上取得了突破性进展。</span>
<span id="cb4-677"><a href="#cb4-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-678"><a href="#cb4-678" aria-hidden="true" tabindex="-1"></a>低秩近似的观察启发了 LoRA 等参数高效微调方法——如果注意力矩阵是低秩的，那么模型的权重矩阵也许同样可以用低秩方式更新。</span>
<span id="cb4-679"><a href="#cb4-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-680"><a href="#cb4-680" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb4-681"><a href="#cb4-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-682"><a href="#cb4-682" aria-hidden="true" tabindex="-1"></a>高效注意力的探索告诉我们：序列建模的核心挑战不仅在于注意力的复杂度，更在于如何让模型在大规模数据上学到有用的表示。这引出了一个更深层的范式转变——**预训练**。无论用什么注意力机制，如何训练一个好的基础模型、如何将一个大模型的知识迁移到具体任务，才是推动 NLP 前进的真正引擎。</span>
<span id="cb4-683"><a href="#cb4-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-684"><a href="#cb4-684" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第10章将回到一个更根本的问题——预训练思想的起源。从 Word2Vec 作为"预训练的雏形"，到计算机视觉领域 ImageNet 预训练的启示，我们将追溯"先训练再迁移"这一范式是如何一步步成形的。</span></span>
<span id="cb4-685"><a href="#cb4-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-686"><a href="#cb4-686" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-687"><a href="#cb4-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-688"><a href="#cb4-688" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb4-689"><a href="#cb4-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-690"><a href="#cb4-690" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb4-691"><a href="#cb4-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-692"><a href="#cb4-692" aria-hidden="true" tabindex="-1"></a>这一章我们探讨了打破 Transformer $O(n^2)$ 复杂度的三条技术路线。核心问题是如何在保持表达能力的同时降低 Self-Attention 的计算代价。</span>
<span id="cb4-693"><a href="#cb4-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-694"><a href="#cb4-694" aria-hidden="true" tabindex="-1"></a>稀疏注意力（Longformer、BigBird）的策略是"只计算重要的注意力对"——通过局部窗口、全局 token 和随机连接的组合，把复杂度降到 $O(n)$，同时在理论上保持通用逼近性和图灵完备性。</span>
<span id="cb4-695"><a href="#cb4-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-696"><a href="#cb4-696" aria-hidden="true" tabindex="-1"></a>线性注意力（Performer、Linear Transformer）的策略是"改变计算顺序"——用核方法近似 softmax，利用矩阵乘法的结合律避免构建 $n \times n$ 矩阵，揭示了 Transformer 与 RNN 的深层对偶性。</span>
<span id="cb4-697"><a href="#cb4-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-698"><a href="#cb4-698" aria-hidden="true" tabindex="-1"></a>低秩方法（Linformer）的策略是"压缩 Key 和 Value"——基于注意力矩阵的低秩性质，用投影降低维度。</span>
<span id="cb4-699"><a href="#cb4-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-700"><a href="#cb4-700" aria-hidden="true" tabindex="-1"></a>这些方法的意义不仅在于效率优化本身，更在于它们揭示的理论洞察——注意力的稀疏性、低秩性、以及 Transformer-RNN 对偶性——深刻影响了后续的架构设计和研究方向。</span>
<span id="cb4-701"><a href="#cb4-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-702"><a href="#cb4-702" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb4-703"><a href="#cb4-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-704"><a href="#cb4-704" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 方法 <span class="pp">|</span> 核心公式 <span class="pp">|</span></span>
<span id="cb4-705"><a href="#cb4-705" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|</span></span>
<span id="cb4-706"><a href="#cb4-706" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 标准注意力 <span class="pp">|</span> $\text{Attn} = \text{softmax}(QK^\top / \sqrt{d_k}) V$ <span class="pp">|</span></span>
<span id="cb4-707"><a href="#cb4-707" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 稀疏注意力 <span class="pp">|</span> $\text{Attn}_i = \text{softmax}_{j \in \mathcal{S}_i}(q_i^\top k_j / \sqrt{d_k}) \cdot v_j$ <span class="pp">|</span></span>
<span id="cb4-708"><a href="#cb4-708" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 线性注意力 <span class="pp">|</span> $\text{Attn}_i = \frac{\phi(q_i)^\top \sum_j \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_j \phi(k_j)}$ <span class="pp">|</span></span>
<span id="cb4-709"><a href="#cb4-709" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Linformer <span class="pp">|</span> $\text{Attn} = \text{softmax}(Q(EK)^\top / \sqrt{d_k}) \cdot EV$ <span class="pp">|</span></span>
<span id="cb4-710"><a href="#cb4-710" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 因果线性注意力 <span class="pp">|</span> $S_i = S_{i-1} + \phi(k_i) v_i^\top$ （RNN递归形式） <span class="pp">|</span></span>
<span id="cb4-711"><a href="#cb4-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-712"><a href="#cb4-712" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb4-713"><a href="#cb4-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-714"><a href="#cb4-714" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么稀疏注意力需要"全局 token"？如果只用滑动窗口而没有全局 token，模型在处理什么类型的任务时会遇到困难？</span>
<span id="cb4-715"><a href="#cb4-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-716"><a href="#cb4-716" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 推导线性注意力的复杂度。证明：当使用特征映射 $\phi: \mathbb{R}^d \to \mathbb{R}^m$ 时，通过改变矩阵乘法的结合顺序，可以将注意力的复杂度从 $O(n^2 d)$ 降至 $O(nmd)$。</span>
<span id="cb4-717"><a href="#cb4-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-718"><a href="#cb4-718" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[数学推导-进阶]** 证明 Katharopoulos et al. 的核心结论：因果线性注意力等价于一个隐状态维度为 $d_k \times d_v$ 的 RNN。写出这个 RNN 的状态转移方程和输出方程。</span>
<span id="cb4-719"><a href="#cb4-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-720"><a href="#cb4-720" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[工程实践]** 在同一数据集（如 WikiText-2）上对比标准 Transformer 和 Longformer 的困惑度（perplexity），分别在序列长度 512、2048、8192 下测试。你观察到了什么趋势？</span>
<span id="cb4-721"><a href="#cb4-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-722"><a href="#cb4-722" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[研究思考]** Flash Attention 通过优化内存访问模式加速了标准全注意力，使得大多数高效注意力方法在实际速度上失去了优势。这是否意味着算法层面的注意力优化不再重要？考虑以下场景：(a) 序列长度超过100万（基因组学）；(b) 边缘设备上的推理；(c) 在线/流式处理任务。</span>
<span id="cb4-723"><a href="#cb4-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-724"><a href="#cb4-724" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-725"><a href="#cb4-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-726"><a href="#cb4-726" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb4-727"><a href="#cb4-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-728"><a href="#cb4-728" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb4-729"><a href="#cb4-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-730"><a href="#cb4-730" aria-hidden="true" tabindex="-1"></a>**Beltagy et al. (2020). "Longformer: The Long-Document Transformer"**（<span class="co">[</span><span class="ot">arXiv:2004.05150</span><span class="co">](https://arxiv.org/abs/2004.05150)</span>）。重点阅读 Section 3 的注意力模式设计和 Section 5 的长文档实验。图1的注意力模式可视化非常直观。</span>
<span id="cb4-731"><a href="#cb4-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-732"><a href="#cb4-732" aria-hidden="true" tabindex="-1"></a>**Zaheer et al. (2020). "Big Bird: Transformers for Longer Sequences"**（<span class="co">[</span><span class="ot">arXiv:2007.14062</span><span class="co">](https://arxiv.org/abs/2007.14062)</span>）。重点阅读 Section 2 的理论分析（图灵完备性证明）和 Section 4 的三种注意力组件设计。理论部分比较硬核但很有价值。</span>
<span id="cb4-733"><a href="#cb4-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-734"><a href="#cb4-734" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb4-735"><a href="#cb4-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-736"><a href="#cb4-736" aria-hidden="true" tabindex="-1"></a>**Choromanski et al. (2020). "Rethinking Attention with Performers"**（<span class="co">[</span><span class="ot">arXiv:2009.14794</span><span class="co">](https://arxiv.org/abs/2009.14794)</span>）。FAVOR+ 算法的详细推导。重点阅读 Section 3 的核方法分析和 Section 4 的正交随机特征。发表于 ICLR 2021。</span>
<span id="cb4-737"><a href="#cb4-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-738"><a href="#cb4-738" aria-hidden="true" tabindex="-1"></a>**Katharopoulos et al. (2020). "Transformers are RNNs"**（<span class="co">[</span><span class="ot">arXiv:2006.16236</span><span class="co">](https://arxiv.org/abs/2006.16236)</span>）。Transformer-RNN 对偶性的发现。重点阅读 Section 3.3 的因果线性注意力推导。发表于 ICML 2020。</span>
<span id="cb4-739"><a href="#cb4-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-740"><a href="#cb4-740" aria-hidden="true" tabindex="-1"></a>**Wang et al. (2020). "Linformer: Self-Attention with Linear Complexity"**（<span class="co">[</span><span class="ot">arXiv:2006.04768</span><span class="co">](https://arxiv.org/abs/2006.04768)</span>）。低秩近似理论的关键分析。重点阅读 Section 3 的 Johnson-Lindenstrauss 引理应用。</span>
<span id="cb4-741"><a href="#cb4-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-742"><a href="#cb4-742" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述</span></span>
<span id="cb4-743"><a href="#cb4-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-744"><a href="#cb4-744" aria-hidden="true" tabindex="-1"></a>**Tay et al. (2022). "Efficient Transformers: A Survey"**（<span class="co">[</span><span class="ot">arXiv:2009.06732</span><span class="co">](https://arxiv.org/abs/2009.06732)</span>）。高效 Transformer 领域最全面的综述，提出了七大类分类体系。如果只读一篇综述，读这篇。发表于 ACM Computing Surveys。</span>
<span id="cb4-745"><a href="#cb4-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-746"><a href="#cb4-746" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb4-747"><a href="#cb4-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-748"><a href="#cb4-748" aria-hidden="true" tabindex="-1"></a>**Dao et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention"**。硬件感知的注意力优化，将在第26章详细讨论。</span>
<span id="cb4-749"><a href="#cb4-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-750"><a href="#cb4-750" aria-hidden="true" tabindex="-1"></a>**Gu &amp; Dao (2023). "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"**。线性注意力思想的自然延伸，用选择性状态空间模型替代注意力。</span>
<span id="cb4-751"><a href="#cb4-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-752"><a href="#cb4-752" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb4-753"><a href="#cb4-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-754"><a href="#cb4-754" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Longformer 官方实现：<span class="co">[</span><span class="ot">github.com/allenai/longformer</span><span class="co">](https://github.com/allenai/longformer)</span></span>
<span id="cb4-755"><a href="#cb4-755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>BigBird 官方实现：<span class="co">[</span><span class="ot">github.com/google-research/bigbird</span><span class="co">](https://github.com/google-research/bigbird)</span></span>
<span id="cb4-756"><a href="#cb4-756" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear Transformer 实现：<span class="co">[</span><span class="ot">linear-transformers.com</span><span class="co">](https://linear-transformers.com/)</span></span>
<span id="cb4-757"><a href="#cb4-757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hugging Face 的高效注意力模型集合：<span class="in">`transformers`</span> 库中的 <span class="in">`LongformerModel`</span>、<span class="in">`BigBirdModel`</span></span>
<span id="cb4-758"><a href="#cb4-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-759"><a href="#cb4-759" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-760"><a href="#cb4-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-761"><a href="#cb4-761" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb4-762"><a href="#cb4-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-763"><a href="#cb4-763" aria-hidden="true" tabindex="-1"></a>2020年可以被称为"X-former 之年"。在短短几个月内，Reformer（1月）、Longformer（4月）、Linformer（6月）、Linear Transformer（6月）、BigBird（7月）、Performer（9月）接连发布。每篇论文都声称找到了打破 $O(n^2)$ 的方法，一时间让人眼花缭乱。Yi Tay 等人及时推出的 survey "Efficient Transformers: A Survey" 帮助社区理清了这片混战的局面。</span>
<span id="cb4-764"><a href="#cb4-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-765"><a href="#cb4-765" aria-hidden="true" tabindex="-1"></a>然而，历史的走向出乎所有人意料。这些精妙的算法层面优化，最终被一个硬件层面的优化——Flash Attention——大幅抢了风头。Tri Dao（Flash Attention 的作者）日后加入了 Princeton，与 Danqi Chen 一起教授 NLP 课程，这大概是学术界对"系统优化也是科学"的最好注脚。</span>
<span id="cb4-766"><a href="#cb4-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-767"><a href="#cb4-767" aria-hidden="true" tabindex="-1"></a>不过，稀疏注意力的思想并未消亡。2023年发布的 Mistral 7B 模型在其架构中采用了滑动窗口注意力，证明了 Longformer 的核心思想在精简形式下仍然具有实用价值。而线性注意力的思想更是通过 Mamba 等状态空间模型获得了"第二次生命"，在长序列建模上展现出巨大的潜力。</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>