<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-25">
<meta name="description" content="从离散符号到分布式语义：Word2Vec、GloVe、FastText如何让机器理解词义，以及静态词向量的根本局限。">

<title>第3章：表示学习的觉醒 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#表示问题的精确定义" id="toc-表示问题的精确定义" class="nav-link" data-scroll-target="#表示问题的精确定义"><span class="header-section-number">2.1</span> 表示问题的精确定义</a></li>
  <li><a href="#之前的尝试为何失败" id="toc-之前的尝试为何失败" class="nav-link" data-scroll-target="#之前的尝试为何失败"><span class="header-section-number">2.2</span> 之前的尝试为何失败？</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#分布式假设语境定义语义" id="toc-分布式假设语境定义语义" class="nav-link" data-scroll-target="#分布式假设语境定义语义"><span class="header-section-number">3.1</span> 分布式假设：语境定义语义</a></li>
  <li><a href="#两种学习范式cbow与skip-gram" id="toc-两种学习范式cbow与skip-gram" class="nav-link" data-scroll-target="#两种学习范式cbow与skip-gram"><span class="header-section-number">3.2</span> 两种学习范式：CBOW与Skip-gram</a></li>
  <li><a href="#一个直觉性的例子" id="toc-一个直觉性的例子" class="nav-link" data-scroll-target="#一个直觉性的例子"><span class="header-section-number">3.3</span> 一个直觉性的例子</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#skip-gram模型的数学形式" id="toc-skip-gram模型的数学形式" class="nav-link" data-scroll-target="#skip-gram模型的数学形式"><span class="header-section-number">4.1</span> Skip-gram模型的数学形式</a></li>
  <li><a href="#计算瓶颈分母太贵" id="toc-计算瓶颈分母太贵" class="nav-link" data-scroll-target="#计算瓶颈分母太贵"><span class="header-section-number">4.2</span> 计算瓶颈：分母太贵</a></li>
  <li><a href="#负采样化softmax为二分类" id="toc-负采样化softmax为二分类" class="nav-link" data-scroll-target="#负采样化softmax为二分类"><span class="header-section-number">4.3</span> 负采样：化softmax为二分类</a></li>
  <li><a href="#负采样分布的设计" id="toc-负采样分布的设计" class="nav-link" data-scroll-target="#负采样分布的设计"><span class="header-section-number">4.4</span> 负采样分布的设计</a></li>
  <li><a href="#cbow模型" id="toc-cbow模型" class="nav-link" data-scroll-target="#cbow模型"><span class="header-section-number">4.5</span> CBOW模型</a></li>
  <li><a href="#数值示例skip-gram训练一步" id="toc-数值示例skip-gram训练一步" class="nav-link" data-scroll-target="#数值示例skip-gram训练一步"><span class="header-section-number">4.6</span> 数值示例：Skip-gram训练一步</a></li>
  <li><a href="#词向量类比的神奇现象" id="toc-词向量类比的神奇现象" class="nav-link" data-scroll-target="#词向量类比的神奇现象"><span class="header-section-number">4.7</span> 词向量类比的神奇现象</a></li>
  </ul></li>
  <li><a href="#glove与fasttext词向量的改进" id="toc-glove与fasttext词向量的改进" class="nav-link" data-scroll-target="#glove与fasttext词向量的改进"><span class="header-section-number">5</span> GloVe与FastText：词向量的改进</a>
  <ul class="collapse">
  <li><a href="#glove全局统计的视角" id="toc-glove全局统计的视角" class="nav-link" data-scroll-target="#glove全局统计的视角"><span class="header-section-number">5.1</span> GloVe：全局统计的视角</a></li>
  <li><a href="#fasttext子词的力量" id="toc-fasttext子词的力量" class="nav-link" data-scroll-target="#fasttext子词的力量"><span class="header-section-number">5.2</span> FastText：子词的力量</a></li>
  </ul></li>
  <li><a href="#工程实践词向量训练与可视化" id="toc-工程实践词向量训练与可视化" class="nav-link" data-scroll-target="#工程实践词向量训练与可视化"><span class="header-section-number">6</span> 工程实践：词向量训练与可视化</a>
  <ul class="collapse">
  <li><a href="#使用gensim训练word2vec" id="toc-使用gensim训练word2vec" class="nav-link" data-scroll-target="#使用gensim训练word2vec"><span class="header-section-number">6.1</span> 使用Gensim训练Word2Vec</a></li>
  <li><a href="#使用预训练词向量" id="toc-使用预训练词向量" class="nav-link" data-scroll-target="#使用预训练词向量"><span class="header-section-number">6.2</span> 使用预训练词向量</a></li>
  <li><a href="#词向量可视化" id="toc-词向量可视化" class="nav-link" data-scroll-target="#词向量可视化"><span class="header-section-number">6.3</span> 词向量可视化</a></li>
  <li><a href="#评估词向量质量" id="toc-评估词向量质量" class="nav-link" data-scroll-target="#评估词向量质量"><span class="header-section-number">6.4</span> 评估词向量质量</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">7</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">7.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#为什么有效实证视角" id="toc-为什么有效实证视角" class="nav-link" data-scroll-target="#为什么有效实证视角"><span class="header-section-number">7.2</span> 为什么有效？——实证视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">7.3</span> 方法的边界条件</a></li>
  <li><a href="#变体与扩展" id="toc-变体与扩展" class="nav-link" data-scroll-target="#变体与扩展"><span class="header-section-number">7.4</span> 变体与扩展</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">7.5</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">8</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#静态表示的根本缺陷" id="toc-静态表示的根本缺陷" class="nav-link" data-scroll-target="#静态表示的根本缺陷"><span class="header-section-number">8.1</span> 静态表示的根本缺陷</a></li>
  <li><a href="#上下文信息的缺失" id="toc-上下文信息的缺失" class="nav-link" data-scroll-target="#上下文信息的缺失"><span class="header-section-number">8.2</span> 上下文信息的缺失</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">8.3</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">9</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">9.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">9.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">9.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">10</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">10.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论分析" id="toc-理论分析" class="nav-link" data-scroll-target="#理论分析"><span class="header-section-number">10.2</span> 理论分析</a></li>
  <li><a href="#改进方法" id="toc-改进方法" class="nav-link" data-scroll-target="#改进方法"><span class="header-section-number">10.3</span> 改进方法</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">10.4</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">10.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">11</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第3章：表示学习的觉醒</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">从离散符号到分布式语义</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Word2Vec</div>
    <div class="quarto-category">GloVe</div>
    <div class="quarto-category">FastText</div>
    <div class="quarto-category">Representation Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    从离散符号到分布式语义：Word2Vec、GloVe、FastText如何让机器理解词义，以及静态词向量的根本局限。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何让计算机理解词的”含义”？如何用数学表示”猫”和”狗”比”猫”和”民主”更相似？</p>
<p><strong>历史坐标</strong>：2013 | Mikolov et al.&nbsp;(Word2Vec) | 从特征工程到表示学习的范式转变</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>在第1章中，我们回顾了前深度学习时代的NLP——那是一个特征工程的时代，研究者需要手工设计特征，把原始文本转换成机器学习模型可以处理的数值向量。第2章则鸟瞰了NLP的核心任务全景，从文本分类到机器翻译、从序列标注到问答系统。我们看到了传统方法的成功，但也看到了它们共同面临的瓶颈——特征工程不可扩展，每个新任务都需要重新设计特征，专家的时间成为了整个领域的天花板。</p>
<p>更根本的问题是：传统方法把词当作离散的符号来处理。在one-hot编码中，“cat”是一个维度上的1，“dog”是另一个维度上的1，它们之间没有任何数学关系。模型无法知道”cat”和”dog”都是动物、都是宠物、都有四条腿。这种离散表示无法捕获语义相似性，导致泛化困难——一个在”猫追老鼠”上训练的模型，无法自动迁移到”狗追松鼠”。</p>
<p>这就引出了一个核心问题：能不能让词的表示本身就蕴含语义信息？让语义相似的词，在数学上也彼此接近？</p>
<p>2013年，Google的研究员Tomas Mikolov给出了一个惊人简洁的答案。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：词的含义可以从它的”邻居”中学习——“You shall know a word by the company it keeps”。通过预测上下文（或被上下文预测），模型可以自动学习词的分布式表示，让语义相似的词在向量空间中彼此接近。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="表示问题的精确定义" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="表示问题的精确定义"><span class="header-section-number">2.1</span> 表示问题的精确定义</h3>
<p>让我们先精确地定义问题。我们希望找到一个映射函数：</p>
<p><span class="math display">\[
f: \mathcal{V} \rightarrow \mathbb{R}^d
\]</span></p>
<p>把词汇表<span class="math inline">\(\mathcal{V}\)</span>中的每个词映射到一个<span class="math inline">\(d\)</span>维的实数向量。这个映射应该满足一个关键性质：<strong>语义相似的词，映射后的向量也相似</strong>。</p>
<p>用数学语言说，如果<span class="math inline">\(w_1\)</span>和<span class="math inline">\(w_2\)</span>语义相似，那么<span class="math inline">\(f(w_1)\)</span>和<span class="math inline">\(f(w_2)\)</span>的余弦相似度应该高：</p>
<p><span class="math display">\[
\text{similarity}(w_1, w_2) \propto \cos(f(w_1), f(w_2)) = \frac{f(w_1) \cdot f(w_2)}{\|f(w_1)\| \|f(w_2)\|}
\]</span></p>
<p>更进一步，我们希望这种表示能够捕获语义关系。比如，“国王”之于”女王”的关系，应该类似于”男人”之于”女人”。这意味着向量空间中应该存在某种规律性的结构。</p>
</section>
<section id="之前的尝试为何失败" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="之前的尝试为何失败"><span class="header-section-number">2.2</span> 之前的尝试为何失败？</h3>
<p>在Word2Vec之前，也有人尝试过学习词的分布式表示，但效果不尽如人意。</p>
<p><strong>One-hot编码</strong>是最简单的表示方法。如果词汇表大小是<span class="math inline">\(|\mathcal{V}|\)</span>，那么每个词都是一个<span class="math inline">\(|\mathcal{V}|\)</span>维的向量，只有对应位置是1，其他都是0。这种表示的问题是显而易见的：任意两个词的向量都是正交的，<span class="math inline">\(\cos(\text{cat}, \text{dog}) = 0\)</span>，没有任何相似性信息。</p>
<p><strong>共现矩阵</strong>是一个更有意思的尝试。思路是：如果两个词经常在相似的上下文中出现，它们可能是相似的。可以构建一个词-词共现矩阵<span class="math inline">\(X\)</span>，其中<span class="math inline">\(X_{ij}\)</span>表示词<span class="math inline">\(i\)</span>和词<span class="math inline">\(j\)</span>在窗口内共同出现的次数。然后对这个矩阵做SVD分解，取前<span class="math inline">\(d\)</span>个奇异向量作为词的表示。这种方法确实能捕获一些语义信息，但有几个问题：矩阵太大（<span class="math inline">\(|\mathcal{V}| \times |\mathcal{V}|\)</span>可能有数十亿元素），SVD计算昂贵，而且新词需要重新计算整个分解。</p>
<p><strong>神经语言模型</strong>（Bengio et al., 2003）是最接近现代方法的先驱。他们用神经网络训练语言模型，词的embedding作为模型的第一层被一起学习。这个想法很正确，但当时的计算能力有限，模型很小，无法在大规模语料上训练，因此效果受限。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>回顾这些尝试，理想的解决方案应该具备以下特性。</p>
<p>首先是<strong>可扩展性</strong>。方法应该能在大规模语料（数十亿词）上高效训练。这意味着不能依赖全局矩阵分解，需要能够用随机梯度下降增量式地学习。</p>
<p>其次是<strong>语义保持</strong>。学到的表示应该自然地捕获语义相似性，不需要额外的语义知识库。</p>
<p>第三是<strong>泛化能力</strong>。表示应该在多种下游任务中有用——情感分析、命名实体识别、问答系统…一次训练，到处使用。</p>
<p>最后是<strong>可解释性</strong>。最好能够理解表示学到了什么，向量的不同维度是否有明确的语义。</p>
<p>Word2Vec惊人地同时满足了前三个要求，并在某种程度上满足了第四个——它发现了著名的”词向量类比”现象。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="分布式假设语境定义语义" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="分布式假设语境定义语义"><span class="header-section-number">3.1</span> 分布式假设：语境定义语义</h3>
<p>Word2Vec的核心洞察来自语言学中的<strong>分布式假设</strong>（Distributional Hypothesis）：</p>
<blockquote class="blockquote">
<p>“You shall know a word by the company it keeps.” — J.R. Firth, 1957</p>
</blockquote>
<p>一个词的含义，可以从它出现的上下文中推断出来。考虑两个你可能不熟悉的词：</p>
<ul>
<li>“The <strong>glorp</strong> chased the mouse across the kitchen.”</li>
<li>“I fed my <strong>glorp</strong> some fish and it purred happily.”</li>
</ul>
<p>即使不知道”glorp”的定义，你也能推断它可能是某种猫科动物。因为它出现在”chased mouse”、“fed fish”、“purred”这样的上下文中，而这些上下文通常与猫相关。</p>
<p>这个洞察的数学含义是：如果两个词经常出现在相似的上下文中，它们应该有相似的向量表示。Word2Vec正是利用这一点来学习词向量。</p>
</section>
<section id="两种学习范式cbow与skip-gram" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="两种学习范式cbow与skip-gram"><span class="header-section-number">3.2</span> 两种学习范式：CBOW与Skip-gram</h3>
<p>Word2Vec提出了两种学习词向量的方法，它们是镜像对称的。</p>
<p><strong>CBOW（Continuous Bag of Words）</strong>：给定上下文词，预测中心词。比如，给定”The cat ___ on the mat”中的上下文词，预测中间的词是”sat”。</p>
<p><strong>Skip-gram</strong>：给定中心词，预测上下文词。比如，给定”sat”，预测它周围可能出现”cat”、“on”、“mat”等词。</p>
<p>直觉上，CBOW是”多个线索猜一个词”，而Skip-gram是”一个词猜多个线索”。实践中，Skip-gram在处理罕见词时效果更好，因为每个罕见词都有多个预测机会（需要预测多个上下文词），获得更多的训练信号。这也是为什么Skip-gram在后来的研究中更常被使用。</p>
<div id="fig-cbow-skipgram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cbow-skipgram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-3/fig3-cbow-vs-skipgram.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cbow-skipgram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: CBOW 与 Skip-gram 架构对比：CBOW 从上下文预测中心词，Skip-gram 从中心词预测上下文。两者是镜像对称的。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>自绘示意图，基于 Mikolov et al.&nbsp;(2013) “Efficient Estimation of Word Representations in Vector Space”. <a href="https://arxiv.org/abs/1301.3781">arXiv:1301.3781</a></em></p>
</div>
</section>
<section id="一个直觉性的例子" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="一个直觉性的例子"><span class="header-section-number">3.3</span> 一个直觉性的例子</h3>
<p>让我们用一个简化的例子来建立直觉。假设语料库是：</p>
<ul>
<li>“I love cats”</li>
<li>“I love dogs”</li>
<li>“cats are cute”</li>
<li>“dogs are loyal”</li>
</ul>
<p>如果我们用Skip-gram训练，以”love”为中心词（窗口大小=1），需要预测”I”和”cats”（来自第一句）以及”I”和”dogs”（来自第二句）。</p>
<p>训练过程会调整词向量，让”love”的向量能够同时预测”cats”和”dogs”。这意味着”cats”和”dogs”需要在向量空间中有相似的位置——因为它们都经常出现在”love”附近。</p>
<p>类似地，“cats”需要预测”are”和”cute”，“dogs”需要预测”are”和”loyal”。它们都需要预测”are”，这进一步拉近了它们在向量空间中的距离。</p>
<p>通过这种方式，即使”cats”和”dogs”从未在同一个句子中共同出现，模型也能学习到它们的相似性——因为它们出现在相似的上下文模式中。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="skip-gram模型的数学形式" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="skip-gram模型的数学形式"><span class="header-section-number">4.1</span> Skip-gram模型的数学形式</h3>
<p>让我们详细推导Skip-gram模型。设词汇表大小为<span class="math inline">\(|\mathcal{V}|\)</span>，嵌入维度为<span class="math inline">\(d\)</span>。模型有两套词向量：</p>
<ul>
<li><strong>中心词向量</strong>（Center word embeddings）：<span class="math inline">\(\mathbf{v}_w \in \mathbb{R}^d\)</span>，用于词<span class="math inline">\(w\)</span>作为中心词时</li>
<li><strong>上下文词向量</strong>（Context word embeddings）：<span class="math inline">\(\mathbf{u}_w \in \mathbb{R}^d\)</span>，用于词<span class="math inline">\(w\)</span>作为上下文词时</li>
</ul>
<p>给定中心词<span class="math inline">\(w_c\)</span>和上下文词<span class="math inline">\(w_o\)</span>，我们用softmax定义条件概率：</p>
<p><span class="math display">\[
P(w_o | w_c) = \frac{\exp(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \mathbf{v}_{w_c})}
\]</span></p>
<p>直觉是：如果<span class="math inline">\(\mathbf{u}_{w_o}\)</span>和<span class="math inline">\(\mathbf{v}_{w_c}\)</span>的点积大（向量方向相近），那么<span class="math inline">\(w_o\)</span>出现在<span class="math inline">\(w_c\)</span>上下文中的概率就高。</p>
<p>给定语料库中的一个位置<span class="math inline">\(t\)</span>，中心词是<span class="math inline">\(w_t\)</span>，上下文窗口大小是<span class="math inline">\(m\)</span>，Skip-gram的目标是最大化：</p>
<p><span class="math display">\[
\prod_{-m \leq j \leq m, j \neq 0} P(w_{t+j} | w_t)
\]</span></p>
<p>对整个语料库，目标函数是：</p>
<p><span class="math display">\[
\mathcal{L} = \sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} | w_t)
\]</span></p>
</section>
<section id="计算瓶颈分母太贵" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="计算瓶颈分母太贵"><span class="header-section-number">4.2</span> 计算瓶颈：分母太贵</h3>
<p>展开log概率：</p>
<p><span class="math display">\[
\log P(w_o | w_c) = \mathbf{u}_{w_o}^\top \mathbf{v}_{w_c} - \log \sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \mathbf{v}_{w_c})
\]</span></p>
<p>问题出在分母——需要对整个词汇表求和。如果词汇表有100万个词，每次计算一个概率就需要100万次点积运算，这是不可接受的。</p>
<p>Word2Vec的关键创新之一就是解决这个计算瓶颈。两种主要方法是<strong>负采样</strong>（Negative Sampling）和<strong>层级Softmax</strong>（Hierarchical Softmax）。</p>
</section>
<section id="负采样化softmax为二分类" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="负采样化softmax为二分类"><span class="header-section-number">4.3</span> 负采样：化softmax为二分类</h3>
<p>负采样的核心思想是：不去计算完整的softmax，而是把问题转化为二分类——区分”真实的上下文词”和”随机采样的噪声词”。</p>
<p>对于一个正样本对<span class="math inline">\((w_c, w_o)\)</span>（<span class="math inline">\(w_o\)</span>确实出现在<span class="math inline">\(w_c\)</span>的上下文中），我们同时采样<span class="math inline">\(k\)</span>个负样本<span class="math inline">\(w_1, w_2, \ldots, w_k\)</span>（随机从词汇表中按某种分布采样）。</p>
<p>新的目标函数是最大化：</p>
<p><span class="math display">\[
\log \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} \left[ \log \sigma(-\mathbf{u}_{w_i}^\top \mathbf{v}_{w_c}) \right]
\]</span></p>
<p>其中<span class="math inline">\(\sigma(x) = \frac{1}{1+e^{-x}}\)</span>是sigmoid函数，<span class="math inline">\(P_n(w)\)</span>是负采样分布。</p>
<p>这个目标的直觉是：让真实上下文词的向量与中心词相近（点积为正），让随机噪声词的向量与中心词远离（点积为负）。</p>
<p><strong>为什么这样做是合理的？</strong></p>
<p>可以证明，当<span class="math inline">\(k\)</span>趋近于无穷时，负采样的目标函数趋近于原始softmax目标的某种近似。直觉上，如果模型能够区分真实上下文和随机噪声，它就学会了词之间的共现模式。</p>
</section>
<section id="负采样分布的设计" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="负采样分布的设计"><span class="header-section-number">4.4</span> 负采样分布的设计</h3>
<p>负采样分布<span class="math inline">\(P_n(w)\)</span>的选择对结果有影响。实践中，Word2Vec使用：</p>
<p><span class="math display">\[
P_n(w) \propto f(w)^{3/4}
\]</span></p>
<p>其中<span class="math inline">\(f(w)\)</span>是词<span class="math inline">\(w\)</span>在语料库中的频率。</p>
<p><strong>为什么用3/4次方？</strong></p>
<p>如果直接用频率<span class="math inline">\(f(w)\)</span>，高频词（如”the”、“a”）会被过度采样，低频词几乎不会被采样。用<span class="math inline">\(f(w)^{3/4}\)</span>是一个”平滑”：它降低了高频词的权重，增加了低频词的权重，使得采样分布更均匀。</p>
<p>实验表明，0.75是一个神奇的数字——它在多个数据集上都表现良好。这个选择更多是经验性的，没有很强的理论解释。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Skip-gram with Negative Sampling (SGNS) (Mikolov et al., 2013)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>：语料库 <span class="math inline">\(\mathcal{D}\)</span>，词汇表 <span class="math inline">\(\mathcal{V}\)</span>，嵌入维度 <span class="math inline">\(d\)</span>，窗口大小 <span class="math inline">\(m\)</span>，负采样数 <span class="math inline">\(k\)</span>，学习率 <span class="math inline">\(\eta\)</span></p>
<p><strong>输出</strong>：词向量矩阵 <span class="math inline">\(\mathbf{V} \in \mathbb{R}^{|\mathcal{V}| \times d}\)</span>（中心词），<span class="math inline">\(\mathbf{U} \in \mathbb{R}^{|\mathcal{V}| \times d}\)</span>（上下文词）</p>
<p><strong>1. 初始化</strong>：</p>
<pre><code>随机初始化 V, U ∈ ℝ^(|V| × d)（通常使用小的随机值）
构建负采样分布 Pn(w) ∝ f(w)^0.75</code></pre>
<p><strong>2. 训练循环</strong>：</p>
<pre><code>for epoch = 1 to num_epochs:
    for 每个位置 t = 1 to T in 语料库:
        wc ← 中心词（位置 t 的词）

        for 每个上下文位置 j ∈ [-m, m], j ≠ 0:
            wo ← 上下文词（位置 t+j 的词）

            # 正样本更新
            score_pos ← u_wo · v_wc
            grad_pos ← (1 - σ(score_pos))

            # 负采样
            for i = 1 to k:
                wn ← 从 Pn(w) 采样一个负样本词
                score_neg ← u_wn · v_wc
                grad_neg ← σ(score_neg)

                # 更新负样本的上下文向量
                u_wn ← u_wn - η · grad_neg · v_wc

            # 更新正样本的上下文向量
            u_wo ← u_wo + η · grad_pos · v_wc

            # 更新中心词向量
            v_wc ← v_wc + η · (grad_pos · u_wo - Σ grad_neg · u_wn)</code></pre>
<p><strong>3. 输出</strong>：返回 <span class="math inline">\(\mathbf{V}\)</span>（或 <span class="math inline">\(\frac{\mathbf{V} + \mathbf{U}}{2}\)</span>）作为最终词向量</p>
<p><strong>关键参数选择</strong>（来自原论文）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>参数</th>
<th>小数据集</th>
<th>大数据集</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>负采样数 <span class="math inline">\(k\)</span></td>
<td>5-20</td>
<td>2-5</td>
</tr>
<tr class="even">
<td>窗口大小 <span class="math inline">\(m\)</span></td>
<td>5</td>
<td>5-10</td>
</tr>
<tr class="odd">
<td>嵌入维度 <span class="math inline">\(d\)</span></td>
<td>100-300</td>
<td>300</td>
</tr>
</tbody>
</table>
<p><em>Adapted from: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). “Distributed Representations of Words and Phrases and their Compositionality”. NeurIPS 2013. <a href="https://arxiv.org/abs/1310.4546">arXiv:1310.4546</a></em></p>
</div>
</div>
</section>
<section id="cbow模型" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="cbow模型"><span class="header-section-number">4.5</span> CBOW模型</h3>
<p>CBOW与Skip-gram方向相反：给定上下文词，预测中心词。</p>
<p>设上下文窗口内的词是<span class="math inline">\(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\)</span>，CBOW首先计算上下文的平均向量：</p>
<p><span class="math display">\[
\bar{\mathbf{v}} = \frac{1}{2m} \sum_{-m \leq j \leq m, j \neq 0} \mathbf{v}_{w_{c+j}}
\]</span></p>
<p>然后用这个平均向量预测中心词：</p>
<p><span class="math display">\[
P(w_c | \text{context}) = \frac{\exp(\mathbf{u}_{w_c}^\top \bar{\mathbf{v}})}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \bar{\mathbf{v}})}
\]</span></p>
<p>CBOW的计算效率比Skip-gram高（每个位置只需一次预测），但在捕获罕见词方面通常不如Skip-gram。</p>
</section>
<section id="数值示例skip-gram训练一步" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="数值示例skip-gram训练一步"><span class="header-section-number">4.6</span> 数值示例：Skip-gram训练一步</h3>
<p>让我们用一个极简的数值例子来理解Skip-gram的梯度更新是如何工作的。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>词汇表：{I, love, cats, dogs}，大小<span class="math inline">\(|\mathcal{V}| = 4\)</span></li>
<li>嵌入维度：<span class="math inline">\(d = 2\)</span></li>
<li>学习率：<span class="math inline">\(\eta = 0.1\)</span></li>
<li>负采样数：<span class="math inline">\(k = 1\)</span></li>
</ul>
<p><strong>初始化词向量</strong>（随机小值）：</p>
<p>中心词向量<span class="math inline">\(\mathbf{v}\)</span>：</p>
<p><span class="math display">\[
\mathbf{v}_{\text{I}} = \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix}, \quad
\mathbf{v}_{\text{love}} = \begin{bmatrix} 0.3 \\ -0.1 \end{bmatrix}, \quad
\mathbf{v}_{\text{cats}} = \begin{bmatrix} -0.2 \\ 0.4 \end{bmatrix}, \quad
\mathbf{v}_{\text{dogs}} = \begin{bmatrix} 0.0 \\ 0.3 \end{bmatrix}
\]</span></p>
<p>上下文词向量<span class="math inline">\(\mathbf{u}\)</span>（初始化相同值，简化演示）：</p>
<p><span class="math display">\[
\mathbf{u}_{\text{I}} = \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix}, \quad
\mathbf{u}_{\text{love}} = \begin{bmatrix} 0.3 \\ -0.1 \end{bmatrix}, \quad
\mathbf{u}_{\text{cats}} = \begin{bmatrix} -0.2 \\ 0.4 \end{bmatrix}, \quad
\mathbf{u}_{\text{dogs}} = \begin{bmatrix} 0.0 \\ 0.3 \end{bmatrix}
\]</span></p>
<p><strong>训练样本</strong>：句子”I love cats”，窗口大小=1</p>
<ul>
<li>中心词：love</li>
<li>正样本上下文词：cats（右边一个词）</li>
<li>负采样词：dogs（随机采样）</li>
</ul>
<p><strong>Step 1：计算点积</strong></p>
<p>正样本：<span class="math inline">\(\mathbf{u}_{\text{cats}}^\top \mathbf{v}_{\text{love}}\)</span></p>
<p><span class="math display">\[
= \begin{bmatrix} -0.2 &amp; 0.4 \end{bmatrix} \begin{bmatrix} 0.3 \\ -0.1 \end{bmatrix}
= (-0.2)(0.3) + (0.4)(-0.1) = -0.06 - 0.04 = -0.10
\]</span></p>
<p>负样本：<span class="math inline">\(\mathbf{u}_{\text{dogs}}^\top \mathbf{v}_{\text{love}}\)</span></p>
<p><span class="math display">\[
= \begin{bmatrix} 0.0 &amp; 0.3 \end{bmatrix} \begin{bmatrix} 0.3 \\ -0.1 \end{bmatrix}
= (0.0)(0.3) + (0.3)(-0.1) = 0 - 0.03 = -0.03
\]</span></p>
<p><strong>Step 2：计算Sigmoid</strong></p>
<p><span class="math display">\[
\sigma(-0.10) = \frac{1}{1 + e^{0.10}} \approx \frac{1}{1.105} \approx 0.475
\]</span></p>
<p><span class="math display">\[
\sigma(-0.03) = \frac{1}{1 + e^{0.03}} \approx \frac{1}{1.030} \approx 0.493
\]</span></p>
<p>对于负样本，我们需要<span class="math inline">\(\sigma(-\mathbf{u}_{\text{dogs}}^\top \mathbf{v}_{\text{love}}) = \sigma(0.03) \approx 0.507\)</span></p>
<p><strong>Step 3：计算梯度</strong></p>
<p>对于负采样目标函数：</p>
<p><span class="math display">\[
\mathcal{L} = \log \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}) + \log \sigma(-\mathbf{u}_{w_{\text{neg}}}^\top \mathbf{v}_{w_c})
\]</span></p>
<p>梯度计算：</p>
<ul>
<li><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}}\)</span>：需要拉近cats，推远dogs</li>
<li><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{u}_{\text{cats}}}\)</span>：需要拉近love</li>
<li><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{u}_{\text{dogs}}}\)</span>：需要推远love</li>
</ul>
<p>对于正样本，梯度为<span class="math inline">\((1 - \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})) \cdot \mathbf{u}_{w_o}\)</span>：</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}} \big|_{\text{positive}} = (1 - 0.475) \cdot \begin{bmatrix} -0.2 \\ 0.4 \end{bmatrix} = 0.525 \cdot \begin{bmatrix} -0.2 \\ 0.4 \end{bmatrix} = \begin{bmatrix} -0.105 \\ 0.210 \end{bmatrix}
\]</span></p>
<p>对于负样本，梯度为<span class="math inline">\(-\sigma(\mathbf{u}_{w_{\text{neg}}}^\top \mathbf{v}_{w_c}) \cdot \mathbf{u}_{w_{\text{neg}}}\)</span>：</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}} \big|_{\text{negative}} = -0.493 \cdot \begin{bmatrix} 0.0 \\ 0.3 \end{bmatrix} = \begin{bmatrix} 0.0 \\ -0.148 \end{bmatrix}
\]</span></p>
<p>总梯度：</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}} = \begin{bmatrix} -0.105 \\ 0.210 \end{bmatrix} + \begin{bmatrix} 0.0 \\ -0.148 \end{bmatrix} = \begin{bmatrix} -0.105 \\ 0.062 \end{bmatrix}
\]</span></p>
<p><strong>Step 4：更新词向量</strong></p>
<p><span class="math display">\[
\mathbf{v}_{\text{love}}^{\text{new}} = \mathbf{v}_{\text{love}} + \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}}
= \begin{bmatrix} 0.3 \\ -0.1 \end{bmatrix} + 0.1 \cdot \begin{bmatrix} -0.105 \\ 0.062 \end{bmatrix}
= \begin{bmatrix} 0.290 \\ -0.094 \end{bmatrix}
\]</span></p>
<p><strong>观察</strong>：</p>
<ul>
<li>“love”的向量第一维减小了（从0.3到0.29），这会使它与”cats”的点积增大（因为cats第一维是负的）</li>
<li>“love”的向量第二维增大了（从-0.1到-0.094），这也会使它与”cats”的点积增大（因为cats第二维是正的）</li>
<li>同时，这些变化会使”love”与”dogs”的点积变化不大或减小</li>
</ul>
<p>这就是Word2Vec学习的本质：通过大量这样的小更新，逐渐调整词向量，让共现词的向量彼此接近。</p>
</section>
<section id="词向量类比的神奇现象" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="词向量类比的神奇现象"><span class="header-section-number">4.7</span> 词向量类比的神奇现象</h3>
<p>Word2Vec最令人惊叹的发现是<strong>词向量类比</strong>（word analogy）。训练好的词向量呈现出惊人的规律性：</p>
<p><span class="math display">\[
\mathbf{v}_{\text{king}} - \mathbf{v}_{\text{man}} + \mathbf{v}_{\text{woman}} \approx \mathbf{v}_{\text{queen}}
\]</span></p>
<p>这意味着”国王-男人+女人≈女王”。向量空间中存在一个”性别方向”，沿着这个方向移动可以改变词的性别属性，同时保持其他语义属性（如”皇室成员”）不变。</p>
<p>类似的关系还有很多：</p>
<ul>
<li><span class="math inline">\(\mathbf{v}_{\text{Paris}} - \mathbf{v}_{\text{France}} + \mathbf{v}_{\text{Italy}} \approx \mathbf{v}_{\text{Rome}}\)</span>（首都关系）</li>
<li><span class="math inline">\(\mathbf{v}_{\text{walking}} - \mathbf{v}_{\text{walk}} + \mathbf{v}_{\text{swim}} \approx \mathbf{v}_{\text{swimming}}\)</span>（时态变化）</li>
</ul>
<p>这个发现说明，词向量空间不仅捕获了词的相似性，还学习到了词之间的语义关系。这是传统特征工程无法实现的。</p>
<p><strong>为什么会出现这种现象？</strong></p>
<p>一个直觉解释是：语言中的规律性反映在共现模式中。“king”和”queen”出现在相似的上下文中（都与皇室、统治相关），但”king”更常与”he”、“his”共现，“queen”更常与”she”、“her”共现。类似地，“man”和”woman”也有这种共现模式的差异。通过大量语料的训练，这些规律性被编码进了向量空间的结构中。</p>
<p>更正式的理论分析将在后面的”深入理解”部分讨论。</p>
<hr>
</section>
</section>
<section id="glove与fasttext词向量的改进" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="glove与fasttext词向量的改进"><span class="header-section-number">5</span> GloVe与FastText：词向量的改进</h2>
<section id="glove全局统计的视角" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="glove全局统计的视角"><span class="header-section-number">5.1</span> GloVe：全局统计的视角</h3>
<p>Word2Vec是一个<strong>预测模型</strong>：它通过预测上下文来学习词向量。2014年，Stanford的Pennington等人提出了GloVe（Global Vectors），从一个不同的角度来看这个问题。</p>
<p>GloVe的核心洞察是：词向量应该直接编码共现统计信息。</p>
<p>设<span class="math inline">\(X_{ij}\)</span>是词<span class="math inline">\(i\)</span>和词<span class="math inline">\(j\)</span>在窗口内共同出现的次数，定义<span class="math inline">\(P_{ij} = P(j|i) = X_{ij} / X_i\)</span>为词<span class="math inline">\(j\)</span>出现在词<span class="math inline">\(i\)</span>上下文中的概率。GloVe关注的是<strong>概率比值</strong>：</p>
<p><span class="math display">\[
\frac{P_{ik}}{P_{jk}} = \frac{P(k|i)}{P(k|j)}
\]</span></p>
<p>这个比值揭示了词之间的关系。考虑词”ice”和”steam”，以及探测词”solid”和”gas”：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(P(\cdot|\text{ice})\)</span></th>
<th><span class="math inline">\(P(\cdot|\text{steam})\)</span></th>
<th>比值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>solid</td>
<td>高（冰是固体）</td>
<td>低</td>
<td>大</td>
</tr>
<tr class="even">
<td>gas</td>
<td>低</td>
<td>高（蒸汽是气体）</td>
<td>小</td>
</tr>
<tr class="odd">
<td>water</td>
<td>高</td>
<td>高</td>
<td>≈1</td>
</tr>
<tr class="even">
<td>fashion</td>
<td>低</td>
<td>低</td>
<td>≈1</td>
</tr>
</tbody>
</table>
<p>GloVe的目标是让词向量能够编码这种比值关系：</p>
<p><span class="math display">\[
\mathbf{w}_i^\top \mathbf{w}_j + b_i + b_j = \log X_{ij}
\]</span></p>
<p>最终的损失函数是：</p>
<p><span class="math display">\[
\mathcal{L} = \sum_{i,j=1}^{|\mathcal{V}|} f(X_{ij}) \left( \mathbf{w}_i^\top \mathbf{w}_j + b_i + b_j - \log X_{ij} \right)^2
\]</span></p>
<p>其中<span class="math inline">\(f(x)\)</span>是权重函数，用于降低高频共现对的影响（否则”the”、“a”等词会主导训练）。</p>
<p><strong>GloVe vs Word2Vec</strong>：</p>
<p>GloVe是<strong>基于统计</strong>的：它直接利用全局共现矩阵，一步到位地优化。Word2Vec是<strong>基于预测</strong>的：它通过局部的滑动窗口，增量式地学习。</p>
<p>实践中，两者效果相当。GloVe的优势是利用了全局统计信息，可能在某些任务上更稳定。Word2Vec的优势是可以流式训练，不需要先构建完整的共现矩阵。</p>
</section>
<section id="fasttext子词的力量" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="fasttext子词的力量"><span class="header-section-number">5.2</span> FastText：子词的力量</h3>
<p>Word2Vec和GloVe都把词当作原子单位。但语言中有很多词共享相同的词根或词缀，比如”teach”、“teacher”、“teaching”、“teaches”。能不能利用这种结构？</p>
<p>2016年，Facebook的Bojanowski等人提出了FastText，核心思想是：把词表示为<strong>子词（subword）的组合</strong>。</p>
<p>具体来说，FastText把每个词分解成字符n-gram。比如，“where”（加上边界符号&lt;和&gt;）分解为：</p>
<ul>
<li>3-gram: &lt;wh, whe, her, ere, re&gt;</li>
<li>4-gram: &lt;whe, wher, here, ere&gt;</li>
<li>5-gram: &lt;wher, where, here&gt;</li>
</ul>
<p>词”where”的向量是所有这些n-gram向量的和，再加上词本身的向量（如果存在的话）：</p>
<p><span class="math display">\[
\mathbf{v}_{\text{where}} = \mathbf{z}_{\text{where}} + \sum_{g \in G(\text{where})} \mathbf{z}_g
\]</span></p>
<p><strong>FastText的优势</strong>：</p>
<p>首先是<strong>处理OOV（Out-of-Vocabulary）词</strong>。传统词向量无法处理训练时没见过的词。但FastText可以：即使”unfriendliness”没出现过，只要它的子词（如”un”、“friend”、“ness”）出现过，就能组合出一个合理的向量。</p>
<p>其次是<strong>形态丰富的语言</strong>。对于德语、土耳其语等形态变化丰富的语言，同一个词根可能有几十种变形。FastText通过共享子词表示，可以更有效地学习这些语言。</p>
<p>第三是<strong>处理拼写错误</strong>。“freind”和”friend”共享很多子词，因此它们的向量会很相似，模型对拼写错误更鲁棒。</p>
<p><strong>FastText vs Word2Vec</strong>：</p>
<p>FastText的代价是参数量增加（需要存储所有n-gram的向量），训练也更慢。在英语等形态变化较少的语言上，FastText相对Word2Vec的优势不太明显。但在形态丰富的语言上，FastText通常显著更好。</p>
<p>下图展示了 FastText 如何通过子词捕获词之间的相似性。热力图显示了不同 n-gram 之间的相似度：共享更多子词的词对（如 “rarity” 和 “scarceness”）在向量空间中更接近。</p>
<div id="fig-fasttext-similarity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fasttext-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-3/original/fig4-fasttext-similarity-rarity-scarceness.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fasttext-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: FastText 子词相似度热力图：展示了 “rarity” 和 “scarceness” 两个词的各个 n-gram 之间的相似度。共享相似子词结构的词会有更高的相似度。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Bojanowski et al.&nbsp;(2017) “Enriching Word Vectors with Subword Information”, Figure 1</em></p>
</div>
<hr>
</section>
</section>
<section id="工程实践词向量训练与可视化" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="工程实践词向量训练与可视化"><span class="header-section-number">6</span> 工程实践：词向量训练与可视化</h2>
<section id="使用gensim训练word2vec" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="使用gensim训练word2vec"><span class="header-section-number">6.1</span> 使用Gensim训练Word2Vec</h3>
<div id="16dffdcf" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.word2vec <span class="im">import</span> LineSentence</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置日志，观察训练进度</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> : </span><span class="sc">%(levelname)s</span><span class="st"> : </span><span class="sc">%(message)s</span><span class="st">'</span>, level<span class="op">=</span>logging.INFO)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备训练数据：每行一个句子，词用空格分隔</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 这里用一个简单的示例，实际应该用大规模语料</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'I'</span>, <span class="st">'love'</span>, <span class="st">'machine'</span>, <span class="st">'learning'</span>],</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'deep'</span>, <span class="st">'learning'</span>, <span class="st">'is'</span>, <span class="st">'a'</span>, <span class="st">'subset'</span>, <span class="st">'of'</span>, <span class="st">'machine'</span>, <span class="st">'learning'</span>],</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'natural'</span>, <span class="st">'language'</span>, <span class="st">'processing'</span>, <span class="st">'uses'</span>, <span class="st">'deep'</span>, <span class="st">'learning'</span>],</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'word'</span>, <span class="st">'vectors'</span>, <span class="st">'are'</span>, <span class="st">'learned'</span>, <span class="st">'representations'</span>],</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'word2vec'</span>, <span class="st">'learns'</span>, <span class="st">'word'</span>, <span class="st">'vectors'</span>, <span class="st">'from'</span>, <span class="st">'text'</span>],</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'cats'</span>, <span class="st">'and'</span>, <span class="st">'dogs'</span>, <span class="st">'are'</span>, <span class="st">'pets'</span>],</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'cats'</span>, <span class="st">'chase'</span>, <span class="st">'mice'</span>],</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'dogs'</span>, <span class="st">'chase'</span>, <span class="st">'cats'</span>, <span class="st">'sometimes'</span>],</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练Word2Vec模型</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    sentences,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">50</span>,      <span class="co"># 词向量维度</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">3</span>,            <span class="co"># 上下文窗口大小</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">1</span>,         <span class="co"># 忽略频率低于此值的词</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    workers<span class="op">=</span><span class="dv">4</span>,           <span class="co"># 训练线程数</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">1</span>,                <span class="co"># 1=Skip-gram, 0=CBOW</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    negative<span class="op">=</span><span class="dv">5</span>,          <span class="co"># 负采样数量</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>           <span class="co"># 训练轮数</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看词向量</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"'learning'的词向量前5维:"</span>, model.wv[<span class="st">'learning'</span>][:<span class="dv">5</span>])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 找相似词</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">与'learning'最相似的词:"</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> model.wv.most_similar(<span class="st">'learning'</span>, topn<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 词向量运算</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'cats'</span> <span class="kw">in</span> model.wv <span class="kw">and</span> <span class="st">'dogs'</span> <span class="kw">in</span> model.wv:</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    similarity <span class="op">=</span> model.wv.similarity(<span class="st">'cats'</span>, <span class="st">'dogs'</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">'cats'和'dogs'的相似度: </span><span class="sc">{</span>similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存和加载模型</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">"word2vec.model"</span>)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="co"># loaded_model = Word2Vec.load("word2vec.model")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="使用预训练词向量" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="使用预训练词向量"><span class="header-section-number">6.2</span> 使用预训练词向量</h3>
<p>实际应用中，通常使用在大规模语料上预训练好的词向量，而不是自己从头训练。</p>
<div id="6db3ed52" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 下载预训练的Word2Vec模型（Google News, 300维, 300万词）</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：这个模型约1.6GB，首次下载需要时间</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model = api.load("word2vec-google-news-300")</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 或者使用更小的GloVe模型</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># model = api.load("glove-wiki-gigaword-100")</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 演示用一个小模型</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"glove-twitter-25"</span>)  <span class="co"># 25维，较小</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 词向量运算示例</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"king - man + woman = ?"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> result:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Paris - France + Italy = ?"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'paris'</span>, <span class="st">'italy'</span>], negative<span class="op">=</span>[<span class="st">'france'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> result:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 找不相似的词</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">哪个词不属于这一组？['breakfast', 'lunch', 'dinner', 'computer']"</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>odd_one <span class="op">=</span> model.doesnt_match([<span class="st">'breakfast'</span>, <span class="st">'lunch'</span>, <span class="st">'dinner'</span>, <span class="st">'computer'</span>])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  答案: </span><span class="sc">{</span>odd_one<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="词向量可视化" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="词向量可视化"><span class="header-section-number">6.3</span> 词向量可视化</h3>
<p>高维词向量无法直接可视化，需要用降维技术投影到2D或3D。</p>
<div id="23b7118b" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练模型</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"glove-twitter-25"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 选择一些词进行可视化</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 动物</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cat'</span>, <span class="st">'dog'</span>, <span class="st">'bird'</span>, <span class="st">'fish'</span>, <span class="st">'horse'</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 国家</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'china'</span>, <span class="st">'japan'</span>, <span class="st">'france'</span>, <span class="st">'germany'</span>, <span class="st">'italy'</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 颜色</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'black'</span>,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 数字</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'one'</span>, <span class="st">'two'</span>, <span class="st">'three'</span>, <span class="st">'four'</span>, <span class="st">'five'</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 获取词向量</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> np.array([model[w] <span class="cf">for</span> w <span class="kw">in</span> words <span class="cf">if</span> w <span class="kw">in</span> model])</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> words <span class="cf">if</span> w <span class="kw">in</span> model]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE降维</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>vectors_2d <span class="op">=</span> tsne.fit_transform(word_vectors)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.scatter(vectors_2d[:, <span class="dv">0</span>], vectors_2d[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    plt.annotate(word, xy<span class="op">=</span>(vectors_2d[i, <span class="dv">0</span>], vectors_2d[i, <span class="dv">1</span>]),</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">12</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Word Vectors Visualization (t-SNE)"</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Dimension 1"</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Dimension 2"</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"word_vectors_tsne.png"</span>, dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="评估词向量质量" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="评估词向量质量"><span class="header-section-number">6.4</span> 评估词向量质量</h3>
<p>词向量的质量可以通过多种基准测试评估。</p>
<div id="940e55aa" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"glove-wiki-gigaword-100"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 词类比任务（Word Analogy）</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 格式：A:B :: C:D，测试 A-B+C=D</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>analogies <span class="op">=</span> [</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'king'</span>, <span class="st">'man'</span>, <span class="st">'woman'</span>, <span class="st">'queen'</span>),</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'paris'</span>, <span class="st">'france'</span>, <span class="st">'berlin'</span>, <span class="st">'germany'</span>),</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'good'</span>, <span class="st">'better'</span>, <span class="st">'bad'</span>, <span class="st">'worse'</span>),</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"词类比测试:"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b, c, expected <span class="kw">in</span> analogies:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[a, c], negative<span class="op">=</span>[b], topn<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">=</span> <span class="st">"✓"</span> <span class="cf">if</span> result <span class="op">==</span> expected <span class="cf">else</span> <span class="st">"✗"</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss"> :: </span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">:? → </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss"> (期望: </span><span class="sc">{</span>expected<span class="sc">}</span><span class="ss">) </span><span class="sc">{</span>correct<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyError</span> <span class="im">as</span> e:</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  词不在词汇表中: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 词相似度任务（Word Similarity）</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 与人类判断的相关性</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>word_pairs <span class="op">=</span> [</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'car'</span>, <span class="st">'automobile'</span>, <span class="st">'high'</span>),</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'car'</span>, <span class="st">'bicycle'</span>, <span class="st">'medium'</span>),</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'car'</span>, <span class="st">'democracy'</span>, <span class="st">'low'</span>),</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">词相似度测试:"</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w1, w2, expected_level <span class="kw">in</span> word_pairs:</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        sim <span class="op">=</span> model.similarity(w1, w2)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>w1<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>w2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>sim<span class="sc">:.4f}</span><span class="ss"> (预期: </span><span class="sc">{</span>expected_level<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyError</span> <span class="im">as</span> e:</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  词不在词汇表中: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 聚类质量</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 语义相似的词应该聚在一起</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>words_by_category <span class="op">=</span> {</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">'animals'</span>: [<span class="st">'cat'</span>, <span class="st">'dog'</span>, <span class="st">'bird'</span>, <span class="st">'fish'</span>, <span class="st">'horse'</span>],</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">'countries'</span>: [<span class="st">'china'</span>, <span class="st">'japan'</span>, <span class="st">'france'</span>, <span class="st">'germany'</span>, <span class="st">'italy'</span>],</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">'colors'</span>: [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'black'</span>],</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>all_words <span class="op">=</span> []</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (category, words) <span class="kw">in</span> <span class="bu">enumerate</span>(words_by_category.items()):</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> model:</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>            all_words.append(word)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>            true_labels.append(i)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> np.array([model[w] <span class="cf">for</span> w <span class="kw">in</span> all_words])</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a><span class="co"># K-means聚类</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> kmeans.fit_predict(vectors)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算聚类纯度</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>ari <span class="op">=</span> adjusted_rand_score(true_labels, pred_labels)</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">聚类质量 (Adjusted Rand Index): </span><span class="sc">{</span>ari<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">7</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨词向量的理论基础、边界条件和开放问题</p>
</blockquote>
<section id="为什么有效理论视角" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">7.1</span> 为什么有效？——理论视角</h3>
<p>词向量的成功有深刻的理论基础。</p>
<p><strong>分布式假设的数学化</strong>。Levy和Goldberg（2014）的重要工作表明，Word2Vec的Skip-gram负采样目标实际上等价于对点互信息（PMI）矩阵的隐式分解：</p>
<p><span class="math display">\[
\mathbf{w}_i \cdot \mathbf{c}_j = \text{PMI}(i, j) - \log k
\]</span></p>
<p>其中PMI定义为：</p>
<p><span class="math display">\[
\text{PMI}(i, j) = \log \frac{P(i, j)}{P(i)P(j)} = \log \frac{\#(i, j) \cdot |D|}{\#(i) \cdot \#(j)}
\]</span></p>
<p>这意味着Word2Vec本质上是在学习词对的PMI——一种统计相关性度量。PMI高意味着两个词比随机共现更频繁地一起出现，这正是语义相关的信号。</p>
<p><strong>与矩阵分解的联系</strong>。进一步，可以证明GloVe也在做类似的事情：对共现统计的某种变换进行低秩分解。这解释了为什么两种表面上不同的方法能得到相似的结果——它们都在捕获相同的统计结构。</p>
<p><strong>线性结构的来源</strong>。词向量类比（king - man + woman ≈ queen）的线性结构可以从PMI的性质推导。如果”king”和”queen”有相似的上下文，但”king”更常与”he”共现、“queen”更常与”she”共现，那么：</p>
<p><span class="math display">\[
\text{PMI}(\text{king}, \text{he}) - \text{PMI}(\text{queen}, \text{he}) \approx \text{PMI}(\text{man}, \text{he}) - \text{PMI}(\text{woman}, \text{he})
\]</span></p>
<p>这种对称性导致了向量空间中的平行结构。</p>
</section>
<section id="为什么有效实证视角" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="为什么有效实证视角"><span class="header-section-number">7.2</span> 为什么有效？——实证视角</h3>
<p>大量实验揭示了词向量有效的条件。</p>
<p><strong>数据量的影响</strong>。更多的训练数据通常带来更好的词向量。Mikolov等人报告，在Google News语料（1000亿词）上训练的向量显著优于在较小语料上训练的向量。这并不意外——更多数据意味着更准确的共现统计估计。</p>
<p><strong>维度的影响</strong>。词向量维度通常在100-300之间效果最好。太低维度无法捕获足够的信息，太高维度可能过拟合或引入噪声。300维似乎是一个”甜点”，在多数任务上表现良好。</p>
<p><strong>窗口大小的影响</strong>。较小的窗口（2-5）倾向于捕获语法关系（如词性），较大的窗口（5-10）倾向于捕获语义关系（如主题）。选择取决于下游任务。</p>
<p><strong>负采样数量的影响</strong>。对于小数据集，较多的负样本（15-20）可能有帮助。对于大数据集，较少的负样本（5-10）通常就够了。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">7.3</span> 方法的边界条件</h3>
<p>词向量有几个重要的隐含假设和局限性。</p>
<p><strong>静态表示假设</strong>。每个词只有一个向量，不论它出现在什么上下文中。这意味着一词多义无法处理。“bank”（银行）和”bank”（河岸）有相同的向量，这显然不合理。这个问题在第11章（ELMo）和第13章（BERT）才得到解决。</p>
<p><strong>词袋假设</strong>。词向量训练通常忽略词序，把上下文当作词袋处理。这丢失了语法信息，比如”dog bites man”和”man bites dog”的区分。</p>
<p><strong>频率偏差</strong>。高频词（如”the”、“is”）可能主导训练，导致它们的向量质量过高，而低频词（真正有语义内容的词）质量不足。虽然负采样分布的设计试图缓解这个问题，但偏差仍然存在。</p>
<p><strong>文化和历史偏见</strong>。词向量会学习语料库中的偏见。如果语料库中”doctor”更常与”he”共现、“nurse”更常与”she”共现，词向量就会编码这种性别偏见。这在某些应用中可能是问题。</p>
</section>
<section id="变体与扩展" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="变体与扩展"><span class="header-section-number">7.4</span> 变体与扩展</h3>
<p>词向量的成功催生了大量后续工作。</p>
<p><strong>句子向量</strong>。如何表示整个句子？简单的方法是对词向量取平均，但这丢失了词序信息。Le和Mikolov（2014）提出了Doc2Vec，用类似Word2Vec的方法学习文档向量。</p>
<p><strong>跨语言词向量</strong>。能否让不同语言的词在同一个向量空间中？如果能，机器翻译就变成了简单的最近邻查找。研究表明，通过少量双语词典或平行语料，可以学习跨语言的词向量对齐。</p>
<p><strong>知识增强</strong>。纯粹从文本学习可能错过一些常识知识。研究者尝试将知识图谱信息融入词向量，比如让”巴黎”和”法国”的向量编码”首都”关系。</p>
</section>
<section id="开放研究问题" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">7.5</span> 开放研究问题</h3>
<p>如果你要在词向量方向做研究，可以考虑这些问题。</p>
<p>第一，<strong>最优的训练目标是什么？</strong> Skip-gram、CBOW、GloVe、FastText…都有各自的优缺点。是否存在一个理论最优的目标函数？目前还没有定论。</p>
<p>第二，<strong>如何处理一词多义？</strong> 静态词向量的根本局限是无法区分同一个词的不同含义。在ELMo和BERT之前，有一些工作尝试为每个词学习多个向量（如Sense2Vec），但效果有限。</p>
<p>第三，<strong>词向量编码了什么信息？</strong> 虽然我们知道词向量能完成类比任务，但它们到底学到了什么语言知识？是语法？语义？世界知识？这仍然是一个活跃的研究问题。</p>
<p>第四，<strong>如何减少偏见？</strong> 词向量中的性别、种族偏见如何检测和消除？简单的”去偏”方法（如减去性别方向）可能只是表面上消除偏见，深层的偏见仍然存在。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">8</span> 局限性与未解决的问题</h2>
<section id="静态表示的根本缺陷" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="静态表示的根本缺陷"><span class="header-section-number">8.1</span> 静态表示的根本缺陷</h3>
<p>词向量最大的局限是<strong>静态性</strong>：每个词只有一个固定的向量表示，不论它出现在什么上下文中。</p>
<p>考虑”bank”这个词：</p>
<ul>
<li>“I went to the <strong>bank</strong> to deposit money.”（银行）</li>
<li>“We sat on the <strong>bank</strong> of the river.”（河岸）</li>
</ul>
<p>这两个”bank”显然有不同的含义，但在Word2Vec中它们只有一个向量。这个向量是所有含义的某种”平均”，对于每个具体用法都不够精确。</p>
<p>问题在高度多义词上尤其严重。英语中很多常用词都有多个含义（“run”有超过100个义项），一个静态向量无法捕获这种复杂性。</p>
</section>
<section id="上下文信息的缺失" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="上下文信息的缺失"><span class="header-section-number">8.2</span> 上下文信息的缺失</h3>
<p>词向量训练使用的是固定窗口内的局部上下文，而忽略了更广泛的语篇信息。</p>
<p>考虑这个例子：</p>
<ul>
<li>“The trophy would not fit in the suitcase because <strong>it</strong> was too big.”</li>
<li>“The trophy would not fit in the suitcase because <strong>it</strong> was too small.”</li>
</ul>
<p>第一句中”it”指代trophy，第二句中”it”指代suitcase。理解这种指代需要跨越整个句子的推理，而不仅仅是局部窗口内的共现。</p>
<p>词向量的训练目标（预测局部上下文）天然无法捕获这种长距离依赖。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">8.3</span> 这些局限导向了什么？</h3>
<p>静态词向量的成功和局限同时存在，这引出了一个自然的问题：能不能让词的表示<strong>动态地依赖于上下文</strong>？</p>
<p>这正是接下来几章要讨论的问题。第11章的ELMo将展示如何用双向LSTM生成上下文相关的词表示，第13章的BERT将用Transformer的双向注意力实现更强大的上下文编码。这些方法被称为”上下文化词向量”（Contextualized Word Embeddings），它们建立在Word2Vec的基础上，但解决了静态表示的根本缺陷。</p>
<blockquote class="blockquote">
<p>下一章预告：第4章将讨论一个常被忽视但极其重要的问题——Tokenization。词向量假设我们已经有了”词”，但什么是一个”词”？中文要不要分词？形态丰富的语言如何处理？这些问题比看起来复杂得多，而Tokenizer的设计直接影响模型的能力。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">9</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">9.1</span> 核心要点回顾</h3>
<p>这一章我们见证了NLP历史上的一次重要突破：从手工设计特征到自动学习表示。</p>
<p>Word2Vec的核心洞察极其简洁：<strong>一个词的含义由它的上下文定义</strong>。通过训练模型预测词的上下文（Skip-gram）或从上下文预测词（CBOW），模型自动学习到了词的分布式表示。这些表示捕获了语义相似性——“cat”和”dog”的向量彼此接近，因为它们出现在相似的上下文中。</p>
<p>更神奇的是，词向量空间呈现出规律的线性结构：king - man + woman ≈ queen。这说明向量空间不仅编码了相似性，还编码了语义关系。</p>
<p>GloVe从全局统计的角度重新推导了词向量，FastText通过子词分解解决了OOV问题。这些改进丰富了词向量的生态。</p>
<p>然而，所有这些方法都面临一个根本局限：<strong>静态表示无法处理一词多义</strong>。每个词只有一个向量，无论它出现在什么上下文中。这个问题的解决要等到ELMo和BERT的出现。</p>
</section>
<section id="关键公式速查" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">9.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P(w_o \| w_c) = \frac{\exp(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})}{\sum_w \exp(\mathbf{u}_w^\top \mathbf{v}_{w_c})}\)</span></td>
<td>Skip-gram的Softmax概率</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\log \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}) + \sum_{i=1}^{k} \log \sigma(-\mathbf{u}_{w_i}^\top \mathbf{v}_{w_c})\)</span></td>
<td>负采样目标函数</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P_n(w) \propto f(w)^{3/4}\)</span></td>
<td>负采样分布</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{w}_i^\top \mathbf{w}_j + b_i + b_j = \log X_{ij}\)</span></td>
<td>GloVe目标</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">9.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么Skip-gram在处理罕见词时比CBOW更有效？从训练信号的角度分析。</p></li>
<li><p><strong>[数学推导]</strong> 推导负采样目标函数的梯度。给定正样本<span class="math inline">\((w_c, w_o)\)</span>和负样本<span class="math inline">\(w_1, \ldots, w_k\)</span>，写出<span class="math inline">\(\partial \mathcal{L} / \partial \mathbf{v}_{w_c}\)</span>的表达式。</p></li>
<li><p><strong>[工程实践]</strong> 使用Gensim在一个中等规模的语料库（如Wikipedia dump的一部分）上训练Word2Vec模型。比较不同超参数（维度、窗口大小、负采样数）对词类比任务准确率的影响。</p></li>
<li><p><strong>[研究思考]</strong> 词向量捕获了语料库中的偏见（如性别偏见）。你认为应该如何处理这个问题？完全去除偏见是否可行或可取？这个问题有什么更深层的哲学含义？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">10</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">10.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013)</strong>：Word2Vec的原始论文
<ul>
<li>重点读：Section 4（Skip-gram和CBOW的定义）</li>
<li>可跳过：实验的细节</li>
</ul></li>
<li><strong>Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013)</strong>：引入负采样的论文
<ul>
<li>重点读：Section 2.2（负采样）、Section 4（短语向量）</li>
<li>这篇论文比第一篇更重要，因为负采样成为了标准做法</li>
</ul></li>
</ul>
</section>
<section id="理论分析" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="理论分析"><span class="header-section-number">10.2</span> 理论分析</h3>
<ul>
<li><strong>Neural Word Embedding as Implicit Matrix Factorization (Levy &amp; Goldberg, 2014)</strong>：证明Word2Vec等价于PMI矩阵分解
<ul>
<li>这篇论文揭示了Word2Vec成功的数学原因</li>
<li>对理解为什么不同方法效果相近很有帮助</li>
</ul></li>
</ul>
</section>
<section id="改进方法" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="改进方法"><span class="header-section-number">10.3</span> 改进方法</h3>
<ul>
<li><strong>GloVe: Global Vectors for Word Representation (Pennington et al., 2014)</strong>：GloVe的原始论文
<ul>
<li>从不同角度推导词向量，与Word2Vec殊途同归</li>
</ul></li>
<li><strong>Enriching Word Vectors with Subword Information (Bojanowski et al., 2017)</strong>：FastText
<ul>
<li>解决OOV问题的优雅方案</li>
</ul></li>
</ul>
</section>
<section id="综述与教程" class="level3" data-number="10.4">
<h3 data-number="10.4" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">10.4</span> 综述与教程</h3>
<ul>
<li><strong>Word2Vec Tutorial (Chris McCormick)</strong>：最通俗的Word2Vec解释
<ul>
<li>适合入门，有很好的可视化</li>
</ul></li>
<li><strong>Speech and Language Processing, Chapter 6 (Jurafsky &amp; Martin)</strong>：词向量的教科书级介绍</li>
</ul>
</section>
<section id="代码资源" class="level3" data-number="10.5">
<h3 data-number="10.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">10.5</span> 代码资源</h3>
<ul>
<li><strong>Gensim库</strong>：Word2Vec/FastText的标准Python实现</li>
<li><strong>Stanford的GloVe官方代码</strong>：C实现，效率很高</li>
<li><strong>Hugging Face的预训练模型</strong>：可以直接加载各种预训练词向量</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">11</span> 历史注脚</h2>
<p>Word2Vec的成功有一个有趣的历史背景。Tomas Mikolov在发表Word2Vec之前，已经在循环神经网络语言模型上做了多年工作。他发现RNN语言模型的隐藏层可以作为词的表示，但训练太慢、无法扩展。Word2Vec可以看作是把这个思想极简化——用更简单的模型（没有非线性隐藏层），换取在大规模数据上训练的能力。</p>
<p>另一个有趣的细节是，Word2Vec论文被ICLR 2013拒稿了。审稿人认为模型太简单、理论贡献不足。后来Mikolov把论文放到arXiv上，它迅速成为NLP领域引用最多的论文之一。这个故事说明，有时候最有影响力的工作不是最”复杂”的，而是找到了正确的简化。</p>
<p>词向量类比的发现也颇为戏剧性。据说Mikolov在调试代码时偶然发现 king - man + woman 会得到接近queen的向量，起初以为是bug。后来才意识到这是一个深刻的发现——向量空间自动学到了语义关系的线性结构。这个发现极大地推动了对词向量的兴趣，因为它暗示着向量空间中可能编码了我们尚未完全理解的语言知识。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第3章：表示学习的觉醒"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "从离散符号到分布式语义"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-25"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Word2Vec, GloVe, FastText, Representation Learning]</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [词向量, Skip-gram, CBOW, 分布式语义, 负采样, 表示学习]</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "从离散符号到分布式语义：Word2Vec、GloVe、FastText如何让机器理解词义，以及静态词向量的根本局限。"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-3/fig3-cbow-vs-skipgram.png"</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何让计算机理解词的"含义"？如何用数学表示"猫"和"狗"比"猫"和"民主"更相似？</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2013 </span><span class="pp">|</span><span class="at"> Mikolov et al. (Word2Vec) </span><span class="pp">|</span><span class="at"> 从特征工程到表示学习的范式转变</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>在第1章中，我们回顾了前深度学习时代的NLP——那是一个特征工程的时代，研究者需要手工设计特征，把原始文本转换成机器学习模型可以处理的数值向量。第2章则鸟瞰了NLP的核心任务全景，从文本分类到机器翻译、从序列标注到问答系统。我们看到了传统方法的成功，但也看到了它们共同面临的瓶颈——特征工程不可扩展，每个新任务都需要重新设计特征，专家的时间成为了整个领域的天花板。</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>更根本的问题是：传统方法把词当作离散的符号来处理。在one-hot编码中，"cat"是一个维度上的1，"dog"是另一个维度上的1，它们之间没有任何数学关系。模型无法知道"cat"和"dog"都是动物、都是宠物、都有四条腿。这种离散表示无法捕获语义相似性，导致泛化困难——一个在"猫追老鼠"上训练的模型，无法自动迁移到"狗追松鼠"。</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>这就引出了一个核心问题：能不能让词的表示本身就蕴含语义信息？让语义相似的词，在数学上也彼此接近？</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>2013年，Google的研究员Tomas Mikolov给出了一个惊人简洁的答案。</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：词的含义可以从它的"邻居"中学习——"You shall know a word by the company it keeps"。通过预测上下文（或被上下文预测），模型可以自动学习词的分布式表示，让语义相似的词在向量空间中彼此接近。</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### 表示问题的精确定义</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>让我们先精确地定义问题。我们希望找到一个映射函数：</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>f: \mathcal{V} \rightarrow \mathbb{R}^d</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>把词汇表$\mathcal{V}$中的每个词映射到一个$d$维的实数向量。这个映射应该满足一个关键性质：**语义相似的词，映射后的向量也相似**。</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>用数学语言说，如果$w_1$和$w_2$语义相似，那么$f(w_1)$和$f(w_2)$的余弦相似度应该高：</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>\text{similarity}(w_1, w_2) \propto \cos(f(w_1), f(w_2)) = \frac{f(w_1) \cdot f(w_2)}{<span class="sc">\|</span>f(w_1)<span class="sc">\|</span> <span class="sc">\|</span>f(w_2)<span class="sc">\|</span>}</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>更进一步，我们希望这种表示能够捕获语义关系。比如，"国王"之于"女王"的关系，应该类似于"男人"之于"女人"。这意味着向量空间中应该存在某种规律性的结构。</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### 之前的尝试为何失败？</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>在Word2Vec之前，也有人尝试过学习词的分布式表示，但效果不尽如人意。</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>**One-hot编码**是最简单的表示方法。如果词汇表大小是$|\mathcal{V}|$，那么每个词都是一个$|\mathcal{V}|$维的向量，只有对应位置是1，其他都是0。这种表示的问题是显而易见的：任意两个词的向量都是正交的，$\cos(\text{cat}, \text{dog}) = 0$，没有任何相似性信息。</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>**共现矩阵**是一个更有意思的尝试。思路是：如果两个词经常在相似的上下文中出现，它们可能是相似的。可以构建一个词-词共现矩阵$X$，其中$X_{ij}$表示词$i$和词$j$在窗口内共同出现的次数。然后对这个矩阵做SVD分解，取前$d$个奇异向量作为词的表示。这种方法确实能捕获一些语义信息，但有几个问题：矩阵太大（$|\mathcal{V}| \times |\mathcal{V}|$可能有数十亿元素），SVD计算昂贵，而且新词需要重新计算整个分解。</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>**神经语言模型**（Bengio et al., 2003）是最接近现代方法的先驱。他们用神经网络训练语言模型，词的embedding作为模型的第一层被一起学习。这个想法很正确，但当时的计算能力有限，模型很小，无法在大规模语料上训练，因此效果受限。</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>回顾这些尝试，理想的解决方案应该具备以下特性。</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>首先是**可扩展性**。方法应该能在大规模语料（数十亿词）上高效训练。这意味着不能依赖全局矩阵分解，需要能够用随机梯度下降增量式地学习。</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>其次是**语义保持**。学到的表示应该自然地捕获语义相似性，不需要额外的语义知识库。</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>第三是**泛化能力**。表示应该在多种下游任务中有用——情感分析、命名实体识别、问答系统...一次训练，到处使用。</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>最后是**可解释性**。最好能够理解表示学到了什么，向量的不同维度是否有明确的语义。</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>Word2Vec惊人地同时满足了前三个要求，并在某种程度上满足了第四个——它发现了著名的"词向量类比"现象。</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### 分布式假设：语境定义语义</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>Word2Vec的核心洞察来自语言学中的**分布式假设**（Distributional Hypothesis）：</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "You shall know a word by the company it keeps." — J.R. Firth, 1957</span></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>一个词的含义，可以从它出现的上下文中推断出来。考虑两个你可能不熟悉的词：</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **glorp** chased the mouse across the kitchen."</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I fed my **glorp** some fish and it purred happily."</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>即使不知道"glorp"的定义，你也能推断它可能是某种猫科动物。因为它出现在"chased mouse"、"fed fish"、"purred"这样的上下文中，而这些上下文通常与猫相关。</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>这个洞察的数学含义是：如果两个词经常出现在相似的上下文中，它们应该有相似的向量表示。Word2Vec正是利用这一点来学习词向量。</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="fu">### 两种学习范式：CBOW与Skip-gram</span></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>Word2Vec提出了两种学习词向量的方法，它们是镜像对称的。</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>**CBOW（Continuous Bag of Words）**：给定上下文词，预测中心词。比如，给定"The cat ___ on the mat"中的上下文词，预测中间的词是"sat"。</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>**Skip-gram**：给定中心词，预测上下文词。比如，给定"sat"，预测它周围可能出现"cat"、"on"、"mat"等词。</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>直觉上，CBOW是"多个线索猜一个词"，而Skip-gram是"一个词猜多个线索"。实践中，Skip-gram在处理罕见词时效果更好，因为每个罕见词都有多个预测机会（需要预测多个上下文词），获得更多的训练信号。这也是为什么Skip-gram在后来的研究中更常被使用。</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a><span class="al">![CBOW 与 Skip-gram 架构对比：CBOW 从上下文预测中心词，Skip-gram 从中心词预测上下文。两者是镜像对称的。](figures/chapter-3/fig3-cbow-vs-skipgram.png)</span>{#fig-cbow-skipgram width=90%}</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>*自绘示意图，基于 Mikolov et al. (2013) "Efficient Estimation of Word Representations in Vector Space". [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)*</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个直觉性的例子</span></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>让我们用一个简化的例子来建立直觉。假设语料库是：</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I love cats"</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I love dogs"</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"cats are cute"</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"dogs are loyal"</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>如果我们用Skip-gram训练，以"love"为中心词（窗口大小=1），需要预测"I"和"cats"（来自第一句）以及"I"和"dogs"（来自第二句）。</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>训练过程会调整词向量，让"love"的向量能够同时预测"cats"和"dogs"。这意味着"cats"和"dogs"需要在向量空间中有相似的位置——因为它们都经常出现在"love"附近。</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>类似地，"cats"需要预测"are"和"cute"，"dogs"需要预测"are"和"loyal"。它们都需要预测"are"，这进一步拉近了它们在向量空间中的距离。</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>通过这种方式，即使"cats"和"dogs"从未在同一个句子中共同出现，模型也能学习到它们的相似性——因为它们出现在相似的上下文模式中。</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a><span class="fu">### Skip-gram模型的数学形式</span></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>让我们详细推导Skip-gram模型。设词汇表大小为$|\mathcal{V}|$，嵌入维度为$d$。模型有两套词向量：</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**中心词向量**（Center word embeddings）：$\mathbf{v}_w \in \mathbb{R}^d$，用于词$w$作为中心词时</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**上下文词向量**（Context word embeddings）：$\mathbf{u}_w \in \mathbb{R}^d$，用于词$w$作为上下文词时</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>给定中心词$w_c$和上下文词$w_o$，我们用softmax定义条件概率：</span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>P(w_o | w_c) = \frac{\exp(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \mathbf{v}_{w_c})}</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>直觉是：如果$\mathbf{u}_{w_o}$和$\mathbf{v}_{w_c}$的点积大（向量方向相近），那么$w_o$出现在$w_c$上下文中的概率就高。</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>给定语料库中的一个位置$t$，中心词是$w_t$，上下文窗口大小是$m$，Skip-gram的目标是最大化：</span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>\prod_{-m \leq j \leq m, j \neq 0} P(w_{t+j} | w_t)</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a>对整个语料库，目标函数是：</span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} | w_t)</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a><span class="fu">### 计算瓶颈：分母太贵</span></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a>展开log概率：</span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>\log P(w_o | w_c) = \mathbf{u}_{w_o}^\top \mathbf{v}_{w_c} - \log \sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \mathbf{v}_{w_c})</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>问题出在分母——需要对整个词汇表求和。如果词汇表有100万个词，每次计算一个概率就需要100万次点积运算，这是不可接受的。</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>Word2Vec的关键创新之一就是解决这个计算瓶颈。两种主要方法是**负采样**（Negative Sampling）和**层级Softmax**（Hierarchical Softmax）。</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a><span class="fu">### 负采样：化softmax为二分类</span></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>负采样的核心思想是：不去计算完整的softmax，而是把问题转化为二分类——区分"真实的上下文词"和"随机采样的噪声词"。</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>对于一个正样本对$(w_c, w_o)$（$w_o$确实出现在$w_c$的上下文中），我们同时采样$k$个负样本$w_1, w_2, \ldots, w_k$（随机从词汇表中按某种分布采样）。</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>新的目标函数是最大化：</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>\log \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} \left[ \log \sigma(-\mathbf{u}_{w_i}^\top \mathbf{v}_{w_c}) \right]</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>其中$\sigma(x) = \frac{1}{1+e^{-x}}$是sigmoid函数，$P_n(w)$是负采样分布。</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>这个目标的直觉是：让真实上下文词的向量与中心词相近（点积为正），让随机噪声词的向量与中心词远离（点积为负）。</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a>**为什么这样做是合理的？**</span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>可以证明，当$k$趋近于无穷时，负采样的目标函数趋近于原始softmax目标的某种近似。直觉上，如果模型能够区分真实上下文和随机噪声，它就学会了词之间的共现模式。</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a><span class="fu">### 负采样分布的设计</span></span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>负采样分布$P_n(w)$的选择对结果有影响。实践中，Word2Vec使用：</span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>P_n(w) \propto f(w)^{3/4}</span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>其中$f(w)$是词$w$在语料库中的频率。</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>**为什么用3/4次方？**</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>如果直接用频率$f(w)$，高频词（如"the"、"a"）会被过度采样，低频词几乎不会被采样。用$f(w)^{3/4}$是一个"平滑"：它降低了高频词的权重，增加了低频词的权重，使得采样分布更均匀。</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>实验表明，0.75是一个神奇的数字——它在多个数据集上都表现良好。这个选择更多是经验性的，没有很强的理论解释。</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Skip-gram with Negative Sampling (SGNS) (Mikolov et al., 2013)</span></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>**输入**：语料库 $\mathcal{D}$，词汇表 $\mathcal{V}$，嵌入维度 $d$，窗口大小 $m$，负采样数 $k$，学习率 $\eta$</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>**输出**：词向量矩阵 $\mathbf{V} \in \mathbb{R}^{|\mathcal{V}| \times d}$（中心词），$\mathbf{U} \in \mathbb{R}^{|\mathcal{V}| \times d}$（上下文词）</span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a>**1. 初始化**：</span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a><span class="in">随机初始化 V, U ∈ ℝ^(|V| × d)（通常使用小的随机值）</span></span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a><span class="in">构建负采样分布 Pn(w) ∝ f(w)^0.75</span></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>**2. 训练循环**：</span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a><span class="in">for epoch = 1 to num_epochs:</span></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a><span class="in">    for 每个位置 t = 1 to T in 语料库:</span></span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a><span class="in">        wc ← 中心词（位置 t 的词）</span></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a><span class="in">        for 每个上下文位置 j ∈ [-m, m], j ≠ 0:</span></span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a><span class="in">            wo ← 上下文词（位置 t+j 的词）</span></span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a><span class="in">            # 正样本更新</span></span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a><span class="in">            score_pos ← u_wo · v_wc</span></span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a><span class="in">            grad_pos ← (1 - σ(score_pos))</span></span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a><span class="in">            # 负采样</span></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a><span class="in">            for i = 1 to k:</span></span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a><span class="in">                wn ← 从 Pn(w) 采样一个负样本词</span></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a><span class="in">                score_neg ← u_wn · v_wc</span></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a><span class="in">                grad_neg ← σ(score_neg)</span></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a><span class="in">                # 更新负样本的上下文向量</span></span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="in">                u_wn ← u_wn - η · grad_neg · v_wc</span></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a><span class="in">            # 更新正样本的上下文向量</span></span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a><span class="in">            u_wo ← u_wo + η · grad_pos · v_wc</span></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a><span class="in">            # 更新中心词向量</span></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a><span class="in">            v_wc ← v_wc + η · (grad_pos · u_wo - Σ grad_neg · u_wn)</span></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a>**3. 输出**：返回 $\mathbf{V}$（或 $\frac{\mathbf{V} + \mathbf{U}}{2}$）作为最终词向量</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a>**关键参数选择**（来自原论文）：</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 参数 <span class="pp">|</span> 小数据集 <span class="pp">|</span> 大数据集 <span class="pp">|</span></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|----------|</span></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 负采样数 $k$ <span class="pp">|</span> 5-20 <span class="pp">|</span> 2-5 <span class="pp">|</span></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 窗口大小 $m$ <span class="pp">|</span> 5 <span class="pp">|</span> 5-10 <span class="pp">|</span></span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 嵌入维度 $d$ <span class="pp">|</span> 100-300 <span class="pp">|</span> 300 <span class="pp">|</span></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a>*Adapted from: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). "Distributed Representations of Words and Phrases and their Compositionality". NeurIPS 2013. [arXiv:1310.4546](https://arxiv.org/abs/1310.4546)*</span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a><span class="fu">### CBOW模型</span></span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a>CBOW与Skip-gram方向相反：给定上下文词，预测中心词。</span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a>设上下文窗口内的词是$w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}$，CBOW首先计算上下文的平均向量：</span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a>\bar{\mathbf{v}} = \frac{1}{2m} \sum_{-m \leq j \leq m, j \neq 0} \mathbf{v}_{w_{c+j}}</span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>然后用这个平均向量预测中心词：</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a>P(w_c | \text{context}) = \frac{\exp(\mathbf{u}_{w_c}^\top \bar{\mathbf{v}})}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \bar{\mathbf{v}})}</span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a>CBOW的计算效率比Skip-gram高（每个位置只需一次预测），但在捕获罕见词方面通常不如Skip-gram。</span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数值示例：Skip-gram训练一步</span></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a>让我们用一个极简的数值例子来理解Skip-gram的梯度更新是如何工作的。</span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>词汇表：{I, love, cats, dogs}，大小$|\mathcal{V}| = 4$</span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>嵌入维度：$d = 2$</span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>学习率：$\eta = 0.1$</span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>负采样数：$k = 1$</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a>**初始化词向量**（随机小值）：</span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>中心词向量$\mathbf{v}$：</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{I}} = \begin{bmatrix} 0.1 <span class="sc">\\</span> 0.2 \end{bmatrix}, \quad</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{love}} = \begin{bmatrix} 0.3 <span class="sc">\\</span> -0.1 \end{bmatrix}, \quad</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{cats}} = \begin{bmatrix} -0.2 <span class="sc">\\</span> 0.4 \end{bmatrix}, \quad</span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{dogs}} = \begin{bmatrix} 0.0 <span class="sc">\\</span> 0.3 \end{bmatrix}</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a>上下文词向量$\mathbf{u}$（初始化相同值，简化演示）：</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_{\text{I}} = \begin{bmatrix} 0.1 <span class="sc">\\</span> 0.2 \end{bmatrix}, \quad</span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_{\text{love}} = \begin{bmatrix} 0.3 <span class="sc">\\</span> -0.1 \end{bmatrix}, \quad</span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_{\text{cats}} = \begin{bmatrix} -0.2 <span class="sc">\\</span> 0.4 \end{bmatrix}, \quad</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_{\text{dogs}} = \begin{bmatrix} 0.0 <span class="sc">\\</span> 0.3 \end{bmatrix}</span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a>**训练样本**：句子"I love cats"，窗口大小=1</span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>中心词：love</span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>正样本上下文词：cats（右边一个词）</span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>负采样词：dogs（随机采样）</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a>**Step 1：计算点积**</span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a>正样本：$\mathbf{u}_{\text{cats}}^\top \mathbf{v}_{\text{love}}$</span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a>= \begin{bmatrix} -0.2 &amp; 0.4 \end{bmatrix} \begin{bmatrix} 0.3 <span class="sc">\\</span> -0.1 \end{bmatrix}</span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a>= (-0.2)(0.3) + (0.4)(-0.1) = -0.06 - 0.04 = -0.10</span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a>负样本：$\mathbf{u}_{\text{dogs}}^\top \mathbf{v}_{\text{love}}$</span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a>= \begin{bmatrix} 0.0 &amp; 0.3 \end{bmatrix} \begin{bmatrix} 0.3 <span class="sc">\\</span> -0.1 \end{bmatrix}</span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a>= (0.0)(0.3) + (0.3)(-0.1) = 0 - 0.03 = -0.03</span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>**Step 2：计算Sigmoid**</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a>\sigma(-0.10) = \frac{1}{1 + e^{0.10}} \approx \frac{1}{1.105} \approx 0.475</span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a>\sigma(-0.03) = \frac{1}{1 + e^{0.03}} \approx \frac{1}{1.030} \approx 0.493</span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a>对于负样本，我们需要$\sigma(-\mathbf{u}_{\text{dogs}}^\top \mathbf{v}_{\text{love}}) = \sigma(0.03) \approx 0.507$</span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a>**Step 3：计算梯度**</span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a>对于负采样目标函数：</span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \log \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}) + \log \sigma(-\mathbf{u}_{w_{\text{neg}}}^\top \mathbf{v}_{w_c})</span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a>梯度计算：</span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}}$：需要拉近cats，推远dogs</span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\frac{\partial \mathcal{L}}{\partial \mathbf{u}_{\text{cats}}}$：需要拉近love</span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\frac{\partial \mathcal{L}}{\partial \mathbf{u}_{\text{dogs}}}$：需要推远love</span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a>对于正样本，梯度为$(1 - \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})) \cdot \mathbf{u}_{w_o}$：</span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}} \big|_{\text{positive}} = (1 - 0.475) \cdot \begin{bmatrix} -0.2 <span class="sc">\\</span> 0.4 \end{bmatrix} = 0.525 \cdot \begin{bmatrix} -0.2 <span class="sc">\\</span> 0.4 \end{bmatrix} = \begin{bmatrix} -0.105 <span class="sc">\\</span> 0.210 \end{bmatrix}</span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a>对于负样本，梯度为$-\sigma(\mathbf{u}_{w_{\text{neg}}}^\top \mathbf{v}_{w_c}) \cdot \mathbf{u}_{w_{\text{neg}}}$：</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}} \big|_{\text{negative}} = -0.493 \cdot \begin{bmatrix} 0.0 <span class="sc">\\</span> 0.3 \end{bmatrix} = \begin{bmatrix} 0.0 <span class="sc">\\</span> -0.148 \end{bmatrix}</span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a>总梯度：</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}} = \begin{bmatrix} -0.105 <span class="sc">\\</span> 0.210 \end{bmatrix} + \begin{bmatrix} 0.0 <span class="sc">\\</span> -0.148 \end{bmatrix} = \begin{bmatrix} -0.105 <span class="sc">\\</span> 0.062 \end{bmatrix}</span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a>**Step 4：更新词向量**</span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{love}}^{\text{new}} = \mathbf{v}_{\text{love}} + \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{v}_{\text{love}}}</span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a>= \begin{bmatrix} 0.3 <span class="sc">\\</span> -0.1 \end{bmatrix} + 0.1 \cdot \begin{bmatrix} -0.105 <span class="sc">\\</span> 0.062 \end{bmatrix}</span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a>= \begin{bmatrix} 0.290 <span class="sc">\\</span> -0.094 \end{bmatrix}</span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a>**观察**：</span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"love"的向量第一维减小了（从0.3到0.29），这会使它与"cats"的点积增大（因为cats第一维是负的）</span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"love"的向量第二维增大了（从-0.1到-0.094），这也会使它与"cats"的点积增大（因为cats第二维是正的）</span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>同时，这些变化会使"love"与"dogs"的点积变化不大或减小</span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>这就是Word2Vec学习的本质：通过大量这样的小更新，逐渐调整词向量，让共现词的向量彼此接近。</span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### 词向量类比的神奇现象</span></span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a>Word2Vec最令人惊叹的发现是**词向量类比**（word analogy）。训练好的词向量呈现出惊人的规律性：</span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{king}} - \mathbf{v}_{\text{man}} + \mathbf{v}_{\text{woman}} \approx \mathbf{v}_{\text{queen}}</span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a>这意味着"国王-男人+女人≈女王"。向量空间中存在一个"性别方向"，沿着这个方向移动可以改变词的性别属性，同时保持其他语义属性（如"皇室成员"）不变。</span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a>类似的关系还有很多：</span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{v}_{\text{Paris}} - \mathbf{v}_{\text{France}} + \mathbf{v}_{\text{Italy}} \approx \mathbf{v}_{\text{Rome}}$（首都关系）</span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{v}_{\text{walking}} - \mathbf{v}_{\text{walk}} + \mathbf{v}_{\text{swim}} \approx \mathbf{v}_{\text{swimming}}$（时态变化）</span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>这个发现说明，词向量空间不仅捕获了词的相似性，还学习到了词之间的语义关系。这是传统特征工程无法实现的。</span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a>**为什么会出现这种现象？**</span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a>一个直觉解释是：语言中的规律性反映在共现模式中。"king"和"queen"出现在相似的上下文中（都与皇室、统治相关），但"king"更常与"he"、"his"共现，"queen"更常与"she"、"her"共现。类似地，"man"和"woman"也有这种共现模式的差异。通过大量语料的训练，这些规律性被编码进了向量空间的结构中。</span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a>更正式的理论分析将在后面的"深入理解"部分讨论。</span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a><span class="fu">## GloVe与FastText：词向量的改进</span></span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a><span class="fu">### GloVe：全局统计的视角</span></span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a>Word2Vec是一个**预测模型**：它通过预测上下文来学习词向量。2014年，Stanford的Pennington等人提出了GloVe（Global Vectors），从一个不同的角度来看这个问题。</span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a>GloVe的核心洞察是：词向量应该直接编码共现统计信息。</span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a>设$X_{ij}$是词$i$和词$j$在窗口内共同出现的次数，定义$P_{ij} = P(j|i) = X_{ij} / X_i$为词$j$出现在词$i$上下文中的概率。GloVe关注的是**概率比值**：</span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a>\frac{P_{ik}}{P_{jk}} = \frac{P(k|i)}{P(k|j)}</span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a>这个比值揭示了词之间的关系。考虑词"ice"和"steam"，以及探测词"solid"和"gas"：</span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="pp">|</span> $P(\cdot<span class="pp">|</span>\text{ice})$ <span class="pp">|</span> $P(\cdot<span class="pp">|</span>\text{steam})$ <span class="pp">|</span> 比值 <span class="pp">|</span></span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|---|</span></span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> solid <span class="pp">|</span> 高（冰是固体） <span class="pp">|</span> 低 <span class="pp">|</span> 大 <span class="pp">|</span></span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> gas <span class="pp">|</span> 低 <span class="pp">|</span> 高（蒸汽是气体） <span class="pp">|</span> 小 <span class="pp">|</span></span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> water <span class="pp">|</span> 高 <span class="pp">|</span> 高 <span class="pp">|</span> ≈1 <span class="pp">|</span></span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> fashion <span class="pp">|</span> 低 <span class="pp">|</span> 低 <span class="pp">|</span> ≈1 <span class="pp">|</span></span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>GloVe的目标是让词向量能够编码这种比值关系：</span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a>\mathbf{w}_i^\top \mathbf{w}_j + b_i + b_j = \log X_{ij}</span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a>最终的损失函数是：</span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \sum_{i,j=1}^{|\mathcal{V}|} f(X_{ij}) \left( \mathbf{w}_i^\top \mathbf{w}_j + b_i + b_j - \log X_{ij} \right)^2</span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a>其中$f(x)$是权重函数，用于降低高频共现对的影响（否则"the"、"a"等词会主导训练）。</span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a>**GloVe vs Word2Vec**：</span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>GloVe是**基于统计**的：它直接利用全局共现矩阵，一步到位地优化。Word2Vec是**基于预测**的：它通过局部的滑动窗口，增量式地学习。</span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a>实践中，两者效果相当。GloVe的优势是利用了全局统计信息，可能在某些任务上更稳定。Word2Vec的优势是可以流式训练，不需要先构建完整的共现矩阵。</span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a><span class="fu">### FastText：子词的力量</span></span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>Word2Vec和GloVe都把词当作原子单位。但语言中有很多词共享相同的词根或词缀，比如"teach"、"teacher"、"teaching"、"teaches"。能不能利用这种结构？</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a>2016年，Facebook的Bojanowski等人提出了FastText，核心思想是：把词表示为**子词（subword）的组合**。</span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a>具体来说，FastText把每个词分解成字符n-gram。比如，"where"（加上边界符号&lt;和&gt;）分解为：</span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>3-gram: &lt;wh, whe, her, ere, re&gt;</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>4-gram: &lt;whe, wher, here, ere&gt;</span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>5-gram: &lt;wher, where, here&gt;</span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a>词"where"的向量是所有这些n-gram向量的和，再加上词本身的向量（如果存在的话）：</span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_{\text{where}} = \mathbf{z}_{\text{where}} + \sum_{g \in G(\text{where})} \mathbf{z}_g</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a>**FastText的优势**：</span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a>首先是**处理OOV（Out-of-Vocabulary）词**。传统词向量无法处理训练时没见过的词。但FastText可以：即使"unfriendliness"没出现过，只要它的子词（如"un"、"friend"、"ness"）出现过，就能组合出一个合理的向量。</span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a>其次是**形态丰富的语言**。对于德语、土耳其语等形态变化丰富的语言，同一个词根可能有几十种变形。FastText通过共享子词表示，可以更有效地学习这些语言。</span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a>第三是**处理拼写错误**。"freind"和"friend"共享很多子词，因此它们的向量会很相似，模型对拼写错误更鲁棒。</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a>**FastText vs Word2Vec**：</span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a>FastText的代价是参数量增加（需要存储所有n-gram的向量），训练也更慢。在英语等形态变化较少的语言上，FastText相对Word2Vec的优势不太明显。但在形态丰富的语言上，FastText通常显著更好。</span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a>下图展示了 FastText 如何通过子词捕获词之间的相似性。热力图显示了不同 n-gram 之间的相似度：共享更多子词的词对（如 "rarity" 和 "scarceness"）在向量空间中更接近。</span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a><span class="al">![FastText 子词相似度热力图：展示了 "rarity" 和 "scarceness" 两个词的各个 n-gram 之间的相似度。共享相似子词结构的词会有更高的相似度。](figures/chapter-3/original/fig4-fasttext-similarity-rarity-scarceness.png)</span>{#fig-fasttext-similarity width=70%}</span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a>*Source: Bojanowski et al. (2017) "Enriching Word Vectors with Subword Information", Figure 1*</span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：词向量训练与可视化</span></span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Gensim训练Word2Vec</span></span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.word2vec <span class="im">import</span> LineSentence</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置日志，观察训练进度</span></span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> : </span><span class="sc">%(levelname)s</span><span class="st"> : </span><span class="sc">%(message)s</span><span class="st">'</span>, level<span class="op">=</span>logging.INFO)</span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a><span class="co"># 准备训练数据：每行一个句子，词用空格分隔</span></span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a><span class="co"># 这里用一个简单的示例，实际应该用大规模语料</span></span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'I'</span>, <span class="st">'love'</span>, <span class="st">'machine'</span>, <span class="st">'learning'</span>],</span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'deep'</span>, <span class="st">'learning'</span>, <span class="st">'is'</span>, <span class="st">'a'</span>, <span class="st">'subset'</span>, <span class="st">'of'</span>, <span class="st">'machine'</span>, <span class="st">'learning'</span>],</span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'natural'</span>, <span class="st">'language'</span>, <span class="st">'processing'</span>, <span class="st">'uses'</span>, <span class="st">'deep'</span>, <span class="st">'learning'</span>],</span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'word'</span>, <span class="st">'vectors'</span>, <span class="st">'are'</span>, <span class="st">'learned'</span>, <span class="st">'representations'</span>],</span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'word2vec'</span>, <span class="st">'learns'</span>, <span class="st">'word'</span>, <span class="st">'vectors'</span>, <span class="st">'from'</span>, <span class="st">'text'</span>],</span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'cats'</span>, <span class="st">'and'</span>, <span class="st">'dogs'</span>, <span class="st">'are'</span>, <span class="st">'pets'</span>],</span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'cats'</span>, <span class="st">'chase'</span>, <span class="st">'mice'</span>],</span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'dogs'</span>, <span class="st">'chase'</span>, <span class="st">'cats'</span>, <span class="st">'sometimes'</span>],</span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练Word2Vec模型</span></span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(</span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a>    sentences,</span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">50</span>,      <span class="co"># 词向量维度</span></span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">3</span>,            <span class="co"># 上下文窗口大小</span></span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">1</span>,         <span class="co"># 忽略频率低于此值的词</span></span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a>    workers<span class="op">=</span><span class="dv">4</span>,           <span class="co"># 训练线程数</span></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">1</span>,                <span class="co"># 1=Skip-gram, 0=CBOW</span></span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a>    negative<span class="op">=</span><span class="dv">5</span>,          <span class="co"># 负采样数量</span></span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>           <span class="co"># 训练轮数</span></span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看词向量</span></span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"'learning'的词向量前5维:"</span>, model.wv[<span class="st">'learning'</span>][:<span class="dv">5</span>])</span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a><span class="co"># 找相似词</span></span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">与'learning'最相似的词:"</span>)</span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> model.wv.most_similar(<span class="st">'learning'</span>, topn<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a><span class="co"># 词向量运算</span></span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'cats'</span> <span class="kw">in</span> model.wv <span class="kw">and</span> <span class="st">'dogs'</span> <span class="kw">in</span> model.wv:</span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a>    similarity <span class="op">=</span> model.wv.similarity(<span class="st">'cats'</span>, <span class="st">'dogs'</span>)</span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">'cats'和'dogs'的相似度: </span><span class="sc">{</span>similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存和加载模型</span></span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">"word2vec.model"</span>)</span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a><span class="co"># loaded_model = Word2Vec.load("word2vec.model")</span></span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用预训练词向量</span></span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a>实际应用中，通常使用在大规模语料上预训练好的词向量，而不是自己从头训练。</span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a><span class="co"># 下载预训练的Word2Vec模型（Google News, 300维, 300万词）</span></span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：这个模型约1.6GB，首次下载需要时间</span></span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a><span class="co"># model = api.load("word2vec-google-news-300")</span></span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a><span class="co"># 或者使用更小的GloVe模型</span></span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a><span class="co"># model = api.load("glove-wiki-gigaword-100")</span></span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a><span class="co"># 演示用一个小模型</span></span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"glove-twitter-25"</span>)  <span class="co"># 25维，较小</span></span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a><span class="co"># 词向量运算示例</span></span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"king - man + woman = ?"</span>)</span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> result:</span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Paris - France + Italy = ?"</span>)</span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'paris'</span>, <span class="st">'italy'</span>], negative<span class="op">=</span>[<span class="st">'france'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> result:</span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a><span class="co"># 找不相似的词</span></span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">哪个词不属于这一组？['breakfast', 'lunch', 'dinner', 'computer']"</span>)</span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a>odd_one <span class="op">=</span> model.doesnt_match([<span class="st">'breakfast'</span>, <span class="st">'lunch'</span>, <span class="st">'dinner'</span>, <span class="st">'computer'</span>])</span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  答案: </span><span class="sc">{</span>odd_one<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a><span class="fu">### 词向量可视化</span></span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a>高维词向量无法直接可视化，需要用降维技术投影到2D或3D。</span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练模型</span></span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"glove-twitter-25"</span>)</span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a><span class="co"># 选择一些词进行可视化</span></span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 动物</span></span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cat'</span>, <span class="st">'dog'</span>, <span class="st">'bird'</span>, <span class="st">'fish'</span>, <span class="st">'horse'</span>,</span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 国家</span></span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a>    <span class="st">'china'</span>, <span class="st">'japan'</span>, <span class="st">'france'</span>, <span class="st">'germany'</span>, <span class="st">'italy'</span>,</span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 颜色</span></span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a>    <span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'black'</span>,</span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 数字</span></span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a>    <span class="st">'one'</span>, <span class="st">'two'</span>, <span class="st">'three'</span>, <span class="st">'four'</span>, <span class="st">'five'</span></span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a><span class="co"># 获取词向量</span></span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> np.array([model[w] <span class="cf">for</span> w <span class="kw">in</span> words <span class="cf">if</span> w <span class="kw">in</span> model])</span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> words <span class="cf">if</span> w <span class="kw">in</span> model]</span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE降维</span></span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a>vectors_2d <span class="op">=</span> tsne.fit_transform(word_vectors)</span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化</span></span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a>plt.scatter(vectors_2d[:, <span class="dv">0</span>], vectors_2d[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a>    plt.annotate(word, xy<span class="op">=</span>(vectors_2d[i, <span class="dv">0</span>], vectors_2d[i, <span class="dv">1</span>]),</span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">12</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Word Vectors Visualization (t-SNE)"</span>)</span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Dimension 1"</span>)</span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Dimension 2"</span>)</span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"word_vectors_tsne.png"</span>, dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a><span class="fu">### 评估词向量质量</span></span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a>词向量的质量可以通过多种基准测试评估。</span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载模型</span></span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"glove-wiki-gigaword-100"</span>)</span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 词类比任务（Word Analogy）</span></span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a><span class="co"># 格式：A:B :: C:D，测试 A-B+C=D</span></span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a>analogies <span class="op">=</span> [</span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'king'</span>, <span class="st">'man'</span>, <span class="st">'woman'</span>, <span class="st">'queen'</span>),</span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'paris'</span>, <span class="st">'france'</span>, <span class="st">'berlin'</span>, <span class="st">'germany'</span>),</span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'good'</span>, <span class="st">'better'</span>, <span class="st">'bad'</span>, <span class="st">'worse'</span>),</span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"词类比测试:"</span>)</span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b, c, expected <span class="kw">in</span> analogies:</span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[a, c], negative<span class="op">=</span>[b], topn<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">=</span> <span class="st">"✓"</span> <span class="cf">if</span> result <span class="op">==</span> expected <span class="cf">else</span> <span class="st">"✗"</span></span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss"> :: </span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">:? → </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss"> (期望: </span><span class="sc">{</span>expected<span class="sc">}</span><span class="ss">) </span><span class="sc">{</span>correct<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyError</span> <span class="im">as</span> e:</span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  词不在词汇表中: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 词相似度任务（Word Similarity）</span></span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a><span class="co"># 与人类判断的相关性</span></span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a>word_pairs <span class="op">=</span> [</span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'car'</span>, <span class="st">'automobile'</span>, <span class="st">'high'</span>),</span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'car'</span>, <span class="st">'bicycle'</span>, <span class="st">'medium'</span>),</span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'car'</span>, <span class="st">'democracy'</span>, <span class="st">'low'</span>),</span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">词相似度测试:"</span>)</span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w1, w2, expected_level <span class="kw">in</span> word_pairs:</span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a>        sim <span class="op">=</span> model.similarity(w1, w2)</span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>w1<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>w2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>sim<span class="sc">:.4f}</span><span class="ss"> (预期: </span><span class="sc">{</span>expected_level<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyError</span> <span class="im">as</span> e:</span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  词不在词汇表中: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 聚类质量</span></span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a><span class="co"># 语义相似的词应该聚在一起</span></span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a>words_by_category <span class="op">=</span> {</span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a>    <span class="st">'animals'</span>: [<span class="st">'cat'</span>, <span class="st">'dog'</span>, <span class="st">'bird'</span>, <span class="st">'fish'</span>, <span class="st">'horse'</span>],</span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a>    <span class="st">'countries'</span>: [<span class="st">'china'</span>, <span class="st">'japan'</span>, <span class="st">'france'</span>, <span class="st">'germany'</span>, <span class="st">'italy'</span>],</span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a>    <span class="st">'colors'</span>: [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'yellow'</span>, <span class="st">'black'</span>],</span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a>all_words <span class="op">=</span> []</span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (category, words) <span class="kw">in</span> <span class="bu">enumerate</span>(words_by_category.items()):</span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> model:</span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a>            all_words.append(word)</span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a>            true_labels.append(i)</span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> np.array([model[w] <span class="cf">for</span> w <span class="kw">in</span> all_words])</span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a><span class="co"># K-means聚类</span></span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> kmeans.fit_predict(vectors)</span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-736"><a href="#cb7-736" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算聚类纯度</span></span>
<span id="cb7-737"><a href="#cb7-737" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb7-738"><a href="#cb7-738" aria-hidden="true" tabindex="-1"></a>ari <span class="op">=</span> adjusted_rand_score(true_labels, pred_labels)</span>
<span id="cb7-739"><a href="#cb7-739" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">聚类质量 (Adjusted Rand Index): </span><span class="sc">{</span>ari<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-740"><a href="#cb7-740" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-741"><a href="#cb7-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-742"><a href="#cb7-742" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-743"><a href="#cb7-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-744"><a href="#cb7-744" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb7-745"><a href="#cb7-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-746"><a href="#cb7-746" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨词向量的理论基础、边界条件和开放问题</span></span>
<span id="cb7-747"><a href="#cb7-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-748"><a href="#cb7-748" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb7-749"><a href="#cb7-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-750"><a href="#cb7-750" aria-hidden="true" tabindex="-1"></a>词向量的成功有深刻的理论基础。</span>
<span id="cb7-751"><a href="#cb7-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-752"><a href="#cb7-752" aria-hidden="true" tabindex="-1"></a>**分布式假设的数学化**。Levy和Goldberg（2014）的重要工作表明，Word2Vec的Skip-gram负采样目标实际上等价于对点互信息（PMI）矩阵的隐式分解：</span>
<span id="cb7-753"><a href="#cb7-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-754"><a href="#cb7-754" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-755"><a href="#cb7-755" aria-hidden="true" tabindex="-1"></a>\mathbf{w}_i \cdot \mathbf{c}_j = \text{PMI}(i, j) - \log k</span>
<span id="cb7-756"><a href="#cb7-756" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-757"><a href="#cb7-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-758"><a href="#cb7-758" aria-hidden="true" tabindex="-1"></a>其中PMI定义为：</span>
<span id="cb7-759"><a href="#cb7-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-760"><a href="#cb7-760" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-761"><a href="#cb7-761" aria-hidden="true" tabindex="-1"></a>\text{PMI}(i, j) = \log \frac{P(i, j)}{P(i)P(j)} = \log \frac{<span class="sc">\#</span>(i, j) \cdot |D|}{<span class="sc">\#</span>(i) \cdot <span class="sc">\#</span>(j)}</span>
<span id="cb7-762"><a href="#cb7-762" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-763"><a href="#cb7-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-764"><a href="#cb7-764" aria-hidden="true" tabindex="-1"></a>这意味着Word2Vec本质上是在学习词对的PMI——一种统计相关性度量。PMI高意味着两个词比随机共现更频繁地一起出现，这正是语义相关的信号。</span>
<span id="cb7-765"><a href="#cb7-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-766"><a href="#cb7-766" aria-hidden="true" tabindex="-1"></a>**与矩阵分解的联系**。进一步，可以证明GloVe也在做类似的事情：对共现统计的某种变换进行低秩分解。这解释了为什么两种表面上不同的方法能得到相似的结果——它们都在捕获相同的统计结构。</span>
<span id="cb7-767"><a href="#cb7-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-768"><a href="#cb7-768" aria-hidden="true" tabindex="-1"></a>**线性结构的来源**。词向量类比（king - man + woman ≈ queen）的线性结构可以从PMI的性质推导。如果"king"和"queen"有相似的上下文，但"king"更常与"he"共现、"queen"更常与"she"共现，那么：</span>
<span id="cb7-769"><a href="#cb7-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-770"><a href="#cb7-770" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-771"><a href="#cb7-771" aria-hidden="true" tabindex="-1"></a>\text{PMI}(\text{king}, \text{he}) - \text{PMI}(\text{queen}, \text{he}) \approx \text{PMI}(\text{man}, \text{he}) - \text{PMI}(\text{woman}, \text{he})</span>
<span id="cb7-772"><a href="#cb7-772" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-773"><a href="#cb7-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-774"><a href="#cb7-774" aria-hidden="true" tabindex="-1"></a>这种对称性导致了向量空间中的平行结构。</span>
<span id="cb7-775"><a href="#cb7-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-776"><a href="#cb7-776" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——实证视角</span></span>
<span id="cb7-777"><a href="#cb7-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-778"><a href="#cb7-778" aria-hidden="true" tabindex="-1"></a>大量实验揭示了词向量有效的条件。</span>
<span id="cb7-779"><a href="#cb7-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-780"><a href="#cb7-780" aria-hidden="true" tabindex="-1"></a>**数据量的影响**。更多的训练数据通常带来更好的词向量。Mikolov等人报告，在Google News语料（1000亿词）上训练的向量显著优于在较小语料上训练的向量。这并不意外——更多数据意味着更准确的共现统计估计。</span>
<span id="cb7-781"><a href="#cb7-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-782"><a href="#cb7-782" aria-hidden="true" tabindex="-1"></a>**维度的影响**。词向量维度通常在100-300之间效果最好。太低维度无法捕获足够的信息，太高维度可能过拟合或引入噪声。300维似乎是一个"甜点"，在多数任务上表现良好。</span>
<span id="cb7-783"><a href="#cb7-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-784"><a href="#cb7-784" aria-hidden="true" tabindex="-1"></a>**窗口大小的影响**。较小的窗口（2-5）倾向于捕获语法关系（如词性），较大的窗口（5-10）倾向于捕获语义关系（如主题）。选择取决于下游任务。</span>
<span id="cb7-785"><a href="#cb7-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-786"><a href="#cb7-786" aria-hidden="true" tabindex="-1"></a>**负采样数量的影响**。对于小数据集，较多的负样本（15-20）可能有帮助。对于大数据集，较少的负样本（5-10）通常就够了。</span>
<span id="cb7-787"><a href="#cb7-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-788"><a href="#cb7-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb7-789"><a href="#cb7-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-790"><a href="#cb7-790" aria-hidden="true" tabindex="-1"></a>词向量有几个重要的隐含假设和局限性。</span>
<span id="cb7-791"><a href="#cb7-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-792"><a href="#cb7-792" aria-hidden="true" tabindex="-1"></a>**静态表示假设**。每个词只有一个向量，不论它出现在什么上下文中。这意味着一词多义无法处理。"bank"（银行）和"bank"（河岸）有相同的向量，这显然不合理。这个问题在第11章（ELMo）和第13章（BERT）才得到解决。</span>
<span id="cb7-793"><a href="#cb7-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-794"><a href="#cb7-794" aria-hidden="true" tabindex="-1"></a>**词袋假设**。词向量训练通常忽略词序，把上下文当作词袋处理。这丢失了语法信息，比如"dog bites man"和"man bites dog"的区分。</span>
<span id="cb7-795"><a href="#cb7-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-796"><a href="#cb7-796" aria-hidden="true" tabindex="-1"></a>**频率偏差**。高频词（如"the"、"is"）可能主导训练，导致它们的向量质量过高，而低频词（真正有语义内容的词）质量不足。虽然负采样分布的设计试图缓解这个问题，但偏差仍然存在。</span>
<span id="cb7-797"><a href="#cb7-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-798"><a href="#cb7-798" aria-hidden="true" tabindex="-1"></a>**文化和历史偏见**。词向量会学习语料库中的偏见。如果语料库中"doctor"更常与"he"共现、"nurse"更常与"she"共现，词向量就会编码这种性别偏见。这在某些应用中可能是问题。</span>
<span id="cb7-799"><a href="#cb7-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-800"><a href="#cb7-800" aria-hidden="true" tabindex="-1"></a><span class="fu">### 变体与扩展</span></span>
<span id="cb7-801"><a href="#cb7-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-802"><a href="#cb7-802" aria-hidden="true" tabindex="-1"></a>词向量的成功催生了大量后续工作。</span>
<span id="cb7-803"><a href="#cb7-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-804"><a href="#cb7-804" aria-hidden="true" tabindex="-1"></a>**句子向量**。如何表示整个句子？简单的方法是对词向量取平均，但这丢失了词序信息。Le和Mikolov（2014）提出了Doc2Vec，用类似Word2Vec的方法学习文档向量。</span>
<span id="cb7-805"><a href="#cb7-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-806"><a href="#cb7-806" aria-hidden="true" tabindex="-1"></a>**跨语言词向量**。能否让不同语言的词在同一个向量空间中？如果能，机器翻译就变成了简单的最近邻查找。研究表明，通过少量双语词典或平行语料，可以学习跨语言的词向量对齐。</span>
<span id="cb7-807"><a href="#cb7-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-808"><a href="#cb7-808" aria-hidden="true" tabindex="-1"></a>**知识增强**。纯粹从文本学习可能错过一些常识知识。研究者尝试将知识图谱信息融入词向量，比如让"巴黎"和"法国"的向量编码"首都"关系。</span>
<span id="cb7-809"><a href="#cb7-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-810"><a href="#cb7-810" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb7-811"><a href="#cb7-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-812"><a href="#cb7-812" aria-hidden="true" tabindex="-1"></a>如果你要在词向量方向做研究，可以考虑这些问题。</span>
<span id="cb7-813"><a href="#cb7-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-814"><a href="#cb7-814" aria-hidden="true" tabindex="-1"></a>第一，**最优的训练目标是什么？** Skip-gram、CBOW、GloVe、FastText...都有各自的优缺点。是否存在一个理论最优的目标函数？目前还没有定论。</span>
<span id="cb7-815"><a href="#cb7-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-816"><a href="#cb7-816" aria-hidden="true" tabindex="-1"></a>第二，**如何处理一词多义？** 静态词向量的根本局限是无法区分同一个词的不同含义。在ELMo和BERT之前，有一些工作尝试为每个词学习多个向量（如Sense2Vec），但效果有限。</span>
<span id="cb7-817"><a href="#cb7-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-818"><a href="#cb7-818" aria-hidden="true" tabindex="-1"></a>第三，**词向量编码了什么信息？** 虽然我们知道词向量能完成类比任务，但它们到底学到了什么语言知识？是语法？语义？世界知识？这仍然是一个活跃的研究问题。</span>
<span id="cb7-819"><a href="#cb7-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-820"><a href="#cb7-820" aria-hidden="true" tabindex="-1"></a>第四，**如何减少偏见？** 词向量中的性别、种族偏见如何检测和消除？简单的"去偏"方法（如减去性别方向）可能只是表面上消除偏见，深层的偏见仍然存在。</span>
<span id="cb7-821"><a href="#cb7-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-822"><a href="#cb7-822" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-823"><a href="#cb7-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-824"><a href="#cb7-824" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb7-825"><a href="#cb7-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-826"><a href="#cb7-826" aria-hidden="true" tabindex="-1"></a><span class="fu">### 静态表示的根本缺陷</span></span>
<span id="cb7-827"><a href="#cb7-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-828"><a href="#cb7-828" aria-hidden="true" tabindex="-1"></a>词向量最大的局限是**静态性**：每个词只有一个固定的向量表示，不论它出现在什么上下文中。</span>
<span id="cb7-829"><a href="#cb7-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-830"><a href="#cb7-830" aria-hidden="true" tabindex="-1"></a>考虑"bank"这个词：</span>
<span id="cb7-831"><a href="#cb7-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-832"><a href="#cb7-832" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I went to the **bank** to deposit money."（银行）</span>
<span id="cb7-833"><a href="#cb7-833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"We sat on the **bank** of the river."（河岸）</span>
<span id="cb7-834"><a href="#cb7-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-835"><a href="#cb7-835" aria-hidden="true" tabindex="-1"></a>这两个"bank"显然有不同的含义，但在Word2Vec中它们只有一个向量。这个向量是所有含义的某种"平均"，对于每个具体用法都不够精确。</span>
<span id="cb7-836"><a href="#cb7-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-837"><a href="#cb7-837" aria-hidden="true" tabindex="-1"></a>问题在高度多义词上尤其严重。英语中很多常用词都有多个含义（"run"有超过100个义项），一个静态向量无法捕获这种复杂性。</span>
<span id="cb7-838"><a href="#cb7-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-839"><a href="#cb7-839" aria-hidden="true" tabindex="-1"></a><span class="fu">### 上下文信息的缺失</span></span>
<span id="cb7-840"><a href="#cb7-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-841"><a href="#cb7-841" aria-hidden="true" tabindex="-1"></a>词向量训练使用的是固定窗口内的局部上下文，而忽略了更广泛的语篇信息。</span>
<span id="cb7-842"><a href="#cb7-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-843"><a href="#cb7-843" aria-hidden="true" tabindex="-1"></a>考虑这个例子：</span>
<span id="cb7-844"><a href="#cb7-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-845"><a href="#cb7-845" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The trophy would not fit in the suitcase because **it** was too big."</span>
<span id="cb7-846"><a href="#cb7-846" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The trophy would not fit in the suitcase because **it** was too small."</span>
<span id="cb7-847"><a href="#cb7-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-848"><a href="#cb7-848" aria-hidden="true" tabindex="-1"></a>第一句中"it"指代trophy，第二句中"it"指代suitcase。理解这种指代需要跨越整个句子的推理，而不仅仅是局部窗口内的共现。</span>
<span id="cb7-849"><a href="#cb7-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-850"><a href="#cb7-850" aria-hidden="true" tabindex="-1"></a>词向量的训练目标（预测局部上下文）天然无法捕获这种长距离依赖。</span>
<span id="cb7-851"><a href="#cb7-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-852"><a href="#cb7-852" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb7-853"><a href="#cb7-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-854"><a href="#cb7-854" aria-hidden="true" tabindex="-1"></a>静态词向量的成功和局限同时存在，这引出了一个自然的问题：能不能让词的表示**动态地依赖于上下文**？</span>
<span id="cb7-855"><a href="#cb7-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-856"><a href="#cb7-856" aria-hidden="true" tabindex="-1"></a>这正是接下来几章要讨论的问题。第11章的ELMo将展示如何用双向LSTM生成上下文相关的词表示，第13章的BERT将用Transformer的双向注意力实现更强大的上下文编码。这些方法被称为"上下文化词向量"（Contextualized Word Embeddings），它们建立在Word2Vec的基础上，但解决了静态表示的根本缺陷。</span>
<span id="cb7-857"><a href="#cb7-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-858"><a href="#cb7-858" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第4章将讨论一个常被忽视但极其重要的问题——Tokenization。词向量假设我们已经有了"词"，但什么是一个"词"？中文要不要分词？形态丰富的语言如何处理？这些问题比看起来复杂得多，而Tokenizer的设计直接影响模型的能力。</span></span>
<span id="cb7-859"><a href="#cb7-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-860"><a href="#cb7-860" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-861"><a href="#cb7-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-862"><a href="#cb7-862" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb7-863"><a href="#cb7-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-864"><a href="#cb7-864" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb7-865"><a href="#cb7-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-866"><a href="#cb7-866" aria-hidden="true" tabindex="-1"></a>这一章我们见证了NLP历史上的一次重要突破：从手工设计特征到自动学习表示。</span>
<span id="cb7-867"><a href="#cb7-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-868"><a href="#cb7-868" aria-hidden="true" tabindex="-1"></a>Word2Vec的核心洞察极其简洁：**一个词的含义由它的上下文定义**。通过训练模型预测词的上下文（Skip-gram）或从上下文预测词（CBOW），模型自动学习到了词的分布式表示。这些表示捕获了语义相似性——"cat"和"dog"的向量彼此接近，因为它们出现在相似的上下文中。</span>
<span id="cb7-869"><a href="#cb7-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-870"><a href="#cb7-870" aria-hidden="true" tabindex="-1"></a>更神奇的是，词向量空间呈现出规律的线性结构：king - man + woman ≈ queen。这说明向量空间不仅编码了相似性，还编码了语义关系。</span>
<span id="cb7-871"><a href="#cb7-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-872"><a href="#cb7-872" aria-hidden="true" tabindex="-1"></a>GloVe从全局统计的角度重新推导了词向量，FastText通过子词分解解决了OOV问题。这些改进丰富了词向量的生态。</span>
<span id="cb7-873"><a href="#cb7-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-874"><a href="#cb7-874" aria-hidden="true" tabindex="-1"></a>然而，所有这些方法都面临一个根本局限：**静态表示无法处理一词多义**。每个词只有一个向量，无论它出现在什么上下文中。这个问题的解决要等到ELMo和BERT的出现。</span>
<span id="cb7-875"><a href="#cb7-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-876"><a href="#cb7-876" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb7-877"><a href="#cb7-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-878"><a href="#cb7-878" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 含义 <span class="pp">|</span></span>
<span id="cb7-879"><a href="#cb7-879" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb7-880"><a href="#cb7-880" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P(w_o <span class="sc">\|</span> w_c) = \frac{\exp(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})}{\sum_w \exp(\mathbf{u}_w^\top \mathbf{v}_{w_c})}$ <span class="pp">|</span> Skip-gram的Softmax概率 <span class="pp">|</span></span>
<span id="cb7-881"><a href="#cb7-881" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\log \sigma(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}) + \sum_{i=1}^{k} \log \sigma(-\mathbf{u}_{w_i}^\top \mathbf{v}_{w_c})$ <span class="pp">|</span> 负采样目标函数 <span class="pp">|</span></span>
<span id="cb7-882"><a href="#cb7-882" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $P_n(w) \propto f(w)^{3/4}$ <span class="pp">|</span> 负采样分布 <span class="pp">|</span></span>
<span id="cb7-883"><a href="#cb7-883" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathbf{w}_i^\top \mathbf{w}_j + b_i + b_j = \log X_{ij}$ <span class="pp">|</span> GloVe目标 <span class="pp">|</span></span>
<span id="cb7-884"><a href="#cb7-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-885"><a href="#cb7-885" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb7-886"><a href="#cb7-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-887"><a href="#cb7-887" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么Skip-gram在处理罕见词时比CBOW更有效？从训练信号的角度分析。</span>
<span id="cb7-888"><a href="#cb7-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-889"><a href="#cb7-889" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 推导负采样目标函数的梯度。给定正样本$(w_c, w_o)$和负样本$w_1, \ldots, w_k$，写出$\partial \mathcal{L} / \partial \mathbf{v}_{w_c}$的表达式。</span>
<span id="cb7-890"><a href="#cb7-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-891"><a href="#cb7-891" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用Gensim在一个中等规模的语料库（如Wikipedia dump的一部分）上训练Word2Vec模型。比较不同超参数（维度、窗口大小、负采样数）对词类比任务准确率的影响。</span>
<span id="cb7-892"><a href="#cb7-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-893"><a href="#cb7-893" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** 词向量捕获了语料库中的偏见（如性别偏见）。你认为应该如何处理这个问题？完全去除偏见是否可行或可取？这个问题有什么更深层的哲学含义？</span>
<span id="cb7-894"><a href="#cb7-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-895"><a href="#cb7-895" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-896"><a href="#cb7-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-897"><a href="#cb7-897" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb7-898"><a href="#cb7-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-899"><a href="#cb7-899" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb7-900"><a href="#cb7-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-901"><a href="#cb7-901" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013)**：Word2Vec的原始论文</span>
<span id="cb7-902"><a href="#cb7-902" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 4（Skip-gram和CBOW的定义）</span>
<span id="cb7-903"><a href="#cb7-903" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>可跳过：实验的细节</span>
<span id="cb7-904"><a href="#cb7-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-905"><a href="#cb7-905" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013)**：引入负采样的论文</span>
<span id="cb7-906"><a href="#cb7-906" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2.2（负采样）、Section 4（短语向量）</span>
<span id="cb7-907"><a href="#cb7-907" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>这篇论文比第一篇更重要，因为负采样成为了标准做法</span>
<span id="cb7-908"><a href="#cb7-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-909"><a href="#cb7-909" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论分析</span></span>
<span id="cb7-910"><a href="#cb7-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-911"><a href="#cb7-911" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Neural Word Embedding as Implicit Matrix Factorization (Levy &amp; Goldberg, 2014)**：证明Word2Vec等价于PMI矩阵分解</span>
<span id="cb7-912"><a href="#cb7-912" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>这篇论文揭示了Word2Vec成功的数学原因</span>
<span id="cb7-913"><a href="#cb7-913" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>对理解为什么不同方法效果相近很有帮助</span>
<span id="cb7-914"><a href="#cb7-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-915"><a href="#cb7-915" aria-hidden="true" tabindex="-1"></a><span class="fu">### 改进方法</span></span>
<span id="cb7-916"><a href="#cb7-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-917"><a href="#cb7-917" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GloVe: Global Vectors for Word Representation (Pennington et al., 2014)**：GloVe的原始论文</span>
<span id="cb7-918"><a href="#cb7-918" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>从不同角度推导词向量，与Word2Vec殊途同归</span>
<span id="cb7-919"><a href="#cb7-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-920"><a href="#cb7-920" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Enriching Word Vectors with Subword Information (Bojanowski et al., 2017)**：FastText</span>
<span id="cb7-921"><a href="#cb7-921" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>解决OOV问题的优雅方案</span>
<span id="cb7-922"><a href="#cb7-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-923"><a href="#cb7-923" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb7-924"><a href="#cb7-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-925"><a href="#cb7-925" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Word2Vec Tutorial (Chris McCormick)**：最通俗的Word2Vec解释</span>
<span id="cb7-926"><a href="#cb7-926" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>适合入门，有很好的可视化</span>
<span id="cb7-927"><a href="#cb7-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-928"><a href="#cb7-928" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Speech and Language Processing, Chapter 6 (Jurafsky &amp; Martin)**：词向量的教科书级介绍</span>
<span id="cb7-929"><a href="#cb7-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-930"><a href="#cb7-930" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb7-931"><a href="#cb7-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-932"><a href="#cb7-932" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gensim库**：Word2Vec/FastText的标准Python实现</span>
<span id="cb7-933"><a href="#cb7-933" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford的GloVe官方代码**：C实现，效率很高</span>
<span id="cb7-934"><a href="#cb7-934" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face的预训练模型**：可以直接加载各种预训练词向量</span>
<span id="cb7-935"><a href="#cb7-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-936"><a href="#cb7-936" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-937"><a href="#cb7-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-938"><a href="#cb7-938" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb7-939"><a href="#cb7-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-940"><a href="#cb7-940" aria-hidden="true" tabindex="-1"></a>Word2Vec的成功有一个有趣的历史背景。Tomas Mikolov在发表Word2Vec之前，已经在循环神经网络语言模型上做了多年工作。他发现RNN语言模型的隐藏层可以作为词的表示，但训练太慢、无法扩展。Word2Vec可以看作是把这个思想极简化——用更简单的模型（没有非线性隐藏层），换取在大规模数据上训练的能力。</span>
<span id="cb7-941"><a href="#cb7-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-942"><a href="#cb7-942" aria-hidden="true" tabindex="-1"></a>另一个有趣的细节是，Word2Vec论文被ICLR 2013拒稿了。审稿人认为模型太简单、理论贡献不足。后来Mikolov把论文放到arXiv上，它迅速成为NLP领域引用最多的论文之一。这个故事说明，有时候最有影响力的工作不是最"复杂"的，而是找到了正确的简化。</span>
<span id="cb7-943"><a href="#cb7-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-944"><a href="#cb7-944" aria-hidden="true" tabindex="-1"></a>词向量类比的发现也颇为戏剧性。据说Mikolov在调试代码时偶然发现 king - man + woman 会得到接近queen的向量，起初以为是bug。后来才意识到这是一个深刻的发现——向量空间自动学到了语义关系的线性结构。这个发现极大地推动了对词向量的兴趣，因为它暗示着向量空间中可能编码了我们尚未完全理解的语言知识。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>