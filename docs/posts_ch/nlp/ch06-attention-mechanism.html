<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-25">
<meta name="description" content="注意力机制的完整故事：从Bahdanau打破Seq2Seq信息瓶颈，到Luong的系统性探索（加性vs乘性、全局vs局部、软vs硬），再到注意力独立于RNN的前奏。">

<title>第6章：注意力机制的诞生与演进 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#问题的精确定义" id="toc-问题的精确定义" class="nav-link" data-scroll-target="#问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</a></li>
  <li><a href="#之前的尝试为何失败" id="toc-之前的尝试为何失败" class="nav-link" data-scroll-target="#之前的尝试为何失败"><span class="header-section-number">2.2</span> 之前的尝试为何失败？</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#关键洞察动态的基于内容的寻址" id="toc-关键洞察动态的基于内容的寻址" class="nav-link" data-scroll-target="#关键洞察动态的基于内容的寻址"><span class="header-section-number">3.1</span> 关键洞察：动态的、基于内容的寻址</a></li>
  <li><a href="#直觉解释聚光灯与图书馆" id="toc-直觉解释聚光灯与图书馆" class="nav-link" data-scroll-target="#直觉解释聚光灯与图书馆"><span class="header-section-number">3.2</span> 直觉解释：聚光灯与图书馆</a></li>
  <li><a href="#另一个类比加权投票" id="toc-另一个类比加权投票" class="nav-link" data-scroll-target="#另一个类比加权投票"><span class="header-section-number">3.3</span> 另一个类比：加权投票</a></li>
  <li><a href="#设计动机为什么选择软注意力" id="toc-设计动机为什么选择软注意力" class="nav-link" data-scroll-target="#设计动机为什么选择软注意力"><span class="header-section-number">3.4</span> 设计动机：为什么选择软注意力？</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#bahdanau-attention加性注意力" id="toc-bahdanau-attention加性注意力" class="nav-link" data-scroll-target="#bahdanau-attention加性注意力"><span class="header-section-number">4.1</span> Bahdanau Attention：加性注意力</a></li>
  <li><a href="#完整数值示例bahdanau-attention计算" id="toc-完整数值示例bahdanau-attention计算" class="nav-link" data-scroll-target="#完整数值示例bahdanau-attention计算"><span class="header-section-number">4.2</span> 完整数值示例：Bahdanau Attention计算</a></li>
  <li><a href="#解码器的完整流程" id="toc-解码器的完整流程" class="nav-link" data-scroll-target="#解码器的完整流程"><span class="header-section-number">4.3</span> 解码器的完整流程</a></li>
  <li><a href="#luong-attention乘性注意力的系统性探索" id="toc-luong-attention乘性注意力的系统性探索" class="nav-link" data-scroll-target="#luong-attention乘性注意力的系统性探索"><span class="header-section-number">4.4</span> Luong Attention：乘性注意力的系统性探索</a></li>
  <li><a href="#三种对齐函数的数值对比" id="toc-三种对齐函数的数值对比" class="nav-link" data-scroll-target="#三种对齐函数的数值对比"><span class="header-section-number">4.5</span> 三种对齐函数的数值对比</a></li>
  <li><a href="#global-vs-local-attention" id="toc-global-vs-local-attention" class="nav-link" data-scroll-target="#global-vs-local-attention"><span class="header-section-number">4.6</span> Global vs Local Attention</a></li>
  <li><a href="#hard-vs-soft-attention" id="toc-hard-vs-soft-attention" class="nav-link" data-scroll-target="#hard-vs-soft-attention"><span class="header-section-number">4.7</span> Hard vs Soft Attention</a></li>
  <li><a href="#复杂度分析" id="toc-复杂度分析" class="nav-link" data-scroll-target="#复杂度分析"><span class="header-section-number">4.8</span> 复杂度分析</a></li>
  </ul></li>
  <li><a href="#注意力可视化模型在看什么" id="toc-注意力可视化模型在看什么" class="nav-link" data-scroll-target="#注意力可视化模型在看什么"><span class="header-section-number">5</span> 注意力可视化：模型在”看”什么？</a>
  <ul class="collapse">
  <li><a href="#对齐矩阵" id="toc-对齐矩阵" class="nav-link" data-scroll-target="#对齐矩阵"><span class="header-section-number">5.1</span> 对齐矩阵</a></li>
  <li><a href="#对齐模式的语言学意义" id="toc-对齐模式的语言学意义" class="nav-link" data-scroll-target="#对齐模式的语言学意义"><span class="header-section-number">5.2</span> 对齐模式的语言学意义</a></li>
  <li><a href="#可视化的局限性" id="toc-可视化的局限性" class="nav-link" data-scroll-target="#可视化的局限性"><span class="header-section-number">5.3</span> 可视化的局限性</a></li>
  </ul></li>
  <li><a href="#工程实践带attention的seq2seq" id="toc-工程实践带attention的seq2seq" class="nav-link" data-scroll-target="#工程实践带attention的seq2seq"><span class="header-section-number">6</span> 工程实践：带Attention的Seq2Seq</a>
  <ul class="collapse">
  <li><a href="#编码器" id="toc-编码器" class="nav-link" data-scroll-target="#编码器"><span class="header-section-number">6.1</span> 编码器</a></li>
  <li><a href="#bahdanau-attention层" id="toc-bahdanau-attention层" class="nav-link" data-scroll-target="#bahdanau-attention层"><span class="header-section-number">6.2</span> Bahdanau Attention层</a></li>
  <li><a href="#解码器" id="toc-解码器" class="nav-link" data-scroll-target="#解码器"><span class="header-section-number">6.3</span> 解码器</a></li>
  <li><a href="#完整的seq2seq模型" id="toc-完整的seq2seq模型" class="nav-link" data-scroll-target="#完整的seq2seq模型"><span class="header-section-number">6.4</span> 完整的Seq2Seq模型</a></li>
  <li><a href="#luong-attention实现" id="toc-luong-attention实现" class="nav-link" data-scroll-target="#luong-attention实现"><span class="header-section-number">6.5</span> Luong Attention实现</a></li>
  <li><a href="#local-attention实现" id="toc-local-attention实现" class="nav-link" data-scroll-target="#local-attention实现"><span class="header-section-number">6.6</span> Local Attention实现</a></li>
  <li><a href="#对比实验" id="toc-对比实验" class="nav-link" data-scroll-target="#对比实验"><span class="header-section-number">6.7</span> 对比实验</a></li>
  <li><a href="#关键实现细节" id="toc-关键实现细节" class="nav-link" data-scroll-target="#关键实现细节"><span class="header-section-number">6.8</span> 关键实现细节</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">7</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么attention有效理论视角" id="toc-为什么attention有效理论视角" class="nav-link" data-scroll-target="#为什么attention有效理论视角"><span class="header-section-number">7.1</span> 为什么Attention有效？——理论视角</a></li>
  <li><a href="#为什么点积注意力能工作" id="toc-为什么点积注意力能工作" class="nav-link" data-scroll-target="#为什么点积注意力能工作"><span class="header-section-number">7.2</span> 为什么点积注意力能工作？</a></li>
  <li><a href="#为什么需要缩放" id="toc-为什么需要缩放" class="nav-link" data-scroll-target="#为什么需要缩放"><span class="header-section-number">7.3</span> 为什么需要缩放？</a></li>
  <li><a href="#边界条件与失效模式" id="toc-边界条件与失效模式" class="nav-link" data-scroll-target="#边界条件与失效模式"><span class="header-section-number">7.4</span> 边界条件与失效模式</a></li>
  </ul></li>
  <li><a href="#局限性与展望" id="toc-局限性与展望" class="nav-link" data-scroll-target="#局限性与展望"><span class="header-section-number">8</span> 局限性与展望</a></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">9</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">9.1</span> 关键公式速查</a></li>
  </ul></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">10</span> 思考题</a></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">11</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">11.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">11.3</span> 后续发展</a></li>
  <li><a href="#注意力可解释性" id="toc-注意力可解释性" class="nav-link" data-scroll-target="#注意力可解释性"><span class="header-section-number">11.4</span> 注意力可解释性</a></li>
  <li><a href="#大规模实证对比" id="toc-大规模实证对比" class="nav-link" data-scroll-target="#大规模实证对比"><span class="header-section-number">11.5</span> 大规模实证对比</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">12</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第6章：注意力机制的诞生与演进</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">从信息瓶颈到动态聚焦：Bahdanau加性注意力、Luong乘性注意力，以及注意力设计空间的系统探索</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Attention</div>
    <div class="quarto-category">Seq2Seq</div>
    <div class="quarto-category">机器翻译</div>
    <div class="quarto-category">Bahdanau</div>
    <div class="quarto-category">Luong</div>
  </div>
  </div>

<div>
  <div class="description">
    注意力机制的完整故事：从Bahdanau打破Seq2Seq信息瓶颈，到Luong的系统性探索（加性vs乘性、全局vs局部、软vs硬），再到注意力独立于RNN的前奏。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：如何让解码器在生成每个词时，能够访问输入序列的不同部分，而不是只依赖一个压缩后的向量？进一步地，计算”注意力”的最佳方式是什么——加性、乘性、全局还是局部？</p>
<p><strong>历史坐标</strong>：2014-2015 | Bahdanau, Cho, Bengio → Luong, Pham, Manning | 从注意力的诞生到系统性探索</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了RNN的辉煌与困境。LSTM和GRU通过门控机制解决了梯度消失问题，Seq2Seq架构让神经网络能够处理翻译、摘要等序列到序列的任务。</p>
<p>但Seq2Seq有一个致命的设计缺陷：<strong>信息瓶颈</strong>。</p>
<p>回顾Seq2Seq的工作方式：编码器读取整个输入序列，将所有信息压缩到一个固定长度的上下文向量 <span class="math inline">\(\mathbf{c}\)</span> 中；解码器仅凭这个向量，逐词生成输出。这意味着，无论输入是5个词还是50个词，所有信息都要塞进同一个维度的向量。</p>
<p>Sutskever等人(2014)的实验清楚地展示了这个问题：当输入句子超过20个词时，翻译质量急剧下降。更长的句子包含更多信息，而固定大小的向量无法承载。</p>
<p>让我们用一个具体的例子感受这个问题。考虑翻译任务：</p>
<blockquote class="blockquote">
<p><strong>英语</strong>：The agreement on the European Economic Area was signed in August 1992.</p>
<p><strong>法语</strong>：L’accord sur la zone économique européenne a été signé en août 1992.</p>
</blockquote>
<p>当解码器生成”août”（八月）时，它需要知道原文中的”August”。但在标准Seq2Seq中，“August”这个词首先被编码进隐藏状态，然后与其他所有词的信息混合在一起，最终压缩成上下文向量 <span class="math inline">\(\mathbf{c}\)</span>。解码器要从这个压缩后的向量中”挖出”August的信息——这就像从一锅汤里找回原来的食材。</p>
<p>更糟糕的是，句子中的某些词对当前生成的词更重要。翻译”August”时，模型最需要关注的是原文中的”August”，而不是”The”或”was”。但标准Seq2Seq对所有输入位置一视同仁——它们都被同等地压缩进了 <span class="math inline">\(\mathbf{c}\)</span>。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：解码器在生成每个词时，应该能够<strong>有选择地关注</strong>输入序列的不同位置。不同的输出词需要关注不同的输入词——这就是”注意力”的本质。本章将追溯注意力机制从诞生（Bahdanau, 2014）到系统性探索（Luong, 2015）的完整历程，揭示不同设计选择背后的权衡。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="问题的精确定义" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</h3>
<p>让我们形式化地描述Seq2Seq的信息瓶颈问题。</p>
<p>在标准Seq2Seq中，编码器产生一系列隐藏状态 <span class="math inline">\(\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, \ldots, \mathbf{h}_T^{enc}\)</span>，但只有最后一个状态 <span class="math inline">\(\mathbf{h}_T^{enc}\)</span> 被传递给解码器作为上下文向量：</p>
<p><span class="math display">\[
\mathbf{c} = \mathbf{h}_T^{enc}
\]</span></p>
<p>解码器的每一步都使用这同一个 <span class="math inline">\(\mathbf{c}\)</span>：</p>
<p><span class="math display">\[
\mathbf{h}_t^{dec} = f(\mathbf{h}_{t-1}^{dec}, y_{t-1}, \mathbf{c})
\]</span></p>
<p>问题在于：<span class="math inline">\(\mathbf{c}\)</span> 是一个<strong>静态</strong>的、<strong>全局</strong>的表示。它在解码的每一步都保持不变，无法根据当前生成的词动态调整。</p>
<p>从信息论的角度看，如果输入序列 <span class="math inline">\(\mathbf{x}\)</span> 的信息熵是 <span class="math inline">\(H(\mathbf{x})\)</span>，而 <span class="math inline">\(\mathbf{c}\)</span> 的维度是 <span class="math inline">\(d\)</span>，那么 <span class="math inline">\(\mathbf{c}\)</span> 最多能携带 <span class="math inline">\(O(d)\)</span> 的信息量。当 <span class="math inline">\(H(\mathbf{x}) &gt; O(d)\)</span> 时，信息丢失是不可避免的。</p>
</section>
<section id="之前的尝试为何失败" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="之前的尝试为何失败"><span class="header-section-number">2.2</span> 之前的尝试为何失败？</h3>
<p>在Attention出现之前，研究者尝试过一些缓解信息瓶颈的方法：</p>
<p><strong>增加上下文向量维度</strong>：直觉上，更大的 <span class="math inline">\(\mathbf{c}\)</span> 可以携带更多信息。但这只是延缓问题，而非解决问题。而且更大的向量意味着更多参数，更容易过拟合。</p>
<p><strong>使用双向RNN</strong>：让编码器同时从左到右和从右到左读取输入，然后拼接两个方向的最终隐藏状态。这确实能捕获更多上下文，但仍然是压缩到一个固定向量——只是这个向量稍微大了一点。</p>
<p><strong>输入反转</strong>：Sutskever等人发现，将输入序列反转后再输入编码器，翻译效果更好。这是因为输入的最后几个词（反转后变成最先输入的词）与输出的最先几个词往往有更强的对应关系。但这只是一个启发式技巧，不能从根本上解决问题。</p>
<p>这些方法都没有触及问题的核心：<strong>解码器只能看到一个固定的、全局的表示，无法动态地访问输入的不同部分</strong>。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>理想的解决方案应该具备以下特性：</p>
<ol type="1">
<li><strong>动态性</strong>：解码器在生成不同词时，应该能够关注输入的不同位置</li>
<li><strong>软选择</strong>：不是硬性地选择某一个位置，而是对所有位置计算一个重要性分布</li>
<li><strong>端到端可训练</strong>：整个机制应该可以通过反向传播优化</li>
<li><strong>可解释性</strong>：模型关注哪些位置应该是可以观察和理解的</li>
</ol>
<p>这些特性正是Attention机制所提供的。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="关键洞察动态的基于内容的寻址" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="关键洞察动态的基于内容的寻址"><span class="header-section-number">3.1</span> 关键洞察：动态的、基于内容的寻址</h3>
<p>Attention的核心洞察可以用一句话概括：</p>
<blockquote class="blockquote">
<p><strong>让解码器在每一步都能”回头看”编码器的所有位置，并根据当前需要动态决定关注哪些位置。</strong></p>
</blockquote>
<p>这个想法听起来简单，但它彻底改变了序列到序列学习的范式。</p>
</section>
<section id="直觉解释聚光灯与图书馆" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="直觉解释聚光灯与图书馆"><span class="header-section-number">3.2</span> 直觉解释：聚光灯与图书馆</h3>
<p>想象你在一个黑暗的图书馆里找书。传统Seq2Seq就像是：你先用手电筒快速扫过所有书架，然后关掉手电筒，仅凭记忆去取书。你对整个图书馆有一个模糊的整体印象，但细节很容易遗忘。</p>
<p>Attention机制则像是：你手里有一个<strong>可调节的聚光灯</strong>。当你需要找某本书时，你可以把聚光灯照向相关的书架，仔细查看那里的书名。不同的查询需求会让你把光照向不同的位置。</p>
<p>更具体地说，当解码器生成”août”（八月）这个词时，Attention机制会：</p>
<ol type="1">
<li>查看编码器的所有隐藏状态（图书馆的所有书架）</li>
<li>计算每个位置与当前任务的相关性（判断每个书架是否可能有你要的书）</li>
<li>把”聚光灯”主要照向相关的位置（“August”对应的编码器状态）</li>
<li>从这些位置汇总信息，辅助生成当前词</li>
</ol>
</section>
<section id="另一个类比加权投票" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="另一个类比加权投票"><span class="header-section-number">3.3</span> 另一个类比：加权投票</h3>
<p>你也可以把Attention理解为一种加权投票机制。</p>
<p>想象解码器是一个领导，需要做一个决定（生成下一个词）。它有一个顾问团队（编码器的各个隐藏状态），每个顾问掌握输入序列不同部分的信息。</p>
<p>传统Seq2Seq：只听一个”总顾问”的意见（上下文向量 <span class="math inline">\(\mathbf{c}\)</span>），这个总顾问要综合所有人的信息。</p>
<p>Attention机制：直接征询每个顾问的意见，然后根据议题相关性给不同顾问的意见赋予不同权重，加权求和得出最终决定。</p>
</section>
<section id="设计动机为什么选择软注意力" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="设计动机为什么选择软注意力"><span class="header-section-number">3.4</span> 设计动机：为什么选择软注意力？</h3>
<p>Attention机制有两种基本变体：</p>
<ul>
<li><strong>软注意力（Soft Attention）</strong>：对所有位置计算概率分布，加权求和</li>
<li><strong>硬注意力（Hard Attention）</strong>：选择一个位置，只看那里的信息</li>
</ul>
<p>Bahdanau等人选择了软注意力，原因是：</p>
<ol type="1">
<li><strong>可微分</strong>：软注意力的加权求和是可微的，可以用标准的反向传播训练</li>
<li><strong>稳定</strong>：硬注意力需要采样或强化学习方法训练，方差大，不稳定</li>
<li><strong>信息更丰富</strong>：软注意力可以同时利用多个位置的信息，而不是非此即彼</li>
</ol>
<p>硬注意力也有其优势（计算更高效，更稀疏），但在实践中，软注意力因其简单和有效成为了主流。我们将在后面的小节中更详细地对比两者。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="bahdanau-attention加性注意力" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="bahdanau-attention加性注意力"><span class="header-section-number">4.1</span> Bahdanau Attention：加性注意力</h3>
<p>2014年，Bahdanau、Cho和Bengio提出了第一个成功的注意力机制用于机器翻译。让我们详细看看它是如何工作的。</p>
<p>首先，编码器使用<strong>双向RNN</strong>，在每个位置 <span class="math inline">\(j\)</span> 产生一个隐藏状态：</p>
<p><span class="math display">\[
\mathbf{h}_j = [\overrightarrow{\mathbf{h}}_j; \overleftarrow{\mathbf{h}}_j]
\]</span></p>
<p>其中 <span class="math inline">\(\overrightarrow{\mathbf{h}}_j\)</span> 是前向RNN的隐藏状态，<span class="math inline">\(\overleftarrow{\mathbf{h}}_j\)</span> 是后向RNN的隐藏状态。拼接后，<span class="math inline">\(\mathbf{h}_j\)</span> 同时包含了位置 <span class="math inline">\(j\)</span> 的左侧和右侧上下文。</p>
<p>在解码的第 <span class="math inline">\(i\)</span> 步，我们计算一个<strong>动态的上下文向量</strong> <span class="math inline">\(\mathbf{c}_i\)</span>（注意：不再是固定的 <span class="math inline">\(\mathbf{c}\)</span>，而是每一步都不同的 <span class="math inline">\(\mathbf{c}_i\)</span>）：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
\]</span></p>
<p>其中 <span class="math inline">\(\alpha_{ij}\)</span> 是第 <span class="math inline">\(i\)</span> 步解码时，对输入位置 <span class="math inline">\(j\)</span> 的注意力权重。</p>
<p>那么 <span class="math inline">\(\alpha_{ij}\)</span> 是怎么计算的呢？这是Attention机制的核心。</p>
<p>首先，计算一个<strong>对齐分数（alignment score）</strong> <span class="math inline">\(e_{ij}\)</span>，衡量解码器当前状态与编码器位置 <span class="math inline">\(j\)</span> 的相关性：</p>
<p><span class="math display">\[
e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{s}_{i-1}\)</span> 是解码器在第 <span class="math inline">\(i-1\)</span> 步的隐藏状态，<span class="math inline">\(a\)</span> 是一个<strong>对齐模型（alignment model）</strong>。</p>
<p>Bahdanau使用了一个单层前馈网络作为对齐模型：</p>
<p><span class="math display">\[
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
\]</span></p>
<p>这被称为<strong>加性注意力（additive attention）</strong>，因为 <span class="math inline">\(\mathbf{s}_{i-1}\)</span> 和 <span class="math inline">\(\mathbf{h}_j\)</span> 是通过加法结合的。</p>
<p>然后，对所有位置的分数做softmax归一化，得到注意力权重：</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
\]</span></p>
<p>softmax确保了：</p>
<ul>
<li>所有权重都是正数：<span class="math inline">\(\alpha_{ij} &gt; 0\)</span></li>
<li>权重之和为1：<span class="math inline">\(\sum_j \alpha_{ij} = 1\)</span></li>
</ul>
<p>这样，<span class="math inline">\(\alpha_{ij}\)</span> 可以解释为一个概率分布——解码器在第 <span class="math inline">\(i\)</span> 步”关注”输入位置 <span class="math inline">\(j\)</span> 的概率。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Bahdanau Attention (Bahdanau et al., 2015)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Bahdanau (加性) 注意力机制</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        s_prev: 解码器上一步的隐藏状态 [batch, dec_hidden]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        W_a, U_a, v_a: 可学习参数</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        context: 上下文向量 [batch, enc_hidden]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_weights: 注意力权重 [batch, src_len]</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 计算对齐分数</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># s_prev 广播到所有源位置</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> v_a <span class="op">@</span> tanh(W_a <span class="op">@</span> s_prev <span class="op">+</span> U_a <span class="op">@</span> encoder_outputs)  <span class="co"># [batch, src_len]</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Softmax 归一化</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 加权求和</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> attention_weights <span class="op">@</span> encoder_outputs  <span class="co"># [batch, enc_hidden]</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Bahdanau, Cho, &amp; Bengio (2015) “Neural Machine Translation by Jointly Learning to Align and Translate”, ICLR 2015. <a href="https://arxiv.org/abs/1409.0473">arXiv:1409.0473</a></em></p>
</div>
</div>
<p>下图展示了带Bahdanau Attention的RNN Encoder-Decoder架构：</p>
<div id="fig-attention-mechanism" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-mechanism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-6/original/fig-bahdanau-attention-d2l.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-mechanism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 带Bahdanau Attention的RNN Encoder-Decoder架构。编码器（底部）使用双向RNN处理输入序列，产生隐藏状态序列。解码器（顶部）在每一步通过Attention机制动态计算上下文向量：将当前解码器状态与所有编码器状态比较，得到注意力权重，加权求和得到上下文向量<span class="math inline">\(c_t\)</span>，辅助生成下一个词。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Dive into Deep Learning, Figure 11.4.2. <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">d2l.ai</a></em></p>
</div>
</section>
<section id="完整数值示例bahdanau-attention计算" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="完整数值示例bahdanau-attention计算"><span class="header-section-number">4.2</span> 完整数值示例：Bahdanau Attention计算</h3>
<p>让我们用一个小例子走一遍完整的Attention计算过程。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>输入序列：3个词（“I love NLP”），编码后得到3个隐藏状态</li>
<li>解码器隐藏状态维度：<span class="math inline">\(d_s = 4\)</span></li>
<li>编码器隐藏状态维度：<span class="math inline">\(d_h = 4\)</span></li>
<li>注意力中间维度：<span class="math inline">\(d_a = 3\)</span></li>
</ul>
<p><strong>编码器输出</strong>（假设已经计算好）：</p>
<p><span class="math display">\[
\mathbf{h}_1 = [0.2, 0.5, -0.3, 0.8]^\top \quad \text{("I")}
\]</span></p>
<p><span class="math display">\[
\mathbf{h}_2 = [0.7, -0.2, 0.4, 0.1]^\top \quad \text{("love")}
\]</span></p>
<p><span class="math display">\[
\mathbf{h}_3 = [-0.1, 0.6, 0.5, -0.4]^\top \quad \text{("NLP")}
\]</span></p>
<p><strong>解码器当前状态</strong>（正在生成第一个目标词）：</p>
<p><span class="math display">\[
\mathbf{s}_0 = [0.1, -0.3, 0.4, 0.2]^\top
\]</span></p>
<p><strong>参数</strong>（简化的随机值）：</p>
<p><span class="math display">\[
\mathbf{W}_a = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.1 \\ 0.2 &amp; 0.1 &amp; -0.1 &amp; 0.2 \\ -0.1 &amp; 0.3 &amp; 0.2 &amp; -0.2 \end{bmatrix}, \quad
\mathbf{U}_a = \begin{bmatrix} 0.2 &amp; 0.1 &amp; -0.2 &amp; 0.3 \\ -0.1 &amp; 0.2 &amp; 0.1 &amp; 0.1 \\ 0.3 &amp; -0.1 &amp; 0.2 &amp; -0.1 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{v}_a = [0.5, -0.3, 0.4]^\top
\]</span></p>
<p><strong>Step 1：计算 <span class="math inline">\(\mathbf{W}_a \mathbf{s}_0\)</span></strong></p>
<p><span class="math display">\[
\mathbf{W}_a \mathbf{s}_0 = \begin{bmatrix} 0.1 \cdot 0.1 + (-0.2) \cdot (-0.3) + 0.3 \cdot 0.4 + 0.1 \cdot 0.2 \\ \vdots \end{bmatrix} = \begin{bmatrix} 0.21 \\ 0.03 \\ 0.04 \end{bmatrix}
\]</span></p>
<p><strong>Step 2：对每个编码器状态计算 <span class="math inline">\(\mathbf{U}_a \mathbf{h}_j\)</span></strong></p>
<p><span class="math display">\[
\mathbf{U}_a \mathbf{h}_1 = [0.33, 0.14, -0.05]^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{U}_a \mathbf{h}_2 = [0.13, 0.06, 0.29]^\top
\]</span></p>
<p><span class="math display">\[
\mathbf{U}_a \mathbf{h}_3 = [-0.15, 0.17, 0.05]^\top
\]</span></p>
<p><strong>Step 3：计算对齐分数 <span class="math inline">\(e_{1j}\)</span></strong></p>
<p><span class="math display">\[
e_{11} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_0 + \mathbf{U}_a \mathbf{h}_1) = \mathbf{v}_a^\top \tanh([0.54, 0.17, -0.01]^\top)
\]</span></p>
<p><span class="math display">\[
= [0.5, -0.3, 0.4] \cdot [\tanh(0.54), \tanh(0.17), \tanh(-0.01)]^\top
\]</span></p>
<p><span class="math display">\[
= [0.5, -0.3, 0.4] \cdot [0.49, 0.17, -0.01]^\top = 0.24 - 0.05 - 0.004 \approx 0.19
\]</span></p>
<p>类似地计算 <span class="math inline">\(e_{12}\)</span> 和 <span class="math inline">\(e_{13}\)</span>：</p>
<p><span class="math display">\[
e_{12} \approx 0.25, \quad e_{13} \approx 0.08
\]</span></p>
<p><strong>Step 4：Softmax归一化</strong></p>
<p><span class="math display">\[
\alpha_{11} = \frac{\exp(0.19)}{\exp(0.19) + \exp(0.25) + \exp(0.08)} = \frac{1.21}{1.21 + 1.28 + 1.08} = \frac{1.21}{3.57} \approx 0.34
\]</span></p>
<p><span class="math display">\[
\alpha_{12} = \frac{1.28}{3.57} \approx 0.36, \quad \alpha_{13} = \frac{1.08}{3.57} \approx 0.30
\]</span></p>
<p><strong>Step 5：计算上下文向量</strong></p>
<p><span class="math display">\[
\mathbf{c}_1 = \alpha_{11} \mathbf{h}_1 + \alpha_{12} \mathbf{h}_2 + \alpha_{13} \mathbf{h}_3
\]</span></p>
<p><span class="math display">\[
= 0.34 \cdot [0.2, 0.5, -0.3, 0.8]^\top + 0.36 \cdot [0.7, -0.2, 0.4, 0.1]^\top + 0.30 \cdot [-0.1, 0.6, 0.5, -0.4]^\top
\]</span></p>
<p><span class="math display">\[
\approx [0.29, 0.18, 0.09, 0.19]^\top
\]</span></p>
<p><strong>解读</strong>：在这个例子中，模型对”love”的关注最多（0.36），其次是”I”（0.34）和”NLP”（0.30）。注意力权重相对均匀，这可能是因为我们用的是随机参数。在训练后的真实模型中，权重分布会更加尖锐——模型会学会在需要时聚焦于特定位置。</p>
</section>
<section id="解码器的完整流程" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="解码器的完整流程"><span class="header-section-number">4.3</span> 解码器的完整流程</h3>
<p>有了Attention机制，解码器的每一步工作流程变为：</p>
<ol type="1">
<li><strong>计算注意力权重</strong> <span class="math inline">\(\alpha_{ij}\)</span>：基于当前解码器状态和所有编码器状态</li>
<li><strong>计算上下文向量</strong> <span class="math inline">\(\mathbf{c}_i\)</span>：对编码器状态加权求和</li>
<li><strong>更新解码器状态</strong>：结合上下文向量、前一步输出、前一步状态</li>
</ol>
<p><span class="math display">\[
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)
\]</span></p>
<ol start="4" type="1">
<li><strong>生成输出</strong>：基于新的解码器状态</li>
</ol>
<p><span class="math display">\[
P(y_i | y_{&lt;i}, \mathbf{x}) = g(\mathbf{s}_i, y_{i-1}, \mathbf{c}_i)
\]</span></p>
<p>关键区别是：<strong>每一步都有一个不同的上下文向量 <span class="math inline">\(\mathbf{c}_i\)</span></strong>，它是根据当前任务动态计算的。</p>
</section>
<section id="luong-attention乘性注意力的系统性探索" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="luong-attention乘性注意力的系统性探索"><span class="header-section-number">4.4</span> Luong Attention：乘性注意力的系统性探索</h3>
<p>Bahdanau的加性注意力虽然有效，但它的计算量不小——每次计算对齐分数都需要一个前馈网络（两个矩阵乘法、一个非线性激活、一个向量点积）。一个自然的问题随之浮现：能否用更简单的操作来衡量两个状态的相关性？</p>
<p>2015年，距Bahdanau论文发表仅一年，斯坦福大学的Luong、Pham和Manning发表了一篇系统性的研究，回答了这个问题。他们提出了计算效率更高的<strong>乘性注意力（multiplicative attention）</strong>，并系统比较了三种对齐函数：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>名称</th>
<th>公式</th>
<th>参数</th>
<th>计算效率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Dot</strong></td>
<td><span class="math inline">\(\mathbf{s}^\top \mathbf{h}\)</span></td>
<td>无</td>
<td>最快</td>
</tr>
<tr class="even">
<td><strong>General</strong></td>
<td><span class="math inline">\(\mathbf{s}^\top \mathbf{W}_a \mathbf{h}\)</span></td>
<td><span class="math inline">\(\mathbf{W}_a\)</span></td>
<td>中等</td>
</tr>
<tr class="odd">
<td><strong>Concat</strong></td>
<td><span class="math inline">\(\mathbf{v}_a^\top \tanh(\mathbf{W}_a [\mathbf{s}; \mathbf{h}])\)</span></td>
<td><span class="math inline">\(\mathbf{v}_a, \mathbf{W}_a\)</span></td>
<td>最慢</td>
</tr>
</tbody>
</table>
<p><strong>Dot（点积）</strong>：最简单的形式，直接计算两个向量的内积 <span class="math inline">\(e_{ij} = \mathbf{s}_i^\top \mathbf{h}_j\)</span>。没有任何可学习参数，计算最快，但要求解码器和编码器的隐藏维度必须相同。直觉上，点积衡量的是两个向量的”相似度”——如果它们指向相似的方向，点积就大；如果正交，点积为零。</p>
<p><strong>General（一般形式）</strong>：引入一个可学习矩阵 <span class="math inline">\(\mathbf{W}_a\)</span>，允许不同维度的状态相互比较，也增加了模型的表达能力。实质上是问：“<span class="math inline">\(\mathbf{s}\)</span> 和 <span class="math inline">\(\mathbf{W}_a \mathbf{h}\)</span>（<span class="math inline">\(\mathbf{h}\)</span> 的一个线性变换）有多相似？”</p>
<p><strong>Concat（拼接）</strong>：这就是Bahdanau的加性注意力。表达能力最强——它可以学习任意非线性的对齐函数——但计算最慢。</p>
<p>除了对齐函数，Luong还发现了另一个重要的设计差异：<strong>注意力在解码器中的使用位置</strong>。</p>
<p>Bahdanau的方式是先算注意力再更新RNN状态：<span class="math inline">\(\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)\)</span>。Luong的方式则更加模块化——先用RNN计算新状态，再基于新状态计算注意力：</p>
<p><span class="math display">\[
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1})
\]</span> <span class="math display">\[
\mathbf{c}_i = \text{Attention}(\mathbf{s}_i, \mathbf{H})
\]</span> <span class="math display">\[
\tilde{\mathbf{s}}_i = \tanh(\mathbf{W}_c [\mathbf{c}_i; \mathbf{s}_i])
\]</span></p>
<p>这种设计让RNN和Attention解耦，便于分析和调试。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Luong Attention Variants (Luong et al., 2015)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> luong_attention(decoder_state, encoder_outputs, method<span class="op">=</span><span class="st">'dot'</span>, W_a<span class="op">=</span><span class="va">None</span>, v_a<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Luong 注意力机制的三种变体</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_state: 解码器当前隐藏状态 [batch, dec_hidden]</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">        method: 'dot', 'general', 或 'concat'</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        W_a: 可学习参数（general和concat需要）</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        v_a: 可学习参数（concat需要）</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">        context: 上下文向量 [batch, enc_hidden]</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_weights: 注意力权重 [batch, src_len]</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> method <span class="op">==</span> <span class="st">'dot'</span>:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 点积: s^T h</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, dec_hidden] @ [batch, enc_hidden, src_len] -&gt; [batch, src_len]</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>                          encoder_outputs.transpose(<span class="dv">1</span>, <span class="dv">2</span>)).squeeze(<span class="dv">1</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'general'</span>:</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 一般形式: s^T W h</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 先计算 W @ h: [batch, src_len, dec_hidden]</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        transformed <span class="op">=</span> encoder_outputs <span class="op">@</span> W_a.T</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                          transformed.transpose(<span class="dv">1</span>, <span class="dv">2</span>)).squeeze(<span class="dv">1</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'concat'</span>:</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接形式: v^T tanh(W [s; h])</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 扩展 decoder_state 到所有位置</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        s_expanded <span class="op">=</span> decoder_state.unsqueeze(<span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>, encoder_outputs.size(<span class="dv">1</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        concat <span class="op">=</span> torch.cat([s_expanded, encoder_outputs], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> v_a <span class="op">@</span> torch.tanh(concat <span class="op">@</span> W_a.T).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.squeeze(<span class="dv">1</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Softmax 归一化</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 加权求和</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> torch.bmm(attention_weights.unsqueeze(<span class="dv">1</span>), encoder_outputs).squeeze(<span class="dv">1</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Luong, Pham, &amp; Manning (2015) “Effective Approaches to Attention-based Neural Machine Translation”, EMNLP 2015. <a href="https://arxiv.org/abs/1508.04025">arXiv:1508.04025</a></em></p>
</div>
</div>
</section>
<section id="三种对齐函数的数值对比" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="三种对齐函数的数值对比"><span class="header-section-number">4.5</span> 三种对齐函数的数值对比</h3>
<p>让我们用同一组数据，比较点积对齐和加性对齐的差异。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>解码器状态：<span class="math inline">\(\mathbf{s} = [0.5, -0.3, 0.8, 0.2]^\top\)</span></li>
<li>编码器状态（3个位置）：
<ul>
<li><span class="math inline">\(\mathbf{h}_1 = [0.2, 0.4, 0.1, -0.3]^\top\)</span></li>
<li><span class="math inline">\(\mathbf{h}_2 = [0.6, -0.1, 0.7, 0.3]^\top\)</span></li>
<li><span class="math inline">\(\mathbf{h}_3 = [-0.2, 0.5, 0.3, 0.1]^\top\)</span></li>
</ul></li>
</ul>
<p><strong>Dot-Product 计算</strong>：</p>
<p><span class="math display">\[
e_1 = \mathbf{s}^\top \mathbf{h}_1 = 0.5 \times 0.2 + (-0.3) \times 0.4 + 0.8 \times 0.1 + 0.2 \times (-0.3)
\]</span> <span class="math display">\[
= 0.10 - 0.12 + 0.08 - 0.06 = 0.00
\]</span></p>
<p><span class="math display">\[
e_2 = \mathbf{s}^\top \mathbf{h}_2 = 0.5 \times 0.6 + (-0.3) \times (-0.1) + 0.8 \times 0.7 + 0.2 \times 0.3
\]</span> <span class="math display">\[
= 0.30 + 0.03 + 0.56 + 0.06 = 0.95
\]</span></p>
<p><span class="math display">\[
e_3 = \mathbf{s}^\top \mathbf{h}_3 = 0.5 \times (-0.2) + (-0.3) \times 0.5 + 0.8 \times 0.3 + 0.2 \times 0.1
\]</span> <span class="math display">\[
= -0.10 - 0.15 + 0.24 + 0.02 = 0.01
\]</span></p>
<p><strong>Softmax 归一化</strong>：</p>
<p><span class="math display">\[
\alpha_1 = \frac{e^{0.00}}{e^{0.00} + e^{0.95} + e^{0.01}} = \frac{1.00}{1.00 + 2.59 + 1.01} = \frac{1.00}{4.60} \approx 0.22
\]</span></p>
<p><span class="math display">\[
\alpha_2 = \frac{e^{0.95}}{4.60} = \frac{2.59}{4.60} \approx 0.56
\]</span></p>
<p><span class="math display">\[
\alpha_3 = \frac{e^{0.01}}{4.60} = \frac{1.01}{4.60} \approx 0.22
\]</span></p>
<p><strong>解读</strong>：使用点积注意力，模型将56%的注意力放在位置2，这是因为 <span class="math inline">\(\mathbf{h}_2\)</span> 与 <span class="math inline">\(\mathbf{s}\)</span> 在向量空间中最”相似”——它们的方向最为一致。注意，点积注意力不需要任何可学习参数，仅靠向量的几何关系就能区分出最相关的位置。而当 <span class="math inline">\(\mathbf{W}_a = \mathbf{I}\)</span>（单位矩阵）时，General退化为Dot-Product；在一般情况下，<span class="math inline">\(\mathbf{W}_a\)</span> 允许模型学习更复杂的相关性模式。</p>
</section>
<section id="global-vs-local-attention" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="global-vs-local-attention"><span class="header-section-number">4.6</span> Global vs Local Attention</h3>
<p>Luong还提出了另一个重要的设计维度：<strong>注意力的范围</strong>。</p>
<p><strong>Global Attention</strong> 关注源序列的所有位置，这是Bahdanau的做法，也是前面讨论的默认方式：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
\]</span></p>
<p><strong>Local Attention</strong> 只关注源序列的一个<strong>窗口</strong>。核心思想是：在每个解码步，先预测一个对齐位置 <span class="math inline">\(p_i\)</span>，然后只计算以 <span class="math inline">\(p_i\)</span> 为中心、宽度为 <span class="math inline">\(2D+1\)</span> 的窗口内的注意力：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=p_i-D}^{p_i+D} \alpha_{ij} \mathbf{h}_j
\]</span></p>
<p>对齐位置 <span class="math inline">\(p_i\)</span> 可以通过两种方式确定：</p>
<p><strong>Local-m（单调）</strong>：假设源和目标大致对齐，简单设置 <span class="math inline">\(p_i = i\)</span>。</p>
<p><strong>Local-p（预测）</strong>：学习一个函数来预测 <span class="math inline">\(p_i\)</span>：</p>
<p><span class="math display">\[
p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))
\]</span></p>
<p>其中 <span class="math inline">\(\sigma\)</span> 是sigmoid函数，确保 <span class="math inline">\(p_i \in [0, T_x]\)</span>。为了让注意力在窗口中心附近更集中，Local Attention还引入了一个高斯偏置：</p>
<p><span class="math display">\[
\alpha_{ij} = \text{align}(\mathbf{s}_i, \mathbf{h}_j) \cdot \exp\left(-\frac{(j - p_i)^2}{2\sigma^2}\right)
\]</span></p>
<div id="fig-global-local" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-global-local-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-6/original/fig-global-local-attention.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-global-local-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Luong论文中的Global vs Local Attention对比。左边是Global Attention：解码器状态 <span class="math inline">\(h_t\)</span> 与所有源位置计算注意力，生成上下文向量 <span class="math inline">\(c_t\)</span>。右边是Local Attention：先预测对齐位置 <span class="math inline">\(p_t\)</span>，只计算窗口 <span class="math inline">\([p_t - D, p_t + D]\)</span> 内的注意力。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Luong, Pham, &amp; Manning (2015) “Effective Approaches to Attention-based Neural Machine Translation”, Figure 2 &amp; 3. <a href="https://arxiv.org/abs/1508.04025">arXiv:1508.04025</a></em></p>
</div>
</section>
<section id="hard-vs-soft-attention" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="hard-vs-soft-attention"><span class="header-section-number">4.7</span> Hard vs Soft Attention</h3>
<p>除了Global/Local的维度，还有另一个基本区分：<strong>软注意力（Soft Attention）</strong> vs <strong>硬注意力（Hard Attention）</strong>。</p>
<p><strong>Soft Attention</strong> 计算所有位置的注意力权重（一个概率分布），然后加权求和：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_j \alpha_{ij} \mathbf{h}_j = \mathbb{E}_{p(j | \mathbf{s}_i, \mathbf{H})}[\mathbf{h}_j]
\]</span></p>
<p><strong>Hard Attention</strong> 从注意力分布中<strong>采样</strong>一个位置 <span class="math inline">\(j^*\)</span>，只使用那个位置的信息：</p>
<p><span class="math display">\[
j^* \sim \text{Categorical}(\alpha_{i1}, \alpha_{i2}, \ldots, \alpha_{iT_x})
\]</span> <span class="math display">\[
\mathbf{c}_i = \mathbf{h}_{j^*}
\]</span></p>
<p>两者的核心区别在于<strong>可微分性</strong>。Soft Attention是可微分的——加权求和是一个连续操作，梯度可以通过 <span class="math inline">\(\alpha_{ij}\)</span> 流向对齐函数的参数。Hard Attention不可微分——采样操作是离散的，需要强化学习方法（如REINFORCE）来训练，带来高方差和训练不稳定的问题。</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Hard Attention的训练困难
</div>
</div>
<div class="callout-body-container callout-body">
<p>Hard Attention虽然在概念上更接近人类的”注意”（我们真的只看一个地方，而不是模糊地看所有地方），但它的训练需要强化学习技术：</p>
<p><span class="math display">\[
\nabla_\theta J = \mathbb{E}_{j^* \sim p(j|\theta)} \left[ \nabla_\theta \log p(j^* | \theta) \cdot R(j^*) \right]
\]</span></p>
<p>其中 <span class="math inline">\(R(j^*)\)</span> 是选择位置 <span class="math inline">\(j^*\)</span> 带来的”奖励”。这个梯度估计的方差很大，需要大量采样才能稳定。</p>
<p>实践中，<strong>Soft Attention几乎总是更好的选择</strong>，因为：</p>
<ol type="1">
<li>端到端可微分，训练简单</li>
<li>梯度估计没有方差问题</li>
<li>可以同时利用多个位置的信息</li>
</ol>
</div>
</div>
</section>
<section id="复杂度分析" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="复杂度分析"><span class="header-section-number">4.8</span> 复杂度分析</h3>
<p>不同注意力变体的计算复杂度汇总如下：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 38%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>变体</th>
<th>对齐计算</th>
<th>总复杂度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Global + Dot</strong></td>
<td><span class="math inline">\(O(T_x \cdot d)\)</span> per step</td>
<td><span class="math inline">\(O(T_x \cdot T_y \cdot d)\)</span></td>
</tr>
<tr class="even">
<td><strong>Global + General</strong></td>
<td><span class="math inline">\(O(T_x \cdot d^2)\)</span> per step</td>
<td><span class="math inline">\(O(T_x \cdot T_y \cdot d^2)\)</span></td>
</tr>
<tr class="odd">
<td><strong>Global + Concat</strong></td>
<td><span class="math inline">\(O(T_x \cdot d^2)\)</span> per step</td>
<td><span class="math inline">\(O(T_x \cdot T_y \cdot d^2)\)</span></td>
</tr>
<tr class="even">
<td><strong>Local</strong></td>
<td><span class="math inline">\(O(D \cdot d)\)</span> per step</td>
<td><span class="math inline">\(O(D \cdot T_y \cdot d)\)</span></td>
</tr>
</tbody>
</table>
<p>其中 <span class="math inline">\(T_x\)</span> 是源序列长度，<span class="math inline">\(T_y\)</span> 是目标序列长度，<span class="math inline">\(d\)</span> 是隐藏维度，<span class="math inline">\(D\)</span> 是局部窗口大小。</p>
<p>与标准Seq2Seq相比，Global Attention增加了 <span class="math inline">\(O(T_x \cdot T_y)\)</span> 的计算量，同时需要存储所有编码器隐藏状态（<span class="math inline">\(O(T_x \cdot d)\)</span> 空间）。这是用空间换取性能的典型例子。Local Attention的优势在长序列时尤为明显：当 <span class="math inline">\(T_x = 1000\)</span> 而 <span class="math inline">\(D = 50\)</span> 时，计算量减少了20倍。</p>
<hr>
</section>
</section>
<section id="注意力可视化模型在看什么" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="注意力可视化模型在看什么"><span class="header-section-number">5</span> 注意力可视化：模型在”看”什么？</h2>
<section id="对齐矩阵" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="对齐矩阵"><span class="header-section-number">5.1</span> 对齐矩阵</h3>
<p>Attention机制的一个美妙特性是<strong>可解释性</strong>。注意力权重 <span class="math inline">\(\alpha_{ij}\)</span> 直接告诉我们：在生成第 <span class="math inline">\(i\)</span> 个目标词时，模型关注了哪些源词。</p>
<p>我们可以把所有的注意力权重排列成一个矩阵，横轴是源序列，纵轴是目标序列。这个矩阵被称为<strong>对齐矩阵（alignment matrix）</strong>。</p>
<div id="fig-alignment-visualization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alignment-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-6/original/fig3-alignment-visualization.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alignment-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 对齐可视化：四个英法翻译例子的注意力权重热力图。横轴是英语源句子，纵轴是法语目标句子。白色表示高注意力权重，黑色表示低权重。注意对角线模式（单词一一对应）和偏离对角线的区域（语序调整）。例如(a)中”August”对应”août”，“European Economic Area”对应”zone économique européenne”。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Bahdanau, Cho, &amp; Bengio (2015) “Neural Machine Translation by Jointly Learning to Align and Translate”, Figure 3. <a href="https://arxiv.org/abs/1409.0473">arXiv:1409.0473</a></em></p>
</div>
</section>
<section id="对齐模式的语言学意义" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="对齐模式的语言学意义"><span class="header-section-number">5.2</span> 对齐模式的语言学意义</h3>
<p>通过观察对齐矩阵，我们可以发现一些有趣的语言学模式：</p>
<p><strong>单调对齐</strong>：对于语序相似的语言对（如英语到德语的某些结构），对齐矩阵接近对角线——第1个源词对应第1个目标词，第2个对应第2个，依此类推。</p>
<p><strong>语序调整</strong>：当源语言和目标语言的词序不同时，对齐矩阵会偏离对角线。例如，英语的”red car”翻译成法语是”voiture rouge”（车 红），对齐矩阵会显示交叉模式。</p>
<p><strong>一对多和多对一</strong>：某些词没有直接对应，或一个词对应多个词。例如，英语的”going to”可能对应法语的单个词”va”。</p>
<p><strong>空对齐</strong>：某些目标词（如冠词）可能没有明确的源词对应，它们的注意力权重会分散在多个位置。</p>
</section>
<section id="可视化的局限性" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="可视化的局限性"><span class="header-section-number">5.3</span> 可视化的局限性</h3>
<p>虽然注意力可视化很吸引人，但我们要谨慎解读：</p>
<ol type="1">
<li><strong>注意力不等于解释</strong>：高注意力权重不一定意味着模型”理解”了那个位置的内容</li>
<li><strong>可能有多重因素</strong>：模型可能通过其他机制（如位置信息）做出决定</li>
<li><strong>训练目标的影响</strong>：注意力权重是为了最小化翻译损失而学习的，不一定反映人类的对齐直觉</li>
</ol>
<p>后来的研究（如Jain &amp; Wallace, 2019）对注意力的可解释性提出了质疑。但作为一个诊断工具，注意力可视化仍然非常有价值。</p>
<hr>
</section>
</section>
<section id="工程实践带attention的seq2seq" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="工程实践带attention的seq2seq"><span class="header-section-number">6</span> 工程实践：带Attention的Seq2Seq</h2>
<p>让我们用PyTorch实现一个完整的带Attention的Seq2Seq模型，涵盖Bahdanau和Luong两种注意力机制。</p>
<section id="编码器" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="编码器"><span class="header-section-number">6.1</span> 编码器</h3>
<div id="5b3133b0" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, hidden_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            embed_dim, hidden_dim,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,  <span class="co"># 双向</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 将双向的隐藏状态压缩到单向维度</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim <span class="op">*</span> <span class="dv">2</span>, hidden_dim)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_lengths<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src: [batch_size, src_len]</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(src))  <span class="co"># [batch, src_len, embed_dim]</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> src_lengths <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            packed <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>                embedded, src_lengths.cpu(), batch_first<span class="op">=</span><span class="va">True</span>, enforce_sorted<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            packed_outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(packed)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            outputs, _ <span class="op">=</span> nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(embedded)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs: [batch, src_len, hidden_dim * 2] (双向拼接)</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [num_layers * 2, batch, hidden_dim]</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并前向和后向的最终隐藏状态</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-2] 是最后一层前向，hidden[-1] 是最后一层后向</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> torch.tanh(<span class="va">self</span>.fc(torch.cat([hidden[<span class="op">-</span><span class="dv">2</span>], hidden[<span class="op">-</span><span class="dv">1</span>]], dim<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, hidden_dim]</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, hidden</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="bahdanau-attention层" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="bahdanau-attention层"><span class="header-section-number">6.2</span> Bahdanau Attention层</h3>
<div id="7c5a652f" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BahdanauAttention(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, attention_dim):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加性注意力的参数</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(dec_hidden_dim, attention_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.U_a <span class="op">=</span> nn.Linear(enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>, attention_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># 双向编码器</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_a <span class="op">=</span> nn.Linear(attention_dim, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_hidden: [batch, dec_hidden]</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, src_len], True表示需要mask的位置（padding）</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decoder_hidden 扩展到所有源位置</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, dec_hidden] -&gt; [batch, src_len, dec_hidden]</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        decoder_hidden <span class="op">=</span> decoder_hidden.unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, src_len, <span class="dv">1</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算对齐分数</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, attention_dim]</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        energy <span class="op">=</span> torch.tanh(<span class="va">self</span>.W_a(decoder_hidden) <span class="op">+</span> <span class="va">self</span>.U_a(encoder_outputs))</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, 1] -&gt; [batch, src_len]</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> <span class="va">self</span>.v_a(energy).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用mask（将padding位置的分数设为很小的负数）</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            attention_scores <span class="op">=</span> attention_scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax归一化</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(attention_scores, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算上下文向量</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -&gt; [batch, 1, enc_hidden*2]</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(attention_weights.unsqueeze(<span class="dv">1</span>), encoder_outputs)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> context.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, enc_hidden * 2]</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="解码器" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="解码器"><span class="header-section-number">6.3</span> 解码器</h3>
<div id="bef63bdf" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionDecoder(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                 attention_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># GRU输入是：embedded + context</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            embed_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>,  <span class="co"># 双向编码器</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            dec_hidden_dim,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(dec_hidden_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> embed_dim, vocab_size)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_token, hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">        单步解码</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">        input_token: [batch] - 上一步的输出token</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden: [1, batch, dec_hidden] - 上一步的隐藏状态</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Embedding</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(input_token))  <span class="co"># [batch, embed_dim]</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-1] 取最后一层，[batch, dec_hidden]</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        context, attention_weights <span class="op">=</span> <span class="va">self</span>.attention(hidden[<span class="op">-</span><span class="dv">1</span>], encoder_outputs, mask)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接embedded和context作为RNN输入</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        rnn_input <span class="op">=</span> torch.cat([embedded, context], dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch, 1, embed+ctx]</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RNN</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        output, hidden <span class="op">=</span> <span class="va">self</span>.rnn(rnn_input, hidden)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, dec_hidden]</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="va">self</span>.fc(torch.cat([output, context, embedded], dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction, hidden, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="完整的seq2seq模型" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="完整的seq2seq模型"><span class="header-section-number">6.4</span> 完整的Seq2Seq模型</h3>
<div id="74e6fc61" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2SeqAttention(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, device):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg, teacher_forcing_ratio<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">        src: [batch, src_len]</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">        trg: [batch, trg_len]</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> src.shape[<span class="dv">0</span>]</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        trg_vocab_size <span class="op">=</span> <span class="va">self</span>.decoder.vocab_size</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 存储输出</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> torch.zeros(batch_size, trg_len, trg_vocab_size).to(<span class="va">self</span>.device)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        attentions <span class="op">=</span> []</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 编码</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        encoder_outputs, hidden <span class="op">=</span> <span class="va">self</span>.encoder(src)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, dec_hidden] -&gt; [1, batch, dec_hidden]</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> hidden.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 第一个解码输入是 &lt;sos&gt; token</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        input_token <span class="op">=</span> trg[:, <span class="dv">0</span>]</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, trg_len):</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>            prediction, hidden, attention <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>                input_token, hidden, encoder_outputs</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>            outputs[:, t] <span class="op">=</span> prediction</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>            attentions.append(attention)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Teacher forcing</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>            teacher_force <span class="op">=</span> torch.rand(<span class="dv">1</span>).item() <span class="op">&lt;</span> teacher_forcing_ratio</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>            top1 <span class="op">=</span> prediction.argmax(<span class="dv">1</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>            input_token <span class="op">=</span> trg[:, t] <span class="cf">if</span> teacher_force <span class="cf">else</span> top1</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, torch.stack(attentions, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建模型示例</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>, hidden_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> AttentionDecoder(</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    enc_hidden_dim<span class="op">=</span><span class="dv">512</span>, dec_hidden_dim<span class="op">=</span><span class="dv">512</span>, attention_dim<span class="op">=</span><span class="dv">256</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2SeqAttention(encoder, decoder, device).to(device)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"编码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> encoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"解码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"总参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>编码器参数: 5,450,240
解码器参数: 23,639,056
总参数: 29,089,296</code></pre>
</div>
</div>
</section>
<section id="luong-attention实现" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="luong-attention实现"><span class="header-section-number">6.5</span> Luong Attention实现</h3>
<div id="96072431" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LuongAttention(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Luong 注意力机制，支持三种对齐方式</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, method<span class="op">=</span><span class="st">'dot'</span>):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.method <span class="op">=</span> method</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_hidden_dim <span class="op">=</span> enc_hidden_dim</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dec_hidden_dim <span class="op">=</span> dec_hidden_dim</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> method <span class="op">==</span> <span class="st">'general'</span>:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(enc_hidden_dim, dec_hidden_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'concat'</span>:</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(enc_hidden_dim <span class="op">+</span> dec_hidden_dim, dec_hidden_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.v_a <span class="op">=</span> nn.Linear(dec_hidden_dim, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_state, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_state: [batch, dec_hidden]</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden]</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, src_len], True表示padding位置</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.method <span class="op">==</span> <span class="st">'dot'</span>:</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 点积: s^T h</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 需要 dec_hidden == enc_hidden</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> torch.bmm(</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>                decoder_state.unsqueeze(<span class="dv">1</span>),  <span class="co"># [batch, 1, dec_hidden]</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>                encoder_outputs.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># [batch, enc_hidden, src_len]</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>            ).squeeze(<span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.method <span class="op">==</span> <span class="st">'general'</span>:</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 一般形式: s^T W h</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># W 将 enc_hidden 映射到 dec_hidden</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>            transformed <span class="op">=</span> <span class="va">self</span>.W_a(encoder_outputs)  <span class="co"># [batch, src_len, dec_hidden]</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> torch.bmm(</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>                decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>                transformed.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>            ).squeeze(<span class="dv">1</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.method <span class="op">==</span> <span class="st">'concat'</span>:</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 拼接形式: v^T tanh(W [s; h])</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>            decoder_expanded <span class="op">=</span> decoder_state.unsqueeze(<span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>, src_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>            concat <span class="op">=</span> torch.cat([decoder_expanded, encoder_outputs], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>            energy <span class="op">=</span> torch.tanh(<span class="va">self</span>.W_a(concat))  <span class="co"># [batch, src_len, dec_hidden]</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="va">self</span>.v_a(energy).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用 mask</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 上下文向量</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>            attention_weights.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>            encoder_outputs</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">1</span>)</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="local-attention实现" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="local-attention实现"><span class="header-section-number">6.6</span> Local Attention实现</h3>
<div id="d040a376" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LocalAttention(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Luong 的 Local Attention（预测型）</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, window_size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size  <span class="co"># D: 窗口半径</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_hidden_dim <span class="op">=</span> enc_hidden_dim</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 位置预测网络</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_p <span class="op">=</span> nn.Linear(dec_hidden_dim, dec_hidden_dim)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_p <span class="op">=</span> nn.Linear(dec_hidden_dim, <span class="dv">1</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 对齐函数（使用 general）</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(enc_hidden_dim, dec_hidden_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 高斯标准差</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigma <span class="op">=</span> window_size <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_state, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_state: [batch, dec_hidden]</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden]</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> decoder_state.device</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 预测对齐位置 p</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># p = S * sigmoid(v^T tanh(W_p s))</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> src_len <span class="op">*</span> torch.sigmoid(</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.v_p(torch.tanh(<span class="va">self</span>.W_p(decoder_state)))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch]</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 计算所有位置的对齐分数</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        transformed <span class="op">=</span> <span class="va">self</span>.W_a(encoder_outputs)  <span class="co"># [batch, src_len, dec_hidden]</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            transformed.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 应用高斯窗口</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 生成位置索引 [0, 1, 2, ..., src_len-1]</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(src_len, device<span class="op">=</span>device).<span class="bu">float</span>()</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> positions.unsqueeze(<span class="dv">0</span>).expand(batch_size, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 高斯权重: exp(-(j - p)^2 / (2 * sigma^2))</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        gaussian <span class="op">=</span> torch.exp(<span class="op">-</span>((positions <span class="op">-</span> p.unsqueeze(<span class="dv">1</span>)) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.sigma <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 窗口mask（只保留 [p-D, p+D] 范围内的位置）</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        window_mask <span class="op">=</span> (positions <span class="op">&gt;=</span> (p.unsqueeze(<span class="dv">1</span>) <span class="op">-</span> <span class="va">self</span>.window_size)) <span class="op">&amp;</span> <span class="op">\</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>                      (positions <span class="op">&lt;=</span> (p.unsqueeze(<span class="dv">1</span>) <span class="op">+</span> <span class="va">self</span>.window_size))</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用窗口mask</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.masked_fill(<span class="op">~</span>window_mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Softmax + 高斯加权</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">*</span> gaussian</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 重新归一化</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> attention_weights <span class="op">/</span> (attention_weights.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="fl">1e-10</span>)</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 上下文向量</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>            attention_weights.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>            encoder_outputs</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">1</span>)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights, p</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="对比实验" class="level3" data-number="6.7">
<h3 data-number="6.7" class="anchored" data-anchor-id="对比实验"><span class="header-section-number">6.7</span> 对比实验</h3>
<div id="c374f817" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建测试数据</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>src_len <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>enc_hidden <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>dec_hidden <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>encoder_outputs <span class="op">=</span> torch.randn(batch_size, src_len, enc_hidden)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>decoder_state <span class="op">=</span> torch.randn(batch_size, dec_hidden)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试三种 Luong Attention</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method <span class="kw">in</span> [<span class="st">'dot'</span>, <span class="st">'general'</span>, <span class="st">'concat'</span>]:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> LuongAttention(enc_hidden, dec_hidden, method<span class="op">=</span>method)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    context, weights <span class="op">=</span> attn(decoder_state, encoder_outputs)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>method<span class="sc">:8s}</span><span class="ss">: context shape = </span><span class="sc">{</span>context<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, weights sum = </span><span class="sc">{</span>weights<span class="sc">.</span><span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试 Local Attention</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>local_attn <span class="op">=</span> LocalAttention(enc_hidden, dec_hidden, window_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>context, weights, p <span class="op">=</span> local_attn(decoder_state, encoder_outputs)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'local'</span><span class="sc">:8s}</span><span class="ss">: context shape = </span><span class="sc">{</span>context<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, predicted p = </span><span class="sc">{</span>p<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>dot     : context shape = torch.Size([2, 64]), weights sum = tensor([1., 1.])
general : context shape = torch.Size([2, 64]), weights sum = tensor([1., 1.], grad_fn=&lt;SumBackward1&gt;)
concat  : context shape = torch.Size([2, 64]), weights sum = tensor([1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)
local   : context shape = torch.Size([2, 64]), predicted p = [5.452468395233154, 5.0841288566589355]</code></pre>
</div>
</div>
</section>
<section id="关键实现细节" class="level3" data-number="6.8">
<h3 data-number="6.8" class="anchored" data-anchor-id="关键实现细节"><span class="header-section-number">6.8</span> 关键实现细节</h3>
<p><strong>Mask处理</strong>：在实际应用中，batch中的序列长度不同，需要padding。计算注意力时，padding位置不应该获得任何权重。我们通过mask将这些位置的分数设为很大的负数，softmax后它们的权重趋近于0。</p>
<p><strong>Teacher Forcing</strong>：训练时，解码器的输入可以是真实的上一个词（teacher forcing）或模型预测的词。<code>teacher_forcing_ratio</code> 控制两者的混合比例。较高的比例加速训练，但可能导致exposure bias。</p>
<p><strong>双向编码器</strong>：我们使用双向GRU，编码器输出的维度是 <code>hidden_dim * 2</code>。这让每个位置都包含完整的上下文信息。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">7</span> 深入理解</h2>
<section id="为什么attention有效理论视角" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="为什么attention有效理论视角"><span class="header-section-number">7.1</span> 为什么Attention有效？——理论视角</h3>
<p><strong>信息论视角</strong>：标准Seq2Seq的上下文向量 <span class="math inline">\(\mathbf{c}\)</span> 是输入 <span class="math inline">\(\mathbf{x}\)</span> 的一个<strong>充分统计量（sufficient statistic）</strong>——如果 <span class="math inline">\(\mathbf{c}\)</span> 完美，它应该包含关于 <span class="math inline">\(\mathbf{y}\)</span> 的所有必要信息。但在实践中，有限维度的 <span class="math inline">\(\mathbf{c}\)</span> 无法做到这一点。Attention通过让解码器访问所有的 <span class="math inline">\(\mathbf{h}_j\)</span>，实际上是在说：<strong>不要求一个充分统计量，而是让模型在需要时直接查询原始信息</strong>。这绕过了信息瓶颈。</p>
<p><strong>记忆寻址视角</strong>：可以把编码器的隐藏状态看作一个<strong>外部记忆（external memory）</strong>，每个 <span class="math inline">\(\mathbf{h}_j\)</span> 是一个记忆槽。Attention机制实现了<strong>基于内容的软寻址（content-based soft addressing）</strong>——根据当前查询（解码器状态）检索相关的记忆。这个视角后来被显式化为Memory Networks和Neural Turing Machine。</p>
<p><strong>梯度流视角</strong>：从优化角度，Attention提供了一条从解码器到编码器特定位置的<strong>直接路径</strong>。在标准Seq2Seq中，梯度要从解码器流回编码器，必须经过 <span class="math inline">\(\mathbf{c}\)</span>，再经过整个编码过程。Attention创造了”捷径”——梯度可以通过注意力权重直接传到相关的编码器位置。</p>
</section>
<section id="为什么点积注意力能工作" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="为什么点积注意力能工作"><span class="header-section-number">7.2</span> 为什么点积注意力能工作？</h3>
<p>点积注意力的有效性可以从多个角度理解。</p>
<p><strong>余弦相似度视角</strong>：当向量被归一化后，点积就是余弦相似度：</p>
<p><span class="math display">\[
\mathbf{s}^\top \mathbf{h} = \|\mathbf{s}\| \|\mathbf{h}\| \cos(\theta)
\]</span></p>
<p>余弦相似度是衡量两个向量”方向一致性”的经典指标。神经网络在训练过程中，会学习让相关的状态指向相似的方向。</p>
<p><strong>核方法视角</strong>：点积可以看作一个线性核（linear kernel）。在核方法的框架下，注意力权重实际上是在一个特征空间中计算相似度。General Attention引入的可学习矩阵 <span class="math inline">\(\mathbf{W}_a\)</span> 相当于学习一个Mahalanobis距离。</p>
<p><strong>信息检索视角</strong>：点积注意力可以类比为向量空间模型中的查询-文档匹配。解码器状态是”查询”，编码器状态是”文档”，点积衡量查询与文档的相关性。</p>
</section>
<section id="为什么需要缩放" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="为什么需要缩放"><span class="header-section-number">7.3</span> 为什么需要缩放？</h3>
<p>Luong的论文没有讨论这个问题，但后来的Transformer论文（Vaswani et al., 2017）指出了点积注意力的一个潜在问题。</p>
<p>当向量维度 <span class="math inline">\(d\)</span> 很大时，点积的方差会很大。假设 <span class="math inline">\(\mathbf{s}\)</span> 和 <span class="math inline">\(\mathbf{h}\)</span> 的每个分量都是独立的、均值为0、方差为1的随机变量，那么：</p>
<p><span class="math display">\[
\text{Var}(\mathbf{s}^\top \mathbf{h}) = d
\]</span></p>
<p>当 <span class="math inline">\(d = 512\)</span> 时，点积的标准差是 <span class="math inline">\(\sqrt{512} \approx 22.6\)</span>。这意味着点积可能产生很大的正值或负值，导致softmax输出接近one-hot分布，梯度变得很小。</p>
<p>解决方案是<strong>缩放</strong>：</p>
<p><span class="math display">\[
\text{score}(\mathbf{s}, \mathbf{h}) = \frac{\mathbf{s}^\top \mathbf{h}}{\sqrt{d}}
\]</span></p>
<p>这就是Transformer中的<strong>Scaled Dot-Product Attention</strong>。Luong的论文使用的维度较小（500左右），问题不太明显；但在Transformer的大维度设置下，缩放变得必要。这个设计选择将在第8章Transformer中发挥核心作用。</p>
</section>
<section id="边界条件与失效模式" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="边界条件与失效模式"><span class="header-section-number">7.4</span> 边界条件与失效模式</h3>
<p><strong>单调对齐假设</strong>：Bahdanau Attention隐含假设源和目标之间存在某种对齐关系。对于翻译任务这通常成立，但对于某些任务（如摘要），这个假设可能不成立——摘要需要整合分散在各处的信息，而不是”对齐”到特定位置。</p>
<p><strong>复杂度限制</strong>：当源序列很长时（如文档级翻译），计算所有位置的注意力权重变得昂贵。<span class="math inline">\(O(T_x \cdot T_y)\)</span> 的复杂度在 <span class="math inline">\(T_x = 10000\)</span> 时是不可接受的。Local Attention是一种缓解方案，但窗口大小的选择需要根据任务调整。</p>
<p><strong>分布偏移</strong>：训练时，解码器看到的上下文向量分布与推理时可能不同（因为teacher forcing）。这可能导致注意力权重在推理时不够准确。</p>
<p><strong>Dot-Product的维度限制</strong>：解码器和编码器的隐藏维度必须相同，否则无法计算点积。General Attention通过引入 <span class="math inline">\(\mathbf{W}_a\)</span> 解除了这个限制，但增加了参数量。</p>
<p><strong>Local Attention的位置预测错误</strong>：如果位置预测函数学习不好，会系统性地错过重要信息。对于语序差异大的语言对，局部窗口可能覆盖不到正确位置。</p>
<hr>
</section>
</section>
<section id="局限性与展望" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="局限性与展望"><span class="header-section-number">8</span> 局限性与展望</h2>
<p>经过Bahdanau（2014）和Luong（2015）的工作，注意力机制已经在机器翻译中确立了核心地位。但两个根本性的局限逐渐浮现，指向更深层的问题。</p>
<p><strong>注意力仍然是RNN的”附属品”</strong>。无论是Bahdanau还是Luong的注意力，都是Seq2Seq架构的增强组件。编码和解码的核心仍然依赖RNN。这意味着顺序计算无法避免——RNN必须逐步处理序列，无法并行。虽然Attention缓解了长距离依赖问题，但RNN本身的梯度流困难并没有根本解决。</p>
<p><strong>注意力只在编码器-解码器之间</strong>。当前的Attention只让解码器关注编码器。但一个更自然的问题是：<strong>编码器内部的各个位置能否相互关注？</strong> 一个词的理解可能依赖于同一句话中的其他词（例如代词的指代消解），而当前的架构没有提供这种”自我关注”的机制。</p>
<p><strong>位置信息是隐式的</strong>。Attention本身不包含位置信息。位置信息完全依赖RNN的顺序处理来隐式编码。如果抛弃RNN，位置信息将完全丢失——这暗示未来的架构需要某种显式的位置编码方案。</p>
<p>这些局限引发了两个深刻的问题。第一：<strong>能否让Attention独立于RNN？</strong> 如果Attention如此有效，为什么还需要RNN作为载体？第二：<strong>能否让序列中的每个位置相互关注？</strong> 如果注意力不局限于编码器-解码器之间，而是扩展到同一序列内——这就是<strong>Self-Attention（自注意力）</strong>。</p>
<p>下一章将揭晓答案。Self-Attention让每个位置都能直接与其他所有位置交互，不再需要RNN作为中介。而在此之后的Transformer架构（第8章）将彻底兑现”Attention Is All You Need”的承诺——完全抛弃循环结构，用纯粹的注意力机制构建整个模型。</p>
<blockquote class="blockquote">
<p>从加性到乘性，从全局到局部，从RNN的附属到独立的核心——注意力机制走过了从”辅助工具”到”架构基石”的完整历程。Bahdanau打破了信息瓶颈，Luong拓展了设计空间，而真正的革命还在后面。</p>
</blockquote>
<hr>
</section>
<section id="本章小结" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">9</span> 本章小结</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>核心要点
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>问题</strong>：Seq2Seq的信息瓶颈——所有输入信息压缩到一个固定向量，导致长序列信息丢失</li>
<li><strong>洞察</strong>：解码器应该能够动态地、有选择地关注输入的不同位置；进一步地，注意力的计算方式可以多样化</li>
<li><strong>Bahdanau (2014)</strong>：加性注意力打破了固定向量的限制，大幅提升了长序列翻译质量</li>
<li><strong>Luong (2015)</strong>：系统比较了dot/general/concat三种对齐函数和global/local两种范围策略，确立了点积注意力的效率优势</li>
<li><strong>意义</strong>：注意力从Seq2Seq的”补丁”发展为核心组件，为后续的Self-Attention和Transformer奠定了基础</li>
</ul>
</div>
</div>
<section id="关键公式速查" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">9.1</span> 关键公式速查</h3>
<p><strong>Bahdanau加性注意力</strong>：</p>
<p><span class="math display">\[
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
\]</span></p>
<p><strong>Dot-Product Attention</strong>：</p>
<p><span class="math display">\[
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{h}
\]</span></p>
<p><strong>General Attention</strong>：</p>
<p><span class="math display">\[
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{W}_a \mathbf{h}
\]</span></p>
<p><strong>注意力权重</strong>：</p>
<p><span class="math display">\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
\]</span></p>
<p><strong>上下文向量</strong>：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
\]</span></p>
<p><strong>Local Attention位置预测</strong>：</p>
<p><span class="math display">\[
p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))
\]</span></p>
<hr>
</section>
</section>
<section id="思考题" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="思考题"><span class="header-section-number">10</span> 思考题</h2>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么说Attention实现了”软寻址”？它与计算机内存的硬寻址有什么本质区别？这种软寻址的优势和劣势是什么？</p></li>
<li><p><strong>[数学推导]</strong> 点积注意力和加性注意力在表达能力上有什么本质区别？设计一个简单的例子，展示加性注意力能学习而点积注意力无法学习的相关性模式。更一般地，如果General Attention中 <span class="math inline">\(\mathbf{W}_a = \mathbf{U}\mathbf{V}^\top\)</span>（秩-<span class="math inline">\(r\)</span>分解），这对注意力模式有什么影响？</p></li>
<li><p><strong>[工程实践]</strong> 在实现Attention时，为什么要对padding位置应用mask？如果不做mask会有什么后果？如何正确实现mask（考虑数值稳定性）？</p></li>
<li><p><strong>[批判思考]</strong> Local Attention假设对齐是大致单调的（源和目标的位置对应）。对于哪些语言对或任务，这个假设会严重失效？能否设计一种”非单调局部注意力”？</p></li>
<li><p><strong>[开放问题]</strong> Hard Attention虽然训练困难，但它有一个优势：稀疏性可以提高可解释性。有没有方法既保持Soft Attention的可微分性，又能获得接近Hard Attention的稀疏性？（提示：考虑稀疏softmax、Gumbel-softmax）</p></li>
</ol>
<hr>
</section>
<section id="延伸阅读" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">11</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">11.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate</strong>
<ul>
<li>Attention机制在NMT中的开创性工作</li>
<li>重点读：Section 3（模型架构）、Section 5（可视化分析）</li>
<li>arXiv: <a href="https://arxiv.org/abs/1409.0473">1409.0473</a></li>
</ul></li>
<li><strong>[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation</strong>
<ul>
<li>系统比较不同注意力变体，确立设计准则</li>
<li>重点读：Section 3（Global vs Local）、Section 4（实验对比）</li>
<li>arXiv: <a href="https://arxiv.org/abs/1508.04025">1508.04025</a></li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">11.2</span> 理论基础</h3>
<ul>
<li><strong>[Graves et al., 2014] Neural Turing Machines</strong>
<ul>
<li>提出了基于内容的软寻址，是Attention的理论先驱</li>
<li>重点读：Section 3.1（Attention机制）</li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">11.3</span> 后续发展</h3>
<ul>
<li><strong>[Vaswani et al., 2017] Attention Is All You Need</strong>
<ul>
<li>提出Scaled Dot-Product Attention和Multi-Head Attention，完全抛弃RNN</li>
<li>本教材第8章的核心内容</li>
<li>arXiv: <a href="https://arxiv.org/abs/1706.03762">1706.03762</a></li>
</ul></li>
<li><strong>[Xu et al., 2015] Show, Attend and Tell</strong>
<ul>
<li>Hard Attention在图像描述中的应用，对比Soft和Hard Attention的效果</li>
<li>arXiv: <a href="https://arxiv.org/abs/1502.03044">1502.03044</a></li>
</ul></li>
</ul>
</section>
<section id="注意力可解释性" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="注意力可解释性"><span class="header-section-number">11.4</span> 注意力可解释性</h3>
<ul>
<li><strong>[Jain &amp; Wallace, 2019] Attention is not Explanation</strong>
<ul>
<li>质疑Attention权重作为解释的可靠性</li>
<li>arXiv: <a href="https://arxiv.org/abs/1902.10186">1902.10186</a></li>
</ul></li>
<li><strong>[Wiegreffe &amp; Pinter, 2019] Attention is not not Explanation</strong>
<ul>
<li>对上述论文的回应，更细致地讨论Attention的解释性</li>
<li>arXiv: <a href="https://arxiv.org/abs/1908.04626">1908.04626</a></li>
</ul></li>
</ul>
</section>
<section id="大规模实证对比" class="level3" data-number="11.5">
<h3 data-number="11.5" class="anchored" data-anchor-id="大规模实证对比"><span class="header-section-number">11.5</span> 大规模实证对比</h3>
<ul>
<li><strong>[Britz et al., 2017] Massive Exploration of Neural Machine Translation Architectures</strong>
<ul>
<li>大规模对比NMT的各种设计选择，包括注意力变体的实证对比</li>
<li>arXiv: <a href="https://arxiv.org/abs/1703.03906">1703.03906</a></li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">12</span> 历史注脚</h2>
<p>Attention机制的灵感部分来自人类视觉系统。当我们看一幅复杂的图像时，我们不会同时处理所有像素，而是会”聚焦”在感兴趣的区域。这种选择性注意（selective attention）是认知科学研究的经典课题。</p>
<p>Bahdanau在2014年将这个思想引入神经机器翻译时，并没有预料到它会成为深度学习最核心的组件之一。在论文中，他们谦虚地称之为”对齐模型”（alignment model），而不是”注意力”。“Attention”这个术语是后来被社区广泛采用的。</p>
<p>仅一年后，斯坦福的Luong、Pham和Manning发表了系统性的探索工作。他们不仅提出了计算更高效的乘性注意力，更重要的是<strong>系统性地比较和总结</strong>了当时的各种方法，为后来者提供了清晰的设计指南。有趣的是，Luong论文中提到的点积注意力因为过于简单而在当时没有受到太多关注——主流仍然偏好参数化的对齐函数。但两年后，当Transformer论文提出”Scaled Dot-Product Attention”时，点积注意力终于登上了历史舞台的中央。</p>
<p>从某种意义上说，这两篇论文共同完成了Attention发展的第一阶段：Bahdanau开创了注意力机制，Luong拓展了设计空间并建立了系统性的理解。然而，一个更大胆的想法正在酝酿：如果Attention如此强大，我们是否还需要RNN？下一章将讲述Self-Attention的诞生，而不久之后，Vaswani等人将给出决定性的回答——“Attention Is All You Need”。这篇论文不仅在技术上革新了序列建模，其标题本身也成为了深度学习历史上最具影响力的金句之一。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第6章：注意力机制的诞生与演进"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "从信息瓶颈到动态聚焦：Bahdanau加性注意力、Luong乘性注意力，以及注意力设计空间的系统探索"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-25"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Attention, Seq2Seq, 机器翻译, Bahdanau, Luong]</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [注意力机制, 加性注意力, 乘性注意力, 点积注意力, 对齐模型, 上下文向量, 神经机器翻译, Global Attention, Local Attention, Hard Attention, Soft Attention]</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "注意力机制的完整故事：从Bahdanau打破Seq2Seq信息瓶颈，到Luong的系统性探索（加性vs乘性、全局vs局部、软vs硬），再到注意力独立于RNN的前奏。"</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：如何让解码器在生成每个词时，能够访问输入序列的不同部分，而不是只依赖一个压缩后的向量？进一步地，计算"注意力"的最佳方式是什么——加性、乘性、全局还是局部？</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2014-2015 </span><span class="pp">|</span><span class="at"> Bahdanau, Cho, Bengio → Luong, Pham, Manning </span><span class="pp">|</span><span class="at"> 从注意力的诞生到系统性探索</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>上一章我们见证了RNN的辉煌与困境。LSTM和GRU通过门控机制解决了梯度消失问题，Seq2Seq架构让神经网络能够处理翻译、摘要等序列到序列的任务。</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>但Seq2Seq有一个致命的设计缺陷：**信息瓶颈**。</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>回顾Seq2Seq的工作方式：编码器读取整个输入序列，将所有信息压缩到一个固定长度的上下文向量 $\mathbf{c}$ 中；解码器仅凭这个向量，逐词生成输出。这意味着，无论输入是5个词还是50个词，所有信息都要塞进同一个维度的向量。</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>Sutskever等人(2014)的实验清楚地展示了这个问题：当输入句子超过20个词时，翻译质量急剧下降。更长的句子包含更多信息，而固定大小的向量无法承载。</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的例子感受这个问题。考虑翻译任务：</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **英语**：The agreement on the European Economic Area was signed in August 1992.</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **法语**：L'accord sur la zone économique européenne a été signé en août 1992.</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>当解码器生成"août"（八月）时，它需要知道原文中的"August"。但在标准Seq2Seq中，"August"这个词首先被编码进隐藏状态，然后与其他所有词的信息混合在一起，最终压缩成上下文向量 $\mathbf{c}$。解码器要从这个压缩后的向量中"挖出"August的信息——这就像从一锅汤里找回原来的食材。</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>更糟糕的是，句子中的某些词对当前生成的词更重要。翻译"August"时，模型最需要关注的是原文中的"August"，而不是"The"或"was"。但标准Seq2Seq对所有输入位置一视同仁——它们都被同等地压缩进了 $\mathbf{c}$。</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：解码器在生成每个词时，应该能够**有选择地关注**输入序列的不同位置。不同的输出词需要关注不同的输入词——这就是"注意力"的本质。本章将追溯注意力机制从诞生（Bahdanau, 2014）到系统性探索（Luong, 2015）的完整历程，揭示不同设计选择背后的权衡。</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的精确定义</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>让我们形式化地描述Seq2Seq的信息瓶颈问题。</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>在标准Seq2Seq中，编码器产生一系列隐藏状态 $\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, \ldots, \mathbf{h}_T^{enc}$，但只有最后一个状态 $\mathbf{h}_T^{enc}$ 被传递给解码器作为上下文向量：</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>\mathbf{c} = \mathbf{h}_T^{enc}</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>解码器的每一步都使用这同一个 $\mathbf{c}$：</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t^{dec} = f(\mathbf{h}_{t-1}^{dec}, y_{t-1}, \mathbf{c})</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>问题在于：$\mathbf{c}$ 是一个**静态**的、**全局**的表示。它在解码的每一步都保持不变，无法根据当前生成的词动态调整。</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>从信息论的角度看，如果输入序列 $\mathbf{x}$ 的信息熵是 $H(\mathbf{x})$，而 $\mathbf{c}$ 的维度是 $d$，那么 $\mathbf{c}$ 最多能携带 $O(d)$ 的信息量。当 $H(\mathbf{x}) &gt; O(d)$ 时，信息丢失是不可避免的。</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### 之前的尝试为何失败？</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>在Attention出现之前，研究者尝试过一些缓解信息瓶颈的方法：</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>**增加上下文向量维度**：直觉上，更大的 $\mathbf{c}$ 可以携带更多信息。但这只是延缓问题，而非解决问题。而且更大的向量意味着更多参数，更容易过拟合。</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>**使用双向RNN**：让编码器同时从左到右和从右到左读取输入，然后拼接两个方向的最终隐藏状态。这确实能捕获更多上下文，但仍然是压缩到一个固定向量——只是这个向量稍微大了一点。</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>**输入反转**：Sutskever等人发现，将输入序列反转后再输入编码器，翻译效果更好。这是因为输入的最后几个词（反转后变成最先输入的词）与输出的最先几个词往往有更强的对应关系。但这只是一个启发式技巧，不能从根本上解决问题。</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>这些方法都没有触及问题的核心：**解码器只能看到一个固定的、全局的表示，无法动态地访问输入的不同部分**。</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>理想的解决方案应该具备以下特性：</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**动态性**：解码器在生成不同词时，应该能够关注输入的不同位置</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**软选择**：不是硬性地选择某一个位置，而是对所有位置计算一个重要性分布</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**端到端可训练**：整个机制应该可以通过反向传播优化</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**可解释性**：模型关注哪些位置应该是可以观察和理解的</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>这些特性正是Attention机制所提供的。</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键洞察：动态的、基于内容的寻址</span></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>Attention的核心洞察可以用一句话概括：</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **让解码器在每一步都能"回头看"编码器的所有位置，并根据当前需要动态决定关注哪些位置。**</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>这个想法听起来简单，但它彻底改变了序列到序列学习的范式。</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a><span class="fu">### 直觉解释：聚光灯与图书馆</span></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>想象你在一个黑暗的图书馆里找书。传统Seq2Seq就像是：你先用手电筒快速扫过所有书架，然后关掉手电筒，仅凭记忆去取书。你对整个图书馆有一个模糊的整体印象，但细节很容易遗忘。</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>Attention机制则像是：你手里有一个**可调节的聚光灯**。当你需要找某本书时，你可以把聚光灯照向相关的书架，仔细查看那里的书名。不同的查询需求会让你把光照向不同的位置。</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>更具体地说，当解码器生成"août"（八月）这个词时，Attention机制会：</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>查看编码器的所有隐藏状态（图书馆的所有书架）</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>计算每个位置与当前任务的相关性（判断每个书架是否可能有你要的书）</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>把"聚光灯"主要照向相关的位置（"August"对应的编码器状态）</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>从这些位置汇总信息，辅助生成当前词</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### 另一个类比：加权投票</span></span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>你也可以把Attention理解为一种加权投票机制。</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>想象解码器是一个领导，需要做一个决定（生成下一个词）。它有一个顾问团队（编码器的各个隐藏状态），每个顾问掌握输入序列不同部分的信息。</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>传统Seq2Seq：只听一个"总顾问"的意见（上下文向量 $\mathbf{c}$），这个总顾问要综合所有人的信息。</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>Attention机制：直接征询每个顾问的意见，然后根据议题相关性给不同顾问的意见赋予不同权重，加权求和得出最终决定。</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="fu">### 设计动机：为什么选择软注意力？</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>Attention机制有两种基本变体：</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**软注意力（Soft Attention）**：对所有位置计算概率分布，加权求和</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**硬注意力（Hard Attention）**：选择一个位置，只看那里的信息</span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>Bahdanau等人选择了软注意力，原因是：</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**可微分**：软注意力的加权求和是可微的，可以用标准的反向传播训练</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**稳定**：硬注意力需要采样或强化学习方法训练，方差大，不稳定</span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**信息更丰富**：软注意力可以同时利用多个位置的信息，而不是非此即彼</span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>硬注意力也有其优势（计算更高效，更稀疏），但在实践中，软注意力因其简单和有效成为了主流。我们将在后面的小节中更详细地对比两者。</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bahdanau Attention：加性注意力</span></span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a>2014年，Bahdanau、Cho和Bengio提出了第一个成功的注意力机制用于机器翻译。让我们详细看看它是如何工作的。</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>首先，编码器使用**双向RNN**，在每个位置 $j$ 产生一个隐藏状态：</span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_j = <span class="co">[</span><span class="ot">\overrightarrow{\mathbf{h}}_j; \overleftarrow{\mathbf{h}}_j</span><span class="co">]</span></span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>其中 $\overrightarrow{\mathbf{h}}_j$ 是前向RNN的隐藏状态，$\overleftarrow{\mathbf{h}}_j$ 是后向RNN的隐藏状态。拼接后，$\mathbf{h}_j$ 同时包含了位置 $j$ 的左侧和右侧上下文。</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a>在解码的第 $i$ 步，我们计算一个**动态的上下文向量** $\mathbf{c}_i$（注意：不再是固定的 $\mathbf{c}$，而是每一步都不同的 $\mathbf{c}_i$）：</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j</span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>其中 $\alpha_{ij}$ 是第 $i$ 步解码时，对输入位置 $j$ 的注意力权重。</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>那么 $\alpha_{ij}$ 是怎么计算的呢？这是Attention机制的核心。</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a>首先，计算一个**对齐分数（alignment score）** $e_{ij}$，衡量解码器当前状态与编码器位置 $j$ 的相关性：</span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)</span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{s}_{i-1}$ 是解码器在第 $i-1$ 步的隐藏状态，$a$ 是一个**对齐模型（alignment model）**。</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>Bahdanau使用了一个单层前馈网络作为对齐模型：</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a>这被称为**加性注意力（additive attention）**，因为 $\mathbf{s}_{i-1}$ 和 $\mathbf{h}_j$ 是通过加法结合的。</span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a>然后，对所有位置的分数做softmax归一化，得到注意力权重：</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}</span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a>softmax确保了：</span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>所有权重都是正数：$\alpha_{ij} &gt; 0$</span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>权重之和为1：$\sum_j \alpha_{ij} = 1$</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a>这样，$\alpha_{ij}$ 可以解释为一个概率分布——解码器在第 $i$ 步"关注"输入位置 $j$ 的概率。</span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Bahdanau Attention (Bahdanau et al., 2015)</span></span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):</span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a><span class="co">    Bahdanau (加性) 注意力机制</span></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a><span class="co">        s_prev: 解码器上一步的隐藏状态 [batch, dec_hidden]</span></span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]</span></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="co">        W_a, U_a, v_a: 可学习参数</span></span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a><span class="co">        context: 上下文向量 [batch, enc_hidden]</span></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_weights: 注意力权重 [batch, src_len]</span></span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 计算对齐分数</span></span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a>    <span class="co"># s_prev 广播到所有源位置</span></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> v_a <span class="op">@</span> tanh(W_a <span class="op">@</span> s_prev <span class="op">+</span> U_a <span class="op">@</span> encoder_outputs)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Softmax 归一化</span></span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 加权求和</span></span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> attention_weights <span class="op">@</span> encoder_outputs  <span class="co"># [batch, enc_hidden]</span></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, attention_weights</span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a>*Source: Bahdanau, Cho, &amp; Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", ICLR 2015. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a>下图展示了带Bahdanau Attention的RNN Encoder-Decoder架构：</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a><span class="al">![带Bahdanau Attention的RNN Encoder-Decoder架构。编码器（底部）使用双向RNN处理输入序列，产生隐藏状态序列。解码器（顶部）在每一步通过Attention机制动态计算上下文向量：将当前解码器状态与所有编码器状态比较，得到注意力权重，加权求和得到上下文向量$c_t$，辅助生成下一个词。](figures/chapter-6/original/fig-bahdanau-attention-d2l.svg)</span>{#fig-attention-mechanism width=70%}</span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a>*Source: Dive into Deep Learning, Figure 11.4.2. [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)*</span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：Bahdanau Attention计算</span></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a>让我们用一个小例子走一遍完整的Attention计算过程。</span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入序列：3个词（"I love NLP"），编码后得到3个隐藏状态</span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>解码器隐藏状态维度：$d_s = 4$</span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>编码器隐藏状态维度：$d_h = 4$</span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>注意力中间维度：$d_a = 3$</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>**编码器输出**（假设已经计算好）：</span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_1 = <span class="co">[</span><span class="ot">0.2, 0.5, -0.3, 0.8</span><span class="co">]</span>^\top \quad \text{("I")}</span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_2 = <span class="co">[</span><span class="ot">0.7, -0.2, 0.4, 0.1</span><span class="co">]</span>^\top \quad \text{("love")}</span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_3 = <span class="co">[</span><span class="ot">-0.1, 0.6, 0.5, -0.4</span><span class="co">]</span>^\top \quad \text{("NLP")}</span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a>**解码器当前状态**（正在生成第一个目标词）：</span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a>\mathbf{s}_0 = <span class="co">[</span><span class="ot">0.1, -0.3, 0.4, 0.2</span><span class="co">]</span>^\top</span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a>**参数**（简化的随机值）：</span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_a = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.1 <span class="sc">\\</span> 0.2 &amp; 0.1 &amp; -0.1 &amp; 0.2 <span class="sc">\\</span> -0.1 &amp; 0.3 &amp; 0.2 &amp; -0.2 \end{bmatrix}, \quad</span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a = \begin{bmatrix} 0.2 &amp; 0.1 &amp; -0.2 &amp; 0.3 <span class="sc">\\</span> -0.1 &amp; 0.2 &amp; 0.1 &amp; 0.1 <span class="sc">\\</span> 0.3 &amp; -0.1 &amp; 0.2 &amp; -0.1 \end{bmatrix}</span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a>\mathbf{v}_a = <span class="co">[</span><span class="ot">0.5, -0.3, 0.4</span><span class="co">]</span>^\top</span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a>**Step 1：计算 $\mathbf{W}_a \mathbf{s}_0$**</span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_a \mathbf{s}_0 = \begin{bmatrix} 0.1 \cdot 0.1 + (-0.2) \cdot (-0.3) + 0.3 \cdot 0.4 + 0.1 \cdot 0.2 <span class="sc">\\</span> \vdots \end{bmatrix} = \begin{bmatrix} 0.21 <span class="sc">\\</span> 0.03 <span class="sc">\\</span> 0.04 \end{bmatrix}</span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a>**Step 2：对每个编码器状态计算 $\mathbf{U}_a \mathbf{h}_j$**</span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a \mathbf{h}_1 = <span class="co">[</span><span class="ot">0.33, 0.14, -0.05</span><span class="co">]</span>^\top</span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a \mathbf{h}_2 = <span class="co">[</span><span class="ot">0.13, 0.06, 0.29</span><span class="co">]</span>^\top</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a>\mathbf{U}_a \mathbf{h}_3 = <span class="co">[</span><span class="ot">-0.15, 0.17, 0.05</span><span class="co">]</span>^\top</span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>**Step 3：计算对齐分数 $e_{1j}$**</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a>e_{11} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_0 + \mathbf{U}_a \mathbf{h}_1) = \mathbf{v}_a^\top \tanh(<span class="co">[</span><span class="ot">0.54, 0.17, -0.01</span><span class="co">]</span>^\top)</span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.5, -0.3, 0.4</span><span class="co">]</span> \cdot <span class="co">[</span><span class="ot">\tanh(0.54), \tanh(0.17), \tanh(-0.01)</span><span class="co">]</span>^\top</span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.5, -0.3, 0.4</span><span class="co">]</span> \cdot <span class="co">[</span><span class="ot">0.49, 0.17, -0.01</span><span class="co">]</span>^\top = 0.24 - 0.05 - 0.004 \approx 0.19</span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a>类似地计算 $e_{12}$ 和 $e_{13}$：</span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a>e_{12} \approx 0.25, \quad e_{13} \approx 0.08</span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a>**Step 4：Softmax归一化**</span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>\alpha_{11} = \frac{\exp(0.19)}{\exp(0.19) + \exp(0.25) + \exp(0.08)} = \frac{1.21}{1.21 + 1.28 + 1.08} = \frac{1.21}{3.57} \approx 0.34</span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>\alpha_{12} = \frac{1.28}{3.57} \approx 0.36, \quad \alpha_{13} = \frac{1.08}{3.57} \approx 0.30</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>**Step 5：计算上下文向量**</span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_1 = \alpha_{11} \mathbf{h}_1 + \alpha_{12} \mathbf{h}_2 + \alpha_{13} \mathbf{h}_3</span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a>= 0.34 \cdot <span class="co">[</span><span class="ot">0.2, 0.5, -0.3, 0.8</span><span class="co">]</span>^\top + 0.36 \cdot <span class="co">[</span><span class="ot">0.7, -0.2, 0.4, 0.1</span><span class="co">]</span>^\top + 0.30 \cdot <span class="co">[</span><span class="ot">-0.1, 0.6, 0.5, -0.4</span><span class="co">]</span>^\top</span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a>\approx <span class="co">[</span><span class="ot">0.29, 0.18, 0.09, 0.19</span><span class="co">]</span>^\top</span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>**解读**：在这个例子中，模型对"love"的关注最多（0.36），其次是"I"（0.34）和"NLP"（0.30）。注意力权重相对均匀，这可能是因为我们用的是随机参数。在训练后的真实模型中，权重分布会更加尖锐——模型会学会在需要时聚焦于特定位置。</span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a><span class="fu">### 解码器的完整流程</span></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>有了Attention机制，解码器的每一步工作流程变为：</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**计算注意力权重** $\alpha_{ij}$：基于当前解码器状态和所有编码器状态</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**计算上下文向量** $\mathbf{c}_i$：对编码器状态加权求和</span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**更新解码器状态**：结合上下文向量、前一步输出、前一步状态</span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a>\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)</span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**生成输出**：基于新的解码器状态</span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a>P(y_i | y_{&lt;i}, \mathbf{x}) = g(\mathbf{s}_i, y_{i-1}, \mathbf{c}_i)</span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>关键区别是：**每一步都有一个不同的上下文向量 $\mathbf{c}_i$**，它是根据当前任务动态计算的。</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="fu">### Luong Attention：乘性注意力的系统性探索</span></span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a>Bahdanau的加性注意力虽然有效，但它的计算量不小——每次计算对齐分数都需要一个前馈网络（两个矩阵乘法、一个非线性激活、一个向量点积）。一个自然的问题随之浮现：能否用更简单的操作来衡量两个状态的相关性？</span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a>2015年，距Bahdanau论文发表仅一年，斯坦福大学的Luong、Pham和Manning发表了一篇系统性的研究，回答了这个问题。他们提出了计算效率更高的**乘性注意力（multiplicative attention）**，并系统比较了三种对齐函数：</span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 名称 <span class="pp">|</span> 公式 <span class="pp">|</span> 参数 <span class="pp">|</span> 计算效率 <span class="pp">|</span></span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|------|----------|</span></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Dot** <span class="pp">|</span> $\mathbf{s}^\top \mathbf{h}$ <span class="pp">|</span> 无 <span class="pp">|</span> 最快 <span class="pp">|</span></span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **General** <span class="pp">|</span> $\mathbf{s}^\top \mathbf{W}_a \mathbf{h}$ <span class="pp">|</span> $\mathbf{W}_a$ <span class="pp">|</span> 中等 <span class="pp">|</span></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Concat** <span class="pp">|</span> $\mathbf{v}_a^\top \tanh(\mathbf{W}_a <span class="co">[</span><span class="ot">\mathbf{s}; \mathbf{h}</span><span class="co">]</span>)$ <span class="pp">|</span> $\mathbf{v}_a, \mathbf{W}_a$ <span class="pp">|</span> 最慢 <span class="pp">|</span></span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a>**Dot（点积）**：最简单的形式，直接计算两个向量的内积 $e_{ij} = \mathbf{s}_i^\top \mathbf{h}_j$。没有任何可学习参数，计算最快，但要求解码器和编码器的隐藏维度必须相同。直觉上，点积衡量的是两个向量的"相似度"——如果它们指向相似的方向，点积就大；如果正交，点积为零。</span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>**General（一般形式）**：引入一个可学习矩阵 $\mathbf{W}_a$，允许不同维度的状态相互比较，也增加了模型的表达能力。实质上是问："$\mathbf{s}$ 和 $\mathbf{W}_a \mathbf{h}$（$\mathbf{h}$ 的一个线性变换）有多相似？"</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a>**Concat（拼接）**：这就是Bahdanau的加性注意力。表达能力最强——它可以学习任意非线性的对齐函数——但计算最慢。</span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a>除了对齐函数，Luong还发现了另一个重要的设计差异：**注意力在解码器中的使用位置**。</span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a>Bahdanau的方式是先算注意力再更新RNN状态：$\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)$。Luong的方式则更加模块化——先用RNN计算新状态，再基于新状态计算注意力：</span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a>\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1})</span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \text{Attention}(\mathbf{s}_i, \mathbf{H})</span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbf{s}}_i = \tanh(\mathbf{W}_c <span class="co">[</span><span class="ot">\mathbf{c}_i; \mathbf{s}_i</span><span class="co">]</span>)</span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a>这种设计让RNN和Attention解耦，便于分析和调试。</span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Luong Attention Variants (Luong et al., 2015)</span></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> luong_attention(decoder_state, encoder_outputs, method<span class="op">=</span><span class="st">'dot'</span>, W_a<span class="op">=</span><span class="va">None</span>, v_a<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a><span class="co">    Luong 注意力机制的三种变体</span></span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_state: 解码器当前隐藏状态 [batch, dec_hidden]</span></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]</span></span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a><span class="co">        method: 'dot', 'general', 或 'concat'</span></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a><span class="co">        W_a: 可学习参数（general和concat需要）</span></span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a><span class="co">        v_a: 可学习参数（concat需要）</span></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a><span class="co">        context: 上下文向量 [batch, enc_hidden]</span></span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_weights: 注意力权重 [batch, src_len]</span></span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> method <span class="op">==</span> <span class="st">'dot'</span>:</span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 点积: s^T h</span></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, dec_hidden] @ [batch, enc_hidden, src_len] -&gt; [batch, src_len]</span></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a>                          encoder_outputs.transpose(<span class="dv">1</span>, <span class="dv">2</span>)).squeeze(<span class="dv">1</span>)</span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'general'</span>:</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 一般形式: s^T W h</span></span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 先计算 W @ h: [batch, src_len, dec_hidden]</span></span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a>        transformed <span class="op">=</span> encoder_outputs <span class="op">@</span> W_a.T</span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a>                          transformed.transpose(<span class="dv">1</span>, <span class="dv">2</span>)).squeeze(<span class="dv">1</span>)</span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'concat'</span>:</span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接形式: v^T tanh(W [s; h])</span></span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 扩展 decoder_state 到所有位置</span></span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a>        s_expanded <span class="op">=</span> decoder_state.unsqueeze(<span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>, encoder_outputs.size(<span class="dv">1</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a>        concat <span class="op">=</span> torch.cat([s_expanded, encoder_outputs], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> v_a <span class="op">@</span> torch.tanh(concat <span class="op">@</span> W_a.T).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.squeeze(<span class="dv">1</span>)</span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Softmax 归一化</span></span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 加权求和</span></span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> torch.bmm(attention_weights.unsqueeze(<span class="dv">1</span>), encoder_outputs).squeeze(<span class="dv">1</span>)</span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, attention_weights</span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a>*Source: Luong, Pham, &amp; Manning (2015) "Effective Approaches to Attention-based Neural Machine Translation", EMNLP 2015. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*</span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a><span class="fu">### 三种对齐函数的数值对比</span></span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>让我们用同一组数据，比较点积对齐和加性对齐的差异。</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>解码器状态：$\mathbf{s} = <span class="co">[</span><span class="ot">0.5, -0.3, 0.8, 0.2</span><span class="co">]</span>^\top$</span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>编码器状态（3个位置）：</span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\mathbf{h}_1 = <span class="co">[</span><span class="ot">0.2, 0.4, 0.1, -0.3</span><span class="co">]</span>^\top$</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\mathbf{h}_2 = <span class="co">[</span><span class="ot">0.6, -0.1, 0.7, 0.3</span><span class="co">]</span>^\top$</span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\mathbf{h}_3 = <span class="co">[</span><span class="ot">-0.2, 0.5, 0.3, 0.1</span><span class="co">]</span>^\top$</span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a>**Dot-Product 计算**：</span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a>e_1 = \mathbf{s}^\top \mathbf{h}_1 = 0.5 \times 0.2 + (-0.3) \times 0.4 + 0.8 \times 0.1 + 0.2 \times (-0.3)</span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a>= 0.10 - 0.12 + 0.08 - 0.06 = 0.00</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a>e_2 = \mathbf{s}^\top \mathbf{h}_2 = 0.5 \times 0.6 + (-0.3) \times (-0.1) + 0.8 \times 0.7 + 0.2 \times 0.3</span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a>= 0.30 + 0.03 + 0.56 + 0.06 = 0.95</span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a>e_3 = \mathbf{s}^\top \mathbf{h}_3 = 0.5 \times (-0.2) + (-0.3) \times 0.5 + 0.8 \times 0.3 + 0.2 \times 0.1</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a>= -0.10 - 0.15 + 0.24 + 0.02 = 0.01</span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a>**Softmax 归一化**：</span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>\alpha_1 = \frac{e^{0.00}}{e^{0.00} + e^{0.95} + e^{0.01}} = \frac{1.00}{1.00 + 2.59 + 1.01} = \frac{1.00}{4.60} \approx 0.22</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a>\alpha_2 = \frac{e^{0.95}}{4.60} = \frac{2.59}{4.60} \approx 0.56</span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>\alpha_3 = \frac{e^{0.01}}{4.60} = \frac{1.01}{4.60} \approx 0.22</span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a>**解读**：使用点积注意力，模型将56%的注意力放在位置2，这是因为 $\mathbf{h}_2$ 与 $\mathbf{s}$ 在向量空间中最"相似"——它们的方向最为一致。注意，点积注意力不需要任何可学习参数，仅靠向量的几何关系就能区分出最相关的位置。而当 $\mathbf{W}_a = \mathbf{I}$（单位矩阵）时，General退化为Dot-Product；在一般情况下，$\mathbf{W}_a$ 允许模型学习更复杂的相关性模式。</span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a><span class="fu">### Global vs Local Attention</span></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a>Luong还提出了另一个重要的设计维度：**注意力的范围**。</span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a>**Global Attention** 关注源序列的所有位置，这是Bahdanau的做法，也是前面讨论的默认方式：</span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a>**Local Attention** 只关注源序列的一个**窗口**。核心思想是：在每个解码步，先预测一个对齐位置 $p_i$，然后只计算以 $p_i$ 为中心、宽度为 $2D+1$ 的窗口内的注意力：</span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=p_i-D}^{p_i+D} \alpha_{ij} \mathbf{h}_j</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>对齐位置 $p_i$ 可以通过两种方式确定：</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a>**Local-m（单调）**：假设源和目标大致对齐，简单设置 $p_i = i$。</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>**Local-p（预测）**：学习一个函数来预测 $p_i$：</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a>p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))</span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a>其中 $\sigma$ 是sigmoid函数，确保 $p_i \in <span class="co">[</span><span class="ot">0, T_x</span><span class="co">]</span>$。为了让注意力在窗口中心附近更集中，Local Attention还引入了一个高斯偏置：</span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \text{align}(\mathbf{s}_i, \mathbf{h}_j) \cdot \exp\left(-\frac{(j - p_i)^2}{2\sigma^2}\right)</span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Luong论文中的Global vs Local Attention对比。左边是Global Attention：解码器状态 $h_t$ 与所有源位置计算注意力，生成上下文向量 $c_t$。右边是Local Attention：先预测对齐位置 $p_t$，只计算窗口 $[p_t - D, p_t + D]$ 内的注意力。</span><span class="co">](figures/chapter-6/original/fig-global-local-attention.png)</span>{#fig-global-local width=80%}</span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>*Source: Luong, Pham, &amp; Manning (2015) "Effective Approaches to Attention-based Neural Machine Translation", Figure 2 &amp; 3. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hard vs Soft Attention</span></span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a>除了Global/Local的维度，还有另一个基本区分：**软注意力（Soft Attention）** vs **硬注意力（Hard Attention）**。</span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a>**Soft Attention** 计算所有位置的注意力权重（一个概率分布），然后加权求和：</span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_j \alpha_{ij} \mathbf{h}_j = \mathbb{E}_{p(j | \mathbf{s}_i, \mathbf{H})}<span class="co">[</span><span class="ot">\mathbf{h}_j</span><span class="co">]</span></span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a>**Hard Attention** 从注意力分布中**采样**一个位置 $j^*$，只使用那个位置的信息：</span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a>j^* \sim \text{Categorical}(\alpha_{i1}, \alpha_{i2}, \ldots, \alpha_{iT_x})</span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \mathbf{h}_{j^*}</span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a>两者的核心区别在于**可微分性**。Soft Attention是可微分的——加权求和是一个连续操作，梯度可以通过 $\alpha_{ij}$ 流向对齐函数的参数。Hard Attention不可微分——采样操作是离散的，需要强化学习方法（如REINFORCE）来训练，带来高方差和训练不稳定的问题。</span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hard Attention的训练困难</span></span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>Hard Attention虽然在概念上更接近人类的"注意"（我们真的只看一个地方，而不是模糊地看所有地方），但它的训练需要强化学习技术：</span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a>\nabla_\theta J = \mathbb{E}_{j^* \sim p(j|\theta)} \left[ \nabla_\theta \log p(j^* | \theta) \cdot R(j^*) \right]</span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a>其中 $R(j^*)$ 是选择位置 $j^*$ 带来的"奖励"。这个梯度估计的方差很大，需要大量采样才能稳定。</span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a>实践中，**Soft Attention几乎总是更好的选择**，因为：</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>端到端可微分，训练简单</span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>梯度估计没有方差问题</span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>可以同时利用多个位置的信息</span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度分析</span></span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a>不同注意力变体的计算复杂度汇总如下：</span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 变体 <span class="pp">|</span> 对齐计算 <span class="pp">|</span> 总复杂度 <span class="pp">|</span></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|----------|</span></span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Global + Dot** <span class="pp">|</span> $O(T_x \cdot d)$ per step <span class="pp">|</span> $O(T_x \cdot T_y \cdot d)$ <span class="pp">|</span></span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Global + General** <span class="pp">|</span> $O(T_x \cdot d^2)$ per step <span class="pp">|</span> $O(T_x \cdot T_y \cdot d^2)$ <span class="pp">|</span></span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Global + Concat** <span class="pp">|</span> $O(T_x \cdot d^2)$ per step <span class="pp">|</span> $O(T_x \cdot T_y \cdot d^2)$ <span class="pp">|</span></span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Local** <span class="pp">|</span> $O(D \cdot d)$ per step <span class="pp">|</span> $O(D \cdot T_y \cdot d)$ <span class="pp">|</span></span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>其中 $T_x$ 是源序列长度，$T_y$ 是目标序列长度，$d$ 是隐藏维度，$D$ 是局部窗口大小。</span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>与标准Seq2Seq相比，Global Attention增加了 $O(T_x \cdot T_y)$ 的计算量，同时需要存储所有编码器隐藏状态（$O(T_x \cdot d)$ 空间）。这是用空间换取性能的典型例子。Local Attention的优势在长序列时尤为明显：当 $T_x = 1000$ 而 $D = 50$ 时，计算量减少了20倍。</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a><span class="fu">## 注意力可视化：模型在"看"什么？</span></span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对齐矩阵</span></span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a>Attention机制的一个美妙特性是**可解释性**。注意力权重 $\alpha_{ij}$ 直接告诉我们：在生成第 $i$ 个目标词时，模型关注了哪些源词。</span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a>我们可以把所有的注意力权重排列成一个矩阵，横轴是源序列，纵轴是目标序列。这个矩阵被称为**对齐矩阵（alignment matrix）**。</span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a><span class="al">![对齐可视化：四个英法翻译例子的注意力权重热力图。横轴是英语源句子，纵轴是法语目标句子。白色表示高注意力权重，黑色表示低权重。注意对角线模式（单词一一对应）和偏离对角线的区域（语序调整）。例如(a)中"August"对应"août"，"European Economic Area"对应"zone économique européenne"。](figures/chapter-6/original/fig3-alignment-visualization.png)</span>{#fig-alignment-visualization width=95%}</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a>*Source: Bahdanau, Cho, &amp; Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", Figure 3. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*</span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对齐模式的语言学意义</span></span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a>通过观察对齐矩阵，我们可以发现一些有趣的语言学模式：</span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>**单调对齐**：对于语序相似的语言对（如英语到德语的某些结构），对齐矩阵接近对角线——第1个源词对应第1个目标词，第2个对应第2个，依此类推。</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a>**语序调整**：当源语言和目标语言的词序不同时，对齐矩阵会偏离对角线。例如，英语的"red car"翻译成法语是"voiture rouge"（车 红），对齐矩阵会显示交叉模式。</span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a>**一对多和多对一**：某些词没有直接对应，或一个词对应多个词。例如，英语的"going to"可能对应法语的单个词"va"。</span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a>**空对齐**：某些目标词（如冠词）可能没有明确的源词对应，它们的注意力权重会分散在多个位置。</span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a><span class="fu">### 可视化的局限性</span></span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a>虽然注意力可视化很吸引人，但我们要谨慎解读：</span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**注意力不等于解释**：高注意力权重不一定意味着模型"理解"了那个位置的内容</span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**可能有多重因素**：模型可能通过其他机制（如位置信息）做出决定</span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**训练目标的影响**：注意力权重是为了最小化翻译损失而学习的，不一定反映人类的对齐直觉</span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a>后来的研究（如Jain &amp; Wallace, 2019）对注意力的可解释性提出了质疑。但作为一个诊断工具，注意力可视化仍然非常有价值。</span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-636"><a href="#cb12-636" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-637"><a href="#cb12-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践：带Attention的Seq2Seq</span></span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a>让我们用PyTorch实现一个完整的带Attention的Seq2Seq模型，涵盖Bahdanau和Luong两种注意力机制。</span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a><span class="fu">### 编码器</span></span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-646"><a href="#cb12-646" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, hidden_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a>            embed_dim, hidden_dim,</span>
<span id="cb12-658"><a href="#cb12-658" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb12-659"><a href="#cb12-659" aria-hidden="true" tabindex="-1"></a>            bidirectional<span class="op">=</span><span class="va">True</span>,  <span class="co"># 双向</span></span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 将双向的隐藏状态压缩到单向维度</span></span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_dim <span class="op">*</span> <span class="dv">2</span>, hidden_dim)</span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, src_lengths<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a>        <span class="co"># src: [batch_size, src_len]</span></span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(src))  <span class="co"># [batch, src_len, embed_dim]</span></span>
<span id="cb12-670"><a href="#cb12-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-671"><a href="#cb12-671" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> src_lengths <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a>            packed <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a>                embedded, src_lengths.cpu(), batch_first<span class="op">=</span><span class="va">True</span>, enforce_sorted<span class="op">=</span><span class="va">False</span></span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a>            packed_outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(packed)</span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a>            outputs, _ <span class="op">=</span> nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a>            outputs, hidden <span class="op">=</span> <span class="va">self</span>.rnn(embedded)</span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs: [batch, src_len, hidden_dim * 2] (双向拼接)</span></span>
<span id="cb12-681"><a href="#cb12-681" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [num_layers * 2, batch, hidden_dim]</span></span>
<span id="cb12-682"><a href="#cb12-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 合并前向和后向的最终隐藏状态</span></span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-2] 是最后一层前向，hidden[-1] 是最后一层后向</span></span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> torch.tanh(<span class="va">self</span>.fc(torch.cat([hidden[<span class="op">-</span><span class="dv">2</span>], hidden[<span class="op">-</span><span class="dv">1</span>]], dim<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, hidden_dim]</span></span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, hidden</span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bahdanau Attention层</span></span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-696"><a href="#cb12-696" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-697"><a href="#cb12-697" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BahdanauAttention(nn.Module):</span>
<span id="cb12-698"><a href="#cb12-698" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, attention_dim):</span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加性注意力的参数</span></span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(dec_hidden_dim, attention_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.U_a <span class="op">=</span> nn.Linear(enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>, attention_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># 双向编码器</span></span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_a <span class="op">=</span> nn.Linear(attention_dim, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-704"><a href="#cb12-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-705"><a href="#cb12-705" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-707"><a href="#cb12-707" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_hidden: [batch, dec_hidden]</span></span>
<span id="cb12-708"><a href="#cb12-708" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, src_len], True表示需要mask的位置（padding）</span></span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decoder_hidden 扩展到所有源位置</span></span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, dec_hidden] -&gt; [batch, src_len, dec_hidden]</span></span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a>        decoder_hidden <span class="op">=</span> decoder_hidden.unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, src_len, <span class="dv">1</span>)</span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算对齐分数</span></span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, attention_dim]</span></span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a>        energy <span class="op">=</span> torch.tanh(<span class="va">self</span>.W_a(decoder_hidden) <span class="op">+</span> <span class="va">self</span>.U_a(encoder_outputs))</span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, src_len, 1] -&gt; [batch, src_len]</span></span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> <span class="va">self</span>.v_a(energy).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-722"><a href="#cb12-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-723"><a href="#cb12-723" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用mask（将padding位置的分数设为很小的负数）</span></span>
<span id="cb12-724"><a href="#cb12-724" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-725"><a href="#cb12-725" aria-hidden="true" tabindex="-1"></a>            attention_scores <span class="op">=</span> attention_scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb12-726"><a href="#cb12-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-727"><a href="#cb12-727" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax归一化</span></span>
<span id="cb12-728"><a href="#cb12-728" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(attention_scores, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-729"><a href="#cb12-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-730"><a href="#cb12-730" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算上下文向量</span></span>
<span id="cb12-731"><a href="#cb12-731" aria-hidden="true" tabindex="-1"></a>        <span class="co"># [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -&gt; [batch, 1, enc_hidden*2]</span></span>
<span id="cb12-732"><a href="#cb12-732" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(attention_weights.unsqueeze(<span class="dv">1</span>), encoder_outputs)</span>
<span id="cb12-733"><a href="#cb12-733" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> context.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, enc_hidden * 2]</span></span>
<span id="cb12-734"><a href="#cb12-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-735"><a href="#cb12-735" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights</span>
<span id="cb12-736"><a href="#cb12-736" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-737"><a href="#cb12-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-738"><a href="#cb12-738" aria-hidden="true" tabindex="-1"></a><span class="fu">### 解码器</span></span>
<span id="cb12-739"><a href="#cb12-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-742"><a href="#cb12-742" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-743"><a href="#cb12-743" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-744"><a href="#cb12-744" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionDecoder(nn.Module):</span>
<span id="cb12-745"><a href="#cb12-745" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,</span>
<span id="cb12-746"><a href="#cb12-746" aria-hidden="true" tabindex="-1"></a>                 attention_dim, num_layers<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb12-747"><a href="#cb12-747" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-748"><a href="#cb12-748" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb12-749"><a href="#cb12-749" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)</span>
<span id="cb12-750"><a href="#cb12-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-751"><a href="#cb12-751" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb12-752"><a href="#cb12-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-753"><a href="#cb12-753" aria-hidden="true" tabindex="-1"></a>        <span class="co"># GRU输入是：embedded + context</span></span>
<span id="cb12-754"><a href="#cb12-754" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb12-755"><a href="#cb12-755" aria-hidden="true" tabindex="-1"></a>            embed_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span>,  <span class="co"># 双向编码器</span></span>
<span id="cb12-756"><a href="#cb12-756" aria-hidden="true" tabindex="-1"></a>            dec_hidden_dim,</span>
<span id="cb12-757"><a href="#cb12-757" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb12-758"><a href="#cb12-758" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-759"><a href="#cb12-759" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout <span class="cf">if</span> num_layers <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb12-760"><a href="#cb12-760" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-761"><a href="#cb12-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-762"><a href="#cb12-762" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb12-763"><a href="#cb12-763" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(dec_hidden_dim <span class="op">+</span> enc_hidden_dim <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> embed_dim, vocab_size)</span>
<span id="cb12-764"><a href="#cb12-764" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb12-765"><a href="#cb12-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-766"><a href="#cb12-766" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_token, hidden, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-767"><a href="#cb12-767" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-768"><a href="#cb12-768" aria-hidden="true" tabindex="-1"></a><span class="co">        单步解码</span></span>
<span id="cb12-769"><a href="#cb12-769" aria-hidden="true" tabindex="-1"></a><span class="co">        input_token: [batch] - 上一步的输出token</span></span>
<span id="cb12-770"><a href="#cb12-770" aria-hidden="true" tabindex="-1"></a><span class="co">        hidden: [1, batch, dec_hidden] - 上一步的隐藏状态</span></span>
<span id="cb12-771"><a href="#cb12-771" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden * 2]</span></span>
<span id="cb12-772"><a href="#cb12-772" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-773"><a href="#cb12-773" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Embedding</span></span>
<span id="cb12-774"><a href="#cb12-774" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(input_token))  <span class="co"># [batch, embed_dim]</span></span>
<span id="cb12-775"><a href="#cb12-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-776"><a href="#cb12-776" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention</span></span>
<span id="cb12-777"><a href="#cb12-777" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden[-1] 取最后一层，[batch, dec_hidden]</span></span>
<span id="cb12-778"><a href="#cb12-778" aria-hidden="true" tabindex="-1"></a>        context, attention_weights <span class="op">=</span> <span class="va">self</span>.attention(hidden[<span class="op">-</span><span class="dv">1</span>], encoder_outputs, mask)</span>
<span id="cb12-779"><a href="#cb12-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-780"><a href="#cb12-780" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接embedded和context作为RNN输入</span></span>
<span id="cb12-781"><a href="#cb12-781" aria-hidden="true" tabindex="-1"></a>        rnn_input <span class="op">=</span> torch.cat([embedded, context], dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch, 1, embed+ctx]</span></span>
<span id="cb12-782"><a href="#cb12-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-783"><a href="#cb12-783" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RNN</span></span>
<span id="cb12-784"><a href="#cb12-784" aria-hidden="true" tabindex="-1"></a>        output, hidden <span class="op">=</span> <span class="va">self</span>.rnn(rnn_input, hidden)</span>
<span id="cb12-785"><a href="#cb12-785" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.squeeze(<span class="dv">1</span>)  <span class="co"># [batch, dec_hidden]</span></span>
<span id="cb12-786"><a href="#cb12-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-787"><a href="#cb12-787" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出层</span></span>
<span id="cb12-788"><a href="#cb12-788" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="va">self</span>.fc(torch.cat([output, context, embedded], dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb12-789"><a href="#cb12-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-790"><a href="#cb12-790" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction, hidden, attention_weights</span>
<span id="cb12-791"><a href="#cb12-791" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-792"><a href="#cb12-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-793"><a href="#cb12-793" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整的Seq2Seq模型</span></span>
<span id="cb12-794"><a href="#cb12-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-797"><a href="#cb12-797" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-798"><a href="#cb12-798" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-799"><a href="#cb12-799" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2SeqAttention(nn.Module):</span>
<span id="cb12-800"><a href="#cb12-800" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, device):</span>
<span id="cb12-801"><a href="#cb12-801" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-802"><a href="#cb12-802" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb12-803"><a href="#cb12-803" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb12-804"><a href="#cb12-804" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb12-805"><a href="#cb12-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-806"><a href="#cb12-806" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg, teacher_forcing_ratio<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb12-807"><a href="#cb12-807" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-808"><a href="#cb12-808" aria-hidden="true" tabindex="-1"></a><span class="co">        src: [batch, src_len]</span></span>
<span id="cb12-809"><a href="#cb12-809" aria-hidden="true" tabindex="-1"></a><span class="co">        trg: [batch, trg_len]</span></span>
<span id="cb12-810"><a href="#cb12-810" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-811"><a href="#cb12-811" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> src.shape[<span class="dv">0</span>]</span>
<span id="cb12-812"><a href="#cb12-812" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb12-813"><a href="#cb12-813" aria-hidden="true" tabindex="-1"></a>        trg_vocab_size <span class="op">=</span> <span class="va">self</span>.decoder.vocab_size</span>
<span id="cb12-814"><a href="#cb12-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-815"><a href="#cb12-815" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 存储输出</span></span>
<span id="cb12-816"><a href="#cb12-816" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> torch.zeros(batch_size, trg_len, trg_vocab_size).to(<span class="va">self</span>.device)</span>
<span id="cb12-817"><a href="#cb12-817" aria-hidden="true" tabindex="-1"></a>        attentions <span class="op">=</span> []</span>
<span id="cb12-818"><a href="#cb12-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-819"><a href="#cb12-819" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 编码</span></span>
<span id="cb12-820"><a href="#cb12-820" aria-hidden="true" tabindex="-1"></a>        encoder_outputs, hidden <span class="op">=</span> <span class="va">self</span>.encoder(src)</span>
<span id="cb12-821"><a href="#cb12-821" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden: [batch, dec_hidden] -&gt; [1, batch, dec_hidden]</span></span>
<span id="cb12-822"><a href="#cb12-822" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> hidden.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb12-823"><a href="#cb12-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-824"><a href="#cb12-824" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 第一个解码输入是 &lt;sos&gt; token</span></span>
<span id="cb12-825"><a href="#cb12-825" aria-hidden="true" tabindex="-1"></a>        input_token <span class="op">=</span> trg[:, <span class="dv">0</span>]</span>
<span id="cb12-826"><a href="#cb12-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-827"><a href="#cb12-827" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, trg_len):</span>
<span id="cb12-828"><a href="#cb12-828" aria-hidden="true" tabindex="-1"></a>            prediction, hidden, attention <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb12-829"><a href="#cb12-829" aria-hidden="true" tabindex="-1"></a>                input_token, hidden, encoder_outputs</span>
<span id="cb12-830"><a href="#cb12-830" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb12-831"><a href="#cb12-831" aria-hidden="true" tabindex="-1"></a>            outputs[:, t] <span class="op">=</span> prediction</span>
<span id="cb12-832"><a href="#cb12-832" aria-hidden="true" tabindex="-1"></a>            attentions.append(attention)</span>
<span id="cb12-833"><a href="#cb12-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-834"><a href="#cb12-834" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Teacher forcing</span></span>
<span id="cb12-835"><a href="#cb12-835" aria-hidden="true" tabindex="-1"></a>            teacher_force <span class="op">=</span> torch.rand(<span class="dv">1</span>).item() <span class="op">&lt;</span> teacher_forcing_ratio</span>
<span id="cb12-836"><a href="#cb12-836" aria-hidden="true" tabindex="-1"></a>            top1 <span class="op">=</span> prediction.argmax(<span class="dv">1</span>)</span>
<span id="cb12-837"><a href="#cb12-837" aria-hidden="true" tabindex="-1"></a>            input_token <span class="op">=</span> trg[:, t] <span class="cf">if</span> teacher_force <span class="cf">else</span> top1</span>
<span id="cb12-838"><a href="#cb12-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-839"><a href="#cb12-839" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, torch.stack(attentions, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-840"><a href="#cb12-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-841"><a href="#cb12-841" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建模型示例</span></span>
<span id="cb12-842"><a href="#cb12-842" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb12-843"><a href="#cb12-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-844"><a href="#cb12-844" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>, hidden_dim<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb12-845"><a href="#cb12-845" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> AttentionDecoder(</span>
<span id="cb12-846"><a href="#cb12-846" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb12-847"><a href="#cb12-847" aria-hidden="true" tabindex="-1"></a>    enc_hidden_dim<span class="op">=</span><span class="dv">512</span>, dec_hidden_dim<span class="op">=</span><span class="dv">512</span>, attention_dim<span class="op">=</span><span class="dv">256</span></span>
<span id="cb12-848"><a href="#cb12-848" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-849"><a href="#cb12-849" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2SeqAttention(encoder, decoder, device).to(device)</span>
<span id="cb12-850"><a href="#cb12-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-851"><a href="#cb12-851" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"编码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> encoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-852"><a href="#cb12-852" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"解码器参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decoder.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-853"><a href="#cb12-853" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"总参数: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb12-854"><a href="#cb12-854" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-855"><a href="#cb12-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-856"><a href="#cb12-856" aria-hidden="true" tabindex="-1"></a><span class="fu">### Luong Attention实现</span></span>
<span id="cb12-857"><a href="#cb12-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-860"><a href="#cb12-860" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-861"><a href="#cb12-861" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-862"><a href="#cb12-862" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LuongAttention(nn.Module):</span>
<span id="cb12-863"><a href="#cb12-863" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-864"><a href="#cb12-864" aria-hidden="true" tabindex="-1"></a><span class="co">    Luong 注意力机制，支持三种对齐方式</span></span>
<span id="cb12-865"><a href="#cb12-865" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-866"><a href="#cb12-866" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, method<span class="op">=</span><span class="st">'dot'</span>):</span>
<span id="cb12-867"><a href="#cb12-867" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-868"><a href="#cb12-868" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.method <span class="op">=</span> method</span>
<span id="cb12-869"><a href="#cb12-869" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_hidden_dim <span class="op">=</span> enc_hidden_dim</span>
<span id="cb12-870"><a href="#cb12-870" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dec_hidden_dim <span class="op">=</span> dec_hidden_dim</span>
<span id="cb12-871"><a href="#cb12-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-872"><a href="#cb12-872" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> method <span class="op">==</span> <span class="st">'general'</span>:</span>
<span id="cb12-873"><a href="#cb12-873" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(enc_hidden_dim, dec_hidden_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-874"><a href="#cb12-874" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> method <span class="op">==</span> <span class="st">'concat'</span>:</span>
<span id="cb12-875"><a href="#cb12-875" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(enc_hidden_dim <span class="op">+</span> dec_hidden_dim, dec_hidden_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-876"><a href="#cb12-876" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.v_a <span class="op">=</span> nn.Linear(dec_hidden_dim, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-877"><a href="#cb12-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-878"><a href="#cb12-878" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_state, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-879"><a href="#cb12-879" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-880"><a href="#cb12-880" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_state: [batch, dec_hidden]</span></span>
<span id="cb12-881"><a href="#cb12-881" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden]</span></span>
<span id="cb12-882"><a href="#cb12-882" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, src_len], True表示padding位置</span></span>
<span id="cb12-883"><a href="#cb12-883" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-884"><a href="#cb12-884" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb12-885"><a href="#cb12-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-886"><a href="#cb12-886" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.method <span class="op">==</span> <span class="st">'dot'</span>:</span>
<span id="cb12-887"><a href="#cb12-887" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 点积: s^T h</span></span>
<span id="cb12-888"><a href="#cb12-888" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 需要 dec_hidden == enc_hidden</span></span>
<span id="cb12-889"><a href="#cb12-889" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> torch.bmm(</span>
<span id="cb12-890"><a href="#cb12-890" aria-hidden="true" tabindex="-1"></a>                decoder_state.unsqueeze(<span class="dv">1</span>),  <span class="co"># [batch, 1, dec_hidden]</span></span>
<span id="cb12-891"><a href="#cb12-891" aria-hidden="true" tabindex="-1"></a>                encoder_outputs.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># [batch, enc_hidden, src_len]</span></span>
<span id="cb12-892"><a href="#cb12-892" aria-hidden="true" tabindex="-1"></a>            ).squeeze(<span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-893"><a href="#cb12-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-894"><a href="#cb12-894" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.method <span class="op">==</span> <span class="st">'general'</span>:</span>
<span id="cb12-895"><a href="#cb12-895" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 一般形式: s^T W h</span></span>
<span id="cb12-896"><a href="#cb12-896" aria-hidden="true" tabindex="-1"></a>            <span class="co"># W 将 enc_hidden 映射到 dec_hidden</span></span>
<span id="cb12-897"><a href="#cb12-897" aria-hidden="true" tabindex="-1"></a>            transformed <span class="op">=</span> <span class="va">self</span>.W_a(encoder_outputs)  <span class="co"># [batch, src_len, dec_hidden]</span></span>
<span id="cb12-898"><a href="#cb12-898" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> torch.bmm(</span>
<span id="cb12-899"><a href="#cb12-899" aria-hidden="true" tabindex="-1"></a>                decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-900"><a href="#cb12-900" aria-hidden="true" tabindex="-1"></a>                transformed.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb12-901"><a href="#cb12-901" aria-hidden="true" tabindex="-1"></a>            ).squeeze(<span class="dv">1</span>)</span>
<span id="cb12-902"><a href="#cb12-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-903"><a href="#cb12-903" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.method <span class="op">==</span> <span class="st">'concat'</span>:</span>
<span id="cb12-904"><a href="#cb12-904" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 拼接形式: v^T tanh(W [s; h])</span></span>
<span id="cb12-905"><a href="#cb12-905" aria-hidden="true" tabindex="-1"></a>            decoder_expanded <span class="op">=</span> decoder_state.unsqueeze(<span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>, src_len, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-906"><a href="#cb12-906" aria-hidden="true" tabindex="-1"></a>            concat <span class="op">=</span> torch.cat([decoder_expanded, encoder_outputs], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-907"><a href="#cb12-907" aria-hidden="true" tabindex="-1"></a>            energy <span class="op">=</span> torch.tanh(<span class="va">self</span>.W_a(concat))  <span class="co"># [batch, src_len, dec_hidden]</span></span>
<span id="cb12-908"><a href="#cb12-908" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> <span class="va">self</span>.v_a(energy).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-909"><a href="#cb12-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-910"><a href="#cb12-910" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用 mask</span></span>
<span id="cb12-911"><a href="#cb12-911" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-912"><a href="#cb12-912" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb12-913"><a href="#cb12-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-914"><a href="#cb12-914" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax</span></span>
<span id="cb12-915"><a href="#cb12-915" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-916"><a href="#cb12-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-917"><a href="#cb12-917" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 上下文向量</span></span>
<span id="cb12-918"><a href="#cb12-918" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(</span>
<span id="cb12-919"><a href="#cb12-919" aria-hidden="true" tabindex="-1"></a>            attention_weights.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-920"><a href="#cb12-920" aria-hidden="true" tabindex="-1"></a>            encoder_outputs</span>
<span id="cb12-921"><a href="#cb12-921" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">1</span>)</span>
<span id="cb12-922"><a href="#cb12-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-923"><a href="#cb12-923" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights</span>
<span id="cb12-924"><a href="#cb12-924" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-925"><a href="#cb12-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-926"><a href="#cb12-926" aria-hidden="true" tabindex="-1"></a><span class="fu">### Local Attention实现</span></span>
<span id="cb12-927"><a href="#cb12-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-930"><a href="#cb12-930" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-931"><a href="#cb12-931" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-932"><a href="#cb12-932" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LocalAttention(nn.Module):</span>
<span id="cb12-933"><a href="#cb12-933" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-934"><a href="#cb12-934" aria-hidden="true" tabindex="-1"></a><span class="co">    Luong 的 Local Attention（预测型）</span></span>
<span id="cb12-935"><a href="#cb12-935" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-936"><a href="#cb12-936" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, enc_hidden_dim, dec_hidden_dim, window_size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb12-937"><a href="#cb12-937" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-938"><a href="#cb12-938" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size  <span class="co"># D: 窗口半径</span></span>
<span id="cb12-939"><a href="#cb12-939" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc_hidden_dim <span class="op">=</span> enc_hidden_dim</span>
<span id="cb12-940"><a href="#cb12-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-941"><a href="#cb12-941" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 位置预测网络</span></span>
<span id="cb12-942"><a href="#cb12-942" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_p <span class="op">=</span> nn.Linear(dec_hidden_dim, dec_hidden_dim)</span>
<span id="cb12-943"><a href="#cb12-943" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v_p <span class="op">=</span> nn.Linear(dec_hidden_dim, <span class="dv">1</span>)</span>
<span id="cb12-944"><a href="#cb12-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-945"><a href="#cb12-945" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 对齐函数（使用 general）</span></span>
<span id="cb12-946"><a href="#cb12-946" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_a <span class="op">=</span> nn.Linear(enc_hidden_dim, dec_hidden_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-947"><a href="#cb12-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-948"><a href="#cb12-948" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 高斯标准差</span></span>
<span id="cb12-949"><a href="#cb12-949" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigma <span class="op">=</span> window_size <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb12-950"><a href="#cb12-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-951"><a href="#cb12-951" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, decoder_state, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-952"><a href="#cb12-952" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-953"><a href="#cb12-953" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_state: [batch, dec_hidden]</span></span>
<span id="cb12-954"><a href="#cb12-954" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_outputs: [batch, src_len, enc_hidden]</span></span>
<span id="cb12-955"><a href="#cb12-955" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-956"><a href="#cb12-956" aria-hidden="true" tabindex="-1"></a>        batch_size, src_len, _ <span class="op">=</span> encoder_outputs.shape</span>
<span id="cb12-957"><a href="#cb12-957" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> decoder_state.device</span>
<span id="cb12-958"><a href="#cb12-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-959"><a href="#cb12-959" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 预测对齐位置 p</span></span>
<span id="cb12-960"><a href="#cb12-960" aria-hidden="true" tabindex="-1"></a>        <span class="co"># p = S * sigmoid(v^T tanh(W_p s))</span></span>
<span id="cb12-961"><a href="#cb12-961" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> src_len <span class="op">*</span> torch.sigmoid(</span>
<span id="cb12-962"><a href="#cb12-962" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.v_p(torch.tanh(<span class="va">self</span>.W_p(decoder_state)))</span>
<span id="cb12-963"><a href="#cb12-963" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch]</span></span>
<span id="cb12-964"><a href="#cb12-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-965"><a href="#cb12-965" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 计算所有位置的对齐分数</span></span>
<span id="cb12-966"><a href="#cb12-966" aria-hidden="true" tabindex="-1"></a>        transformed <span class="op">=</span> <span class="va">self</span>.W_a(encoder_outputs)  <span class="co"># [batch, src_len, dec_hidden]</span></span>
<span id="cb12-967"><a href="#cb12-967" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(</span>
<span id="cb12-968"><a href="#cb12-968" aria-hidden="true" tabindex="-1"></a>            decoder_state.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-969"><a href="#cb12-969" aria-hidden="true" tabindex="-1"></a>            transformed.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb12-970"><a href="#cb12-970" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-971"><a href="#cb12-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-972"><a href="#cb12-972" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 应用高斯窗口</span></span>
<span id="cb12-973"><a href="#cb12-973" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 生成位置索引 [0, 1, 2, ..., src_len-1]</span></span>
<span id="cb12-974"><a href="#cb12-974" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(src_len, device<span class="op">=</span>device).<span class="bu">float</span>()</span>
<span id="cb12-975"><a href="#cb12-975" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> positions.unsqueeze(<span class="dv">0</span>).expand(batch_size, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch, src_len]</span></span>
<span id="cb12-976"><a href="#cb12-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-977"><a href="#cb12-977" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 高斯权重: exp(-(j - p)^2 / (2 * sigma^2))</span></span>
<span id="cb12-978"><a href="#cb12-978" aria-hidden="true" tabindex="-1"></a>        gaussian <span class="op">=</span> torch.exp(<span class="op">-</span>((positions <span class="op">-</span> p.unsqueeze(<span class="dv">1</span>)) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.sigma <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb12-979"><a href="#cb12-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-980"><a href="#cb12-980" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 窗口mask（只保留 [p-D, p+D] 范围内的位置）</span></span>
<span id="cb12-981"><a href="#cb12-981" aria-hidden="true" tabindex="-1"></a>        window_mask <span class="op">=</span> (positions <span class="op">&gt;=</span> (p.unsqueeze(<span class="dv">1</span>) <span class="op">-</span> <span class="va">self</span>.window_size)) <span class="op">&amp;</span> <span class="op">\</span></span>
<span id="cb12-982"><a href="#cb12-982" aria-hidden="true" tabindex="-1"></a>                      (positions <span class="op">&lt;=</span> (p.unsqueeze(<span class="dv">1</span>) <span class="op">+</span> <span class="va">self</span>.window_size))</span>
<span id="cb12-983"><a href="#cb12-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-984"><a href="#cb12-984" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用窗口mask</span></span>
<span id="cb12-985"><a href="#cb12-985" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.masked_fill(<span class="op">~</span>window_mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb12-986"><a href="#cb12-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-987"><a href="#cb12-987" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Softmax + 高斯加权</span></span>
<span id="cb12-988"><a href="#cb12-988" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">*</span> gaussian</span>
<span id="cb12-989"><a href="#cb12-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-990"><a href="#cb12-990" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 重新归一化</span></span>
<span id="cb12-991"><a href="#cb12-991" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> attention_weights <span class="op">/</span> (attention_weights.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="fl">1e-10</span>)</span>
<span id="cb12-992"><a href="#cb12-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-993"><a href="#cb12-993" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 上下文向量</span></span>
<span id="cb12-994"><a href="#cb12-994" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> torch.bmm(</span>
<span id="cb12-995"><a href="#cb12-995" aria-hidden="true" tabindex="-1"></a>            attention_weights.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-996"><a href="#cb12-996" aria-hidden="true" tabindex="-1"></a>            encoder_outputs</span>
<span id="cb12-997"><a href="#cb12-997" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">1</span>)</span>
<span id="cb12-998"><a href="#cb12-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-999"><a href="#cb12-999" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context, attention_weights, p</span>
<span id="cb12-1000"><a href="#cb12-1000" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1001"><a href="#cb12-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1002"><a href="#cb12-1002" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对比实验</span></span>
<span id="cb12-1003"><a href="#cb12-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1006"><a href="#cb12-1006" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-1007"><a href="#cb12-1007" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb12-1008"><a href="#cb12-1008" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建测试数据</span></span>
<span id="cb12-1009"><a href="#cb12-1009" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-1010"><a href="#cb12-1010" aria-hidden="true" tabindex="-1"></a>src_len <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-1011"><a href="#cb12-1011" aria-hidden="true" tabindex="-1"></a>enc_hidden <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb12-1012"><a href="#cb12-1012" aria-hidden="true" tabindex="-1"></a>dec_hidden <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb12-1013"><a href="#cb12-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1014"><a href="#cb12-1014" aria-hidden="true" tabindex="-1"></a>encoder_outputs <span class="op">=</span> torch.randn(batch_size, src_len, enc_hidden)</span>
<span id="cb12-1015"><a href="#cb12-1015" aria-hidden="true" tabindex="-1"></a>decoder_state <span class="op">=</span> torch.randn(batch_size, dec_hidden)</span>
<span id="cb12-1016"><a href="#cb12-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1017"><a href="#cb12-1017" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试三种 Luong Attention</span></span>
<span id="cb12-1018"><a href="#cb12-1018" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method <span class="kw">in</span> [<span class="st">'dot'</span>, <span class="st">'general'</span>, <span class="st">'concat'</span>]:</span>
<span id="cb12-1019"><a href="#cb12-1019" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> LuongAttention(enc_hidden, dec_hidden, method<span class="op">=</span>method)</span>
<span id="cb12-1020"><a href="#cb12-1020" aria-hidden="true" tabindex="-1"></a>    context, weights <span class="op">=</span> attn(decoder_state, encoder_outputs)</span>
<span id="cb12-1021"><a href="#cb12-1021" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>method<span class="sc">:8s}</span><span class="ss">: context shape = </span><span class="sc">{</span>context<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, weights sum = </span><span class="sc">{</span>weights<span class="sc">.</span><span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-1022"><a href="#cb12-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1023"><a href="#cb12-1023" aria-hidden="true" tabindex="-1"></a><span class="co"># 测试 Local Attention</span></span>
<span id="cb12-1024"><a href="#cb12-1024" aria-hidden="true" tabindex="-1"></a>local_attn <span class="op">=</span> LocalAttention(enc_hidden, dec_hidden, window_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-1025"><a href="#cb12-1025" aria-hidden="true" tabindex="-1"></a>context, weights, p <span class="op">=</span> local_attn(decoder_state, encoder_outputs)</span>
<span id="cb12-1026"><a href="#cb12-1026" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'local'</span><span class="sc">:8s}</span><span class="ss">: context shape = </span><span class="sc">{</span>context<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, predicted p = </span><span class="sc">{</span>p<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-1027"><a href="#cb12-1027" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1028"><a href="#cb12-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1029"><a href="#cb12-1029" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键实现细节</span></span>
<span id="cb12-1030"><a href="#cb12-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1031"><a href="#cb12-1031" aria-hidden="true" tabindex="-1"></a>**Mask处理**：在实际应用中，batch中的序列长度不同，需要padding。计算注意力时，padding位置不应该获得任何权重。我们通过mask将这些位置的分数设为很大的负数，softmax后它们的权重趋近于0。</span>
<span id="cb12-1032"><a href="#cb12-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1033"><a href="#cb12-1033" aria-hidden="true" tabindex="-1"></a>**Teacher Forcing**：训练时，解码器的输入可以是真实的上一个词（teacher forcing）或模型预测的词。<span class="in">`teacher_forcing_ratio`</span> 控制两者的混合比例。较高的比例加速训练，但可能导致exposure bias。</span>
<span id="cb12-1034"><a href="#cb12-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1035"><a href="#cb12-1035" aria-hidden="true" tabindex="-1"></a>**双向编码器**：我们使用双向GRU，编码器输出的维度是 <span class="in">`hidden_dim * 2`</span>。这让每个位置都包含完整的上下文信息。</span>
<span id="cb12-1036"><a href="#cb12-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1037"><a href="#cb12-1037" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1038"><a href="#cb12-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1039"><a href="#cb12-1039" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb12-1040"><a href="#cb12-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1041"><a href="#cb12-1041" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么Attention有效？——理论视角</span></span>
<span id="cb12-1042"><a href="#cb12-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1043"><a href="#cb12-1043" aria-hidden="true" tabindex="-1"></a>**信息论视角**：标准Seq2Seq的上下文向量 $\mathbf{c}$ 是输入 $\mathbf{x}$ 的一个**充分统计量（sufficient statistic）**——如果 $\mathbf{c}$ 完美，它应该包含关于 $\mathbf{y}$ 的所有必要信息。但在实践中，有限维度的 $\mathbf{c}$ 无法做到这一点。Attention通过让解码器访问所有的 $\mathbf{h}_j$，实际上是在说：**不要求一个充分统计量，而是让模型在需要时直接查询原始信息**。这绕过了信息瓶颈。</span>
<span id="cb12-1044"><a href="#cb12-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1045"><a href="#cb12-1045" aria-hidden="true" tabindex="-1"></a>**记忆寻址视角**：可以把编码器的隐藏状态看作一个**外部记忆（external memory）**，每个 $\mathbf{h}_j$ 是一个记忆槽。Attention机制实现了**基于内容的软寻址（content-based soft addressing）**——根据当前查询（解码器状态）检索相关的记忆。这个视角后来被显式化为Memory Networks和Neural Turing Machine。</span>
<span id="cb12-1046"><a href="#cb12-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1047"><a href="#cb12-1047" aria-hidden="true" tabindex="-1"></a>**梯度流视角**：从优化角度，Attention提供了一条从解码器到编码器特定位置的**直接路径**。在标准Seq2Seq中，梯度要从解码器流回编码器，必须经过 $\mathbf{c}$，再经过整个编码过程。Attention创造了"捷径"——梯度可以通过注意力权重直接传到相关的编码器位置。</span>
<span id="cb12-1048"><a href="#cb12-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1049"><a href="#cb12-1049" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么点积注意力能工作？</span></span>
<span id="cb12-1050"><a href="#cb12-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1051"><a href="#cb12-1051" aria-hidden="true" tabindex="-1"></a>点积注意力的有效性可以从多个角度理解。</span>
<span id="cb12-1052"><a href="#cb12-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1053"><a href="#cb12-1053" aria-hidden="true" tabindex="-1"></a>**余弦相似度视角**：当向量被归一化后，点积就是余弦相似度：</span>
<span id="cb12-1054"><a href="#cb12-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1055"><a href="#cb12-1055" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1056"><a href="#cb12-1056" aria-hidden="true" tabindex="-1"></a>\mathbf{s}^\top \mathbf{h} = <span class="sc">\|</span>\mathbf{s}<span class="sc">\|</span> <span class="sc">\|</span>\mathbf{h}<span class="sc">\|</span> \cos(\theta)</span>
<span id="cb12-1057"><a href="#cb12-1057" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1058"><a href="#cb12-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1059"><a href="#cb12-1059" aria-hidden="true" tabindex="-1"></a>余弦相似度是衡量两个向量"方向一致性"的经典指标。神经网络在训练过程中，会学习让相关的状态指向相似的方向。</span>
<span id="cb12-1060"><a href="#cb12-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1061"><a href="#cb12-1061" aria-hidden="true" tabindex="-1"></a>**核方法视角**：点积可以看作一个线性核（linear kernel）。在核方法的框架下，注意力权重实际上是在一个特征空间中计算相似度。General Attention引入的可学习矩阵 $\mathbf{W}_a$ 相当于学习一个Mahalanobis距离。</span>
<span id="cb12-1062"><a href="#cb12-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1063"><a href="#cb12-1063" aria-hidden="true" tabindex="-1"></a>**信息检索视角**：点积注意力可以类比为向量空间模型中的查询-文档匹配。解码器状态是"查询"，编码器状态是"文档"，点积衡量查询与文档的相关性。</span>
<span id="cb12-1064"><a href="#cb12-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1065"><a href="#cb12-1065" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么需要缩放？</span></span>
<span id="cb12-1066"><a href="#cb12-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1067"><a href="#cb12-1067" aria-hidden="true" tabindex="-1"></a>Luong的论文没有讨论这个问题，但后来的Transformer论文（Vaswani et al., 2017）指出了点积注意力的一个潜在问题。</span>
<span id="cb12-1068"><a href="#cb12-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1069"><a href="#cb12-1069" aria-hidden="true" tabindex="-1"></a>当向量维度 $d$ 很大时，点积的方差会很大。假设 $\mathbf{s}$ 和 $\mathbf{h}$ 的每个分量都是独立的、均值为0、方差为1的随机变量，那么：</span>
<span id="cb12-1070"><a href="#cb12-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1071"><a href="#cb12-1071" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1072"><a href="#cb12-1072" aria-hidden="true" tabindex="-1"></a>\text{Var}(\mathbf{s}^\top \mathbf{h}) = d</span>
<span id="cb12-1073"><a href="#cb12-1073" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1074"><a href="#cb12-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1075"><a href="#cb12-1075" aria-hidden="true" tabindex="-1"></a>当 $d = 512$ 时，点积的标准差是 $\sqrt{512} \approx 22.6$。这意味着点积可能产生很大的正值或负值，导致softmax输出接近one-hot分布，梯度变得很小。</span>
<span id="cb12-1076"><a href="#cb12-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1077"><a href="#cb12-1077" aria-hidden="true" tabindex="-1"></a>解决方案是**缩放**：</span>
<span id="cb12-1078"><a href="#cb12-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1079"><a href="#cb12-1079" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1080"><a href="#cb12-1080" aria-hidden="true" tabindex="-1"></a>\text{score}(\mathbf{s}, \mathbf{h}) = \frac{\mathbf{s}^\top \mathbf{h}}{\sqrt{d}}</span>
<span id="cb12-1081"><a href="#cb12-1081" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1082"><a href="#cb12-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1083"><a href="#cb12-1083" aria-hidden="true" tabindex="-1"></a>这就是Transformer中的**Scaled Dot-Product Attention**。Luong的论文使用的维度较小（500左右），问题不太明显；但在Transformer的大维度设置下，缩放变得必要。这个设计选择将在第8章Transformer中发挥核心作用。</span>
<span id="cb12-1084"><a href="#cb12-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1085"><a href="#cb12-1085" aria-hidden="true" tabindex="-1"></a><span class="fu">### 边界条件与失效模式</span></span>
<span id="cb12-1086"><a href="#cb12-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1087"><a href="#cb12-1087" aria-hidden="true" tabindex="-1"></a>**单调对齐假设**：Bahdanau Attention隐含假设源和目标之间存在某种对齐关系。对于翻译任务这通常成立，但对于某些任务（如摘要），这个假设可能不成立——摘要需要整合分散在各处的信息，而不是"对齐"到特定位置。</span>
<span id="cb12-1088"><a href="#cb12-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1089"><a href="#cb12-1089" aria-hidden="true" tabindex="-1"></a>**复杂度限制**：当源序列很长时（如文档级翻译），计算所有位置的注意力权重变得昂贵。$O(T_x \cdot T_y)$ 的复杂度在 $T_x = 10000$ 时是不可接受的。Local Attention是一种缓解方案，但窗口大小的选择需要根据任务调整。</span>
<span id="cb12-1090"><a href="#cb12-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1091"><a href="#cb12-1091" aria-hidden="true" tabindex="-1"></a>**分布偏移**：训练时，解码器看到的上下文向量分布与推理时可能不同（因为teacher forcing）。这可能导致注意力权重在推理时不够准确。</span>
<span id="cb12-1092"><a href="#cb12-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1093"><a href="#cb12-1093" aria-hidden="true" tabindex="-1"></a>**Dot-Product的维度限制**：解码器和编码器的隐藏维度必须相同，否则无法计算点积。General Attention通过引入 $\mathbf{W}_a$ 解除了这个限制，但增加了参数量。</span>
<span id="cb12-1094"><a href="#cb12-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1095"><a href="#cb12-1095" aria-hidden="true" tabindex="-1"></a>**Local Attention的位置预测错误**：如果位置预测函数学习不好，会系统性地错过重要信息。对于语序差异大的语言对，局部窗口可能覆盖不到正确位置。</span>
<span id="cb12-1096"><a href="#cb12-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1097"><a href="#cb12-1097" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1098"><a href="#cb12-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1099"><a href="#cb12-1099" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与展望</span></span>
<span id="cb12-1100"><a href="#cb12-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1101"><a href="#cb12-1101" aria-hidden="true" tabindex="-1"></a>经过Bahdanau（2014）和Luong（2015）的工作，注意力机制已经在机器翻译中确立了核心地位。但两个根本性的局限逐渐浮现，指向更深层的问题。</span>
<span id="cb12-1102"><a href="#cb12-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1103"><a href="#cb12-1103" aria-hidden="true" tabindex="-1"></a>**注意力仍然是RNN的"附属品"**。无论是Bahdanau还是Luong的注意力，都是Seq2Seq架构的增强组件。编码和解码的核心仍然依赖RNN。这意味着顺序计算无法避免——RNN必须逐步处理序列，无法并行。虽然Attention缓解了长距离依赖问题，但RNN本身的梯度流困难并没有根本解决。</span>
<span id="cb12-1104"><a href="#cb12-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1105"><a href="#cb12-1105" aria-hidden="true" tabindex="-1"></a>**注意力只在编码器-解码器之间**。当前的Attention只让解码器关注编码器。但一个更自然的问题是：**编码器内部的各个位置能否相互关注？** 一个词的理解可能依赖于同一句话中的其他词（例如代词的指代消解），而当前的架构没有提供这种"自我关注"的机制。</span>
<span id="cb12-1106"><a href="#cb12-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1107"><a href="#cb12-1107" aria-hidden="true" tabindex="-1"></a>**位置信息是隐式的**。Attention本身不包含位置信息。位置信息完全依赖RNN的顺序处理来隐式编码。如果抛弃RNN，位置信息将完全丢失——这暗示未来的架构需要某种显式的位置编码方案。</span>
<span id="cb12-1108"><a href="#cb12-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1109"><a href="#cb12-1109" aria-hidden="true" tabindex="-1"></a>这些局限引发了两个深刻的问题。第一：**能否让Attention独立于RNN？** 如果Attention如此有效，为什么还需要RNN作为载体？第二：**能否让序列中的每个位置相互关注？** 如果注意力不局限于编码器-解码器之间，而是扩展到同一序列内——这就是**Self-Attention（自注意力）**。</span>
<span id="cb12-1110"><a href="#cb12-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1111"><a href="#cb12-1111" aria-hidden="true" tabindex="-1"></a>下一章将揭晓答案。Self-Attention让每个位置都能直接与其他所有位置交互，不再需要RNN作为中介。而在此之后的Transformer架构（第8章）将彻底兑现"Attention Is All You Need"的承诺——完全抛弃循环结构，用纯粹的注意力机制构建整个模型。</span>
<span id="cb12-1112"><a href="#cb12-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1113"><a href="#cb12-1113" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 从加性到乘性，从全局到局部，从RNN的附属到独立的核心——注意力机制走过了从"辅助工具"到"架构基石"的完整历程。Bahdanau打破了信息瓶颈，Luong拓展了设计空间，而真正的革命还在后面。</span></span>
<span id="cb12-1114"><a href="#cb12-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1115"><a href="#cb12-1115" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1116"><a href="#cb12-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1117"><a href="#cb12-1117" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb12-1118"><a href="#cb12-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1119"><a href="#cb12-1119" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb12-1120"><a href="#cb12-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心要点</span></span>
<span id="cb12-1121"><a href="#cb12-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1122"><a href="#cb12-1122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**问题**：Seq2Seq的信息瓶颈——所有输入信息压缩到一个固定向量，导致长序列信息丢失</span>
<span id="cb12-1123"><a href="#cb12-1123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**洞察**：解码器应该能够动态地、有选择地关注输入的不同位置；进一步地，注意力的计算方式可以多样化</span>
<span id="cb12-1124"><a href="#cb12-1124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bahdanau (2014)**：加性注意力打破了固定向量的限制，大幅提升了长序列翻译质量</span>
<span id="cb12-1125"><a href="#cb12-1125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Luong (2015)**：系统比较了dot/general/concat三种对齐函数和global/local两种范围策略，确立了点积注意力的效率优势</span>
<span id="cb12-1126"><a href="#cb12-1126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**意义**：注意力从Seq2Seq的"补丁"发展为核心组件，为后续的Self-Attention和Transformer奠定了基础</span>
<span id="cb12-1127"><a href="#cb12-1127" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1128"><a href="#cb12-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1129"><a href="#cb12-1129" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb12-1130"><a href="#cb12-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1131"><a href="#cb12-1131" aria-hidden="true" tabindex="-1"></a>**Bahdanau加性注意力**：</span>
<span id="cb12-1132"><a href="#cb12-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1133"><a href="#cb12-1133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1134"><a href="#cb12-1134" aria-hidden="true" tabindex="-1"></a>e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)</span>
<span id="cb12-1135"><a href="#cb12-1135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1136"><a href="#cb12-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1137"><a href="#cb12-1137" aria-hidden="true" tabindex="-1"></a>**Dot-Product Attention**：</span>
<span id="cb12-1138"><a href="#cb12-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1139"><a href="#cb12-1139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1140"><a href="#cb12-1140" aria-hidden="true" tabindex="-1"></a>\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{h}</span>
<span id="cb12-1141"><a href="#cb12-1141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1142"><a href="#cb12-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1143"><a href="#cb12-1143" aria-hidden="true" tabindex="-1"></a>**General Attention**：</span>
<span id="cb12-1144"><a href="#cb12-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1145"><a href="#cb12-1145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1146"><a href="#cb12-1146" aria-hidden="true" tabindex="-1"></a>\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{W}_a \mathbf{h}</span>
<span id="cb12-1147"><a href="#cb12-1147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1148"><a href="#cb12-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1149"><a href="#cb12-1149" aria-hidden="true" tabindex="-1"></a>**注意力权重**：</span>
<span id="cb12-1150"><a href="#cb12-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1151"><a href="#cb12-1151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1152"><a href="#cb12-1152" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}</span>
<span id="cb12-1153"><a href="#cb12-1153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1154"><a href="#cb12-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1155"><a href="#cb12-1155" aria-hidden="true" tabindex="-1"></a>**上下文向量**：</span>
<span id="cb12-1156"><a href="#cb12-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1157"><a href="#cb12-1157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1158"><a href="#cb12-1158" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j</span>
<span id="cb12-1159"><a href="#cb12-1159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1160"><a href="#cb12-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1161"><a href="#cb12-1161" aria-hidden="true" tabindex="-1"></a>**Local Attention位置预测**：</span>
<span id="cb12-1162"><a href="#cb12-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1163"><a href="#cb12-1163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1164"><a href="#cb12-1164" aria-hidden="true" tabindex="-1"></a>p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))</span>
<span id="cb12-1165"><a href="#cb12-1165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-1166"><a href="#cb12-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1167"><a href="#cb12-1167" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1168"><a href="#cb12-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1169"><a href="#cb12-1169" aria-hidden="true" tabindex="-1"></a><span class="fu">## 思考题</span></span>
<span id="cb12-1170"><a href="#cb12-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1171"><a href="#cb12-1171" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么说Attention实现了"软寻址"？它与计算机内存的硬寻址有什么本质区别？这种软寻址的优势和劣势是什么？</span>
<span id="cb12-1172"><a href="#cb12-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1173"><a href="#cb12-1173" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 点积注意力和加性注意力在表达能力上有什么本质区别？设计一个简单的例子，展示加性注意力能学习而点积注意力无法学习的相关性模式。更一般地，如果General Attention中 $\mathbf{W}_a = \mathbf{U}\mathbf{V}^\top$（秩-$r$分解），这对注意力模式有什么影响？</span>
<span id="cb12-1174"><a href="#cb12-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1175"><a href="#cb12-1175" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 在实现Attention时，为什么要对padding位置应用mask？如果不做mask会有什么后果？如何正确实现mask（考虑数值稳定性）？</span>
<span id="cb12-1176"><a href="#cb12-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1177"><a href="#cb12-1177" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[批判思考]** Local Attention假设对齐是大致单调的（源和目标的位置对应）。对于哪些语言对或任务，这个假设会严重失效？能否设计一种"非单调局部注意力"？</span>
<span id="cb12-1178"><a href="#cb12-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1179"><a href="#cb12-1179" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放问题]** Hard Attention虽然训练困难，但它有一个优势：稀疏性可以提高可解释性。有没有方法既保持Soft Attention的可微分性，又能获得接近Hard Attention的稀疏性？（提示：考虑稀疏softmax、Gumbel-softmax）</span>
<span id="cb12-1180"><a href="#cb12-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1181"><a href="#cb12-1181" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1182"><a href="#cb12-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1183"><a href="#cb12-1183" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb12-1184"><a href="#cb12-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1185"><a href="#cb12-1185" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb12-1186"><a href="#cb12-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1187"><a href="#cb12-1187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**</span>
<span id="cb12-1188"><a href="#cb12-1188" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Attention机制在NMT中的开创性工作</span>
<span id="cb12-1189"><a href="#cb12-1189" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（模型架构）、Section 5（可视化分析）</span>
<span id="cb12-1190"><a href="#cb12-1190" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1409.0473</span><span class="co">](https://arxiv.org/abs/1409.0473)</span></span>
<span id="cb12-1191"><a href="#cb12-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1192"><a href="#cb12-1192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**</span>
<span id="cb12-1193"><a href="#cb12-1193" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>系统比较不同注意力变体，确立设计准则</span>
<span id="cb12-1194"><a href="#cb12-1194" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3（Global vs Local）、Section 4（实验对比）</span>
<span id="cb12-1195"><a href="#cb12-1195" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1508.04025</span><span class="co">](https://arxiv.org/abs/1508.04025)</span></span>
<span id="cb12-1196"><a href="#cb12-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1197"><a href="#cb12-1197" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb12-1198"><a href="#cb12-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1199"><a href="#cb12-1199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Graves et al., 2014] Neural Turing Machines**</span>
<span id="cb12-1200"><a href="#cb12-1200" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出了基于内容的软寻址，是Attention的理论先驱</span>
<span id="cb12-1201"><a href="#cb12-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3.1（Attention机制）</span>
<span id="cb12-1202"><a href="#cb12-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1203"><a href="#cb12-1203" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb12-1204"><a href="#cb12-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1205"><a href="#cb12-1205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Vaswani et al., 2017] Attention Is All You Need**</span>
<span id="cb12-1206"><a href="#cb12-1206" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出Scaled Dot-Product Attention和Multi-Head Attention，完全抛弃RNN</span>
<span id="cb12-1207"><a href="#cb12-1207" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>本教材第8章的核心内容</span>
<span id="cb12-1208"><a href="#cb12-1208" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1706.03762</span><span class="co">](https://arxiv.org/abs/1706.03762)</span></span>
<span id="cb12-1209"><a href="#cb12-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1210"><a href="#cb12-1210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Xu et al., 2015] Show, Attend and Tell**</span>
<span id="cb12-1211"><a href="#cb12-1211" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Hard Attention在图像描述中的应用，对比Soft和Hard Attention的效果</span>
<span id="cb12-1212"><a href="#cb12-1212" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1502.03044</span><span class="co">](https://arxiv.org/abs/1502.03044)</span></span>
<span id="cb12-1213"><a href="#cb12-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1214"><a href="#cb12-1214" aria-hidden="true" tabindex="-1"></a><span class="fu">### 注意力可解释性</span></span>
<span id="cb12-1215"><a href="#cb12-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1216"><a href="#cb12-1216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Jain &amp; Wallace, 2019] Attention is not Explanation**</span>
<span id="cb12-1217"><a href="#cb12-1217" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>质疑Attention权重作为解释的可靠性</span>
<span id="cb12-1218"><a href="#cb12-1218" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1902.10186</span><span class="co">](https://arxiv.org/abs/1902.10186)</span></span>
<span id="cb12-1219"><a href="#cb12-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1220"><a href="#cb12-1220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Wiegreffe &amp; Pinter, 2019] Attention is not not Explanation**</span>
<span id="cb12-1221"><a href="#cb12-1221" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>对上述论文的回应，更细致地讨论Attention的解释性</span>
<span id="cb12-1222"><a href="#cb12-1222" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1908.04626</span><span class="co">](https://arxiv.org/abs/1908.04626)</span></span>
<span id="cb12-1223"><a href="#cb12-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1224"><a href="#cb12-1224" aria-hidden="true" tabindex="-1"></a><span class="fu">### 大规模实证对比</span></span>
<span id="cb12-1225"><a href="#cb12-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1226"><a href="#cb12-1226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Britz et al., 2017] Massive Exploration of Neural Machine Translation Architectures**</span>
<span id="cb12-1227"><a href="#cb12-1227" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>大规模对比NMT的各种设计选择，包括注意力变体的实证对比</span>
<span id="cb12-1228"><a href="#cb12-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1703.03906</span><span class="co">](https://arxiv.org/abs/1703.03906)</span></span>
<span id="cb12-1229"><a href="#cb12-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1230"><a href="#cb12-1230" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1231"><a href="#cb12-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1232"><a href="#cb12-1232" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb12-1233"><a href="#cb12-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1234"><a href="#cb12-1234" aria-hidden="true" tabindex="-1"></a>Attention机制的灵感部分来自人类视觉系统。当我们看一幅复杂的图像时，我们不会同时处理所有像素，而是会"聚焦"在感兴趣的区域。这种选择性注意（selective attention）是认知科学研究的经典课题。</span>
<span id="cb12-1235"><a href="#cb12-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1236"><a href="#cb12-1236" aria-hidden="true" tabindex="-1"></a>Bahdanau在2014年将这个思想引入神经机器翻译时，并没有预料到它会成为深度学习最核心的组件之一。在论文中，他们谦虚地称之为"对齐模型"（alignment model），而不是"注意力"。"Attention"这个术语是后来被社区广泛采用的。</span>
<span id="cb12-1237"><a href="#cb12-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1238"><a href="#cb12-1238" aria-hidden="true" tabindex="-1"></a>仅一年后，斯坦福的Luong、Pham和Manning发表了系统性的探索工作。他们不仅提出了计算更高效的乘性注意力，更重要的是**系统性地比较和总结**了当时的各种方法，为后来者提供了清晰的设计指南。有趣的是，Luong论文中提到的点积注意力因为过于简单而在当时没有受到太多关注——主流仍然偏好参数化的对齐函数。但两年后，当Transformer论文提出"Scaled Dot-Product Attention"时，点积注意力终于登上了历史舞台的中央。</span>
<span id="cb12-1239"><a href="#cb12-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1240"><a href="#cb12-1240" aria-hidden="true" tabindex="-1"></a>从某种意义上说，这两篇论文共同完成了Attention发展的第一阶段：Bahdanau开创了注意力机制，Luong拓展了设计空间并建立了系统性的理解。然而，一个更大胆的想法正在酝酿：如果Attention如此强大，我们是否还需要RNN？下一章将讲述Self-Attention的诞生，而不久之后，Vaswani等人将给出决定性的回答——"Attention Is All You Need"。这篇论文不仅在技术上革新了序列建模，其标题本身也成为了深度学习历史上最具影响力的金句之一。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>