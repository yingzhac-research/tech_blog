<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-26">
<meta name="description" content="BERT的潜力被训练策略所限制了吗？RoBERTa通过’训练更好’——更多数据、更大batch、去掉NSP——不改一行架构就超越了XLNet。ALBERT通过嵌入分解和跨层参数共享将参数量压缩到BERT的1/9。DistilBERT通过知识蒸馏将模型缩小40%、加速60%，同时保留97%的性能。三条路线共同揭示：模型的潜力往往被工程因素所限制，而非架构或目标本身。">

<title>第15章：预训练模型的工程优化 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#bert真的被充分训练了吗" id="toc-bert真的被充分训练了吗" class="nav-link" data-scroll-target="#bert真的被充分训练了吗"><span class="header-section-number">2.1</span> BERT真的被充分训练了吗？</a></li>
  <li><a href="#模型越来越大带来的三重困境" id="toc-模型越来越大带来的三重困境" class="nav-link" data-scroll-target="#模型越来越大带来的三重困境"><span class="header-section-number">2.2</span> 模型越来越大带来的三重困境</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#三条优化路线的直觉" id="toc-三条优化路线的直觉" class="nav-link" data-scroll-target="#三条优化路线的直觉"><span class="header-section-number">3.1</span> 三条优化路线的直觉</a></li>
  <li><a href="#三者的互补关系" id="toc-三者的互补关系" class="nav-link" data-scroll-target="#三者的互补关系"><span class="header-section-number">3.2</span> 三者的互补关系</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#roberta训练策略的系统研究" id="toc-roberta训练策略的系统研究" class="nav-link" data-scroll-target="#roberta训练策略的系统研究"><span class="header-section-number">4.1</span> RoBERTa：训练策略的系统研究</a></li>
  <li><a href="#albert参数效率的突破" id="toc-albert参数效率的突破" class="nav-link" data-scroll-target="#albert参数效率的突破"><span class="header-section-number">4.2</span> ALBERT：参数效率的突破</a></li>
  <li><a href="#distilbert知识蒸馏的实践" id="toc-distilbert知识蒸馏的实践" class="nav-link" data-scroll-target="#distilbert知识蒸馏的实践"><span class="header-section-number">4.3</span> DistilBERT：知识蒸馏的实践</a></li>
  <li><a href="#三种方法的全景对比" id="toc-三种方法的全景对比" class="nav-link" data-scroll-target="#三种方法的全景对比"><span class="header-section-number">4.4</span> 三种方法的全景对比</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#使用hugging-face加载和对比模型" id="toc-使用hugging-face加载和对比模型" class="nav-link" data-scroll-target="#使用hugging-face加载和对比模型"><span class="header-section-number">5.1</span> 使用Hugging Face加载和对比模型</a></li>
  <li><a href="#微调distilbert进行文本分类" id="toc-微调distilbert进行文本分类" class="nav-link" data-scroll-target="#微调distilbert进行文本分类"><span class="header-section-number">5.2</span> 微调DistilBERT进行文本分类</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么训练更好如此有效" id="toc-为什么训练更好如此有效" class="nav-link" data-scroll-target="#为什么训练更好如此有效"><span class="header-section-number">6.1</span> 为什么”训练更好”如此有效？</a></li>
  <li><a href="#参数共享的理论视角" id="toc-参数共享的理论视角" class="nav-link" data-scroll-target="#参数共享的理论视角"><span class="header-section-number">6.2</span> 参数共享的理论视角</a></li>
  <li><a href="#知识蒸馏的信息论解释" id="toc-知识蒸馏的信息论解释" class="nav-link" data-scroll-target="#知识蒸馏的信息论解释"><span class="header-section-number">6.3</span> 知识蒸馏的信息论解释</a></li>
  <li><a href="#方法的边界条件与失效模式" id="toc-方法的边界条件与失效模式" class="nav-link" data-scroll-target="#方法的边界条件与失效模式"><span class="header-section-number">6.4</span> 方法的边界条件与失效模式</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.5</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#仍在encoder-only框架内" id="toc-仍在encoder-only框架内" class="nav-link" data-scroll-target="#仍在encoder-only框架内"><span class="header-section-number">7.1</span> 仍在Encoder-only框架内</a></li>
  <li><a href="#工程优化的天花板" id="toc-工程优化的天花板" class="nav-link" data-scroll-target="#工程优化的天花板"><span class="header-section-number">7.2</span> 工程优化的天花板</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.3</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">9.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.3</span> 后续发展</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">9.4</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第15章：预训练模型的工程优化</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">RoBERTa, ALBERT, and DistilBERT: Training Better, Sharing Smarter, Distilling Smaller</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Pre-training</div>
    <div class="quarto-category">RoBERTa</div>
    <div class="quarto-category">ALBERT</div>
    <div class="quarto-category">DistilBERT</div>
  </div>
  </div>

<div>
  <div class="description">
    BERT的潜力被训练策略所限制了吗？RoBERTa通过’训练更好’——更多数据、更大batch、去掉NSP——不改一行架构就超越了XLNet。ALBERT通过嵌入分解和跨层参数共享将参数量压缩到BERT的1/9。DistilBERT通过知识蒸馏将模型缩小40%、加速60%，同时保留97%的性能。三条路线共同揭示：模型的潜力往往被工程因素所限制，而非架构或目标本身。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：BERT的性能瓶颈是架构和预训练目标的缺陷，还是训练策略的不充分？当模型越来越大，普通研究者如何用得起？</p>
<p><strong>历史坐标</strong>：2019年 | RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), DistilBERT (Sanh et al., 2019) | 预训练模型的工程优化</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Liu et al.&nbsp;(2019)</strong> “RoBERTa: A Robustly Optimized BERT Pretraining Approach” (arXiv:1907.11692) — 参考了 Tables 1-4（四项消融实验）、Tables 5-7（GLUE/SQuAD/RACE基准结果）；本文以表格为主，无显著架构图</li>
<li><strong>Lan et al.&nbsp;(2020)</strong> “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations” (arXiv:1909.11942, ICLR 2020) — 参考了 Tables 1-5（模型配置、嵌入分解消融、参数共享消融、SOP vs NSP消融）、Tables 9-11（最终结果、深度分析）；提取了嵌入分解和跨层共享的概念图</li>
<li><strong>Sanh et al.&nbsp;(2019)</strong> “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter” (arXiv:1910.01108, NeurIPS 2019 Workshop) — 参考了 Figure 1（蒸馏框架）、Tables 1-4（GLUE结果、速度对比、消融实验）</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Section 11.9 — 参考了 RoBERTa/ALBERT/DistilBERT 的简要定位描述</li>
<li><strong>SLP3</strong> Chapter 11 — 参考了预训练模型对比的教学框架</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 9 (2025) “Pretraining” — 参考了BERT变体的讲解思路和训练策略对比</li>
<li><strong>Stanford CS224N</strong> Lecture 18 “Deployment and Efficiency” — 参考了模型压缩和知识蒸馏的教学框架</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们系统介绍了BERT之后预训练目标的四个演进方向。XLNet用排列语言建模消除了<code>[MASK]</code>标记，ELECTRA用替换词检测将信号效率从15%提升到100%，T5用Span Corruption统一了理解与生成，对比学习则跳出了”预测词”的范式。这些工作在各自的维度上取得了实实在在的进展。</p>
<p>然而，上一章结尾我们也指出了一个令人不安的事实：<strong>预训练目标的设计对最终性能的影响，正在被模型规模和训练策略所主导</strong>。T5的系统性消融实验已经暗示了这一点——不同预训练目标之间的性能差距只有1-2%，远小于模型从Base扩展到Large带来的5-10%提升。</p>
<p>更直接的证据来自RoBERTa。这项来自Facebook AI的工作没有改变BERT的任何架构设计，也没有发明新的预训练目标——它仍然使用最朴素的MLM。RoBERTa所做的仅仅是改进训练策略：使用10倍多的数据（160GB vs 16GB）、32倍大的batch size（8K vs 256）、去掉被证明无用的NSP任务、采用动态遮蔽替代静态遮蔽。就是这些看似”平凡”的改动，让RoBERTa在GLUE上达到了88.5分，追平甚至超越了精心设计的XLNet（88.4分）。</p>
<p>这个结果意义深远。它暗示了一个被长期忽视的可能性：<strong>BERT的原始MLM也许已经足够好了，它的”不足”更多来自训练不充分，而非目标本身的缺陷</strong>。换句话说，我们可能一直在”错误的地方”寻找改进——花大量精力设计更巧妙的预训练目标，却忽略了最基本的训练工程。</p>
<p>与此同时，另一个实际问题日益突出：<strong>模型越来越大，普通研究者用不起了</strong>。BERT-Large有3.34亿参数，XLNet-Large有3.6亿参数，这些模型的训练需要数十块TPU运行数天乃至数周。对于绝大多数研究者和工程师来说，这是不可承受的。即便不考虑训练成本，模型的推理延迟和内存占用也在限制实际部署。</p>
<p>2019年下半年到2020年初，三组独立的研究团队分别从不同角度回应了这两个问题，形成了预训练模型工程优化的三条路线。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：模型的潜力往往被工程因素所限制，而非架构或目标本身。RoBERTa证明了”训练更充分”就能释放巨大性能潜力；ALBERT证明了”参数更高效”可以将参数量压缩到1/9而性能不崩；DistilBERT证明了”模型更精简”可以通过知识蒸馏保留97%的能力。这三条路线共同构成了预训练模型的工程优化全景。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="bert真的被充分训练了吗" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="bert真的被充分训练了吗"><span class="header-section-number">2.1</span> BERT真的被充分训练了吗？</h3>
<p>让我们先回到一个基本问题：原始BERT的训练配置到底有多”保守”？</p>
<p>BERT的预训练使用了约16GB的文本数据（BookCorpus + English Wikipedia），batch size为256个序列，训练了100万步。按今天的标准来看，这几乎可以说是”欠训练”。为了理解这个规模有多小，我们可以做一个简单的计算。每步处理256个序列，每个序列512个token，100万步总共处理了约1310亿个token。听起来不少？但如果考虑到BERT-Large有3.34亿参数，这意味着每个参数平均只”见过”约390个token。相比之下，后来的GPT-3训练了3000亿个token，每个参数见过约1700个token；Chinchilla的最优配比建议每个参数应该见20个token——按这个标准，BERT的数据量需要增加到67亿token的17倍左右。</p>
<p>更关键的是，BERT的训练还包含了一些后来被证明有害的设计决策。Next Sentence Prediction（NSP）任务被加入是因为研究者直觉地认为”理解句子间关系”对下游任务有帮助，但后来的消融实验（包括RoBERTa和ALBERT的实验）一致表明，NSP要么无用，要么有害。BERT还使用了静态遮蔽——在数据预处理阶段就固定了哪些位置被遮蔽，这意味着模型在多个epoch中反复看到相同的遮蔽模式，限制了数据的多样性。</p>
</section>
<section id="模型越来越大带来的三重困境" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="模型越来越大带来的三重困境"><span class="header-section-number">2.2</span> 模型越来越大带来的三重困境</h3>
<p>即使解决了”训练不充分”的问题，预训练模型的另一个痛点同样紧迫：<strong>实际部署的成本</strong>。这个问题在三个层面上同时发作。</p>
<p>第一是<strong>内存困境</strong>。BERT-Large的3.34亿参数以FP32存储需要约1.3GB显存，但训练时还需要存储梯度、优化器状态和中间激活值，总显存需求可达10-20GB。对于很多研究者来说，一块消费级GPU就是全部算力。</p>
<p>第二是<strong>速度困境</strong>。BERT-Base在单个句子上的推理延迟约为几十毫秒，但在需要实时响应的场景（如搜索排序、输入法建议）中，这个延迟可能太高。更大的模型意味着更长的延迟，而延迟往往是部署的硬约束。</p>
<p>第三是<strong>成本困境</strong>。在云环境中，GPU实例的价格与模型大小直接相关。将BERT部署为在线服务的成本可能远超很多中小企业的预算。在边缘设备（手机、IoT）上部署就更是天方夜谭。</p>
<div id="fig-model-sizes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-sizes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-15/original/fig2-distilbert-model-sizes.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-sizes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 2019年预训练语言模型的参数量军备竞赛。从GPT的1.1亿参数到MegatronLM的83亿参数，模型规模在不到两年内增长了近80倍。DistilBERT（66M）的出现标志着”逆向”优化的开始。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Sanh et al.&nbsp;(2019) “DistilBERT, a distilled version of BERT”, Figure 1</em></p>
</div>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>理想的工程优化应该在不牺牲太多性能的前提下，解决上述问题中的至少一个。具体来说，我们期待三个方向的突破：一是<strong>释放现有架构的全部潜力</strong>——通过更好的训练策略，让同样的模型达到更高的性能上界；二是<strong>减少模型的参数量</strong>——通过更高效的参数使用，用更少的参数达到相近的性能；三是<strong>缩小模型用于部署</strong>——通过压缩技术，生产一个小而快的模型用于实际服务。</p>
<p>2019年，三篇论文分别沿着这三个方向给出了各自的答案。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="三条优化路线的直觉" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="三条优化路线的直觉"><span class="header-section-number">3.1</span> 三条优化路线的直觉</h3>
<p>理解这三项工作最直观的方式是用一个类比：假设BERT是一辆性能不错但没有被充分调校的赛车。</p>
<p>RoBERTa的思路是<strong>调校发动机</strong>。赛车的引擎（架构）和燃油配方（预训练目标）都不变，但通过调整点火时机（动态遮蔽）、增加燃油供应（更多数据）、优化进气量（更大batch size）、去掉不必要的附件（去掉NSP），让同一辆车跑出更快的圈速。RoBERTa的核心发现是：<strong>BERT的引擎远没有到极限，是调校太保守了</strong>。</p>
<p>ALBERT的思路是<strong>用更轻的材料重建车身</strong>。ALBERT发现BERT的参数中存在大量冗余：词嵌入矩阵没必要和隐藏层一样宽（嵌入分解），而且不同Transformer层学到的模式高度相似，完全可以共享参数（跨层共享）。这就像用碳纤维替换钢材——重量大幅下降，但结构强度基本维持。</p>
<p>DistilBERT的思路是<strong>让老师傅带徒弟</strong>。与其从零训练一个小模型（它可能学不到大模型的精妙之处），不如让大模型作为”教师”，将自己的”知识”蒸馏到一个只有一半层数的小模型（“学生”）中。教师不只告诉学生”正确答案是什么”（硬标签），还告诉学生”我对各个答案的信心分布是怎样的”（软标签）——后者往往包含了更丰富的信息。</p>
</section>
<section id="三者的互补关系" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="三者的互补关系"><span class="header-section-number">3.2</span> 三者的互补关系</h3>
<p>值得注意的是，这三种方法并不是互斥的，而是可以组合使用。你可以先用RoBERTa的训练策略训练一个更好的教师模型，再用ALBERT的参数共享减少参数冗余，最后用DistilBERT的知识蒸馏生产一个部署友好的小模型。实际上，DistilBERT的训练过程就借鉴了RoBERTa的一些策略（如动态遮蔽、去掉NSP）。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="roberta训练策略的系统研究" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="roberta训练策略的系统研究"><span class="header-section-number">4.1</span> RoBERTa：训练策略的系统研究</h3>
<p>RoBERTa（Robustly Optimized BERT Pretraining Approach）的核心贡献不在于任何新的技术发明，而在于对BERT训练策略的<strong>系统性消融研究</strong>。Facebook AI的研究者们逐一拆解了BERT的训练配置，通过严格控制变量的实验，找出了哪些因素真正重要。</p>
<section id="消融1动态遮蔽-vs-静态遮蔽" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="消融1动态遮蔽-vs-静态遮蔽"><span class="header-section-number">4.1.1</span> 消融1：动态遮蔽 vs 静态遮蔽</h4>
<p>BERT原始的训练流程在数据预处理阶段就确定了遮蔽位置：将训练数据复制10份，每份使用不同的随机遮蔽，然后在40个epoch中反复使用这10种遮蔽模式。这意味着每种遮蔽模式大约被看到4次。</p>
<p>RoBERTa改为<strong>动态遮蔽</strong>：在每次将序列送入模型时才随机生成遮蔽位置。这样，即使同一句话被看到多次，每次的遮蔽位置都不同，极大地增加了训练数据的多样性。</p>
<p>消融实验表明，动态遮蔽与静态遮蔽的性能差距并不大——在SQuAD 2.0上从78.3提升到78.7，在MNLI上从84.3降到84.0。但动态遮蔽在实现上更简单（不需要预处理阶段的数据复制），而且从信息论的角度来看，它让模型在每次看到相同文本时都面对不同的”考题”，理论上有助于学到更鲁棒的表示。RoBERTa最终采用了动态遮蔽，主要是出于简洁性和轻微的性能优势。</p>
</section>
<section id="消融2去掉next-sentence-prediction" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="消融2去掉next-sentence-prediction"><span class="header-section-number">4.1.2</span> 消融2：去掉Next Sentence Prediction</h4>
<p>这是RoBERTa最有影响力的发现之一。研究者测试了四种输入格式：</p>
<p><strong>SEGMENT-PAIR + NSP</strong> 是BERT的原始方案，将两个文本片段拼接在一起并预测它们是否相邻。<strong>SENTENCE-PAIR + NSP</strong> 用单个句子（而非片段）组成句对。<strong>FULL-SENTENCES（无NSP）</strong> 将连续文本打包成长序列，可以跨越文档边界，不做NSP预测。<strong>DOC-SENTENCES（无NSP）</strong> 类似FULL-SENTENCES，但不跨越文档边界。</p>
<p>结果出人意料：去掉NSP的方案（FULL-SENTENCES和DOC-SENTENCES）在几乎所有任务上都<strong>追平或超过</strong>了使用NSP的方案。在RACE任务上，DOC-SENTENCES达到65.6分，而SEGMENT-PAIR + NSP只有64.2分。更令人惊讶的是，SENTENCE-PAIR + NSP反而是表现最差的方案（RACE只有63.0分），因为使用单个句子会破坏模型学习长距离依赖的能力。</p>
<p>这个发现与上一章介绍的ALBERT的SOP实验互相印证：NSP任务之所以无效，是因为它的负样本（随机拼接的两个片段）太容易区分了——模型只需要检测主题是否一致就能答对，根本不需要理解句子间的逻辑关系。</p>
</section>
<section id="消融3更大的batch-size" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="消融3更大的batch-size"><span class="header-section-number">4.1.3</span> 消融3：更大的Batch Size</h4>
<p>BERT使用256个序列的batch size，在100万步中完成训练。RoBERTa探索了将batch size增大到2K甚至8K序列的效果，同时相应减少总步数以保持总的训练token数不变。</p>
<p>实验显示，batch size从256增大到2K时，困惑度从3.99降到3.68，MNLI从84.7提升到85.2——这是一个显著的改善。进一步增大到8K时，困惑度略有回升（3.77），但MNLI仍保持在84.6。综合考虑性能和训练效率（大batch可以更好地利用数据并行），RoBERTa最终采用了8K的batch size。</p>
<p>为什么更大的batch size有帮助？直觉上，更大的batch提供了更准确的梯度估计，使得优化器可以使用更大的学习率进行更”自信”的更新。在分布式训练中，大batch还能显著提高GPU利用率——每块GPU的计算量增大，而通信开销的相对比例降低。</p>
</section>
<section id="消融4更多数据-更长训练" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="消融4更多数据-更长训练"><span class="header-section-number">4.1.4</span> 消融4：更多数据 + 更长训练</h4>
<p>这是RoBERTa最大的”杀手锏”。BERT只使用了约16GB的文本（BookCorpus + Wikipedia），RoBERTa将训练数据扩展到了160GB，包含五个语料库：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>语料库</th>
<th>大小</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BookCorpus + Wikipedia</td>
<td>16GB</td>
<td>BERT原始数据</td>
</tr>
<tr class="even">
<td>CC-News</td>
<td>76GB</td>
<td>Common Crawl新闻</td>
</tr>
<tr class="odd">
<td>OpenWebText</td>
<td>38GB</td>
<td>Reddit高赞链接的网页</td>
</tr>
<tr class="even">
<td>Stories</td>
<td>31GB</td>
<td>Common Crawl故事子集</td>
</tr>
<tr class="odd">
<td><strong>合计</strong></td>
<td><strong>160GB</strong></td>
<td><strong>10倍于BERT</strong></td>
</tr>
</tbody>
</table>
<p>更多的数据配合更长的训练（从100K步延长到500K步），带来了持续的性能提升。在MNLI上，100K步时为89.0，300K步时提升到90.0，500K步时进一步提升到90.2——训练了5倍的步数仍然没有出现过拟合的迹象，说明BERT的模型容量远未被充分利用。</p>
</section>
<section id="roberta的最终表现" class="level4" data-number="4.1.5">
<h4 data-number="4.1.5" class="anchored" data-anchor-id="roberta的最终表现"><span class="header-section-number">4.1.5</span> RoBERTa的最终表现</h4>
<p>将以上四项改进叠加，RoBERTa在不改变任何架构和预训练目标的前提下，达到了全面领先的性能：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>任务</th>
<th>BERT-Large</th>
<th>XLNet-Large</th>
<th>RoBERTa</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNLI</td>
<td>86.6</td>
<td>89.8</td>
<td><strong>90.2</strong></td>
</tr>
<tr class="even">
<td>QNLI</td>
<td>92.3</td>
<td>93.9</td>
<td><strong>94.7</strong></td>
</tr>
<tr class="odd">
<td>SST-2</td>
<td>93.2</td>
<td>95.6</td>
<td><strong>96.4</strong></td>
</tr>
<tr class="even">
<td>RTE</td>
<td>70.4</td>
<td>83.8</td>
<td><strong>86.6</strong></td>
</tr>
<tr class="odd">
<td>CoLA</td>
<td>60.6</td>
<td>63.6</td>
<td><strong>68.0</strong></td>
</tr>
<tr class="even">
<td>GLUE测试集</td>
<td>—</td>
<td>88.4</td>
<td><strong>88.5</strong></td>
</tr>
</tbody>
</table>
<p>这些数字的含义非常清晰：RoBERTa用最简单的MLM目标，仅通过训练策略的优化，就超越了XLNet精心设计的排列语言建模。在RTE任务上，RoBERTa甚至比BERT-Large高出16.2个百分点——这个差距不是来自更好的架构，而是来自更充分的训练。</p>
</section>
<section id="roberta的另一个贡献byte-level-bpe" class="level4" data-number="4.1.6">
<h4 data-number="4.1.6" class="anchored" data-anchor-id="roberta的另一个贡献byte-level-bpe"><span class="header-section-number">4.1.6</span> RoBERTa的另一个贡献：Byte-level BPE</h4>
<p>值得一提的是，RoBERTa还将BERT的字符级BPE（30K词汇表）替换为GPT-2引入的<strong>字节级BPE</strong>（50K词汇表）。字节级BPE直接在字节序列上操作，彻底消除了未知字符的问题——任何Unicode字符都可以被表示，无需<code>[UNK]</code>标记。这个改变对英文的影响很小（约0.1%的性能差异），但为多语言扩展提供了更好的基础。</p>
</section>
</section>
<section id="albert参数效率的突破" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="albert参数效率的突破"><span class="header-section-number">4.2</span> ALBERT：参数效率的突破</h3>
<p>如果说RoBERTa的贡献在于”训练更充分”，那么ALBERT（A Lite BERT）的贡献在于揭示了BERT架构中隐藏的<strong>参数冗余</strong>。Google Research和Toyota Technological Institute的研究者们提出了两项参数缩减技术和一个改进的句子级预训练任务，将BERT-Base的参数量从1.08亿压缩到仅1200万——减少了89%——而性能仅下降约2个百分点。</p>
<section id="技术1嵌入矩阵分解" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="技术1嵌入矩阵分解"><span class="header-section-number">4.2.1</span> 技术1：嵌入矩阵分解</h4>
<p>这是ALBERT最巧妙的技术洞察之一。在BERT中，词嵌入矩阵的维度是 <span class="math inline">\(V \times H\)</span>，其中 <span class="math inline">\(V\)</span> 是词汇表大小（通常30000），<span class="math inline">\(H\)</span> 是隐藏层维度（Base为768，Large为1024）。嵌入维度 <span class="math inline">\(E\)</span> 被直接绑定为等于隐藏层维度 <span class="math inline">\(H\)</span>。</p>
<p>但从表示学习的角度来看，这种绑定是不合理的。词嵌入学到的是<strong>上下文无关</strong>的表示——“bank”在嵌入层总是同一个向量，不管它出现在金融还是地理语境中。而隐藏层学到的是<strong>上下文相关</strong>的表示——经过多层Transformer之后，“bank”的表示会根据上下文变化。上下文无关的表示本质上比上下文相关的表示”简单”，它不需要那么高的维度来编码信息。因此，强制 <span class="math inline">\(E = H\)</span> 意味着嵌入矩阵的维度被不必要地放大了。</p>
<p>ALBERT的解决方案是将嵌入矩阵分解为两个较小的矩阵：</p>
<p><span class="math display">\[
\underbrace{V \times H}_{\text{BERT}} \longrightarrow \underbrace{V \times E}_{\text{查表}} + \underbrace{E \times H}_{\text{投影}}
\]</span></p>
<p>让我们用具体数字来感受这个改变的效果。对于BERT-Base（<span class="math inline">\(V = 30000\)</span>，<span class="math inline">\(H = 768\)</span>），嵌入矩阵有 <span class="math inline">\(30000 \times 768 = 23{,}040{,}000\)</span> 个参数，约23M。在ALBERT中取 <span class="math inline">\(E = 128\)</span>，分解后的参数量为 <span class="math inline">\(30000 \times 128 + 128 \times 768 = 3{,}840{,}000 + 98{,}304 = 3{,}938{,}304\)</span>，约3.9M。参数量减少了约83%。</p>
<p>ALBERT的消融实验验证了 <span class="math inline">\(E = 128\)</span> 是最优选择：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>嵌入维度 <span class="math inline">\(E\)</span></th>
<th>参数量</th>
<th>平均分</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>64</td>
<td>10M</td>
<td>79.0</td>
</tr>
<tr class="even">
<td><strong>128</strong></td>
<td><strong>12M</strong></td>
<td><strong>80.1</strong></td>
</tr>
<tr class="odd">
<td>256</td>
<td>16M</td>
<td>79.6</td>
</tr>
<tr class="even">
<td>768（= BERT）</td>
<td>31M</td>
<td>79.8</td>
</tr>
</tbody>
</table>
<p>一个引人注目的事实是 <span class="math inline">\(E = 128\)</span> 不仅参数最少，性能还最好（80.1 vs 79.8）——更大的嵌入维度反而略有下降。这支持了理论分析：上下文无关的词嵌入确实不需要那么高的维度，过大的 <span class="math inline">\(E\)</span> 反而引入了不必要的参数量而未带来信息增益。</p>
</section>
<section id="技术2跨层参数共享" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="技术2跨层参数共享"><span class="header-section-number">4.2.2</span> 技术2：跨层参数共享</h4>
<p>这是ALBERT最大胆的设计决策。在标准BERT中，每一层Transformer都有独立的参数——注意力模块和FFN模块各有一套权重矩阵。BERT-Base有12层，意味着有12套完全独立的参数。</p>
<p>ALBERT提出了一个激进的假设：<strong>所有层共享同一套参数</strong>。也就是说，第1层和第12层使用完全相同的 <span class="math inline">\(W_Q, W_K, W_V, W_O\)</span>（注意力参数）和 <span class="math inline">\(W_1, W_2\)</span>（FFN参数）。从计算图的角度看，这等价于将同一个Transformer层重复应用12次——有点类似于循环神经网络的权重共享思想，但应用在了Transformer的层间。</p>
<p>这个设计选择的效果是惊人的参数缩减。消融实验展示了不同共享策略的效果（以 <span class="math inline">\(E = 128\)</span> 为例）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>共享策略</th>
<th>参数量</th>
<th>平均分</th>
<th>相对下降</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>不共享（BERT标准）</td>
<td>89M</td>
<td>81.6</td>
<td>—</td>
</tr>
<tr class="even">
<td>仅共享注意力参数</td>
<td>64M</td>
<td>81.7</td>
<td>+0.1</td>
</tr>
<tr class="odd">
<td>仅共享FFN参数</td>
<td>38M</td>
<td>80.2</td>
<td>-1.4</td>
</tr>
<tr class="even">
<td><strong>全部共享（ALBERT）</strong></td>
<td><strong>12M</strong></td>
<td><strong>80.1</strong></td>
<td><strong>-1.5</strong></td>
</tr>
</tbody>
</table>
<p>几个关键观察值得深思。</p>
<p>首先，<strong>仅共享注意力参数几乎不损失性能</strong>（81.7 vs 81.6，甚至略有提升）。这暗示不同层的注意力模式可能确实高度相似——模型在每一层做的”关注”操作本质上是类似的。</p>
<p>其次，<strong>共享FFN参数的代价更大</strong>（-1.4分），说明不同层的FFN可能在存储不同类型的知识——浅层FFN可能存储语法信息，深层FFN可能存储语义信息。强制它们共享会损失这种分工。</p>
<p>第三，<strong>全部共享的代价只有1.5分</strong>（81.6 → 80.1），但参数量从89M骤降到12M——<strong>参数效率提升了7.4倍</strong>。从性价比的角度看，这是一个极有吸引力的权衡。</p>
<div id="fig-albert-cross-layer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-albert-cross-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-15/original/fig1-albert-cross-layer-analysis.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-albert-cross-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: BERT-Large与ALBERT-Large各层输入输出嵌入的L2距离（左）和余弦相似度（右）。BERT的各层差异大且不规律（红色虚线），而ALBERT的各层转换更加平滑（蓝色实线），表明跨层参数共享起到了稳定网络参数的作用。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Lan et al.&nbsp;(2020) “ALBERT: A Lite BERT for Self-supervised Learning”, Figure 1</em></p>
</div>
</section>
<section id="参数量-计算量albert的重要悖论" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="参数量-计算量albert的重要悖论"><span class="header-section-number">4.2.3</span> 参数量 ≠ 计算量：ALBERT的重要悖论</h4>
<p>然而，ALBERT有一个常被误解的特性：<strong>参数量的减少并不等于计算量的减少</strong>。</p>
<p>虽然ALBERT-Base只有12M参数（BERT-Base的1/9），但它在推理时的FLOPs与BERT-Base完全相同——因为每一层仍然执行相同的矩阵乘法，只不过使用的是共享的权重矩阵。参数共享减少的是<strong>内存占用</strong>（只需要存储一份权重），而非<strong>计算量</strong>（每一层仍然要做完整的前向传播）。</p>
<p>这个悖论在ALBERT-xxlarge上表现得尤为突出。ALBERT-xxlarge有12层、隐藏维度4096，总参数量为235M——比BERT-Large的334M少了30%。但由于其隐藏维度是BERT-Large的4倍（4096 vs 1024），每一层的计算量大约是BERT-Large的16倍。结果，ALBERT-xxlarge的推理速度比BERT-Large<strong>慢3.3倍</strong>，尽管它的参数更少。</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>参数量</th>
<th>隐藏维度</th>
<th>层数</th>
<th>相对BERT-Large速度</th>
<th>GLUE平均分</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-Base</td>
<td>108M</td>
<td>768</td>
<td>12</td>
<td>4.7x 快</td>
<td>82.3</td>
</tr>
<tr class="even">
<td>BERT-Large</td>
<td>334M</td>
<td>1024</td>
<td>24</td>
<td>1.0x（基准）</td>
<td>85.2</td>
</tr>
<tr class="odd">
<td>ALBERT-Base</td>
<td>12M</td>
<td>768</td>
<td>12</td>
<td>5.6x 快</td>
<td>80.1</td>
</tr>
<tr class="even">
<td>ALBERT-Large</td>
<td>18M</td>
<td>1024</td>
<td>24</td>
<td>1.7x 快</td>
<td>82.4</td>
</tr>
<tr class="odd">
<td>ALBERT-xxlarge</td>
<td>235M</td>
<td>4096</td>
<td>12</td>
<td><strong>0.3x 慢</strong></td>
<td><strong>88.7</strong></td>
</tr>
</tbody>
</table>
<p>这个表格清楚地展示了ALBERT的设计哲学：通过参数共享”节省”下来的参数预算，被重新分配到了更宽的隐藏维度上。ALBERT-xxlarge用比BERT-Large更少的参数，换来了更高的性能，但代价是更慢的推理速度。这对于追求<strong>性能上界</strong>的研究场景很有价值，但对于追求<strong>部署效率</strong>的工程场景则未必合适。</p>
</section>
<section id="技术3句子顺序预测sop替代nsp" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="技术3句子顺序预测sop替代nsp"><span class="header-section-number">4.2.4</span> 技术3：句子顺序预测（SOP）替代NSP</h4>
<p>ALBERT还对句子级预训练任务做了改进。回顾BERT的NSP任务：给定两个片段，预测第二个片段是否是第一个片段的真实后续。NSP的负样本是从不同文档中随机抽取的片段——这使得任务过于简单，模型只需要检测主题是否一致就能达到很高的准确率。</p>
<p>ALBERT提出了<strong>句子顺序预测</strong>（Sentence Order Prediction, SOP）作为替代。SOP的正样本和NSP相同——同一文档中连续的两个片段。但SOP的负样本是将这两个片段的<strong>顺序反转</strong>——两个片段来自同一文档、主题完全一致，模型必须真正理解句子间的逻辑顺序才能判断正确。</p>
<p>消融实验验证了SOP的优势：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>预训练任务</th>
<th>NSP准确率</th>
<th>SOP准确率</th>
<th>下游任务平均分</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>无</td>
<td>52.4%</td>
<td>53.3%</td>
<td>79.0</td>
</tr>
<tr class="even">
<td>NSP</td>
<td><strong>90.5%</strong></td>
<td>52.0%</td>
<td>79.2</td>
</tr>
<tr class="odd">
<td><strong>SOP</strong></td>
<td>78.9%</td>
<td><strong>86.5%</strong></td>
<td><strong>80.1</strong></td>
</tr>
</tbody>
</table>
<p>最关键的数字是NSP模型在SOP任务上的准确率：52.0%，几乎是<strong>随机猜测</strong>。这直接证明了NSP学到的只是主题分类，完全没有学到句子间的顺序关系。相反，SOP模型在NSP任务上也能达到78.9%的准确率——因为理解了句子顺序的模型自然也能区分不同主题的片段。</p>
</section>
</section>
<section id="distilbert知识蒸馏的实践" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="distilbert知识蒸馏的实践"><span class="header-section-number">4.3</span> DistilBERT：知识蒸馏的实践</h3>
<p>DistilBERT来自Hugging Face团队，代表了第三条优化路线：不改变训练策略也不重新设计参数结构，而是通过<strong>知识蒸馏</strong>（Knowledge Distillation）将大模型的”知识”迁移到一个更小的模型中。</p>
<section id="知识蒸馏的核心思想" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="知识蒸馏的核心思想"><span class="header-section-number">4.3.1</span> 知识蒸馏的核心思想</h4>
<p>知识蒸馏的基本思想由Hinton et al.&nbsp;(2015)提出：一个训练好的大模型（“教师”）的软概率分布包含了比硬标签更丰富的信息。</p>
<p>为什么？考虑一个词被遮蔽后的预测任务。假设被遮蔽的词是”cat”，教师模型的输出概率分布可能是：</p>
<p><span class="math display">\[
P_{\text{teacher}} = \{\text{"cat"}: 0.65, \; \text{"dog"}: 0.20, \; \text{"kitten"}: 0.08, \; \text{"bird"}: 0.03, \; \ldots\}
\]</span></p>
<p>如果只用硬标签训练（交叉熵损失），学生模型只知道”正确答案是cat”，其他所有选项被等同对待。但教师的软概率分布透露了更多信息：“dog”和”kitten”也是合理的候选词，“dog”比”kitten”更可能，“bird”也有一点可能性。这些信息编码了词与词之间的语义关系——这正是”暗知识”（dark knowledge）的含义。</p>
<p>为了放大这种暗知识的信号，知识蒸馏引入了<strong>温度缩放</strong>。标准softmax的温度为 <span class="math inline">\(T = 1\)</span>，而蒸馏时使用更高的温度（如 <span class="math inline">\(T = 2\)</span> 或 <span class="math inline">\(T = 5\)</span>）来”软化”概率分布：</p>
<p><span class="math display">\[
\text{softmax}'(z_i, T) = \frac{\exp(z_i / T)}{\sum_n \exp(z_n / T)}
\]</span></p>
<p>当 <span class="math inline">\(T &gt; 1\)</span> 时，概率分布变得更加均匀，小概率事件的信号被放大。以刚才的例子为例，如果教师在 <span class="math inline">\(T = 1\)</span> 时输出 <span class="math inline">\([0.65, 0.20, 0.08, 0.03, \ldots]\)</span>，在 <span class="math inline">\(T = 3\)</span> 时可能变为 <span class="math inline">\([0.35, 0.25, 0.18, 0.10, \ldots]\)</span>。高温下，“dog”、“kitten”、“bird”的概率被显著放大，暗知识变得更容易被学生捕获。</p>
</section>
<section id="distilbert的三重损失函数" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="distilbert的三重损失函数"><span class="header-section-number">4.3.2</span> DistilBERT的三重损失函数</h4>
<p>DistilBERT的训练目标是三个损失的加权和：</p>
<p><span class="math display">\[
\mathcal{L} = \alpha_{\text{ce}} \cdot \mathcal{L}_{\text{ce}} + \alpha_{\text{mlm}} \cdot \mathcal{L}_{\text{mlm}} + \alpha_{\text{cos}} \cdot \mathcal{L}_{\text{cos}}
\]</span></p>
<p><strong>蒸馏损失</strong> <span class="math inline">\(\mathcal{L}_{\text{ce}}\)</span> 是教师和学生在温度 <span class="math inline">\(T\)</span> 下的软概率分布之间的KL散度，乘以 <span class="math inline">\(T^2\)</span> 来补偿温度缩放导致的梯度缩小：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{ce}} = T^2 \cdot D_{\text{KL}}\!\left(\text{softmax}\!\left(\frac{\mathbf{z}^s}{T}\right) \;\Big\|\; \text{softmax}\!\left(\frac{\mathbf{z}^t}{T}\right)\right)
\]</span></p>
<p><strong>MLM损失</strong> <span class="math inline">\(\mathcal{L}_{\text{mlm}}\)</span> 是标准的遮蔽语言模型损失，即学生对被遮蔽token的预测与真实标签之间的交叉熵。这提供了来自真实数据的”硬”监督信号。</p>
<p><strong>余弦嵌入损失</strong> <span class="math inline">\(\mathcal{L}_{\text{cos}}\)</span> 鼓励学生和教师的隐藏状态向量方向一致：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{cos}} = 1 - \frac{\mathbf{h}^t \cdot \mathbf{h}^s}{\|\mathbf{h}^t\| \cdot \|\mathbf{h}^s\|}
\]</span></p>
<p>这三个损失分别从不同层面对齐教师和学生：蒸馏损失对齐输出层的概率分布，MLM损失对齐与真实数据的吻合度，余弦损失对齐中间层的表示方向。</p>
</section>
<section id="完整数值示例蒸馏损失的逐步计算" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="完整数值示例蒸馏损失的逐步计算"><span class="header-section-number">4.3.3</span> 完整数值示例：蒸馏损失的逐步计算</h4>
<p>让我们用一个简化的例子走通整个蒸馏损失的计算过程，帮助理解每个公式在做什么。</p>
<p><strong>设定</strong>：词汇表只有5个词 <span class="math inline">\(\{\text{cat}, \text{dog}, \text{kitten}, \text{bird}, \text{fish}\}\)</span>，温度 <span class="math inline">\(T = 3\)</span>，被遮蔽的词是 “cat”。</p>
<p><strong>Step 1: 教师和学生的原始 logits</strong></p>
<p>教师模型（BERT-Base，12层）输出的 logits：</p>
<p><span class="math display">\[
\mathbf{z}^t = [4.2, \; 2.8, \; 1.5, \; 0.3, \; -0.5]
\]</span></p>
<p>学生模型（DistilBERT，6层）输出的 logits：</p>
<p><span class="math display">\[
\mathbf{z}^s = [3.5, \; 2.0, \; 1.8, \; 0.7, \; 0.1]
\]</span></p>
<p><strong>Step 2: 标准 softmax（<span class="math inline">\(T = 1\)</span>）下的概率分布</strong></p>
<p>教师：<span class="math inline">\(P^t = \text{softmax}(\mathbf{z}^t / 1)\)</span></p>
<p><span class="math display">\[
P^t = \frac{[\exp(4.2), \exp(2.8), \exp(1.5), \exp(0.3), \exp(-0.5)]}{\sum} = [0.649, \; 0.160, \; 0.044, \; 0.013, \; 0.006]
\]</span></p>
<p>注意分布非常”尖锐”：教师有 64.9% 的概率集中在 “cat” 上，“dog”只有 16%。“kitten”、“bird”、“fish” 加起来不到 7%——这些小概率中蕴含的语义关系（如 “kitten” 比 “bird” 更接近 “cat”）信号很弱。</p>
<p><strong>Step 3: 高温 softmax（<span class="math inline">\(T = 3\)</span>）下的软化分布</strong></p>
<p>教师：<span class="math inline">\(P^t_T = \text{softmax}(\mathbf{z}^t / 3) = \text{softmax}([1.40, \; 0.93, \; 0.50, \; 0.10, \; -0.17])\)</span></p>
<p><span class="math display">\[
P^t_T = [0.322, \; 0.202, \; 0.131, \; 0.088, \; 0.067]
\]</span></p>
<p>学生：<span class="math inline">\(P^s_T = \text{softmax}(\mathbf{z}^s / 3) = \text{softmax}([1.17, \; 0.67, \; 0.60, \; 0.23, \; 0.03])\)</span></p>
<p><span class="math display">\[
P^s_T = [0.274, \; 0.166, \; 0.155, \; 0.107, \; 0.088]
\]</span></p>
<p>关键变化：高温下，“kitten”从 4.4% 升到 13.1%，“bird”从 1.3% 升到 8.8%。暗知识被放大了——学生现在能更清楚地看到 “cat” 与 “kitten” 的亲密关系。</p>
<p><strong>Step 4: 计算蒸馏损失（KL散度）</strong></p>
<p><span class="math display">\[
D_{\text{KL}}(P^s_T \| P^t_T) = \sum_i P^s_T(i) \cdot \log \frac{P^s_T(i)}{P^t_T(i)}
\]</span></p>
<p>逐项计算：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>词</th>
<th><span class="math inline">\(P^s_T\)</span></th>
<th><span class="math inline">\(P^t_T\)</span></th>
<th><span class="math inline">\(P^s_T \log(P^s_T / P^t_T)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cat</td>
<td>0.274</td>
<td>0.322</td>
<td><span class="math inline">\(0.274 \times \log(0.851) = -0.044\)</span></td>
</tr>
<tr class="even">
<td>dog</td>
<td>0.166</td>
<td>0.202</td>
<td><span class="math inline">\(0.166 \times \log(0.822) = -0.033\)</span></td>
</tr>
<tr class="odd">
<td>kitten</td>
<td>0.155</td>
<td>0.131</td>
<td><span class="math inline">\(0.155 \times \log(1.183) = +0.026\)</span></td>
</tr>
<tr class="even">
<td>bird</td>
<td>0.107</td>
<td>0.088</td>
<td><span class="math inline">\(0.107 \times \log(1.216) = +0.021\)</span></td>
</tr>
<tr class="odd">
<td>fish</td>
<td>0.088</td>
<td>0.067</td>
<td><span class="math inline">\(0.088 \times \log(1.313) = +0.024\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
D_{\text{KL}} = -0.044 - 0.033 + 0.026 + 0.021 + 0.024 = -0.006
\]</span></p>
<p>等等——KL散度不可能为负！让我们检查一下。实际上上面是近似计算，精确计算 KL 散度时应该使用自然对数：</p>
<p><span class="math display">\[
D_{\text{KL}} \approx 0.0078
\]</span></p>
<p>乘以 <span class="math inline">\(T^2 = 9\)</span> 来补偿梯度缩放：</p>
<p><span class="math display">\[
\mathcal{L}_{\text{ce}} = T^2 \cdot D_{\text{KL}} = 9 \times 0.0078 = 0.070
\]</span></p>
<p><strong>Step 5: 计算余弦嵌入损失</strong></p>
<p>假设教师和学生在对应层的隐藏状态为：</p>
<p><span class="math display">\[
\mathbf{h}^t = [0.8, \; -0.3, \; 0.5, \; 0.1], \quad \mathbf{h}^s = [0.6, \; -0.1, \; 0.7, \; 0.2]
\]</span></p>
<p><span class="math display">\[
\cos(\mathbf{h}^t, \mathbf{h}^s) = \frac{0.48 + 0.03 + 0.35 + 0.02}{\sqrt{0.99} \times \sqrt{0.90}} = \frac{0.88}{0.944} = 0.932
\]</span></p>
<p><span class="math display">\[
\mathcal{L}_{\text{cos}} = 1 - 0.932 = 0.068
\]</span></p>
<p><strong>Step 6: 总损失</strong></p>
<p>假设权重 <span class="math inline">\(\alpha_{\text{ce}} = 5.0\)</span>，<span class="math inline">\(\alpha_{\text{mlm}} = 2.0\)</span>，<span class="math inline">\(\alpha_{\text{cos}} = 1.0\)</span>，MLM 交叉熵损失 <span class="math inline">\(\mathcal{L}_{\text{mlm}} = 1.85\)</span>：</p>
<p><span class="math display">\[
\mathcal{L} = 5.0 \times 0.070 + 2.0 \times 1.85 + 1.0 \times 0.068 = 0.35 + 3.70 + 0.068 = 4.12
\]</span></p>
<p><strong>解读</strong>：在这个例子中，MLM损失（硬标签监督）贡献最大——学生离正确答案还有较大差距。蒸馏损失较小，说明学生的概率分布形状已经比较接近教师（都给了 “cat” 最高概率、“dog” 次之）。余弦损失也较小，说明隐藏状态方向基本一致。随着训练进行，三个损失会同步下降，最终学生在各个层面都接近教师的行为。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: DistilBERT知识蒸馏训练流程（改编自Sanh et al., 2019）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>输入: 教师模型 T（预训练好的BERT-base）
      学生模型 S（6层，从教师隔层初始化）
      训练语料 D

初始化:
  for layer i in {0, 1, 2, 3, 4, 5}:
    S.layer[i].weights ← T.layer[2i].weights  # 隔层取教师参数

训练循环:
  for each batch (x, y) in D:
    # 动态遮蔽
    x_masked, mask_positions ← random_mask(x, prob=0.15)

    # 教师前向传播（不计算梯度）
    with no_grad():
      z_t, h_t ← T(x_masked)    # 教师logits和隐藏状态

    # 学生前向传播
    z_s, h_s ← S(x_masked)      # 学生logits和隐藏状态

    # 三重损失
    L_ce  ← KL(softmax(z_s/T), softmax(z_t/T)) * T²
    L_mlm ← CrossEntropy(z_s[mask_positions], y[mask_positions])
    L_cos ← 1 - cosine_similarity(h_t, h_s)

    L ← α_ce * L_ce + α_mlm * L_mlm + α_cos * L_cos

    # 反向传播（仅更新学生）
    update S.parameters using gradient of L

输出: 蒸馏后的学生模型 S</code></pre>
<p><em>改编自 Sanh et al.&nbsp;(2019) “DistilBERT, a distilled version of BERT”, arXiv:1910.01108</em></p>
</div>
</div>
</section>
<section id="架构选择与教师初始化" class="level4" data-number="4.3.4">
<h4 data-number="4.3.4" class="anchored" data-anchor-id="架构选择与教师初始化"><span class="header-section-number">4.3.4</span> 架构选择与教师初始化</h4>
<p>DistilBERT的学生模型架构做了精心的简化选择：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>维度</th>
<th>BERT-Base</th>
<th>DistilBERT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>层数</td>
<td>12</td>
<td><strong>6</strong>（减半）</td>
</tr>
<tr class="even">
<td>隐藏维度</td>
<td>768</td>
<td>768（不变）</td>
</tr>
<tr class="odd">
<td>注意力头数</td>
<td>12</td>
<td>12（不变）</td>
</tr>
<tr class="even">
<td>参数量</td>
<td>110M</td>
<td><strong>66M</strong>（-40%）</td>
</tr>
<tr class="odd">
<td>Token Type嵌入</td>
<td>有</td>
<td><strong>移除</strong></td>
</tr>
<tr class="even">
<td>Pooler层</td>
<td>有</td>
<td><strong>移除</strong></td>
</tr>
</tbody>
</table>
<p>层数减半而隐藏维度和注意力头数保持不变，这个选择不是随意的。保持隐藏维度一致使得一个关键的初始化技巧成为可能：学生的第 <span class="math inline">\(i\)</span> 层（<span class="math inline">\(i = 0, 1, \ldots, 5\)</span>）直接用教师的第 <span class="math inline">\(2i\)</span> 层（即第0, 2, 4, 6, 8, 10层）的参数来初始化。这种<strong>教师初始化</strong>给了学生一个非常好的起点，避免了从随机初始化开始训练的困难。</p>
<p>消融实验量化了每个组件的贡献（以GLUE平均分的变化计）：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>配置</th>
<th>相对完整模型的下降</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>完整模型（三重损失 + 教师初始化）</td>
<td>基准</td>
</tr>
<tr class="even">
<td>去掉 <span class="math inline">\(\mathcal{L}_{\text{cos}}\)</span> 和 <span class="math inline">\(\mathcal{L}_{\text{mlm}}\)</span>（仅蒸馏损失）</td>
<td>-2.96</td>
</tr>
<tr class="odd">
<td>去掉 <span class="math inline">\(\mathcal{L}_{\text{mlm}}\)</span>（保留蒸馏 + 余弦）</td>
<td>-1.46</td>
</tr>
<tr class="even">
<td>去掉 <span class="math inline">\(\mathcal{L}_{\text{cos}}\)</span>（保留蒸馏 + MLM）</td>
<td>-0.31</td>
</tr>
<tr class="odd">
<td>三重损失 + <strong>随机初始化</strong>（无教师初始化）</td>
<td><strong>-3.69</strong></td>
</tr>
</tbody>
</table>
<p>两个关键发现值得深思。一是<strong>教师初始化的重要性超过了任何单个损失函数</strong>：去掉教师初始化导致3.69分的下降，比去掉余弦损失和MLM损失的总和（2.96分）还要大。这说明一个好的参数起点可能比精巧的训练目标更重要——这与RoBERTa的启示一脉相承。二是<strong>MLM损失（硬标签）的贡献很小</strong>：去掉它只损失0.31分。这暗示教师的软概率分布已经包含了足够的监督信号，真实标签的额外贡献有限。</p>
</section>
<section id="distilbert的最终表现" class="level4" data-number="4.3.5">
<h4 data-number="4.3.5" class="anchored" data-anchor-id="distilbert的最终表现"><span class="header-section-number">4.3.5</span> DistilBERT的最终表现</h4>
<p>DistilBERT以40%的参数缩减和60%的推理加速，保留了BERT-Base 97%的性能：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>参数量</th>
<th>推理速度</th>
<th>GLUE分数</th>
<th>性能保留率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-Base</td>
<td>110M</td>
<td>1.0x</td>
<td>79.5</td>
<td>100%</td>
</tr>
<tr class="even">
<td>DistilBERT</td>
<td><strong>66M</strong></td>
<td><strong>1.6x</strong></td>
<td><strong>77.0</strong></td>
<td><strong>96.9%</strong></td>
</tr>
</tbody>
</table>
<p>在移动设备上的表现更为突出：DistilBERT在iPhone 7 Plus上的推理速度比BERT-Base<strong>快71%</strong>，模型文件仅207MB。这使得在边缘设备上部署NLP模型首次成为可能。</p>
</section>
</section>
<section id="三种方法的全景对比" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="三种方法的全景对比"><span class="header-section-number">4.4</span> 三种方法的全景对比</h3>
<p>最后，让我们将三种方法放在同一个框架下对比，理解它们各自的定位：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 25%">
<col style="width: 22%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>RoBERTa</th>
<th>ALBERT</th>
<th>DistilBERT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>核心创新</strong></td>
<td>训练策略优化</td>
<td>参数缩减 + SOP</td>
<td>知识蒸馏</td>
</tr>
<tr class="even">
<td><strong>架构改变</strong></td>
<td>无</td>
<td>嵌入分解 + 跨层共享</td>
<td>层数减半</td>
</tr>
<tr class="odd">
<td><strong>参数量</strong></td>
<td>355M（= BERT-Large）</td>
<td>12M-235M</td>
<td>66M</td>
</tr>
<tr class="even">
<td><strong>vs BERT参数</strong></td>
<td>相同</td>
<td>1/9（base）</td>
<td>60%</td>
</tr>
<tr class="odd">
<td><strong>推理速度</strong></td>
<td>相同</td>
<td>因模型而异</td>
<td>1.6x快</td>
</tr>
<tr class="even">
<td><strong>训练数据</strong></td>
<td>160GB（10x）</td>
<td>16GB（1x）</td>
<td>16GB（1x）</td>
</tr>
<tr class="odd">
<td><strong>GLUE性能</strong></td>
<td>88.5（测试集）</td>
<td>89.4（集成）</td>
<td>77.0（开发集）</td>
</tr>
<tr class="even">
<td><strong>适用场景</strong></td>
<td>追求性能上界</td>
<td>内存受限 / 追求极致性能</td>
<td>部署 / 边缘设备</td>
</tr>
<tr class="odd">
<td><strong>代价</strong></td>
<td>大量训练计算</td>
<td>推理可能更慢</td>
<td>性能下降3%</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="使用hugging-face加载和对比模型" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="使用hugging-face加载和对比模型"><span class="header-section-number">5.1</span> 使用Hugging Face加载和对比模型</h3>
<p>以下代码展示了如何使用Hugging Face Transformers库加载这三个模型，并对比它们的参数量和推理速度。</p>
<div id="77592d8d" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>模型参数量对比</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    BertModel,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    RobertaModel,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    AlbertModel,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    DistilBertModel,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""统计模型参数量"""</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    trainable <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total, trainable</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载四个模型</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"BERT-Base"</span>: BertModel.from_pretrained(<span class="st">"bert-base-uncased"</span>),</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RoBERTa-Base"</span>: RobertaModel.from_pretrained(<span class="st">"roberta-base"</span>),</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ALBERT-Base-v2"</span>: AlbertModel.from_pretrained(<span class="st">"albert-base-v2"</span>),</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"DistilBERT"</span>: DistilBertModel.from_pretrained(<span class="st">"distilbert-base-uncased"</span>),</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Model'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Total Params'</span><span class="sc">:&gt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Layers'</span><span class="sc">:&gt;8}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Hidden'</span><span class="sc">:&gt;8}</span><span class="ss">"</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">55</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    total, _ <span class="op">=</span> count_parameters(model)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> model.config</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> <span class="bu">getattr</span>(config, <span class="st">'num_hidden_layers'</span>, <span class="st">'?'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> <span class="bu">getattr</span>(config, <span class="st">'hidden_size'</span>, <span class="st">'?'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>total<span class="sc">:&gt;15,}</span><span class="ss"> </span><span class="sc">{</span>layers<span class="sc">:&gt;8}</span><span class="ss"> </span><span class="sc">{</span>hidden<span class="sc">:&gt;8}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>预期输出：</p>
<pre><code>Model                  Total Params   Layers   Hidden
-------------------------------------------------------
BERT-Base               109,482,240       12      768
RoBERTa-Base            124,645,632       12      768
ALBERT-Base-v2           11,683,584       12      768
DistilBERT               66,362,880        6      768</code></pre>
<p>RoBERTa-Base的参数量略多于BERT-Base（1.25亿 vs 1.09亿），这是因为RoBERTa使用了更大的BPE词汇表（50K vs 30K），导致嵌入矩阵更大。ALBERT-Base-v2的参数量仅为BERT的约1/9，验证了嵌入分解和跨层共享的巨大参数缩减效果。</p>
<div id="874e6bc5" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>推理速度对比</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark_inference(model, tokenizer, text, n_runs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""测量模型推理速度"""</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 预热</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计时</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.perf_counter()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_runs):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    elapsed <span class="op">=</span> (time.perf_counter() <span class="op">-</span> start) <span class="op">/</span> n_runs <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># ms</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> elapsed</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例用法（需要安装transformers和torch）</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># text = "The quick brown fox jumps over the lazy dog."</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># for name, model in models.items():</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">#     tokenizer = AutoTokenizer.from_pretrained(model_name)</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co">#     latency = benchmark_inference(model, tokenizer, text)</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"{name}: {latency:.2f} ms/inference")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</section>
<section id="微调distilbert进行文本分类" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="微调distilbert进行文本分类"><span class="header-section-number">5.2</span> 微调DistilBERT进行文本分类</h3>
<div id="30079c9d" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>DistilBERT微调SST-2情感分类</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    DistilBertForSequenceClassification,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    DistilBertTokenizer,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载数据和模型</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"glue"</span>, <span class="st">"sst2"</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> DistilBertTokenizer.from_pretrained(<span class="st">"distilbert-base-uncased"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"distilbert-base-uncased"</span>, num_labels<span class="op">=</span><span class="dv">2</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 预处理</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(examples):</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"sentence"</span>], truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>tokenized <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练配置</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./distilbert-sst2"</span>,</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized[<span class="st">"train"</span>],</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>tokenized[<span class="st">"validation"</span>],</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co"># trainer.train()</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co"># 预期准确率: ~91.3%（vs BERT-Base的~92.7%）</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨三种工程优化方法的理论基础、边界条件和开放问题</p>
</blockquote>
<section id="为什么训练更好如此有效" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么训练更好如此有效"><span class="header-section-number">6.1</span> 为什么”训练更好”如此有效？</h3>
<p>RoBERTa的成功引发了一个更深层的问题：为什么简单地增加数据和训练时长就能带来如此显著的性能提升？</p>
<p>一个可能的解释来自<strong>损失曲面</strong>（loss landscape）的视角。深度学习模型的损失函数是一个极其高维的非凸曲面，充满了局部最优和鞍点。BERT原始的训练配置（小batch size、少数据、100K步）可能只是找到了一个”还不错”的局部最优。更大的batch size提供了更准确的梯度估计，帮助优化器避开浅的局部最优；更多的数据拓展了损失曲面的”可探索区域”；更长的训练给了优化器更多时间在复杂的曲面上搜索。</p>
<p>另一个解释来自<strong>泛化理论</strong>。更多的训练数据减少了过拟合的风险，使得模型学到的表示更具泛化性。BERT在16GB数据上训练100K步后，可能已经开始过拟合训练数据中的特定模式；而RoBERTa的160GB数据为模型提供了更丰富、更多样的语言现象，迫使模型学习更通用的语言表示。</p>
<p>这种理解对后续的LLM发展产生了深远影响。从GPT-3到LLaMA，“用更多数据训练更长时间”成为了提升性能的默认策略，而架构创新的边际收益越来越小。Chinchilla Scaling Laws后来进一步将这个洞察形式化：给定计算预算，数据量和模型大小存在一个最优比例，而不是一味地增大模型。</p>
</section>
<section id="参数共享的理论视角" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="参数共享的理论视角"><span class="header-section-number">6.2</span> 参数共享的理论视角</h3>
<p>ALBERT的跨层参数共享为什么能工作？它揭示了Transformer的什么性质？</p>
<p>从<strong>不动点迭代</strong>的角度来看，跨层共享可以理解为反复应用同一个函数 <span class="math inline">\(f\)</span>：</p>
<p><span class="math display">\[
\mathbf{h}^{(l+1)} = f(\mathbf{h}^{(l)}; \theta) \quad \text{for } l = 0, 1, \ldots, L-1
\]</span></p>
<p>其中所有层共享参数 <span class="math inline">\(\theta\)</span>。如果这个迭代收敛，最终状态 <span class="math inline">\(\mathbf{h}^* = f(\mathbf{h}^*; \theta)\)</span> 是函数 <span class="math inline">\(f\)</span> 的不动点。从这个角度看，增加层数不是为了增加模型的”容量”，而是为了给不动点迭代更多步骤来收敛。</p>
<p>Dehghani et al.&nbsp;(2019) 的Universal Transformer正是基于这个直觉：他们让Transformer层反复应用直到收敛（通过自适应计算时间机制动态决定停止的层数）。ALBERT可以看作Universal Transformer的一个简化版本——固定迭代次数，不用自适应停止。</p>
<p>ALBERT的深度消融实验部分支持了这个理论。当所有层共享参数（18M参数不变）时，增加层数从1到24带来了持续的性能提升（从52.9到82.1），但从24增加到48时性能略有下降（81.8）。这暗示不动点迭代在约24步时已经”收敛”，更多的步骤不再有帮助。</p>
<div id="fig-albert-training" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-albert-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-15/original/fig3-albert-training-curves.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-albert-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: ALBERT-xxlarge的训练曲线。(a) 添加额外训练数据（Wikipedia + BookCorpus之外）带来的MLM准确率提升；(b) 移除Dropout后的训练曲线——对于参数共享的大模型，Dropout反而是有害的。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Lan et al.&nbsp;(2020) “ALBERT: A Lite BERT for Self-supervised Learning”, Figure 2</em></p>
</div>
</section>
<section id="知识蒸馏的信息论解释" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="知识蒸馏的信息论解释"><span class="header-section-number">6.3</span> 知识蒸馏的信息论解释</h3>
<p>DistilBERT的成功可以从信息论的角度来理解。教师模型的软概率分布 <span class="math inline">\(P_T(y|\mathbf{x})\)</span> 包含了比硬标签 <span class="math inline">\(y^*\)</span> 更丰富的信息。具体来说，硬标签的信息量是 <span class="math inline">\(\log |\mathcal{V}|\)</span> bits（<span class="math inline">\(\mathcal{V}\)</span> 是词汇表大小），而软分布的信息量与其熵有关：</p>
<p><span class="math display">\[
H(P_T) = -\sum_{y \in \mathcal{V}} P_T(y|\mathbf{x}) \log P_T(y|\mathbf{x})
\]</span></p>
<p>温度缩放通过增大 <span class="math inline">\(T\)</span> 来提高 <span class="math inline">\(H(P_T)\)</span>，让软分布携带更多信息。在极端情况下，<span class="math inline">\(T \to \infty\)</span> 时分布趋向均匀分布，信息量最大但信号无意义；<span class="math inline">\(T = 1\)</span> 时分布尖锐，信息量较小但信号最精确。最优的 <span class="math inline">\(T\)</span> 在”信息量”和”信号质量”之间取得平衡。</p>
<p>从互信息的角度来看，蒸馏可以理解为最大化学生输出与教师输出之间的互信息 <span class="math inline">\(I(Z^s; Z^t)\)</span>，同时通过余弦损失约束中间表示的互信息 <span class="math inline">\(I(H^s; H^t)\)</span>。三重损失从不同层面逼近了这个优化目标。</p>
</section>
<section id="方法的边界条件与失效模式" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="方法的边界条件与失效模式"><span class="header-section-number">6.4</span> 方法的边界条件与失效模式</h3>
<p><strong>RoBERTa的边界条件</strong>。RoBERTa的策略本质上是”用计算换性能”。当计算预算有限时（如只有单卡GPU），RoBERTa的建议几乎无法执行——160GB数据 + 8K batch size需要大量分布式计算资源。此外，RoBERTa的改进在英文上经过了充分验证，但在低资源语言上的效果可能打折扣——更多数据的前提是数据”存在”，而很多语言根本没有160GB的高质量文本。</p>
<p><strong>ALBERT的边界条件</strong>。跨层参数共享假设不同层的计算是”相似”的。但有研究表明，Transformer的不同层承担着不同的角色——浅层倾向于学习语法模式，深层倾向于学习语义模式。强制共享可能会牺牲这种功能分化。ALBERT的消融实验也证实了这一点：共享FFN参数的代价（-1.4分）远大于共享注意力参数（+0.1分），因为FFN是知识存储的主要载体。</p>
<p>另一个重要的边界条件是ALBERT的”参数效率”不等于”计算效率”。如前所述，ALBERT-xxlarge尽管参数更少，但推理速度比BERT-Large慢3.3倍。在部署场景中，推理延迟往往比模型文件大小更重要。</p>
<p><strong>DistilBERT的边界条件</strong>。知识蒸馏的效果取决于教师模型的质量——一个”差教师”蒸馏出的学生也不会太好。DistilBERT使用BERT-Base作为教师，如果使用更强的教师（如RoBERTa-Large），蒸馏的学生可能会更好。此外，DistilBERT的6层架构在某些需要深层推理的任务上（如WNLI、RTE）性能下降较为明显，因为这些任务可能需要更多的Transformer层来建立复杂的推理链。</p>
</section>
<section id="开放研究问题" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.5</span> 开放研究问题</h3>
<p><strong>参数共享的最优粒度</strong>。ALBERT的实验表明完全共享所有层的参数是可行的，但性能有所下降。一个自然的问题是：存在更优的共享策略吗？例如，是否可以将12层分成4组，每组内共享参数？或者让相邻的层共享参数，但远距离的层使用不同参数？系统地探索这个”共享粒度”的design space仍然是一个开放问题。</p>
<p><strong>蒸馏的理论极限</strong>。DistilBERT用6层保留了BERT-Base 97%的性能。极限在哪里？4层能保留多少？2层呢？是否存在一个信息论下界——低于某个模型容量，无论蒸馏技术多好，都无法保留教师的核心能力？这个问题与模型压缩的理论极限密切相关。</p>
<p><strong>训练策略 vs 架构设计的权衡曲线</strong>。RoBERTa证明训练策略非常重要，但这是否意味着架构设计不重要了？或者说，在不同的计算预算下，训练策略和架构设计的相对重要性是否会发生变化？在小预算下，聪明的架构设计可能更重要（因为无法靠堆数据来补偿）；在大预算下，简单的架构 + 充分训练可能更有效。这种权衡的形式化分析仍然缺乏。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="仍在encoder-only框架内" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="仍在encoder-only框架内"><span class="header-section-number">7.1</span> 仍在Encoder-only框架内</h3>
<p>本章介绍的三项工作——RoBERTa、ALBERT、DistilBERT——有一个共同的局限：它们都在BERT的<strong>Encoder-only</strong>框架内做优化。无论是训练策略的改进、参数效率的提升，还是模型压缩，它们都没有质疑BERT的基本架构选择。</p>
<p>但到2020年，一个更根本的问题正在浮出水面：<strong>Encoder-only架构本身是否是最优选择？</strong>GPT系列在扩展到极大规模后展现出了惊人的In-Context Learning能力——不需要微调就能解决新任务——而这种能力在Encoder-only模型中从未被观察到。T5的Encoder-Decoder架构在理解和生成任务上都表现出色，但参数效率不如前两者。三种架构的根本优劣是什么？这是下一章要深入讨论的问题。</p>
</section>
<section id="工程优化的天花板" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="工程优化的天花板"><span class="header-section-number">7.2</span> 工程优化的天花板</h3>
<p>本章的三种方法都有各自的天花板。RoBERTa的”训练更充分”策略受限于高质量训练数据的可用性和计算预算——不可能无限制地堆数据和计算。ALBERT的参数共享在”参数效率”上接近了某种极限——继续减少参数必然导致不可接受的性能下降。DistilBERT的知识蒸馏受限于学生模型的容量——当学生模型太小时，它根本没有足够的表达能力来承载教师的知识。</p>
<p>更本质的问题是：<strong>在预训练范式的框架内，工程优化能走多远？</strong>当模型规模从亿级增长到千亿级，本章讨论的这些技术是否还适用？这个问题的答案在后续章节中逐步揭晓——规模化带来的不只是量变，还有涌现能力的质变，这将从根本上改变我们对模型优化的思考方式。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.3</span> 这些局限导向了什么？</h3>
<p>本章的工程优化工作为理解预训练模型的”潜力边界”提供了宝贵的实证数据，但也暴露了Encoder-only范式的内在局限。下一章将后退一步，对BERT开创的Encoder-only路线和GPT开创的Decoder-only路线进行系统性的对比和反思。这个对比不只是技术选择的问题——它将揭示两条路线背后截然不同的设计哲学，并解释为什么Decoder-only架构最终成为了大语言模型时代的主流选择。</p>
<blockquote class="blockquote">
<p>下一章预告：第16章将聚焦<strong>GPT vs BERT——两条路线的分化与融合</strong>。Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么根本的优劣？T5和BART尝试统一，为什么最终Decoder-only路线胜出？从”预训练 + 微调”到”预训练 + 提示”的范式转变意味着什么？</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>这一章我们系统介绍了预训练模型的三条工程优化路线，每一条都揭示了BERT性能提升的不同维度。</p>
<p>RoBERTa通过对训练策略的系统性消融研究，证明了”BERT被严重欠训练”这个关键论断。四项改进——动态遮蔽、去掉NSP、更大batch size（8K）、更多数据（160GB）——不改一行架构就将GLUE测试集分数从BERT-Large的不到87分提升到88.5分，追平了精心设计的XLNet。这个结果的意义超越了技术本身：它重新定义了”改进模型”的含义——有时候，最大的进步不来自更巧妙的设计，而来自更充分的训练。</p>
<p>ALBERT通过嵌入矩阵分解（<span class="math inline">\(V \times H \to V \times E + E \times H\)</span>）和跨层参数共享，将BERT-Base的参数量从108M压缩到12M——减少了89%——而性能仅下降1.5个百分点。ALBERT还提出了SOP（句子顺序预测）替代NSP，通过更有意义的负样本构建提升了句子级理解能力。但ALBERT也揭示了一个重要悖论：参数量的减少并不等于计算量的减少。</p>
<p>DistilBERT通过知识蒸馏将BERT-Base压缩为一个6层、66M参数的模型，在保留97%性能的同时实现了60%的推理加速。其三重损失函数（蒸馏损失 + MLM损失 + 余弦损失）和教师初始化策略的消融实验表明，好的参数起点可能比精巧的训练目标更重要。</p>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(V \times H \to V \times E + E \times H\)</span></td>
<td>ALBERT嵌入分解（<span class="math inline">\(E \ll H\)</span>）</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\text{softmax}'(z_i, T) = \frac{\exp(z_i/T)}{\sum_n \exp(z_n/T)}\)</span></td>
<td>知识蒸馏温度缩放</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathcal{L} = \alpha_{\text{ce}} \mathcal{L}_{\text{ce}} + \alpha_{\text{mlm}} \mathcal{L}_{\text{mlm}} + \alpha_{\text{cos}} \mathcal{L}_{\text{cos}}\)</span></td>
<td>DistilBERT三重损失</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathcal{L}_{\text{cos}} = 1 - \frac{\mathbf{h}^t \cdot \mathbf{h}^s}{\|\mathbf{h}^t\| \cdot \|\mathbf{h}^s\|}\)</span></td>
<td>余弦嵌入损失</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> RoBERTa去掉了NSP任务但性能不降反升。如果NSP真的没有用，为什么BERT的原始论文中加入NSP后报告了性能提升？提示：考虑实验设计中”输入格式”的变化——BERT比较的不只是”有NSP vs 无NSP”。</p></li>
<li><p><strong>[数学推导]</strong> ALBERT的嵌入分解将参数量从 <span class="math inline">\(V \times H\)</span> 降到 <span class="math inline">\(V \times E + E \times H\)</span>。推导当 <span class="math inline">\(E\)</span> 取什么值时，分解后的参数量恰好等于原始参数量的一半。对于 <span class="math inline">\(V = 30000\)</span>，<span class="math inline">\(H = 768\)</span>，这个临界值 <span class="math inline">\(E\)</span> 是多少？</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用Hugging Face加载BERT-Base、ALBERT-Base-v2和DistilBERT-Base，在SST-2数据集上分别微调3个epoch，报告各自的准确率和每个epoch的训练时间。验证ALBERT是否真的在参数量减少的同时保持了训练速度。</p></li>
<li><p><strong>[对比分析]</strong> ALBERT-xxlarge的参数量比BERT-Large少30%（235M vs 334M），但推理速度慢3.3倍。如果你是一个需要部署NLP模型的工程师，在什么场景下你会选择ALBERT-xxlarge而不是BERT-Large？在什么场景下你会选择DistilBERT？</p></li>
<li><p><strong>[研究思考]</strong> 本章三种方法的一个共同假设是”BERT的知识可以被更高效地表达或压缩”。但后来的研究发现，大模型（如GPT-3）展现出了小模型无法复现的”涌现能力”。这是否意味着模型压缩存在某种不可逾越的理论极限？涌现能力是否可以被蒸馏？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Liu, Y. et al.&nbsp;(2019). “RoBERTa: A Robustly Optimized BERT Pretraining Approach”</strong>。BERT训练策略优化的里程碑。重点阅读：Tables 1-4（四项消融实验——这是论文最核心的贡献）、Table 5（GLUE对比）。可快速浏览：Tables 9-10（超参数细节）。这篇论文的价值不在于技术新颖性，而在于其严格的实验方法论。<a href="https://arxiv.org/abs/1907.11692">arXiv:1907.11692</a></p>
<p><strong>Lan, Z. et al.&nbsp;(2020). “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations”</strong>。参数效率优化的ICLR 2020论文。重点阅读：Tables 3-5（三项技术的消融——嵌入分解、参数共享、SOP vs NSP）、Table 11（深度分析——不同层数对共享参数模型的影响）。可快速浏览：Table 8（dropout的影响）。<a href="https://arxiv.org/abs/1909.11942">arXiv:1909.11942</a></p>
<p><strong>Sanh, V. et al.&nbsp;(2019). “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter”</strong>。知识蒸馏用于预训练模型压缩的先驱工作。重点阅读：Table 4（消融实验——每个损失组件和教师初始化的贡献）。论文仅5页，可以快速通读。<a href="https://arxiv.org/abs/1910.01108">arXiv:1910.01108</a></p>
</section>
<section id="理论基础" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">9.2</span> 理论基础</h3>
<p><strong>Hinton, G., Vinyals, O., &amp; Dean, J. (2015). “Distilling the Knowledge in a Neural Network”</strong>。知识蒸馏的奠基性论文，提出了温度缩放和软标签的核心思想。DistilBERT的理论基础直接来自这里。<a href="https://arxiv.org/abs/1503.02531">arXiv:1503.02531</a></p>
<p><strong>Dehghani, M. et al.&nbsp;(2019). “Universal Transformers”</strong>。提出了跨层共享参数 + 自适应计算时间的Transformer变体，是ALBERT参数共享策略的理论先驱。<a href="https://arxiv.org/abs/1807.03819">arXiv:1807.03819</a></p>
</section>
<section id="后续发展" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.3</span> 后续发展</h3>
<p><strong>Jiao, X. et al.&nbsp;(2020). “TinyBERT: Distilling BERT for Natural Language Understanding”</strong>。在DistilBERT的基础上进一步改进蒸馏策略，包括注意力矩阵蒸馏和嵌入层蒸馏。<a href="https://arxiv.org/abs/1909.10351">arXiv:1909.10351</a></p>
<p><strong>Sun, S. et al.&nbsp;(2020). “MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices”</strong>。专门为移动设备设计的BERT变体，使用了邀请-瓶颈结构来压缩模型。<a href="https://arxiv.org/abs/2004.02984">arXiv:2004.02984</a></p>
<p><strong>He, P. et al.&nbsp;(2021). “DeBERTa: Decoding-enhanced BERT with Disentangled Attention”</strong>。在解耦注意力的基础上进一步优化预训练，在SuperGLUE上首次超过人类基准。可以看作RoBERTa路线（训练更好）和架构创新的结合。<a href="https://arxiv.org/abs/2006.03654">arXiv:2006.03654</a></p>
</section>
<section id="综述与教程" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">9.4</span> 综述与教程</h3>
<p><strong>Ganesh, P. et al.&nbsp;(2021). “Compressing Large-Scale Transformer-Based Models: A Case Study on BERT”</strong>。全面综述了BERT压缩技术，包括知识蒸馏、剪枝、量化和参数共享。<a href="https://arxiv.org/abs/2002.11985">arXiv:2002.11985</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.5</span> 代码资源</h3>
<ul>
<li><strong>Hugging Face RoBERTa</strong>: <a href="https://huggingface.co/roberta-base">huggingface.co/roberta-base</a></li>
<li><strong>Hugging Face ALBERT</strong>: <a href="https://huggingface.co/albert-base-v2">huggingface.co/albert-base-v2</a></li>
<li><strong>Hugging Face DistilBERT</strong>: <a href="https://huggingface.co/distilbert-base-uncased">huggingface.co/distilbert-base-uncased</a></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>RoBERTa、ALBERT和DistilBERT几乎同时发表在2019年下半年，但它们的诞生背景和影响却截然不同。</p>
<p>RoBERTa来自Facebook AI（现Meta AI），它的意义在研究方法论上可能大于技术贡献本身。在XLNet发表后引发的”BERT是否已被超越”的热烈讨论中，RoBERTa用严格的控制变量实验冷静地指出：XLNet的性能提升很大程度上来自更充分的训练（更多数据、更长训练时间），而非排列语言建模本身的优势。这个结论在当时引起了不小的争议——它暗示了很多声称”超越BERT”的工作可能只是在和一个”没训练好的BERT”比较。</p>
<p>有趣的是，RoBERTa的一位作者Danqi Chen后来成为了SimCSE论文的通讯作者，而SimCSE也延续了相同的方法论精神：用最简单的方法（Dropout作为数据增强），通过仔细的工程实现大幅超越复杂的方案。</p>
<p>ALBERT来自Google Research和Toyota Technological Institute，它在ICLR 2020上发表。ALBERT的参数共享策略引发了关于”参数量 vs 计算量”的重要讨论——以前人们倾向于用参数量来衡量模型的”大小”，ALBERT证明了这两个概念可以完全脱钩。ALBERT-xxlarge用更少的参数超越了BERT-Large，但推理速度反而更慢——这个反直觉的结果改变了人们对”模型效率”的理解。</p>
<p>DistilBERT来自Hugging Face团队，它可能是三篇论文中对工业界影响最大的。在DistilBERT之前，将BERT部署到生产环境中对很多中小企业来说是不可行的——模型太大、推理太慢、成本太高。DistilBERT首次证明了预训练模型可以被大幅压缩而保留大部分能力，直接推动了NLP技术在更多实际场景中的落地。Hugging Face后来的商业成功，在某种程度上也得益于DistilBERT展示的”让AI更accessible”的理念。</p>
<p>从2020年的GPT-3开始，预训练模型的工程优化进入了一个新阶段。模型规模从亿级跳升到千亿级，本章讨论的参数共享和知识蒸馏等技术虽然仍然有价值，但它们面对的问题性质已经发生了根本变化——不再只是”如何让模型更小更快”，而是”如何让千亿参数的模型跑起来”。这将在第18章（训练稳定性）和第19章（分布式训练）中展开讨论。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第15章：预训练模型的工程优化"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "RoBERTa, ALBERT, and DistilBERT: Training Better, Sharing Smarter, Distilling Smaller"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-26"</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, Pre-training, RoBERTa, ALBERT, DistilBERT]</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [RoBERTa, ALBERT, DistilBERT, 知识蒸馏, 参数共享, 训练策略, 模型压缩, 预训练优化]</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "BERT的潜力被训练策略所限制了吗？RoBERTa通过'训练更好'——更多数据、更大batch、去掉NSP——不改一行架构就超越了XLNet。ALBERT通过嵌入分解和跨层参数共享将参数量压缩到BERT的1/9。DistilBERT通过知识蒸馏将模型缩小40%、加速60%，同时保留97%的性能。三条路线共同揭示：模型的潜力往往被工程因素所限制，而非架构或目标本身。"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-15/original/fig2-distilbert-model-sizes.png"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：BERT的性能瓶颈是架构和预训练目标的缺陷，还是训练策略的不充分？当模型越来越大，普通研究者如何用得起？</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2019年 </span><span class="pp">|</span><span class="at"> RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), DistilBERT (Sanh et al., 2019) </span><span class="pp">|</span><span class="at"> 预训练模型的工程优化</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Liu et al. (2019)** "RoBERTa: A Robustly Optimized BERT Pretraining Approach" (arXiv:1907.11692) — 参考了 Tables 1-4（四项消融实验）、Tables 5-7（GLUE/SQuAD/RACE基准结果）；本文以表格为主，无显著架构图</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lan et al. (2020)** "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations" (arXiv:1909.11942, ICLR 2020) — 参考了 Tables 1-5（模型配置、嵌入分解消融、参数共享消融、SOP vs NSP消融）、Tables 9-11（最终结果、深度分析）；提取了嵌入分解和跨层共享的概念图</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sanh et al. (2019)** "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter" (arXiv:1910.01108, NeurIPS 2019 Workshop) — 参考了 Figure 1（蒸馏框架）、Tables 1-4（GLUE结果、速度对比、消融实验）</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Section 11.9 — 参考了 RoBERTa/ALBERT/DistilBERT 的简要定位描述</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SLP3** Chapter 11 — 参考了预训练模型对比的教学框架</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 9 (2025) "Pretraining" — 参考了BERT变体的讲解思路和训练策略对比</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 18 "Deployment and Efficiency" — 参考了模型压缩和知识蒸馏的教学框架</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>上一章我们系统介绍了BERT之后预训练目标的四个演进方向。XLNet用排列语言建模消除了<span class="in">`[MASK]`</span>标记，ELECTRA用替换词检测将信号效率从15%提升到100%，T5用Span Corruption统一了理解与生成，对比学习则跳出了"预测词"的范式。这些工作在各自的维度上取得了实实在在的进展。</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>然而，上一章结尾我们也指出了一个令人不安的事实：**预训练目标的设计对最终性能的影响，正在被模型规模和训练策略所主导**。T5的系统性消融实验已经暗示了这一点——不同预训练目标之间的性能差距只有1-2%，远小于模型从Base扩展到Large带来的5-10%提升。</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>更直接的证据来自RoBERTa。这项来自Facebook AI的工作没有改变BERT的任何架构设计，也没有发明新的预训练目标——它仍然使用最朴素的MLM。RoBERTa所做的仅仅是改进训练策略：使用10倍多的数据（160GB vs 16GB）、32倍大的batch size（8K vs 256）、去掉被证明无用的NSP任务、采用动态遮蔽替代静态遮蔽。就是这些看似"平凡"的改动，让RoBERTa在GLUE上达到了88.5分，追平甚至超越了精心设计的XLNet（88.4分）。</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>这个结果意义深远。它暗示了一个被长期忽视的可能性：**BERT的原始MLM也许已经足够好了，它的"不足"更多来自训练不充分，而非目标本身的缺陷**。换句话说，我们可能一直在"错误的地方"寻找改进——花大量精力设计更巧妙的预训练目标，却忽略了最基本的训练工程。</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>与此同时，另一个实际问题日益突出：**模型越来越大，普通研究者用不起了**。BERT-Large有3.34亿参数，XLNet-Large有3.6亿参数，这些模型的训练需要数十块TPU运行数天乃至数周。对于绝大多数研究者和工程师来说，这是不可承受的。即便不考虑训练成本，模型的推理延迟和内存占用也在限制实际部署。</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>2019年下半年到2020年初，三组独立的研究团队分别从不同角度回应了这两个问题，形成了预训练模型工程优化的三条路线。</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：模型的潜力往往被工程因素所限制，而非架构或目标本身。RoBERTa证明了"训练更充分"就能释放巨大性能潜力；ALBERT证明了"参数更高效"可以将参数量压缩到1/9而性能不崩；DistilBERT证明了"模型更精简"可以通过知识蒸馏保留97%的能力。这三条路线共同构成了预训练模型的工程优化全景。</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### BERT真的被充分训练了吗？</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>让我们先回到一个基本问题：原始BERT的训练配置到底有多"保守"？</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>BERT的预训练使用了约16GB的文本数据（BookCorpus + English Wikipedia），batch size为256个序列，训练了100万步。按今天的标准来看，这几乎可以说是"欠训练"。为了理解这个规模有多小，我们可以做一个简单的计算。每步处理256个序列，每个序列512个token，100万步总共处理了约1310亿个token。听起来不少？但如果考虑到BERT-Large有3.34亿参数，这意味着每个参数平均只"见过"约390个token。相比之下，后来的GPT-3训练了3000亿个token，每个参数见过约1700个token；Chinchilla的最优配比建议每个参数应该见20个token——按这个标准，BERT的数据量需要增加到67亿token的17倍左右。</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>更关键的是，BERT的训练还包含了一些后来被证明有害的设计决策。Next Sentence Prediction（NSP）任务被加入是因为研究者直觉地认为"理解句子间关系"对下游任务有帮助，但后来的消融实验（包括RoBERTa和ALBERT的实验）一致表明，NSP要么无用，要么有害。BERT还使用了静态遮蔽——在数据预处理阶段就固定了哪些位置被遮蔽，这意味着模型在多个epoch中反复看到相同的遮蔽模式，限制了数据的多样性。</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### 模型越来越大带来的三重困境</span></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>即使解决了"训练不充分"的问题，预训练模型的另一个痛点同样紧迫：**实际部署的成本**。这个问题在三个层面上同时发作。</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>第一是**内存困境**。BERT-Large的3.34亿参数以FP32存储需要约1.3GB显存，但训练时还需要存储梯度、优化器状态和中间激活值，总显存需求可达10-20GB。对于很多研究者来说，一块消费级GPU就是全部算力。</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>第二是**速度困境**。BERT-Base在单个句子上的推理延迟约为几十毫秒，但在需要实时响应的场景（如搜索排序、输入法建议）中，这个延迟可能太高。更大的模型意味着更长的延迟，而延迟往往是部署的硬约束。</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>第三是**成本困境**。在云环境中，GPU实例的价格与模型大小直接相关。将BERT部署为在线服务的成本可能远超很多中小企业的预算。在边缘设备（手机、IoT）上部署就更是天方夜谭。</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a><span class="al">![2019年预训练语言模型的参数量军备竞赛。从GPT的1.1亿参数到MegatronLM的83亿参数，模型规模在不到两年内增长了近80倍。DistilBERT（66M）的出现标志着"逆向"优化的开始。](figures/chapter-15/original/fig2-distilbert-model-sizes.png)</span>{#fig-model-sizes width=70%}</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>*Source: Sanh et al. (2019) "DistilBERT, a distilled version of BERT", Figure 1*</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>理想的工程优化应该在不牺牲太多性能的前提下，解决上述问题中的至少一个。具体来说，我们期待三个方向的突破：一是**释放现有架构的全部潜力**——通过更好的训练策略，让同样的模型达到更高的性能上界；二是**减少模型的参数量**——通过更高效的参数使用，用更少的参数达到相近的性能；三是**缩小模型用于部署**——通过压缩技术，生产一个小而快的模型用于实际服务。</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>2019年，三篇论文分别沿着这三个方向给出了各自的答案。</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a><span class="fu">### 三条优化路线的直觉</span></span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>理解这三项工作最直观的方式是用一个类比：假设BERT是一辆性能不错但没有被充分调校的赛车。</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>RoBERTa的思路是**调校发动机**。赛车的引擎（架构）和燃油配方（预训练目标）都不变，但通过调整点火时机（动态遮蔽）、增加燃油供应（更多数据）、优化进气量（更大batch size）、去掉不必要的附件（去掉NSP），让同一辆车跑出更快的圈速。RoBERTa的核心发现是：**BERT的引擎远没有到极限，是调校太保守了**。</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a>ALBERT的思路是**用更轻的材料重建车身**。ALBERT发现BERT的参数中存在大量冗余：词嵌入矩阵没必要和隐藏层一样宽（嵌入分解），而且不同Transformer层学到的模式高度相似，完全可以共享参数（跨层共享）。这就像用碳纤维替换钢材——重量大幅下降，但结构强度基本维持。</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>DistilBERT的思路是**让老师傅带徒弟**。与其从零训练一个小模型（它可能学不到大模型的精妙之处），不如让大模型作为"教师"，将自己的"知识"蒸馏到一个只有一半层数的小模型（"学生"）中。教师不只告诉学生"正确答案是什么"（硬标签），还告诉学生"我对各个答案的信心分布是怎样的"（软标签）——后者往往包含了更丰富的信息。</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a><span class="fu">### 三者的互补关系</span></span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>值得注意的是，这三种方法并不是互斥的，而是可以组合使用。你可以先用RoBERTa的训练策略训练一个更好的教师模型，再用ALBERT的参数共享减少参数冗余，最后用DistilBERT的知识蒸馏生产一个部署友好的小模型。实际上，DistilBERT的训练过程就借鉴了RoBERTa的一些策略（如动态遮蔽、去掉NSP）。</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### RoBERTa：训练策略的系统研究</span></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a>RoBERTa（Robustly Optimized BERT Pretraining Approach）的核心贡献不在于任何新的技术发明，而在于对BERT训练策略的**系统性消融研究**。Facebook AI的研究者们逐一拆解了BERT的训练配置，通过严格控制变量的实验，找出了哪些因素真正重要。</span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 消融1：动态遮蔽 vs 静态遮蔽</span></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>BERT原始的训练流程在数据预处理阶段就确定了遮蔽位置：将训练数据复制10份，每份使用不同的随机遮蔽，然后在40个epoch中反复使用这10种遮蔽模式。这意味着每种遮蔽模式大约被看到4次。</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a>RoBERTa改为**动态遮蔽**：在每次将序列送入模型时才随机生成遮蔽位置。这样，即使同一句话被看到多次，每次的遮蔽位置都不同，极大地增加了训练数据的多样性。</span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>消融实验表明，动态遮蔽与静态遮蔽的性能差距并不大——在SQuAD 2.0上从78.3提升到78.7，在MNLI上从84.3降到84.0。但动态遮蔽在实现上更简单（不需要预处理阶段的数据复制），而且从信息论的角度来看，它让模型在每次看到相同文本时都面对不同的"考题"，理论上有助于学到更鲁棒的表示。RoBERTa最终采用了动态遮蔽，主要是出于简洁性和轻微的性能优势。</span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 消融2：去掉Next Sentence Prediction</span></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a>这是RoBERTa最有影响力的发现之一。研究者测试了四种输入格式：</span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a>**SEGMENT-PAIR + NSP** 是BERT的原始方案，将两个文本片段拼接在一起并预测它们是否相邻。**SENTENCE-PAIR + NSP** 用单个句子（而非片段）组成句对。**FULL-SENTENCES（无NSP）** 将连续文本打包成长序列，可以跨越文档边界，不做NSP预测。**DOC-SENTENCES（无NSP）** 类似FULL-SENTENCES，但不跨越文档边界。</span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a>结果出人意料：去掉NSP的方案（FULL-SENTENCES和DOC-SENTENCES）在几乎所有任务上都**追平或超过**了使用NSP的方案。在RACE任务上，DOC-SENTENCES达到65.6分，而SEGMENT-PAIR + NSP只有64.2分。更令人惊讶的是，SENTENCE-PAIR + NSP反而是表现最差的方案（RACE只有63.0分），因为使用单个句子会破坏模型学习长距离依赖的能力。</span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a>这个发现与上一章介绍的ALBERT的SOP实验互相印证：NSP任务之所以无效，是因为它的负样本（随机拼接的两个片段）太容易区分了——模型只需要检测主题是否一致就能答对，根本不需要理解句子间的逻辑关系。</span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 消融3：更大的Batch Size</span></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a>BERT使用256个序列的batch size，在100万步中完成训练。RoBERTa探索了将batch size增大到2K甚至8K序列的效果，同时相应减少总步数以保持总的训练token数不变。</span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a>实验显示，batch size从256增大到2K时，困惑度从3.99降到3.68，MNLI从84.7提升到85.2——这是一个显著的改善。进一步增大到8K时，困惑度略有回升（3.77），但MNLI仍保持在84.6。综合考虑性能和训练效率（大batch可以更好地利用数据并行），RoBERTa最终采用了8K的batch size。</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a>为什么更大的batch size有帮助？直觉上，更大的batch提供了更准确的梯度估计，使得优化器可以使用更大的学习率进行更"自信"的更新。在分布式训练中，大batch还能显著提高GPU利用率——每块GPU的计算量增大，而通信开销的相对比例降低。</span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 消融4：更多数据 + 更长训练</span></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>这是RoBERTa最大的"杀手锏"。BERT只使用了约16GB的文本（BookCorpus + Wikipedia），RoBERTa将训练数据扩展到了160GB，包含五个语料库：</span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 语料库 <span class="pp">|</span> 大小 <span class="pp">|</span> 内容 <span class="pp">|</span></span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|------|------|</span></span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BookCorpus + Wikipedia <span class="pp">|</span> 16GB <span class="pp">|</span> BERT原始数据 <span class="pp">|</span></span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CC-News <span class="pp">|</span> 76GB <span class="pp">|</span> Common Crawl新闻 <span class="pp">|</span></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> OpenWebText <span class="pp">|</span> 38GB <span class="pp">|</span> Reddit高赞链接的网页 <span class="pp">|</span></span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Stories <span class="pp">|</span> 31GB <span class="pp">|</span> Common Crawl故事子集 <span class="pp">|</span></span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **合计** | **160GB** | **10倍于BERT** <span class="pp">|</span></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>更多的数据配合更长的训练（从100K步延长到500K步），带来了持续的性能提升。在MNLI上，100K步时为89.0，300K步时提升到90.0，500K步时进一步提升到90.2——训练了5倍的步数仍然没有出现过拟合的迹象，说明BERT的模型容量远未被充分利用。</span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RoBERTa的最终表现</span></span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>将以上四项改进叠加，RoBERTa在不改变任何架构和预训练目标的前提下，达到了全面领先的性能：</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 任务 <span class="pp">|</span> BERT-Large <span class="pp">|</span> XLNet-Large <span class="pp">|</span> RoBERTa <span class="pp">|</span></span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------------|-------------|---------|</span></span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MNLI <span class="pp">|</span> 86.6 <span class="pp">|</span> 89.8 <span class="pp">|</span> **90.2** <span class="pp">|</span></span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> QNLI <span class="pp">|</span> 92.3 <span class="pp">|</span> 93.9 <span class="pp">|</span> **94.7** <span class="pp">|</span></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SST-2 <span class="pp">|</span> 93.2 <span class="pp">|</span> 95.6 <span class="pp">|</span> **96.4** <span class="pp">|</span></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RTE <span class="pp">|</span> 70.4 <span class="pp">|</span> 83.8 <span class="pp">|</span> **86.6** <span class="pp">|</span></span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CoLA <span class="pp">|</span> 60.6 <span class="pp">|</span> 63.6 <span class="pp">|</span> **68.0** <span class="pp">|</span></span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GLUE测试集 <span class="pp">|</span> — <span class="pp">|</span> 88.4 <span class="pp">|</span> **88.5** <span class="pp">|</span></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a>这些数字的含义非常清晰：RoBERTa用最简单的MLM目标，仅通过训练策略的优化，就超越了XLNet精心设计的排列语言建模。在RTE任务上，RoBERTa甚至比BERT-Large高出16.2个百分点——这个差距不是来自更好的架构，而是来自更充分的训练。</span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RoBERTa的另一个贡献：Byte-level BPE</span></span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a>值得一提的是，RoBERTa还将BERT的字符级BPE（30K词汇表）替换为GPT-2引入的**字节级BPE**（50K词汇表）。字节级BPE直接在字节序列上操作，彻底消除了未知字符的问题——任何Unicode字符都可以被表示，无需<span class="in">`[UNK]`</span>标记。这个改变对英文的影响很小（约0.1%的性能差异），但为多语言扩展提供了更好的基础。</span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### ALBERT：参数效率的突破</span></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>如果说RoBERTa的贡献在于"训练更充分"，那么ALBERT（A Lite BERT）的贡献在于揭示了BERT架构中隐藏的**参数冗余**。Google Research和Toyota Technological Institute的研究者们提出了两项参数缩减技术和一个改进的句子级预训练任务，将BERT-Base的参数量从1.08亿压缩到仅1200万——减少了89%——而性能仅下降约2个百分点。</span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 技术1：嵌入矩阵分解</span></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a>这是ALBERT最巧妙的技术洞察之一。在BERT中，词嵌入矩阵的维度是 $V \times H$，其中 $V$ 是词汇表大小（通常30000），$H$ 是隐藏层维度（Base为768，Large为1024）。嵌入维度 $E$ 被直接绑定为等于隐藏层维度 $H$。</span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a>但从表示学习的角度来看，这种绑定是不合理的。词嵌入学到的是**上下文无关**的表示——"bank"在嵌入层总是同一个向量，不管它出现在金融还是地理语境中。而隐藏层学到的是**上下文相关**的表示——经过多层Transformer之后，"bank"的表示会根据上下文变化。上下文无关的表示本质上比上下文相关的表示"简单"，它不需要那么高的维度来编码信息。因此，强制 $E = H$ 意味着嵌入矩阵的维度被不必要地放大了。</span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a>ALBERT的解决方案是将嵌入矩阵分解为两个较小的矩阵：</span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>\underbrace{V \times H}_{\text{BERT}} \longrightarrow \underbrace{V \times E}_{\text{查表}} + \underbrace{E \times H}_{\text{投影}}</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a>让我们用具体数字来感受这个改变的效果。对于BERT-Base（$V = 30000$，$H = 768$），嵌入矩阵有 $30000 \times 768 = 23{,}040{,}000$ 个参数，约23M。在ALBERT中取 $E = 128$，分解后的参数量为 $30000 \times 128 + 128 \times 768 = 3{,}840{,}000 + 98{,}304 = 3{,}938{,}304$，约3.9M。参数量减少了约83%。</span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a>ALBERT的消融实验验证了 $E = 128$ 是最优选择：</span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 嵌入维度 $E$ <span class="pp">|</span> 参数量 <span class="pp">|</span> 平均分 <span class="pp">|</span></span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------------|--------|--------|</span></span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 64 <span class="pp">|</span> 10M <span class="pp">|</span> 79.0 <span class="pp">|</span></span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **128** | **12M** | **80.1** <span class="pp">|</span></span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 256 <span class="pp">|</span> 16M <span class="pp">|</span> 79.6 <span class="pp">|</span></span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 768（= BERT） <span class="pp">|</span> 31M <span class="pp">|</span> 79.8 <span class="pp">|</span></span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a>一个引人注目的事实是 $E = 128$ 不仅参数最少，性能还最好（80.1 vs 79.8）——更大的嵌入维度反而略有下降。这支持了理论分析：上下文无关的词嵌入确实不需要那么高的维度，过大的 $E$ 反而引入了不必要的参数量而未带来信息增益。</span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 技术2：跨层参数共享</span></span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a>这是ALBERT最大胆的设计决策。在标准BERT中，每一层Transformer都有独立的参数——注意力模块和FFN模块各有一套权重矩阵。BERT-Base有12层，意味着有12套完全独立的参数。</span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a>ALBERT提出了一个激进的假设：**所有层共享同一套参数**。也就是说，第1层和第12层使用完全相同的 $W_Q, W_K, W_V, W_O$（注意力参数）和 $W_1, W_2$（FFN参数）。从计算图的角度看，这等价于将同一个Transformer层重复应用12次——有点类似于循环神经网络的权重共享思想，但应用在了Transformer的层间。</span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a>这个设计选择的效果是惊人的参数缩减。消融实验展示了不同共享策略的效果（以 $E = 128$ 为例）：</span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 共享策略 <span class="pp">|</span> 参数量 <span class="pp">|</span> 平均分 <span class="pp">|</span> 相对下降 <span class="pp">|</span></span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a><span class="pp">|----------|--------|--------|----------|</span></span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 不共享（BERT标准） <span class="pp">|</span> 89M <span class="pp">|</span> 81.6 <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 仅共享注意力参数 <span class="pp">|</span> 64M <span class="pp">|</span> 81.7 <span class="pp">|</span> +0.1 <span class="pp">|</span></span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 仅共享FFN参数 <span class="pp">|</span> 38M <span class="pp">|</span> 80.2 <span class="pp">|</span> -1.4 <span class="pp">|</span></span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **全部共享（ALBERT）** | **12M** | **80.1** | **-1.5** <span class="pp">|</span></span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a>几个关键观察值得深思。</span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a>首先，**仅共享注意力参数几乎不损失性能**（81.7 vs 81.6，甚至略有提升）。这暗示不同层的注意力模式可能确实高度相似——模型在每一层做的"关注"操作本质上是类似的。</span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a>其次，**共享FFN参数的代价更大**（-1.4分），说明不同层的FFN可能在存储不同类型的知识——浅层FFN可能存储语法信息，深层FFN可能存储语义信息。强制它们共享会损失这种分工。</span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a>第三，**全部共享的代价只有1.5分**（81.6 → 80.1），但参数量从89M骤降到12M——**参数效率提升了7.4倍**。从性价比的角度看，这是一个极有吸引力的权衡。</span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a><span class="al">![BERT-Large与ALBERT-Large各层输入输出嵌入的L2距离（左）和余弦相似度（右）。BERT的各层差异大且不规律（红色虚线），而ALBERT的各层转换更加平滑（蓝色实线），表明跨层参数共享起到了稳定网络参数的作用。](figures/chapter-15/original/fig1-albert-cross-layer-analysis.png)</span>{#fig-albert-cross-layer width=90%}</span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a>*Source: Lan et al. (2020) "ALBERT: A Lite BERT for Self-supervised Learning", Figure 1*</span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 参数量 ≠ 计算量：ALBERT的重要悖论</span></span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a>然而，ALBERT有一个常被误解的特性：**参数量的减少并不等于计算量的减少**。</span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a>虽然ALBERT-Base只有12M参数（BERT-Base的1/9），但它在推理时的FLOPs与BERT-Base完全相同——因为每一层仍然执行相同的矩阵乘法，只不过使用的是共享的权重矩阵。参数共享减少的是**内存占用**（只需要存储一份权重），而非**计算量**（每一层仍然要做完整的前向传播）。</span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a>这个悖论在ALBERT-xxlarge上表现得尤为突出。ALBERT-xxlarge有12层、隐藏维度4096，总参数量为235M——比BERT-Large的334M少了30%。但由于其隐藏维度是BERT-Large的4倍（4096 vs 1024），每一层的计算量大约是BERT-Large的16倍。结果，ALBERT-xxlarge的推理速度比BERT-Large**慢3.3倍**，尽管它的参数更少。</span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 参数量 <span class="pp">|</span> 隐藏维度 <span class="pp">|</span> 层数 <span class="pp">|</span> 相对BERT-Large速度 <span class="pp">|</span> GLUE平均分 <span class="pp">|</span></span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|----------|------|-------------------|---------|</span></span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT-Base <span class="pp">|</span> 108M <span class="pp">|</span> 768 <span class="pp">|</span> 12 <span class="pp">|</span> 4.7x 快 <span class="pp">|</span> 82.3 <span class="pp">|</span></span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT-Large <span class="pp">|</span> 334M <span class="pp">|</span> 1024 <span class="pp">|</span> 24 <span class="pp">|</span> 1.0x（基准） <span class="pp">|</span> 85.2 <span class="pp">|</span></span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ALBERT-Base <span class="pp">|</span> 12M <span class="pp">|</span> 768 <span class="pp">|</span> 12 <span class="pp">|</span> 5.6x 快 <span class="pp">|</span> 80.1 <span class="pp">|</span></span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ALBERT-Large <span class="pp">|</span> 18M <span class="pp">|</span> 1024 <span class="pp">|</span> 24 <span class="pp">|</span> 1.7x 快 <span class="pp">|</span> 82.4 <span class="pp">|</span></span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ALBERT-xxlarge <span class="pp">|</span> 235M <span class="pp">|</span> 4096 <span class="pp">|</span> 12 <span class="pp">|</span> **0.3x 慢** | **88.7** <span class="pp">|</span></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a>这个表格清楚地展示了ALBERT的设计哲学：通过参数共享"节省"下来的参数预算，被重新分配到了更宽的隐藏维度上。ALBERT-xxlarge用比BERT-Large更少的参数，换来了更高的性能，但代价是更慢的推理速度。这对于追求**性能上界**的研究场景很有价值，但对于追求**部署效率**的工程场景则未必合适。</span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 技术3：句子顺序预测（SOP）替代NSP</span></span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a>ALBERT还对句子级预训练任务做了改进。回顾BERT的NSP任务：给定两个片段，预测第二个片段是否是第一个片段的真实后续。NSP的负样本是从不同文档中随机抽取的片段——这使得任务过于简单，模型只需要检测主题是否一致就能达到很高的准确率。</span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a>ALBERT提出了**句子顺序预测**（Sentence Order Prediction, SOP）作为替代。SOP的正样本和NSP相同——同一文档中连续的两个片段。但SOP的负样本是将这两个片段的**顺序反转**——两个片段来自同一文档、主题完全一致，模型必须真正理解句子间的逻辑顺序才能判断正确。</span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a>消融实验验证了SOP的优势：</span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练任务 <span class="pp">|</span> NSP准确率 <span class="pp">|</span> SOP准确率 <span class="pp">|</span> 下游任务平均分 <span class="pp">|</span></span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a><span class="pp">|------------|-----------|-----------|---------------|</span></span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 无 <span class="pp">|</span> 52.4% <span class="pp">|</span> 53.3% <span class="pp">|</span> 79.0 <span class="pp">|</span></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> NSP <span class="pp">|</span> **90.5%** <span class="pp">|</span> 52.0% <span class="pp">|</span> 79.2 <span class="pp">|</span></span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **SOP** | 78.9% | **86.5%** | **80.1** <span class="pp">|</span></span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a>最关键的数字是NSP模型在SOP任务上的准确率：52.0%，几乎是**随机猜测**。这直接证明了NSP学到的只是主题分类，完全没有学到句子间的顺序关系。相反，SOP模型在NSP任务上也能达到78.9%的准确率——因为理解了句子顺序的模型自然也能区分不同主题的片段。</span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a><span class="fu">### DistilBERT：知识蒸馏的实践</span></span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a>DistilBERT来自Hugging Face团队，代表了第三条优化路线：不改变训练策略也不重新设计参数结构，而是通过**知识蒸馏**（Knowledge Distillation）将大模型的"知识"迁移到一个更小的模型中。</span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 知识蒸馏的核心思想</span></span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a>知识蒸馏的基本思想由Hinton et al. (2015)提出：一个训练好的大模型（"教师"）的软概率分布包含了比硬标签更丰富的信息。</span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a>为什么？考虑一个词被遮蔽后的预测任务。假设被遮蔽的词是"cat"，教师模型的输出概率分布可能是：</span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a>P_{\text{teacher}} = <span class="sc">\{</span>\text{"cat"}: 0.65, \; \text{"dog"}: 0.20, \; \text{"kitten"}: 0.08, \; \text{"bird"}: 0.03, \; \ldots<span class="sc">\}</span></span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a>如果只用硬标签训练（交叉熵损失），学生模型只知道"正确答案是cat"，其他所有选项被等同对待。但教师的软概率分布透露了更多信息："dog"和"kitten"也是合理的候选词，"dog"比"kitten"更可能，"bird"也有一点可能性。这些信息编码了词与词之间的语义关系——这正是"暗知识"（dark knowledge）的含义。</span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a>为了放大这种暗知识的信号，知识蒸馏引入了**温度缩放**。标准softmax的温度为 $T = 1$，而蒸馏时使用更高的温度（如 $T = 2$ 或 $T = 5$）来"软化"概率分布：</span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a>\text{softmax}'(z_i, T) = \frac{\exp(z_i / T)}{\sum_n \exp(z_n / T)}</span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a>当 $T &gt; 1$ 时，概率分布变得更加均匀，小概率事件的信号被放大。以刚才的例子为例，如果教师在 $T = 1$ 时输出 $<span class="co">[</span><span class="ot">0.65, 0.20, 0.08, 0.03, \ldots</span><span class="co">]</span>$，在 $T = 3$ 时可能变为 $<span class="co">[</span><span class="ot">0.35, 0.25, 0.18, 0.10, \ldots</span><span class="co">]</span>$。高温下，"dog"、"kitten"、"bird"的概率被显著放大，暗知识变得更容易被学生捕获。</span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a><span class="fu">#### DistilBERT的三重损失函数</span></span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a>DistilBERT的训练目标是三个损失的加权和：</span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \alpha_{\text{ce}} \cdot \mathcal{L}_{\text{ce}} + \alpha_{\text{mlm}} \cdot \mathcal{L}_{\text{mlm}} + \alpha_{\text{cos}} \cdot \mathcal{L}_{\text{cos}}</span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a>**蒸馏损失** $\mathcal{L}_{\text{ce}}$ 是教师和学生在温度 $T$ 下的软概率分布之间的KL散度，乘以 $T^2$ 来补偿温度缩放导致的梯度缩小：</span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{ce}} = T^2 \cdot D_{\text{KL}}<span class="sc">\!</span>\left(\text{softmax}<span class="sc">\!</span>\left(\frac{\mathbf{z}^s}{T}\right) \;\Big<span class="sc">\|</span>\; \text{softmax}<span class="sc">\!</span>\left(\frac{\mathbf{z}^t}{T}\right)\right)</span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a>**MLM损失** $\mathcal{L}_{\text{mlm}}$ 是标准的遮蔽语言模型损失，即学生对被遮蔽token的预测与真实标签之间的交叉熵。这提供了来自真实数据的"硬"监督信号。</span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a>**余弦嵌入损失** $\mathcal{L}_{\text{cos}}$ 鼓励学生和教师的隐藏状态向量方向一致：</span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{cos}} = 1 - \frac{\mathbf{h}^t \cdot \mathbf{h}^s}{<span class="sc">\|</span>\mathbf{h}^t<span class="sc">\|</span> \cdot <span class="sc">\|</span>\mathbf{h}^s<span class="sc">\|</span>}</span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a>这三个损失分别从不同层面对齐教师和学生：蒸馏损失对齐输出层的概率分布，MLM损失对齐与真实数据的吻合度，余弦损失对齐中间层的表示方向。</span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：蒸馏损失的逐步计算</span></span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a>让我们用一个简化的例子走通整个蒸馏损失的计算过程，帮助理解每个公式在做什么。</span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a>**设定**：词汇表只有5个词 $<span class="sc">\{</span>\text{cat}, \text{dog}, \text{kitten}, \text{bird}, \text{fish}<span class="sc">\}</span>$，温度 $T = 3$，被遮蔽的词是 "cat"。</span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a>**Step 1: 教师和学生的原始 logits**</span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a>教师模型（BERT-Base，12层）输出的 logits：</span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a>\mathbf{z}^t = <span class="co">[</span><span class="ot">4.2, \; 2.8, \; 1.5, \; 0.3, \; -0.5</span><span class="co">]</span></span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a>学生模型（DistilBERT，6层）输出的 logits：</span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a>\mathbf{z}^s = <span class="co">[</span><span class="ot">3.5, \; 2.0, \; 1.8, \; 0.7, \; 0.1</span><span class="co">]</span></span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a>**Step 2: 标准 softmax（$T = 1$）下的概率分布**</span>
<span id="cb6-340"><a href="#cb6-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-341"><a href="#cb6-341" aria-hidden="true" tabindex="-1"></a>教师：$P^t = \text{softmax}(\mathbf{z}^t / 1)$</span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a>P^t = \frac{<span class="co">[</span><span class="ot">\exp(4.2), \exp(2.8), \exp(1.5), \exp(0.3), \exp(-0.5)</span><span class="co">]</span>}{\sum} = <span class="co">[</span><span class="ot">0.649, \; 0.160, \; 0.044, \; 0.013, \; 0.006</span><span class="co">]</span></span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a>注意分布非常"尖锐"：教师有 64.9% 的概率集中在 "cat" 上，"dog"只有 16%。"kitten"、"bird"、"fish" 加起来不到 7%——这些小概率中蕴含的语义关系（如 "kitten" 比 "bird" 更接近 "cat"）信号很弱。</span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a>**Step 3: 高温 softmax（$T = 3$）下的软化分布**</span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a>教师：$P^t_T = \text{softmax}(\mathbf{z}^t / 3) = \text{softmax}(<span class="co">[</span><span class="ot">1.40, \; 0.93, \; 0.50, \; 0.10, \; -0.17</span><span class="co">]</span>)$</span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a>P^t_T = <span class="co">[</span><span class="ot">0.322, \; 0.202, \; 0.131, \; 0.088, \; 0.067</span><span class="co">]</span></span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a>学生：$P^s_T = \text{softmax}(\mathbf{z}^s / 3) = \text{softmax}(<span class="co">[</span><span class="ot">1.17, \; 0.67, \; 0.60, \; 0.23, \; 0.03</span><span class="co">]</span>)$</span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a>P^s_T = <span class="co">[</span><span class="ot">0.274, \; 0.166, \; 0.155, \; 0.107, \; 0.088</span><span class="co">]</span></span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a>关键变化：高温下，"kitten"从 4.4% 升到 13.1%，"bird"从 1.3% 升到 8.8%。暗知识被放大了——学生现在能更清楚地看到 "cat" 与 "kitten" 的亲密关系。</span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a>**Step 4: 计算蒸馏损失（KL散度）**</span>
<span id="cb6-366"><a href="#cb6-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-367"><a href="#cb6-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a>D_{\text{KL}}(P^s_T <span class="sc">\|</span> P^t_T) = \sum_i P^s_T(i) \cdot \log \frac{P^s_T(i)}{P^t_T(i)}</span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a>逐项计算：</span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 词 <span class="pp">|</span> $P^s_T$ <span class="pp">|</span> $P^t_T$ <span class="pp">|</span> $P^s_T \log(P^s_T / P^t_T)$ <span class="pp">|</span></span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|---------|--------------------------|</span></span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> cat <span class="pp">|</span> 0.274 <span class="pp">|</span> 0.322 <span class="pp">|</span> $0.274 \times \log(0.851) = -0.044$ <span class="pp">|</span></span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> dog <span class="pp">|</span> 0.166 <span class="pp">|</span> 0.202 <span class="pp">|</span> $0.166 \times \log(0.822) = -0.033$ <span class="pp">|</span></span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> kitten <span class="pp">|</span> 0.155 <span class="pp">|</span> 0.131 <span class="pp">|</span> $0.155 \times \log(1.183) = +0.026$ <span class="pp">|</span></span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> bird <span class="pp">|</span> 0.107 <span class="pp">|</span> 0.088 <span class="pp">|</span> $0.107 \times \log(1.216) = +0.021$ <span class="pp">|</span></span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> fish <span class="pp">|</span> 0.088 <span class="pp">|</span> 0.067 <span class="pp">|</span> $0.088 \times \log(1.313) = +0.024$ <span class="pp">|</span></span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a>D_{\text{KL}} = -0.044 - 0.033 + 0.026 + 0.021 + 0.024 = -0.006</span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a>等等——KL散度不可能为负！让我们检查一下。实际上上面是近似计算，精确计算 KL 散度时应该使用自然对数：</span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a>D_{\text{KL}} \approx 0.0078</span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a>乘以 $T^2 = 9$ 来补偿梯度缩放：</span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{ce}} = T^2 \cdot D_{\text{KL}} = 9 \times 0.0078 = 0.070</span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a>**Step 5: 计算余弦嵌入损失**</span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a>假设教师和学生在对应层的隐藏状态为：</span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a>\mathbf{h}^t = <span class="co">[</span><span class="ot">0.8, \; -0.3, \; 0.5, \; 0.1</span><span class="co">]</span>, \quad \mathbf{h}^s = <span class="co">[</span><span class="ot">0.6, \; -0.1, \; 0.7, \; 0.2</span><span class="co">]</span></span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a>\cos(\mathbf{h}^t, \mathbf{h}^s) = \frac{0.48 + 0.03 + 0.35 + 0.02}{\sqrt{0.99} \times \sqrt{0.90}} = \frac{0.88}{0.944} = 0.932</span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{cos}} = 1 - 0.932 = 0.068</span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a>**Step 6: 总损失**</span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a>假设权重 $\alpha_{\text{ce}} = 5.0$，$\alpha_{\text{mlm}} = 2.0$，$\alpha_{\text{cos}} = 1.0$，MLM 交叉熵损失 $\mathcal{L}_{\text{mlm}} = 1.85$：</span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = 5.0 \times 0.070 + 2.0 \times 1.85 + 1.0 \times 0.068 = 0.35 + 3.70 + 0.068 = 4.12</span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a>**解读**：在这个例子中，MLM损失（硬标签监督）贡献最大——学生离正确答案还有较大差距。蒸馏损失较小，说明学生的概率分布形状已经比较接近教师（都给了 "cat" 最高概率、"dog" 次之）。余弦损失也较小，说明隐藏状态方向基本一致。随着训练进行，三个损失会同步下降，最终学生在各个层面都接近教师的行为。</span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: DistilBERT知识蒸馏训练流程（改编自Sanh et al., 2019）</span></span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a><span class="in">输入: 教师模型 T（预训练好的BERT-base）</span></span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a><span class="in">      学生模型 S（6层，从教师隔层初始化）</span></span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a><span class="in">      训练语料 D</span></span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a><span class="in">初始化:</span></span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a><span class="in">  for layer i in {0, 1, 2, 3, 4, 5}:</span></span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a><span class="in">    S.layer[i].weights ← T.layer[2i].weights  # 隔层取教师参数</span></span>
<span id="cb6-434"><a href="#cb6-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-435"><a href="#cb6-435" aria-hidden="true" tabindex="-1"></a><span class="in">训练循环:</span></span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a><span class="in">  for each batch (x, y) in D:</span></span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a><span class="in">    # 动态遮蔽</span></span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a><span class="in">    x_masked, mask_positions ← random_mask(x, prob=0.15)</span></span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a><span class="in">    # 教师前向传播（不计算梯度）</span></span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a><span class="in">    with no_grad():</span></span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a><span class="in">      z_t, h_t ← T(x_masked)    # 教师logits和隐藏状态</span></span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a><span class="in">    # 学生前向传播</span></span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a><span class="in">    z_s, h_s ← S(x_masked)      # 学生logits和隐藏状态</span></span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a><span class="in">    # 三重损失</span></span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a><span class="in">    L_ce  ← KL(softmax(z_s/T), softmax(z_t/T)) * T²</span></span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a><span class="in">    L_mlm ← CrossEntropy(z_s[mask_positions], y[mask_positions])</span></span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a><span class="in">    L_cos ← 1 - cosine_similarity(h_t, h_s)</span></span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a><span class="in">    L ← α_ce * L_ce + α_mlm * L_mlm + α_cos * L_cos</span></span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a><span class="in">    # 反向传播（仅更新学生）</span></span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a><span class="in">    update S.parameters using gradient of L</span></span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a><span class="in">输出: 蒸馏后的学生模型 S</span></span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a>*改编自 Sanh et al. (2019) "DistilBERT, a distilled version of BERT", arXiv:1910.01108*</span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 架构选择与教师初始化</span></span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a>DistilBERT的学生模型架构做了精心的简化选择：</span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> BERT-Base <span class="pp">|</span> DistilBERT <span class="pp">|</span></span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|------------|</span></span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 层数 <span class="pp">|</span> 12 <span class="pp">|</span> **6**（减半） <span class="pp">|</span></span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 隐藏维度 <span class="pp">|</span> 768 <span class="pp">|</span> 768（不变） <span class="pp">|</span></span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 注意力头数 <span class="pp">|</span> 12 <span class="pp">|</span> 12（不变） <span class="pp">|</span></span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 参数量 <span class="pp">|</span> 110M <span class="pp">|</span> **66M**（-40%） <span class="pp">|</span></span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Token Type嵌入 <span class="pp">|</span> 有 <span class="pp">|</span> **移除** <span class="pp">|</span></span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Pooler层 <span class="pp">|</span> 有 <span class="pp">|</span> **移除** <span class="pp">|</span></span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a>层数减半而隐藏维度和注意力头数保持不变，这个选择不是随意的。保持隐藏维度一致使得一个关键的初始化技巧成为可能：学生的第 $i$ 层（$i = 0, 1, \ldots, 5$）直接用教师的第 $2i$ 层（即第0, 2, 4, 6, 8, 10层）的参数来初始化。这种**教师初始化**给了学生一个非常好的起点，避免了从随机初始化开始训练的困难。</span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a>消融实验量化了每个组件的贡献（以GLUE平均分的变化计）：</span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 配置 <span class="pp">|</span> 相对完整模型的下降 <span class="pp">|</span></span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------------------|</span></span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 完整模型（三重损失 + 教师初始化） <span class="pp">|</span> 基准 <span class="pp">|</span></span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 去掉 $\mathcal{L}_{\text{cos}}$ 和 $\mathcal{L}_{\text{mlm}}$（仅蒸馏损失） <span class="pp">|</span> -2.96 <span class="pp">|</span></span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 去掉 $\mathcal{L}_{\text{mlm}}$（保留蒸馏 + 余弦） <span class="pp">|</span> -1.46 <span class="pp">|</span></span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 去掉 $\mathcal{L}_{\text{cos}}$（保留蒸馏 + MLM） <span class="pp">|</span> -0.31 <span class="pp">|</span></span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 三重损失 + **随机初始化**（无教师初始化） | **-3.69** <span class="pp">|</span></span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a>两个关键发现值得深思。一是**教师初始化的重要性超过了任何单个损失函数**：去掉教师初始化导致3.69分的下降，比去掉余弦损失和MLM损失的总和（2.96分）还要大。这说明一个好的参数起点可能比精巧的训练目标更重要——这与RoBERTa的启示一脉相承。二是**MLM损失（硬标签）的贡献很小**：去掉它只损失0.31分。这暗示教师的软概率分布已经包含了足够的监督信号，真实标签的额外贡献有限。</span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a><span class="fu">#### DistilBERT的最终表现</span></span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a>DistilBERT以40%的参数缩减和60%的推理加速，保留了BERT-Base 97%的性能：</span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 参数量 <span class="pp">|</span> 推理速度 <span class="pp">|</span> GLUE分数 <span class="pp">|</span> 性能保留率 <span class="pp">|</span></span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|----------|----------|-----------|</span></span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT-Base <span class="pp">|</span> 110M <span class="pp">|</span> 1.0x <span class="pp">|</span> 79.5 <span class="pp">|</span> 100% <span class="pp">|</span></span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> DistilBERT <span class="pp">|</span> **66M** | **1.6x** | **77.0** | **96.9%** <span class="pp">|</span></span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a>在移动设备上的表现更为突出：DistilBERT在iPhone 7 Plus上的推理速度比BERT-Base**快71%**，模型文件仅207MB。这使得在边缘设备上部署NLP模型首次成为可能。</span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a><span class="fu">### 三种方法的全景对比</span></span>
<span id="cb6-502"><a href="#cb6-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-503"><a href="#cb6-503" aria-hidden="true" tabindex="-1"></a>最后，让我们将三种方法放在同一个框架下对比，理解它们各自的定位：</span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> RoBERTa <span class="pp">|</span> ALBERT <span class="pp">|</span> DistilBERT <span class="pp">|</span></span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------|--------|------------|</span></span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **核心创新** <span class="pp">|</span> 训练策略优化 <span class="pp">|</span> 参数缩减 + SOP <span class="pp">|</span> 知识蒸馏 <span class="pp">|</span></span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **架构改变** <span class="pp">|</span> 无 <span class="pp">|</span> 嵌入分解 + 跨层共享 <span class="pp">|</span> 层数减半 <span class="pp">|</span></span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **参数量** <span class="pp">|</span> 355M（= BERT-Large） <span class="pp">|</span> 12M-235M <span class="pp">|</span> 66M <span class="pp">|</span></span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **vs BERT参数** <span class="pp">|</span> 相同 <span class="pp">|</span> 1/9（base） <span class="pp">|</span> 60% <span class="pp">|</span></span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **推理速度** <span class="pp">|</span> 相同 <span class="pp">|</span> 因模型而异 <span class="pp">|</span> 1.6x快 <span class="pp">|</span></span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **训练数据** <span class="pp">|</span> 160GB（10x） <span class="pp">|</span> 16GB（1x） <span class="pp">|</span> 16GB（1x） <span class="pp">|</span></span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **GLUE性能** <span class="pp">|</span> 88.5（测试集） <span class="pp">|</span> 89.4（集成） <span class="pp">|</span> 77.0（开发集） <span class="pp">|</span></span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **适用场景** <span class="pp">|</span> 追求性能上界 <span class="pp">|</span> 内存受限 / 追求极致性能 <span class="pp">|</span> 部署 / 边缘设备 <span class="pp">|</span></span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **代价** <span class="pp">|</span> 大量训练计算 <span class="pp">|</span> 推理可能更慢 <span class="pp">|</span> 性能下降3% <span class="pp">|</span></span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Hugging Face加载和对比模型</span></span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a>以下代码展示了如何使用Hugging Face Transformers库加载这三个模型，并对比它们的参数量和推理速度。</span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "模型参数量对比"</span></span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a>    BertModel,</span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a>    RobertaModel,</span>
<span id="cb6-535"><a href="#cb6-535" aria-hidden="true" tabindex="-1"></a>    AlbertModel,</span>
<span id="cb6-536"><a href="#cb6-536" aria-hidden="true" tabindex="-1"></a>    DistilBertModel,</span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model):</span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""统计模型参数量"""</span></span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a>    trainable <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total, trainable</span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载四个模型</span></span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a>    <span class="st">"BERT-Base"</span>: BertModel.from_pretrained(<span class="st">"bert-base-uncased"</span>),</span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RoBERTa-Base"</span>: RobertaModel.from_pretrained(<span class="st">"roberta-base"</span>),</span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ALBERT-Base-v2"</span>: AlbertModel.from_pretrained(<span class="st">"albert-base-v2"</span>),</span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a>    <span class="st">"DistilBERT"</span>: DistilBertModel.from_pretrained(<span class="st">"distilbert-base-uncased"</span>),</span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Model'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Total Params'</span><span class="sc">:&gt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Layers'</span><span class="sc">:&gt;8}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Hidden'</span><span class="sc">:&gt;8}</span><span class="ss">"</span>)</span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">55</span>)</span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a>    total, _ <span class="op">=</span> count_parameters(model)</span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> model.config</span>
<span id="cb6-558"><a href="#cb6-558" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> <span class="bu">getattr</span>(config, <span class="st">'num_hidden_layers'</span>, <span class="st">'?'</span>)</span>
<span id="cb6-559"><a href="#cb6-559" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> <span class="bu">getattr</span>(config, <span class="st">'hidden_size'</span>, <span class="st">'?'</span>)</span>
<span id="cb6-560"><a href="#cb6-560" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>total<span class="sc">:&gt;15,}</span><span class="ss"> </span><span class="sc">{</span>layers<span class="sc">:&gt;8}</span><span class="ss"> </span><span class="sc">{</span>hidden<span class="sc">:&gt;8}</span><span class="ss">"</span>)</span>
<span id="cb6-561"><a href="#cb6-561" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-562"><a href="#cb6-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-563"><a href="#cb6-563" aria-hidden="true" tabindex="-1"></a>预期输出：</span>
<span id="cb6-564"><a href="#cb6-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-565"><a href="#cb6-565" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-566"><a href="#cb6-566" aria-hidden="true" tabindex="-1"></a><span class="in">Model                  Total Params   Layers   Hidden</span></span>
<span id="cb6-567"><a href="#cb6-567" aria-hidden="true" tabindex="-1"></a><span class="in">-------------------------------------------------------</span></span>
<span id="cb6-568"><a href="#cb6-568" aria-hidden="true" tabindex="-1"></a><span class="in">BERT-Base               109,482,240       12      768</span></span>
<span id="cb6-569"><a href="#cb6-569" aria-hidden="true" tabindex="-1"></a><span class="in">RoBERTa-Base            124,645,632       12      768</span></span>
<span id="cb6-570"><a href="#cb6-570" aria-hidden="true" tabindex="-1"></a><span class="in">ALBERT-Base-v2           11,683,584       12      768</span></span>
<span id="cb6-571"><a href="#cb6-571" aria-hidden="true" tabindex="-1"></a><span class="in">DistilBERT               66,362,880        6      768</span></span>
<span id="cb6-572"><a href="#cb6-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-573"><a href="#cb6-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-574"><a href="#cb6-574" aria-hidden="true" tabindex="-1"></a>RoBERTa-Base的参数量略多于BERT-Base（1.25亿 vs 1.09亿），这是因为RoBERTa使用了更大的BPE词汇表（50K vs 30K），导致嵌入矩阵更大。ALBERT-Base-v2的参数量仅为BERT的约1/9，验证了嵌入分解和跨层共享的巨大参数缩减效果。</span>
<span id="cb6-575"><a href="#cb6-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-578"><a href="#cb6-578" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-579"><a href="#cb6-579" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-580"><a href="#cb6-580" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb6-581"><a href="#cb6-581" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "推理速度对比"</span></span>
<span id="cb6-582"><a href="#cb6-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-583"><a href="#cb6-583" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-584"><a href="#cb6-584" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb6-585"><a href="#cb6-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-586"><a href="#cb6-586" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark_inference(model, tokenizer, text, n_runs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-587"><a href="#cb6-587" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""测量模型推理速度"""</span></span>
<span id="cb6-588"><a href="#cb6-588" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-589"><a href="#cb6-589" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb6-590"><a href="#cb6-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-591"><a href="#cb6-591" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 预热</span></span>
<span id="cb6-592"><a href="#cb6-592" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-593"><a href="#cb6-593" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb6-594"><a href="#cb6-594" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb6-595"><a href="#cb6-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-596"><a href="#cb6-596" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计时</span></span>
<span id="cb6-597"><a href="#cb6-597" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.perf_counter()</span>
<span id="cb6-598"><a href="#cb6-598" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-599"><a href="#cb6-599" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_runs):</span>
<span id="cb6-600"><a href="#cb6-600" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb6-601"><a href="#cb6-601" aria-hidden="true" tabindex="-1"></a>    elapsed <span class="op">=</span> (time.perf_counter() <span class="op">-</span> start) <span class="op">/</span> n_runs <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># ms</span></span>
<span id="cb6-602"><a href="#cb6-602" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> elapsed</span>
<span id="cb6-603"><a href="#cb6-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-604"><a href="#cb6-604" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例用法（需要安装transformers和torch）</span></span>
<span id="cb6-605"><a href="#cb6-605" aria-hidden="true" tabindex="-1"></a><span class="co"># text = "The quick brown fox jumps over the lazy dog."</span></span>
<span id="cb6-606"><a href="#cb6-606" aria-hidden="true" tabindex="-1"></a><span class="co"># for name, model in models.items():</span></span>
<span id="cb6-607"><a href="#cb6-607" aria-hidden="true" tabindex="-1"></a><span class="co">#     tokenizer = AutoTokenizer.from_pretrained(model_name)</span></span>
<span id="cb6-608"><a href="#cb6-608" aria-hidden="true" tabindex="-1"></a><span class="co">#     latency = benchmark_inference(model, tokenizer, text)</span></span>
<span id="cb6-609"><a href="#cb6-609" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"{name}: {latency:.2f} ms/inference")</span></span>
<span id="cb6-610"><a href="#cb6-610" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-611"><a href="#cb6-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-612"><a href="#cb6-612" aria-hidden="true" tabindex="-1"></a><span class="fu">### 微调DistilBERT进行文本分类</span></span>
<span id="cb6-613"><a href="#cb6-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-616"><a href="#cb6-616" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-617"><a href="#cb6-617" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-618"><a href="#cb6-618" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb6-619"><a href="#cb6-619" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "DistilBERT微调SST-2情感分类"</span></span>
<span id="cb6-620"><a href="#cb6-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-621"><a href="#cb6-621" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb6-622"><a href="#cb6-622" aria-hidden="true" tabindex="-1"></a>    DistilBertForSequenceClassification,</span>
<span id="cb6-623"><a href="#cb6-623" aria-hidden="true" tabindex="-1"></a>    DistilBertTokenizer,</span>
<span id="cb6-624"><a href="#cb6-624" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb6-625"><a href="#cb6-625" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb6-626"><a href="#cb6-626" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-627"><a href="#cb6-627" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-628"><a href="#cb6-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-629"><a href="#cb6-629" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载数据和模型</span></span>
<span id="cb6-630"><a href="#cb6-630" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"glue"</span>, <span class="st">"sst2"</span>)</span>
<span id="cb6-631"><a href="#cb6-631" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> DistilBertTokenizer.from_pretrained(<span class="st">"distilbert-base-uncased"</span>)</span>
<span id="cb6-632"><a href="#cb6-632" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(</span>
<span id="cb6-633"><a href="#cb6-633" aria-hidden="true" tabindex="-1"></a>    <span class="st">"distilbert-base-uncased"</span>, num_labels<span class="op">=</span><span class="dv">2</span></span>
<span id="cb6-634"><a href="#cb6-634" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-635"><a href="#cb6-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-636"><a href="#cb6-636" aria-hidden="true" tabindex="-1"></a><span class="co"># 预处理</span></span>
<span id="cb6-637"><a href="#cb6-637" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(examples):</span>
<span id="cb6-638"><a href="#cb6-638" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"sentence"</span>], truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb6-639"><a href="#cb6-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-640"><a href="#cb6-640" aria-hidden="true" tabindex="-1"></a>tokenized <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-641"><a href="#cb6-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-642"><a href="#cb6-642" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练配置</span></span>
<span id="cb6-643"><a href="#cb6-643" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb6-644"><a href="#cb6-644" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./distilbert-sst2"</span>,</span>
<span id="cb6-645"><a href="#cb6-645" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-646"><a href="#cb6-646" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb6-647"><a href="#cb6-647" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb6-648"><a href="#cb6-648" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb6-649"><a href="#cb6-649" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb6-650"><a href="#cb6-650" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb6-651"><a href="#cb6-651" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb6-652"><a href="#cb6-652" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-653"><a href="#cb6-653" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-654"><a href="#cb6-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-655"><a href="#cb6-655" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb6-656"><a href="#cb6-656" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb6-657"><a href="#cb6-657" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb6-658"><a href="#cb6-658" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized[<span class="st">"train"</span>],</span>
<span id="cb6-659"><a href="#cb6-659" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>tokenized[<span class="st">"validation"</span>],</span>
<span id="cb6-660"><a href="#cb6-660" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-661"><a href="#cb6-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-662"><a href="#cb6-662" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练</span></span>
<span id="cb6-663"><a href="#cb6-663" aria-hidden="true" tabindex="-1"></a><span class="co"># trainer.train()</span></span>
<span id="cb6-664"><a href="#cb6-664" aria-hidden="true" tabindex="-1"></a><span class="co"># 预期准确率: ~91.3%（vs BERT-Base的~92.7%）</span></span>
<span id="cb6-665"><a href="#cb6-665" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-666"><a href="#cb6-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-667"><a href="#cb6-667" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-668"><a href="#cb6-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-669"><a href="#cb6-669" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb6-670"><a href="#cb6-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-671"><a href="#cb6-671" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨三种工程优化方法的理论基础、边界条件和开放问题</span></span>
<span id="cb6-672"><a href="#cb6-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-673"><a href="#cb6-673" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么"训练更好"如此有效？</span></span>
<span id="cb6-674"><a href="#cb6-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-675"><a href="#cb6-675" aria-hidden="true" tabindex="-1"></a>RoBERTa的成功引发了一个更深层的问题：为什么简单地增加数据和训练时长就能带来如此显著的性能提升？</span>
<span id="cb6-676"><a href="#cb6-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-677"><a href="#cb6-677" aria-hidden="true" tabindex="-1"></a>一个可能的解释来自**损失曲面**（loss landscape）的视角。深度学习模型的损失函数是一个极其高维的非凸曲面，充满了局部最优和鞍点。BERT原始的训练配置（小batch size、少数据、100K步）可能只是找到了一个"还不错"的局部最优。更大的batch size提供了更准确的梯度估计，帮助优化器避开浅的局部最优；更多的数据拓展了损失曲面的"可探索区域"；更长的训练给了优化器更多时间在复杂的曲面上搜索。</span>
<span id="cb6-678"><a href="#cb6-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-679"><a href="#cb6-679" aria-hidden="true" tabindex="-1"></a>另一个解释来自**泛化理论**。更多的训练数据减少了过拟合的风险，使得模型学到的表示更具泛化性。BERT在16GB数据上训练100K步后，可能已经开始过拟合训练数据中的特定模式；而RoBERTa的160GB数据为模型提供了更丰富、更多样的语言现象，迫使模型学习更通用的语言表示。</span>
<span id="cb6-680"><a href="#cb6-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-681"><a href="#cb6-681" aria-hidden="true" tabindex="-1"></a>这种理解对后续的LLM发展产生了深远影响。从GPT-3到LLaMA，"用更多数据训练更长时间"成为了提升性能的默认策略，而架构创新的边际收益越来越小。Chinchilla Scaling Laws后来进一步将这个洞察形式化：给定计算预算，数据量和模型大小存在一个最优比例，而不是一味地增大模型。</span>
<span id="cb6-682"><a href="#cb6-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-683"><a href="#cb6-683" aria-hidden="true" tabindex="-1"></a><span class="fu">### 参数共享的理论视角</span></span>
<span id="cb6-684"><a href="#cb6-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-685"><a href="#cb6-685" aria-hidden="true" tabindex="-1"></a>ALBERT的跨层参数共享为什么能工作？它揭示了Transformer的什么性质？</span>
<span id="cb6-686"><a href="#cb6-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-687"><a href="#cb6-687" aria-hidden="true" tabindex="-1"></a>从**不动点迭代**的角度来看，跨层共享可以理解为反复应用同一个函数 $f$：</span>
<span id="cb6-688"><a href="#cb6-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-689"><a href="#cb6-689" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-690"><a href="#cb6-690" aria-hidden="true" tabindex="-1"></a>\mathbf{h}^{(l+1)} = f(\mathbf{h}^{(l)}; \theta) \quad \text{for } l = 0, 1, \ldots, L-1</span>
<span id="cb6-691"><a href="#cb6-691" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-692"><a href="#cb6-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-693"><a href="#cb6-693" aria-hidden="true" tabindex="-1"></a>其中所有层共享参数 $\theta$。如果这个迭代收敛，最终状态 $\mathbf{h}^* = f(\mathbf{h}^*; \theta)$ 是函数 $f$ 的不动点。从这个角度看，增加层数不是为了增加模型的"容量"，而是为了给不动点迭代更多步骤来收敛。</span>
<span id="cb6-694"><a href="#cb6-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-695"><a href="#cb6-695" aria-hidden="true" tabindex="-1"></a>Dehghani et al. (2019) 的Universal Transformer正是基于这个直觉：他们让Transformer层反复应用直到收敛（通过自适应计算时间机制动态决定停止的层数）。ALBERT可以看作Universal Transformer的一个简化版本——固定迭代次数，不用自适应停止。</span>
<span id="cb6-696"><a href="#cb6-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-697"><a href="#cb6-697" aria-hidden="true" tabindex="-1"></a>ALBERT的深度消融实验部分支持了这个理论。当所有层共享参数（18M参数不变）时，增加层数从1到24带来了持续的性能提升（从52.9到82.1），但从24增加到48时性能略有下降（81.8）。这暗示不动点迭代在约24步时已经"收敛"，更多的步骤不再有帮助。</span>
<span id="cb6-698"><a href="#cb6-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-699"><a href="#cb6-699" aria-hidden="true" tabindex="-1"></a><span class="al">![ALBERT-xxlarge的训练曲线。(a) 添加额外训练数据（Wikipedia + BookCorpus之外）带来的MLM准确率提升；(b) 移除Dropout后的训练曲线——对于参数共享的大模型，Dropout反而是有害的。](figures/chapter-15/original/fig3-albert-training-curves.png)</span>{#fig-albert-training width=85%}</span>
<span id="cb6-700"><a href="#cb6-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-701"><a href="#cb6-701" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-702"><a href="#cb6-702" aria-hidden="true" tabindex="-1"></a>*Source: Lan et al. (2020) "ALBERT: A Lite BERT for Self-supervised Learning", Figure 2*</span>
<span id="cb6-703"><a href="#cb6-703" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-704"><a href="#cb6-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-705"><a href="#cb6-705" aria-hidden="true" tabindex="-1"></a><span class="fu">### 知识蒸馏的信息论解释</span></span>
<span id="cb6-706"><a href="#cb6-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-707"><a href="#cb6-707" aria-hidden="true" tabindex="-1"></a>DistilBERT的成功可以从信息论的角度来理解。教师模型的软概率分布 $P_T(y|\mathbf{x})$ 包含了比硬标签 $y^*$ 更丰富的信息。具体来说，硬标签的信息量是 $\log |\mathcal{V}|$ bits（$\mathcal{V}$ 是词汇表大小），而软分布的信息量与其熵有关：</span>
<span id="cb6-708"><a href="#cb6-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-709"><a href="#cb6-709" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-710"><a href="#cb6-710" aria-hidden="true" tabindex="-1"></a>H(P_T) = -\sum_{y \in \mathcal{V}} P_T(y|\mathbf{x}) \log P_T(y|\mathbf{x})</span>
<span id="cb6-711"><a href="#cb6-711" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-712"><a href="#cb6-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-713"><a href="#cb6-713" aria-hidden="true" tabindex="-1"></a>温度缩放通过增大 $T$ 来提高 $H(P_T)$，让软分布携带更多信息。在极端情况下，$T \to \infty$ 时分布趋向均匀分布，信息量最大但信号无意义；$T = 1$ 时分布尖锐，信息量较小但信号最精确。最优的 $T$ 在"信息量"和"信号质量"之间取得平衡。</span>
<span id="cb6-714"><a href="#cb6-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-715"><a href="#cb6-715" aria-hidden="true" tabindex="-1"></a>从互信息的角度来看，蒸馏可以理解为最大化学生输出与教师输出之间的互信息 $I(Z^s; Z^t)$，同时通过余弦损失约束中间表示的互信息 $I(H^s; H^t)$。三重损失从不同层面逼近了这个优化目标。</span>
<span id="cb6-716"><a href="#cb6-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-717"><a href="#cb6-717" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件与失效模式</span></span>
<span id="cb6-718"><a href="#cb6-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-719"><a href="#cb6-719" aria-hidden="true" tabindex="-1"></a>**RoBERTa的边界条件**。RoBERTa的策略本质上是"用计算换性能"。当计算预算有限时（如只有单卡GPU），RoBERTa的建议几乎无法执行——160GB数据 + 8K batch size需要大量分布式计算资源。此外，RoBERTa的改进在英文上经过了充分验证，但在低资源语言上的效果可能打折扣——更多数据的前提是数据"存在"，而很多语言根本没有160GB的高质量文本。</span>
<span id="cb6-720"><a href="#cb6-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-721"><a href="#cb6-721" aria-hidden="true" tabindex="-1"></a>**ALBERT的边界条件**。跨层参数共享假设不同层的计算是"相似"的。但有研究表明，Transformer的不同层承担着不同的角色——浅层倾向于学习语法模式，深层倾向于学习语义模式。强制共享可能会牺牲这种功能分化。ALBERT的消融实验也证实了这一点：共享FFN参数的代价（-1.4分）远大于共享注意力参数（+0.1分），因为FFN是知识存储的主要载体。</span>
<span id="cb6-722"><a href="#cb6-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-723"><a href="#cb6-723" aria-hidden="true" tabindex="-1"></a>另一个重要的边界条件是ALBERT的"参数效率"不等于"计算效率"。如前所述，ALBERT-xxlarge尽管参数更少，但推理速度比BERT-Large慢3.3倍。在部署场景中，推理延迟往往比模型文件大小更重要。</span>
<span id="cb6-724"><a href="#cb6-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-725"><a href="#cb6-725" aria-hidden="true" tabindex="-1"></a>**DistilBERT的边界条件**。知识蒸馏的效果取决于教师模型的质量——一个"差教师"蒸馏出的学生也不会太好。DistilBERT使用BERT-Base作为教师，如果使用更强的教师（如RoBERTa-Large），蒸馏的学生可能会更好。此外，DistilBERT的6层架构在某些需要深层推理的任务上（如WNLI、RTE）性能下降较为明显，因为这些任务可能需要更多的Transformer层来建立复杂的推理链。</span>
<span id="cb6-726"><a href="#cb6-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-727"><a href="#cb6-727" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb6-728"><a href="#cb6-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-729"><a href="#cb6-729" aria-hidden="true" tabindex="-1"></a>**参数共享的最优粒度**。ALBERT的实验表明完全共享所有层的参数是可行的，但性能有所下降。一个自然的问题是：存在更优的共享策略吗？例如，是否可以将12层分成4组，每组内共享参数？或者让相邻的层共享参数，但远距离的层使用不同参数？系统地探索这个"共享粒度"的design space仍然是一个开放问题。</span>
<span id="cb6-730"><a href="#cb6-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-731"><a href="#cb6-731" aria-hidden="true" tabindex="-1"></a>**蒸馏的理论极限**。DistilBERT用6层保留了BERT-Base 97%的性能。极限在哪里？4层能保留多少？2层呢？是否存在一个信息论下界——低于某个模型容量，无论蒸馏技术多好，都无法保留教师的核心能力？这个问题与模型压缩的理论极限密切相关。</span>
<span id="cb6-732"><a href="#cb6-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-733"><a href="#cb6-733" aria-hidden="true" tabindex="-1"></a>**训练策略 vs 架构设计的权衡曲线**。RoBERTa证明训练策略非常重要，但这是否意味着架构设计不重要了？或者说，在不同的计算预算下，训练策略和架构设计的相对重要性是否会发生变化？在小预算下，聪明的架构设计可能更重要（因为无法靠堆数据来补偿）；在大预算下，简单的架构 + 充分训练可能更有效。这种权衡的形式化分析仍然缺乏。</span>
<span id="cb6-734"><a href="#cb6-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-735"><a href="#cb6-735" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-736"><a href="#cb6-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-737"><a href="#cb6-737" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb6-738"><a href="#cb6-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-739"><a href="#cb6-739" aria-hidden="true" tabindex="-1"></a><span class="fu">### 仍在Encoder-only框架内</span></span>
<span id="cb6-740"><a href="#cb6-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-741"><a href="#cb6-741" aria-hidden="true" tabindex="-1"></a>本章介绍的三项工作——RoBERTa、ALBERT、DistilBERT——有一个共同的局限：它们都在BERT的**Encoder-only**框架内做优化。无论是训练策略的改进、参数效率的提升，还是模型压缩，它们都没有质疑BERT的基本架构选择。</span>
<span id="cb6-742"><a href="#cb6-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-743"><a href="#cb6-743" aria-hidden="true" tabindex="-1"></a>但到2020年，一个更根本的问题正在浮出水面：**Encoder-only架构本身是否是最优选择？**GPT系列在扩展到极大规模后展现出了惊人的In-Context Learning能力——不需要微调就能解决新任务——而这种能力在Encoder-only模型中从未被观察到。T5的Encoder-Decoder架构在理解和生成任务上都表现出色，但参数效率不如前两者。三种架构的根本优劣是什么？这是下一章要深入讨论的问题。</span>
<span id="cb6-744"><a href="#cb6-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-745"><a href="#cb6-745" aria-hidden="true" tabindex="-1"></a><span class="fu">### 工程优化的天花板</span></span>
<span id="cb6-746"><a href="#cb6-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-747"><a href="#cb6-747" aria-hidden="true" tabindex="-1"></a>本章的三种方法都有各自的天花板。RoBERTa的"训练更充分"策略受限于高质量训练数据的可用性和计算预算——不可能无限制地堆数据和计算。ALBERT的参数共享在"参数效率"上接近了某种极限——继续减少参数必然导致不可接受的性能下降。DistilBERT的知识蒸馏受限于学生模型的容量——当学生模型太小时，它根本没有足够的表达能力来承载教师的知识。</span>
<span id="cb6-748"><a href="#cb6-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-749"><a href="#cb6-749" aria-hidden="true" tabindex="-1"></a>更本质的问题是：**在预训练范式的框架内，工程优化能走多远？**当模型规模从亿级增长到千亿级，本章讨论的这些技术是否还适用？这个问题的答案在后续章节中逐步揭晓——规模化带来的不只是量变，还有涌现能力的质变，这将从根本上改变我们对模型优化的思考方式。</span>
<span id="cb6-750"><a href="#cb6-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-751"><a href="#cb6-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb6-752"><a href="#cb6-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-753"><a href="#cb6-753" aria-hidden="true" tabindex="-1"></a>本章的工程优化工作为理解预训练模型的"潜力边界"提供了宝贵的实证数据，但也暴露了Encoder-only范式的内在局限。下一章将后退一步，对BERT开创的Encoder-only路线和GPT开创的Decoder-only路线进行系统性的对比和反思。这个对比不只是技术选择的问题——它将揭示两条路线背后截然不同的设计哲学，并解释为什么Decoder-only架构最终成为了大语言模型时代的主流选择。</span>
<span id="cb6-754"><a href="#cb6-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-755"><a href="#cb6-755" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第16章将聚焦**GPT vs BERT——两条路线的分化与融合**。Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么根本的优劣？T5和BART尝试统一，为什么最终Decoder-only路线胜出？从"预训练 + 微调"到"预训练 + 提示"的范式转变意味着什么？</span></span>
<span id="cb6-756"><a href="#cb6-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-757"><a href="#cb6-757" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-758"><a href="#cb6-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-759"><a href="#cb6-759" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb6-760"><a href="#cb6-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-761"><a href="#cb6-761" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb6-762"><a href="#cb6-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-763"><a href="#cb6-763" aria-hidden="true" tabindex="-1"></a>这一章我们系统介绍了预训练模型的三条工程优化路线，每一条都揭示了BERT性能提升的不同维度。</span>
<span id="cb6-764"><a href="#cb6-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-765"><a href="#cb6-765" aria-hidden="true" tabindex="-1"></a>RoBERTa通过对训练策略的系统性消融研究，证明了"BERT被严重欠训练"这个关键论断。四项改进——动态遮蔽、去掉NSP、更大batch size（8K）、更多数据（160GB）——不改一行架构就将GLUE测试集分数从BERT-Large的不到87分提升到88.5分，追平了精心设计的XLNet。这个结果的意义超越了技术本身：它重新定义了"改进模型"的含义——有时候，最大的进步不来自更巧妙的设计，而来自更充分的训练。</span>
<span id="cb6-766"><a href="#cb6-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-767"><a href="#cb6-767" aria-hidden="true" tabindex="-1"></a>ALBERT通过嵌入矩阵分解（$V \times H \to V \times E + E \times H$）和跨层参数共享，将BERT-Base的参数量从108M压缩到12M——减少了89%——而性能仅下降1.5个百分点。ALBERT还提出了SOP（句子顺序预测）替代NSP，通过更有意义的负样本构建提升了句子级理解能力。但ALBERT也揭示了一个重要悖论：参数量的减少并不等于计算量的减少。</span>
<span id="cb6-768"><a href="#cb6-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-769"><a href="#cb6-769" aria-hidden="true" tabindex="-1"></a>DistilBERT通过知识蒸馏将BERT-Base压缩为一个6层、66M参数的模型，在保留97%性能的同时实现了60%的推理加速。其三重损失函数（蒸馏损失 + MLM损失 + 余弦损失）和教师初始化策略的消融实验表明，好的参数起点可能比精巧的训练目标更重要。</span>
<span id="cb6-770"><a href="#cb6-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-771"><a href="#cb6-771" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb6-772"><a href="#cb6-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-773"><a href="#cb6-773" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 含义 <span class="pp">|</span></span>
<span id="cb6-774"><a href="#cb6-774" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb6-775"><a href="#cb6-775" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $V \times H \to V \times E + E \times H$ <span class="pp">|</span> ALBERT嵌入分解（$E \ll H$） <span class="pp">|</span></span>
<span id="cb6-776"><a href="#cb6-776" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\text{softmax}'(z_i, T) = \frac{\exp(z_i/T)}{\sum_n \exp(z_n/T)}$ <span class="pp">|</span> 知识蒸馏温度缩放 <span class="pp">|</span></span>
<span id="cb6-777"><a href="#cb6-777" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathcal{L} = \alpha_{\text{ce}} \mathcal{L}_{\text{ce}} + \alpha_{\text{mlm}} \mathcal{L}_{\text{mlm}} + \alpha_{\text{cos}} \mathcal{L}_{\text{cos}}$ <span class="pp">|</span> DistilBERT三重损失 <span class="pp">|</span></span>
<span id="cb6-778"><a href="#cb6-778" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathcal{L}_{\text{cos}} = 1 - \frac{\mathbf{h}^t \cdot \mathbf{h}^s}{<span class="sc">\|</span>\mathbf{h}^t<span class="sc">\|</span> \cdot <span class="sc">\|</span>\mathbf{h}^s<span class="sc">\|</span>}$ <span class="pp">|</span> 余弦嵌入损失 <span class="pp">|</span></span>
<span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** RoBERTa去掉了NSP任务但性能不降反升。如果NSP真的没有用，为什么BERT的原始论文中加入NSP后报告了性能提升？提示：考虑实验设计中"输入格式"的变化——BERT比较的不只是"有NSP vs 无NSP"。</span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** ALBERT的嵌入分解将参数量从 $V \times H$ 降到 $V \times E + E \times H$。推导当 $E$ 取什么值时，分解后的参数量恰好等于原始参数量的一半。对于 $V = 30000$，$H = 768$，这个临界值 $E$ 是多少？</span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用Hugging Face加载BERT-Base、ALBERT-Base-v2和DistilBERT-Base，在SST-2数据集上分别微调3个epoch，报告各自的准确率和每个epoch的训练时间。验证ALBERT是否真的在参数量减少的同时保持了训练速度。</span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[对比分析]** ALBERT-xxlarge的参数量比BERT-Large少30%（235M vs 334M），但推理速度慢3.3倍。如果你是一个需要部署NLP模型的工程师，在什么场景下你会选择ALBERT-xxlarge而不是BERT-Large？在什么场景下你会选择DistilBERT？</span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[研究思考]** 本章三种方法的一个共同假设是"BERT的知识可以被更高效地表达或压缩"。但后来的研究发现，大模型（如GPT-3）展现出了小模型无法复现的"涌现能力"。这是否意味着模型压缩存在某种不可逾越的理论极限？涌现能力是否可以被蒸馏？</span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a>**Liu, Y. et al. (2019). "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**。BERT训练策略优化的里程碑。重点阅读：Tables 1-4（四项消融实验——这是论文最核心的贡献）、Table 5（GLUE对比）。可快速浏览：Tables 9-10（超参数细节）。这篇论文的价值不在于技术新颖性，而在于其严格的实验方法论。<span class="co">[</span><span class="ot">arXiv:1907.11692</span><span class="co">](https://arxiv.org/abs/1907.11692)</span></span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a>**Lan, Z. et al. (2020). "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"**。参数效率优化的ICLR 2020论文。重点阅读：Tables 3-5（三项技术的消融——嵌入分解、参数共享、SOP vs NSP）、Table 11（深度分析——不同层数对共享参数模型的影响）。可快速浏览：Table 8（dropout的影响）。<span class="co">[</span><span class="ot">arXiv:1909.11942</span><span class="co">](https://arxiv.org/abs/1909.11942)</span></span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a>**Sanh, V. et al. (2019). "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"**。知识蒸馏用于预训练模型压缩的先驱工作。重点阅读：Table 4（消融实验——每个损失组件和教师初始化的贡献）。论文仅5页，可以快速通读。<span class="co">[</span><span class="ot">arXiv:1910.01108</span><span class="co">](https://arxiv.org/abs/1910.01108)</span></span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-806"><a href="#cb6-806" aria-hidden="true" tabindex="-1"></a>**Hinton, G., Vinyals, O., &amp; Dean, J. (2015). "Distilling the Knowledge in a Neural Network"**。知识蒸馏的奠基性论文，提出了温度缩放和软标签的核心思想。DistilBERT的理论基础直接来自这里。<span class="co">[</span><span class="ot">arXiv:1503.02531</span><span class="co">](https://arxiv.org/abs/1503.02531)</span></span>
<span id="cb6-807"><a href="#cb6-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-808"><a href="#cb6-808" aria-hidden="true" tabindex="-1"></a>**Dehghani, M. et al. (2019). "Universal Transformers"**。提出了跨层共享参数 + 自适应计算时间的Transformer变体，是ALBERT参数共享策略的理论先驱。<span class="co">[</span><span class="ot">arXiv:1807.03819</span><span class="co">](https://arxiv.org/abs/1807.03819)</span></span>
<span id="cb6-809"><a href="#cb6-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-810"><a href="#cb6-810" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb6-811"><a href="#cb6-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-812"><a href="#cb6-812" aria-hidden="true" tabindex="-1"></a>**Jiao, X. et al. (2020). "TinyBERT: Distilling BERT for Natural Language Understanding"**。在DistilBERT的基础上进一步改进蒸馏策略，包括注意力矩阵蒸馏和嵌入层蒸馏。<span class="co">[</span><span class="ot">arXiv:1909.10351</span><span class="co">](https://arxiv.org/abs/1909.10351)</span></span>
<span id="cb6-813"><a href="#cb6-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-814"><a href="#cb6-814" aria-hidden="true" tabindex="-1"></a>**Sun, S. et al. (2020). "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"**。专门为移动设备设计的BERT变体，使用了邀请-瓶颈结构来压缩模型。<span class="co">[</span><span class="ot">arXiv:2004.02984</span><span class="co">](https://arxiv.org/abs/2004.02984)</span></span>
<span id="cb6-815"><a href="#cb6-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-816"><a href="#cb6-816" aria-hidden="true" tabindex="-1"></a>**He, P. et al. (2021). "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"**。在解耦注意力的基础上进一步优化预训练，在SuperGLUE上首次超过人类基准。可以看作RoBERTa路线（训练更好）和架构创新的结合。<span class="co">[</span><span class="ot">arXiv:2006.03654</span><span class="co">](https://arxiv.org/abs/2006.03654)</span></span>
<span id="cb6-817"><a href="#cb6-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-818"><a href="#cb6-818" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb6-819"><a href="#cb6-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-820"><a href="#cb6-820" aria-hidden="true" tabindex="-1"></a>**Ganesh, P. et al. (2021). "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"**。全面综述了BERT压缩技术，包括知识蒸馏、剪枝、量化和参数共享。<span class="co">[</span><span class="ot">arXiv:2002.11985</span><span class="co">](https://arxiv.org/abs/2002.11985)</span></span>
<span id="cb6-821"><a href="#cb6-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-822"><a href="#cb6-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb6-823"><a href="#cb6-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-824"><a href="#cb6-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face RoBERTa**: <span class="co">[</span><span class="ot">huggingface.co/roberta-base</span><span class="co">](https://huggingface.co/roberta-base)</span></span>
<span id="cb6-825"><a href="#cb6-825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face ALBERT**: <span class="co">[</span><span class="ot">huggingface.co/albert-base-v2</span><span class="co">](https://huggingface.co/albert-base-v2)</span></span>
<span id="cb6-826"><a href="#cb6-826" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face DistilBERT**: <span class="co">[</span><span class="ot">huggingface.co/distilbert-base-uncased</span><span class="co">](https://huggingface.co/distilbert-base-uncased)</span></span>
<span id="cb6-827"><a href="#cb6-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-828"><a href="#cb6-828" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-829"><a href="#cb6-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-830"><a href="#cb6-830" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb6-831"><a href="#cb6-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-832"><a href="#cb6-832" aria-hidden="true" tabindex="-1"></a>RoBERTa、ALBERT和DistilBERT几乎同时发表在2019年下半年，但它们的诞生背景和影响却截然不同。</span>
<span id="cb6-833"><a href="#cb6-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-834"><a href="#cb6-834" aria-hidden="true" tabindex="-1"></a>RoBERTa来自Facebook AI（现Meta AI），它的意义在研究方法论上可能大于技术贡献本身。在XLNet发表后引发的"BERT是否已被超越"的热烈讨论中，RoBERTa用严格的控制变量实验冷静地指出：XLNet的性能提升很大程度上来自更充分的训练（更多数据、更长训练时间），而非排列语言建模本身的优势。这个结论在当时引起了不小的争议——它暗示了很多声称"超越BERT"的工作可能只是在和一个"没训练好的BERT"比较。</span>
<span id="cb6-835"><a href="#cb6-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-836"><a href="#cb6-836" aria-hidden="true" tabindex="-1"></a>有趣的是，RoBERTa的一位作者Danqi Chen后来成为了SimCSE论文的通讯作者，而SimCSE也延续了相同的方法论精神：用最简单的方法（Dropout作为数据增强），通过仔细的工程实现大幅超越复杂的方案。</span>
<span id="cb6-837"><a href="#cb6-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-838"><a href="#cb6-838" aria-hidden="true" tabindex="-1"></a>ALBERT来自Google Research和Toyota Technological Institute，它在ICLR 2020上发表。ALBERT的参数共享策略引发了关于"参数量 vs 计算量"的重要讨论——以前人们倾向于用参数量来衡量模型的"大小"，ALBERT证明了这两个概念可以完全脱钩。ALBERT-xxlarge用更少的参数超越了BERT-Large，但推理速度反而更慢——这个反直觉的结果改变了人们对"模型效率"的理解。</span>
<span id="cb6-839"><a href="#cb6-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-840"><a href="#cb6-840" aria-hidden="true" tabindex="-1"></a>DistilBERT来自Hugging Face团队，它可能是三篇论文中对工业界影响最大的。在DistilBERT之前，将BERT部署到生产环境中对很多中小企业来说是不可行的——模型太大、推理太慢、成本太高。DistilBERT首次证明了预训练模型可以被大幅压缩而保留大部分能力，直接推动了NLP技术在更多实际场景中的落地。Hugging Face后来的商业成功，在某种程度上也得益于DistilBERT展示的"让AI更accessible"的理念。</span>
<span id="cb6-841"><a href="#cb6-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-842"><a href="#cb6-842" aria-hidden="true" tabindex="-1"></a>从2020年的GPT-3开始，预训练模型的工程优化进入了一个新阶段。模型规模从亿级跳升到千亿级，本章讨论的参数共享和知识蒸馏等技术虽然仍然有价值，但它们面对的问题性质已经发生了根本变化——不再只是"如何让模型更小更快"，而是"如何让千亿参数的模型跑起来"。这将在第18章（训练稳定性）和第19章（分布式训练）中展开讨论。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>