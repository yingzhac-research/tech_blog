<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-22">
<meta name="description" content="Attention Is All You Need：Transformer如何用纯注意力替代循环结构，Multi-Head、FFN、残差连接的设计智慧，以及这个架构为何改变了一切。">

<title>第8章：Transformer——注意力即一切 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#问题的精确定义" id="toc-问题的精确定义" class="nav-link" data-scroll-target="#问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</a></li>
  <li><a href="#rnn-based-seq2seq的局限" id="toc-rnn-based-seq2seq的局限" class="nav-link" data-scroll-target="#rnn-based-seq2seq的局限"><span class="header-section-number">2.2</span> RNN-based Seq2Seq的局限</a></li>
  <li><a href="#我们需要什么样的架构" id="toc-我们需要什么样的架构" class="nav-link" data-scroll-target="#我们需要什么样的架构"><span class="header-section-number">2.3</span> 我们需要什么样的架构？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#革命性洞察注意力本身就是计算" id="toc-革命性洞察注意力本身就是计算" class="nav-link" data-scroll-target="#革命性洞察注意力本身就是计算"><span class="header-section-number">3.1</span> 革命性洞察：注意力本身就是计算</a></li>
  <li><a href="#核心架构概览" id="toc-核心架构概览" class="nav-link" data-scroll-target="#核心架构概览"><span class="header-section-number">3.2</span> 核心架构概览</a></li>
  <li><a href="#设计动机为什么这些组件" id="toc-设计动机为什么这些组件" class="nav-link" data-scroll-target="#设计动机为什么这些组件"><span class="header-section-number">3.3</span> 设计动机：为什么这些组件？</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#scaled-dot-product-attention" id="toc-scaled-dot-product-attention" class="nav-link" data-scroll-target="#scaled-dot-product-attention"><span class="header-section-number">4.1</span> Scaled Dot-Product Attention</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">4.2</span> Multi-Head Attention</a></li>
  <li><a href="#位置编码在transformer中的角色" id="toc-位置编码在transformer中的角色" class="nav-link" data-scroll-target="#位置编码在transformer中的角色"><span class="header-section-number">4.3</span> 位置编码在Transformer中的角色</a></li>
  <li><a href="#feed-forward-network" id="toc-feed-forward-network" class="nav-link" data-scroll-target="#feed-forward-network"><span class="header-section-number">4.4</span> Feed-Forward Network</a></li>
  <li><a href="#残差连接与layer-normalization" id="toc-残差连接与layer-normalization" class="nav-link" data-scroll-target="#残差连接与layer-normalization"><span class="header-section-number">4.5</span> 残差连接与Layer Normalization</a></li>
  <li><a href="#encoder与decoder的结构差异" id="toc-encoder与decoder的结构差异" class="nav-link" data-scroll-target="#encoder与decoder的结构差异"><span class="header-section-number">4.6</span> Encoder与Decoder的结构差异</a></li>
  <li><a href="#复杂度分析" id="toc-复杂度分析" class="nav-link" data-scroll-target="#复杂度分析"><span class="header-section-number">4.7</span> 复杂度分析</a></li>
  </ul></li>
  <li><a href="#数学推导深入" id="toc-数学推导深入" class="nav-link" data-scroll-target="#数学推导深入"><span class="header-section-number">5</span> 数学推导深入</a>
  <ul class="collapse">
  <li><a href="#矩阵维度的逐步追踪" id="toc-矩阵维度的逐步追踪" class="nav-link" data-scroll-target="#矩阵维度的逐步追踪"><span class="header-section-number">5.1</span> 矩阵维度的逐步追踪</a></li>
  <li><a href="#参数量的详细计算" id="toc-参数量的详细计算" class="nav-link" data-scroll-target="#参数量的详细计算"><span class="header-section-number">5.2</span> 参数量的详细计算</a></li>
  <li><a href="#softmax梯度与缩放因子的数学分析" id="toc-softmax梯度与缩放因子的数学分析" class="nav-link" data-scroll-target="#softmax梯度与缩放因子的数学分析"><span class="header-section-number">5.3</span> Softmax梯度与缩放因子的数学分析</a></li>
  <li><a href="#位置编码的相对位置性质证明" id="toc-位置编码的相对位置性质证明" class="nav-link" data-scroll-target="#位置编码的相对位置性质证明"><span class="header-section-number">5.4</span> 位置编码的相对位置性质证明</a></li>
  <li><a href="#flops计算训练一个transformer需要多少计算" id="toc-flops计算训练一个transformer需要多少计算" class="nav-link" data-scroll-target="#flops计算训练一个transformer需要多少计算"><span class="header-section-number">5.5</span> FLOPs计算：训练一个Transformer需要多少计算？</a></li>
  <li><a href="#复杂度对比总结" id="toc-复杂度对比总结" class="nav-link" data-scroll-target="#复杂度对比总结"><span class="header-section-number">5.6</span> 复杂度对比总结</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">6</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#从零实现transformer" id="toc-从零实现transformer" class="nav-link" data-scroll-target="#从零实现transformer"><span class="header-section-number">6.1</span> 从零实现Transformer</a></li>
  <li><a href="#使用hugging-face-transformers" id="toc-使用hugging-face-transformers" class="nav-link" data-scroll-target="#使用hugging-face-transformers"><span class="header-section-number">6.2</span> 使用Hugging Face Transformers</a></li>
  <li><a href="#复现论文的关键细节" id="toc-复现论文的关键细节" class="nav-link" data-scroll-target="#复现论文的关键细节"><span class="header-section-number">6.3</span> 复现论文的关键细节</a></li>
  <li><a href="#实验验证" id="toc-实验验证" class="nav-link" data-scroll-target="#实验验证"><span class="header-section-number">6.4</span> 实验验证</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">7</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">7.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#为什么有效实证视角" id="toc-为什么有效实证视角" class="nav-link" data-scroll-target="#为什么有效实证视角"><span class="header-section-number">7.2</span> 为什么有效？——实证视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">7.3</span> 方法的边界条件</a></li>
  <li><a href="#变体与扩展" id="toc-变体与扩展" class="nav-link" data-scroll-target="#变体与扩展"><span class="header-section-number">7.4</span> 变体与扩展</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">7.5</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">8</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#计算复杂度on2的诅咒" id="toc-计算复杂度on2的诅咒" class="nav-link" data-scroll-target="#计算复杂度on2的诅咒"><span class="header-section-number">8.1</span> 计算复杂度：<span class="math inline">\(O(n^2)\)</span>的诅咒</a></li>
  <li><a href="#位置编码的外推问题" id="toc-位置编码的外推问题" class="nav-link" data-scroll-target="#位置编码的外推问题"><span class="header-section-number">8.2</span> 位置编码的外推问题</a></li>
  <li><a href="#缺乏显式的层级结构" id="toc-缺乏显式的层级结构" class="nav-link" data-scroll-target="#缺乏显式的层级结构"><span class="header-section-number">8.3</span> 缺乏显式的层级结构</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">8.4</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">9</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">9.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">9.2</span> 关键公式速查</a></li>
  <li><a href="#参数量与复杂度速查" id="toc-参数量与复杂度速查" class="nav-link" data-scroll-target="#参数量与复杂度速查"><span class="header-section-number">9.3</span> 参数量与复杂度速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">9.4</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">10</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">10.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">10.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">10.3</span> 后续发展</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">10.4</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">11</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第8章：Transformer——注意力即一切</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Attention Is All You Need</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Transformer</div>
    <div class="quarto-category">Attention</div>
  </div>
  </div>

<div>
  <div class="description">
    Attention Is All You Need：Transformer如何用纯注意力替代循环结构，Multi-Head、FFN、残差连接的设计智慧，以及这个架构为何改变了一切。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 22, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：能否用纯注意力机制替代循环结构，构建一个并行高效、能捕获任意距离依赖的序列建模架构？</p>
<p><strong>历史坐标</strong>：2017年 | Vaswani et al.&nbsp;“Attention Is All You Need” | Google Brain/Google Research</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们见证了Self-Attention的诞生。这是一个概念性的突破：序列中的每个位置可以直接关注其他任意位置，不再需要通过RNN的逐步传递来建立长距离依赖。Self-Attention彻底打破了循环网络的顺序枷锁，让任意两个位置可以一步到位地直接交互。这不仅解决了长距离依赖的问题，还带来了一个意想不到的副产品：既然不同位置之间的注意力计算彼此独立，它们就可以完全并行——GPU终于能发挥它真正的实力了。更妙的是，注意力权重本身就是可解释的，你可以直接看到模型在处理某个词时”关注”了哪些其他词，这在黑箱横行的深度学习时代尤为珍贵。</p>
<p>然而，Self-Attention仍然面临几个关键问题。首先是位置信息的缺失：纯Self-Attention是置换不变的（permutation invariant），打乱输入顺序，输出也只是相应打乱，模型完全不知道”谁在谁前面”。其次，之前的Self-Attention通常只是作为RNN的辅助模块，而非独立架构。最后，如何堆叠多层？如何处理编码器-解码器结构？如何保证训练稳定？这些架构设计问题都还悬而未决。</p>
<p>2017年，Google的研究团队在论文”Attention Is All You Need”中给出了一个大胆的回答。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：完全抛弃循环结构，用精心设计的注意力模块（Scaled Dot-Product Attention + Multi-Head）配合位置编码、残差连接、层归一化，构建一个完整的序列到序列架构——Transformer。</p>
</blockquote>
<p>这个决定在当时看来相当激进。RNN已经统治序列建模领域多年，放弃它意味着放弃一种直觉上合理的归纳偏置——毕竟，顺序处理符合人类阅读的方式。但实验结果令人震惊：Transformer不仅在机器翻译任务上大幅超越了当时最好的RNN模型，训练速度还快了一个数量级。</p>
<p>这一章，我们将深入理解Transformer的每个设计决策：为什么要这样做？还有什么其他选择？这些选择带来了什么后果？</p>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="问题的精确定义" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</h3>
<p>我们要解决的核心问题是序列到序列建模（Sequence-to-Sequence）：给定输入序列 <span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)</span>，生成输出序列 <span class="math inline">\(\mathbf{y} = (y_1, y_2, \ldots, y_m)\)</span>。这个问题为什么重要？因为几乎所有的自然语言处理任务都可以转化为某种形式的序列到序列问题。机器翻译是把英文句子变成中文句子，文本摘要是把长文档变成短摘要，对话系统是把用户输入变成系统回复，代码生成是把自然语言描述变成代码。找到一个通用的序列建模架构，就等于找到了NLP的”万能钥匙”。</p>
</section>
<section id="rnn-based-seq2seq的局限" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="rnn-based-seq2seq的局限"><span class="header-section-number">2.2</span> RNN-based Seq2Seq的局限</h3>
<p>在Transformer之前，主流的Seq2Seq架构是基于RNN的Encoder-Decoder加上Attention机制（详见第4-6章）。</p>
<div id="fig-rnn-seq2seq" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-seq2seq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/fig-rnn-seq2seq.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-seq2seq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RNN-based Seq2Seq with Attention
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>自绘示意图，基于 Bahdanau et al.&nbsp;(2015) 和 Sutskever et al.&nbsp;(2014) 的 Seq2Seq + Attention 架构</em></p>
</div>
<p>回顾前几章的讨论，这个架构有三个根本性的问题：<strong>顺序计算瓶颈</strong>（第4章）——RNN必须串行计算 <span class="math inline">\(h_t = f(h_{t-1}, x_t)\)</span>，GPU无法并行；<strong>长距离依赖衰减</strong>（第4章）——即使有LSTM/GRU的门控缓解，信息仍需逐步传递，100步的非线性变换不可避免地带来损失；<strong>Attention受限于RNN骨架</strong>（第5-6章）——Attention只是RNN之间的桥梁，Encoder和Decoder内部仍然是顺序处理。</p>
</section>
<section id="我们需要什么样的架构" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的架构"><span class="header-section-number">2.3</span> 我们需要什么样的架构？</h3>
<p>从上述分析可以看出，理想的序列建模架构应该满足几个关键需求。首先是并行计算能力，这样才能充分利用GPU加速训练。其次是直接的长距离连接，让任意两个位置可以一步到位地交互。同时必须保留位置信息，因为序列顺序对语言理解至关重要。当然，表达能力要足够强，至少能匹配或超越RNN。最后，训练要稳定，深层网络要能收敛。</p>
<p>Transformer的设计正是为了同时满足这些需求。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="革命性洞察注意力本身就是计算" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="革命性洞察注意力本身就是计算"><span class="header-section-number">3.1</span> 革命性洞察：注意力本身就是计算</h3>
<p>传统观点把注意力看作一种选择机制——从一堆信息中挑选重要的部分。Transformer的洞察更加深刻：注意力可以是计算本身，不只是选择信息，而是通过加权聚合来生成新的表示。</p>
<p>让我用一个图书馆的类比来解释这个区别。RNN的方式就像你按顺序阅读书架上的每本书，一本接一本，用笔记记录累积的理解。读到第100本书时，你对第1本的记忆已经模糊了。Transformer的方式则完全不同：你同时把所有书摊开在桌上，对于每个问题（Query），快速扫视所有书（Key），找出相关的（高注意力权重），然后综合这些相关内容（Value的加权和）得到答案。这种方式不仅更高效，而且不会遗忘。</p>
</section>
<section id="核心架构概览" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="核心架构概览"><span class="header-section-number">3.2</span> 核心架构概览</h3>
<p>Transformer由两个主要部分组成：Encoder负责理解输入序列，Decoder负责生成输出序列。</p>
<div id="fig-transformer-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/original/fig1-transformer-architecture.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The Transformer — model architecture.
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Vaswani et al.&nbsp;(2017) “Attention Is All You Need”, Figure 1</em></p>
</div>
<p>整个架构由五种核心组件构成。Scaled Dot-Product Attention负责高效地计算注意力。Multi-Head Attention让多个注意力”头”关注不同的子空间。Positional Encoding注入位置信息。Feed-Forward Network提供逐位置的非线性变换。Residual Connection和Layer Norm则保证深层训练的稳定性。</p>
</section>
<section id="设计动机为什么这些组件" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="设计动机为什么这些组件"><span class="header-section-number">3.3</span> 设计动机：为什么这些组件？</h3>
<p>每个组件都是为了解决特定问题而存在的。Scaled Dot-Product用点积计算注意力，比加性注意力更高效，因为可以直接利用矩阵乘法的硬件优化。缩放因子<span class="math inline">\(\sqrt{d_k}\)</span>防止softmax饱和导致的梯度消失。Multi-Head让模型能同时捕获多种不同类型的关系模式。Positional Encoding弥补了纯注意力机制丢失的位置信息。FFN提供必要的非线性变换能力，因为单纯的注意力操作本质上是线性的。Residual和LayerNorm则是深层网络训练的标配，没有它们模型根本无法收敛。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="scaled-dot-product-attention" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="scaled-dot-product-attention"><span class="header-section-number">4.1</span> Scaled Dot-Product Attention</h3>
<section id="基本公式" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="基本公式"><span class="header-section-number">4.1.1</span> 基本公式</h4>
<p>上一章我们建立了Self-Attention的Q-K-V框架和基本计算流程（第7章）。现在让我们看Transformer论文如何将其形式化为一个高效的标准化计算模块。给定Query矩阵 <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span>，Key矩阵 <span class="math inline">\(K \in \mathbb{R}^{m \times d_k}\)</span>，Value矩阵 <span class="math inline">\(V \in \mathbb{R}^{m \times d_v}\)</span>，Scaled Dot-Product Attention的计算公式是：</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]</span></p>
<p>这里<span class="math inline">\(n\)</span>是Query的数量，<span class="math inline">\(m\)</span>是Key/Value的数量，<span class="math inline">\(d_k\)</span>是Key的维度，<span class="math inline">\(d_v\)</span>是Value的维度。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Scaled Dot-Product Attention (Vaswani et al., 2017)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>： - Query 矩阵 <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span> - Key 矩阵 <span class="math inline">\(K \in \mathbb{R}^{m \times d_k}\)</span> - Value 矩阵 <span class="math inline">\(V \in \mathbb{R}^{m \times d_v}\)</span> - 可选的 Mask 矩阵 <span class="math inline">\(M \in \mathbb{R}^{n \times m}\)</span></p>
<p><strong>输出</strong>：注意力输出 <span class="math inline">\(\text{Output} \in \mathbb{R}^{n \times d_v}\)</span></p>
<pre><code>1. 计算注意力分数（点积）:
   S ← Q × K^T                    # S ∈ ℝ^(n × m)

2. 缩放（防止 softmax 饱和）:
   S ← S / √d_k

3. 应用 Mask（可选，用于因果注意力）:
   if Mask is provided:
       S ← S + M                  # M 中非法位置为 -∞

4. Softmax 归一化（按行）:
   A ← softmax(S, dim=-1)         # A ∈ ℝ^(n × m), 每行和为 1

5. 加权聚合:
   Output ← A × V                 # Output ∈ ℝ^(n × d_v)

6. return Output, A               # 返回输出和注意力权重</code></pre>
<p><strong>关键设计</strong>： - 缩放因子 <span class="math inline">\(\sqrt{d_k}\)</span> 确保点积方差为 1，避免 softmax 梯度消失 - Mask 中的 <span class="math inline">\(-\infty\)</span> 经 softmax 变为 0，实现”不可见”效果 - 时间复杂度：<span class="math inline">\(O(n \cdot m \cdot d_k + n \cdot m \cdot d_v) = O(n \cdot m \cdot d)\)</span> - 空间复杂度：<span class="math inline">\(O(n \cdot m)\)</span>（注意力矩阵）</p>
<p><em>Source: Vaswani, A. et al.&nbsp;(2017). “Attention Is All You Need”. NeurIPS 2017. <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></em></p>
</div>
</div>
<p>整个计算流程可以用下图表示：</p>
<div id="fig-scaled-dot-product" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaled-dot-product-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/original/fig2-scaled-attention.png" class="img-fluid figure-img" style="width:35.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaled-dot-product-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Scaled Dot-Product Attention
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Vaswani et al.&nbsp;(2017) “Attention Is All You Need”, Figure 2 (left)</em></p>
</div>
</section>
<section id="为什么要除以-sqrtd_k" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="为什么要除以-sqrtd_k"><span class="header-section-number">4.1.2</span> 为什么要除以 <span class="math inline">\(\sqrt{d_k}\)</span>？</h4>
<p>这是Transformer论文中最精妙的细节之一，值得我们从数学上仔细理解。</p>
<p>问题的根源在于：当<span class="math inline">\(d_k\)</span>较大时，点积<span class="math inline">\(q \cdot k\)</span>的方差会变大。假设<span class="math inline">\(q\)</span>和<span class="math inline">\(k\)</span>的每个分量都是独立同分布的，均值为0，方差为1。那么点积的方差是多少呢？</p>
<p><span class="math display">\[
\text{Var}(q \cdot k) = \text{Var}\left(\sum_{i=1}^{d_k} q_i k_i\right) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k
\]</span></p>
<p>当<span class="math inline">\(d_k = 512\)</span>时，点积的标准差约为<span class="math inline">\(\sqrt{512} \approx 22.6\)</span>。这意味着点积值可能非常大或非常小。</p>
<p>这会带来什么后果？如果某个点积远大于其他点积，softmax输出会接近one-hot分布，梯度会趋近于零（因为softmax在饱和区的梯度很小），学习就会停滞。</p>
<p>解决方案很优雅：除以<span class="math inline">\(\sqrt{d_k}\)</span>，将方差重新标准化为1。</p>
<p><span class="math display">\[
\text{Var}\left(\frac{q \cdot k}{\sqrt{d_k}}\right) = \frac{d_k}{d_k} = 1
\]</span></p>
<p>这保证了无论<span class="math inline">\(d_k\)</span>多大，注意力分数的分布都保持稳定。在<span class="math inline">\(d_k = 64\)</span>的典型设置下，不缩放时点积标准差约为8，softmax容易饱和；缩放后标准差为1，softmax梯度稳定。</p>
</section>
<section id="mask的作用" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="mask的作用"><span class="header-section-number">4.1.3</span> Mask的作用</h4>
<p>在解码器的Self-Attention中，我们需要因果遮罩（Causal Mask）来保证模型只能看到过去，不能看到未来。Mask矩阵的定义是：当<span class="math inline">\(i \geq j\)</span>时为0，当<span class="math inline">\(i &lt; j\)</span>时为<span class="math inline">\(-\infty\)</span>。加上Mask后：</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{Mask}\right)V
\]</span></p>
<p><span class="math inline">\(-\infty\)</span>经过softmax变成0，实现了”看不到未来”的效果。这对于自回归生成至关重要——在生成第<span class="math inline">\(t\)</span>个词时，模型只能依赖已经生成的前<span class="math inline">\(t-1\)</span>个词。</p>
</section>
<section id="完整数值示例从embedding到attention输出" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="完整数值示例从embedding到attention输出"><span class="header-section-number">4.1.4</span> 完整数值示例：从Embedding到Attention输出</h4>
<p>抽象的公式有时让人难以建立直觉。让我们用一个极简的例子，手把手走完从词嵌入到注意力输出的全过程。</p>
<p><strong>设定</strong>：句子 “I love NLP”，共3个词。为了便于手算，我们设 <span class="math inline">\(d_{model} = 4\)</span>（实际模型用512或更大）。</p>
<p><strong>Step 1: 词嵌入（Word Embedding）</strong></p>
<p>假设我们的词嵌入矩阵已经训练好，查表得到每个词的向量：</p>
<p><span class="math display">\[
\begin{aligned}
\text{I} &amp;\rightarrow [1.0, 0.0, 1.0, 0.0] \\
\text{love} &amp;\rightarrow [0.0, 1.0, 0.5, 0.5] \\
\text{NLP} &amp;\rightarrow [1.0, 1.0, 0.0, 0.0]
\end{aligned}
\]</span></p>
<p>写成矩阵形式 <span class="math inline">\(E \in \mathbb{R}^{3 \times 4}\)</span>：</p>
<p><span class="math display">\[
E = \begin{bmatrix}
1.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\
0.0 &amp; 1.0 &amp; 0.5 &amp; 0.5 \\
1.0 &amp; 1.0 &amp; 0.0 &amp; 0.0
\end{bmatrix}
\]</span></p>
<p><strong>Step 2: 位置编码（Positional Encoding）</strong></p>
<p>使用正弦位置编码公式。对于 <span class="math inline">\(d_{model} = 4\)</span>，我们有两对 sin/cos（维度0-1用频率 <span class="math inline">\(\omega_0\)</span>，维度2-3用频率 <span class="math inline">\(\omega_1\)</span>）：</p>
<p><span class="math display">\[
\omega_0 = \frac{1}{10000^{0/4}} = 1, \quad \omega_1 = \frac{1}{10000^{2/4}} = 0.01
\]</span></p>
<p>计算每个位置的编码：</p>
<p><span class="math display">\[
\begin{aligned}
PE_0 &amp;= [\sin(0), \cos(0), \sin(0), \cos(0)] = [0, 1, 0, 1] \\
PE_1 &amp;= [\sin(1), \cos(1), \sin(0.01), \cos(0.01)] \approx [0.84, 0.54, 0.01, 1.00] \\
PE_2 &amp;= [\sin(2), \cos(2), \sin(0.02), \cos(0.02)] \approx [0.91, -0.42, 0.02, 1.00]
\end{aligned}
\]</span></p>
<p>位置编码矩阵 <span class="math inline">\(PE \in \mathbb{R}^{3 \times 4}\)</span>：</p>
<p><span class="math display">\[
PE = \begin{bmatrix}
0.00 &amp; 1.00 &amp; 0.00 &amp; 1.00 \\
0.84 &amp; 0.54 &amp; 0.01 &amp; 1.00 \\
0.91 &amp; -0.42 &amp; 0.02 &amp; 1.00
\end{bmatrix}
\]</span></p>
<p><strong>Step 3: 输入表示 = 词嵌入 + 位置编码</strong></p>
<p><span class="math display">\[
X = E + PE = \begin{bmatrix}
1.00 &amp; 1.00 &amp; 1.00 &amp; 1.00 \\
0.84 &amp; 1.54 &amp; 0.51 &amp; 1.50 \\
1.91 &amp; 0.58 &amp; 0.02 &amp; 1.00
\end{bmatrix}
\]</span></p>
<p>现在每个词的表示既包含语义信息（词嵌入），也包含位置信息（位置编码）。</p>
<p><strong>Step 4: 线性投影得到 Q, K, V</strong></p>
<p>为简化演示，假设投影矩阵 <span class="math inline">\(W^Q, W^K, W^V\)</span> 都是单位矩阵（实际中是可学习参数）：</p>
<p><span class="math display">\[
Q = XW^Q = X, \quad K = XW^K = X, \quad V = XW^V = X
\]</span></p>
<p>在 Self-Attention 中，Q、K、V 都来自同一个输入 <span class="math inline">\(X\)</span>。</p>
<p><strong>Step 5: 计算注意力分数 <span class="math inline">\(QK^\top\)</span></strong></p>
<p>这一步计算每对位置之间的”相似度”：</p>
<p><span class="math display">\[
QK^\top = XX^\top = \begin{bmatrix}
1.00 &amp; 1.00 &amp; 1.00 &amp; 1.00 \\
0.84 &amp; 1.54 &amp; 0.51 &amp; 1.50 \\
1.91 &amp; 0.58 &amp; 0.02 &amp; 1.00
\end{bmatrix}
\begin{bmatrix}
1.00 &amp; 0.84 &amp; 1.91 \\
1.00 &amp; 1.54 &amp; 0.58 \\
1.00 &amp; 0.51 &amp; 0.02 \\
1.00 &amp; 1.50 &amp; 1.00
\end{bmatrix}
\]</span></p>
<p>逐元素计算（以第(1,2)个元素为例）：</p>
<p><span class="math display">\[
(QK^\top)_{12} = 1.0 \times 0.84 + 1.0 \times 1.54 + 1.0 \times 0.51 + 1.0 \times 1.50 = 4.39
\]</span></p>
<p>完整的注意力分数矩阵：</p>
<p><span class="math display">\[
QK^\top = \begin{bmatrix}
4.00 &amp; 4.39 &amp; 3.51 \\
4.39 &amp; 5.06 &amp; 3.64 \\
3.51 &amp; 3.64 &amp; 4.70
\end{bmatrix}
\]</span></p>
<p>观察：对角线元素（自己与自己的相似度）通常较高，但不一定最高。</p>
<p><strong>Step 6: 缩放（除以 <span class="math inline">\(\sqrt{d_k}\)</span>）</strong></p>
<p><span class="math inline">\(d_k = 4\)</span>，所以 <span class="math inline">\(\sqrt{d_k} = 2\)</span>：</p>
<p><span class="math display">\[
\frac{QK^\top}{\sqrt{d_k}} = \begin{bmatrix}
2.00 &amp; 2.20 &amp; 1.76 \\
2.20 &amp; 2.53 &amp; 1.82 \\
1.76 &amp; 1.82 &amp; 2.35
\end{bmatrix}
\]</span></p>
<p>缩放使得数值更加温和，避免 softmax 饱和。</p>
<p><strong>Step 7: Softmax（按行归一化）</strong></p>
<p>对每一行应用 softmax，得到注意力权重 <span class="math inline">\(A\)</span>：</p>
<p><span class="math display">\[
\text{softmax}([2.00, 2.20, 1.76]) = \frac{[e^{2.00}, e^{2.20}, e^{1.76}]}{\sum} = \frac{[7.39, 9.03, 5.81]}{22.23} = [0.33, 0.41, 0.26]
\]</span></p>
<p>完整的注意力权重矩阵：</p>
<p><span class="math display">\[
A = \begin{bmatrix}
0.33 &amp; 0.41 &amp; 0.26 \\
0.30 &amp; 0.42 &amp; 0.28 \\
0.27 &amp; 0.29 &amp; 0.44
\end{bmatrix}
\]</span></p>
<p><strong>解读注意力权重</strong>：</p>
<ul>
<li>第1行：处理 “I” 时，模型最关注 “love”（0.41），其次是自己（0.33），最后是 “NLP”（0.26）</li>
<li>第2行：处理 “love” 时，也最关注自己（0.42），但同时也在看 “I” 和 “NLP”</li>
<li>第3行：处理 “NLP” 时，最关注自己（0.44），这符合直觉——专有名词更依赖自身语义</li>
</ul>
<p><strong>Step 8: 加权求和得到输出</strong></p>
<p>最终输出是 Value 向量的加权和：</p>
<p><span class="math display">\[
\text{Output} = AV = A \cdot X
\]</span></p>
<p>以第一行（“I” 的新表示）为例：</p>
<p><span class="math display">\[
\text{Output}_1 = 0.33 \times [1.00, 1.00, 1.00, 1.00] + 0.41 \times [0.84, 1.54, 0.51, 1.50] + 0.26 \times [1.91, 0.58, 0.02, 1.00]
\]</span></p>
<p><span class="math display">\[
= [0.33, 0.33, 0.33, 0.33] + [0.34, 0.63, 0.21, 0.62] + [0.50, 0.15, 0.01, 0.26] = [1.17, 1.11, 0.55, 1.21]
\]</span></p>
<p>完整输出矩阵：</p>
<p><span class="math display">\[
\text{Output} = \begin{bmatrix}
1.17 &amp; 1.11 &amp; 0.55 &amp; 1.21 \\
1.15 &amp; 1.15 &amp; 0.54 &amp; 1.22 \\
1.31 &amp; 0.99 &amp; 0.44 &amp; 1.13
\end{bmatrix}
\]</span></p>
<p><strong>关键洞察</strong>：</p>
<p>注意力机制做了什么？比较输入 <span class="math inline">\(X\)</span> 和输出：</p>
<ul>
<li><strong>信息融合</strong>：每个词的输出不再只是自己的表示，而是融合了其他词的信息。“I” 的新表示包含了 “love” 和 “NLP” 的成分。</li>
<li><strong>上下文感知</strong>：同一个词 “I” 在不同句子中会有不同的输出，因为它会融合不同的上下文。</li>
<li><strong>软选择</strong>：通过 softmax 权重实现”软”的信息选择，而非硬性地只看某一个词。</li>
</ul>
<p>这就是 Self-Attention 的核心：让每个词都能”看到”整个句子，并根据相关性选择性地融合信息。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>实际模型的规模
</div>
</div>
<div class="callout-body-container callout-body">
<p>上面的例子用 <span class="math inline">\(d_{model} = 4\)</span> 是为了手算方便。实际的 Transformer Base 用 <span class="math inline">\(d_{model} = 512\)</span>，序列长度可达数百甚至数千。注意力矩阵的大小是 <span class="math inline">\(n \times n\)</span>，这就是 <span class="math inline">\(O(n^2)\)</span> 复杂度的来源。</p>
</div>
</div>
</section>
</section>
<section id="multi-head-attention" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">4.2</span> Multi-Head Attention</h3>
<section id="动机为什么需要多头" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="动机为什么需要多头"><span class="header-section-number">4.2.1</span> 动机：为什么需要多头？</h4>
<p>单头注意力有一个根本性的局限：每个位置只能有一种”关注模式”。但在自然语言中，一个词可能同时需要关注多种不同类型的信息。比如动词需要关注它的主语（语法关系），代词需要关注它的先行词（指代关系），同义词之间也需要相互关注（语义关系）。单头难以同时捕获这些不同类型的依赖，这就是Multi-Head的动机。</p>
</section>
<section id="数学形式" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="数学形式"><span class="header-section-number">4.2.2</span> 数学形式</h4>
<p>Multi-Head Attention的核心思想是将<span class="math inline">\(d_{model}\)</span>维的Q、K、V分别投影到<span class="math inline">\(h\)</span>个子空间，在每个子空间独立计算注意力，最后拼接起来：</p>
<p><span class="math display">\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
\]</span></p>
<p>其中每个head的计算是：</p>
<p><span class="math display">\[
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\]</span></p>
<p>参数矩阵的维度设计很讲究：<span class="math inline">\(W_i^Q\)</span>和<span class="math inline">\(W_i^K\)</span>都是<span class="math inline">\(d_{model} \times d_k\)</span>，<span class="math inline">\(W_i^V\)</span>是<span class="math inline">\(d_{model} \times d_v\)</span>，输出投影<span class="math inline">\(W^O\)</span>是<span class="math inline">\(hd_v \times d_{model}\)</span>。通常设置<span class="math inline">\(d_k = d_v = d_{model}/h\)</span>，这样保证了总参数量不变。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Multi-Head Attention (Vaswani et al., 2017)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>： - Query 矩阵 <span class="math inline">\(Q \in \mathbb{R}^{n \times d_{model}}\)</span> - Key 矩阵 <span class="math inline">\(K \in \mathbb{R}^{m \times d_{model}}\)</span> - Value 矩阵 <span class="math inline">\(V \in \mathbb{R}^{m \times d_{model}}\)</span> - 头数 <span class="math inline">\(h\)</span>，每头维度 <span class="math inline">\(d_k = d_v = d_{model} / h\)</span></p>
<p><strong>参数</strong>： - 投影矩阵 <span class="math inline">\(W_i^Q, W_i^K \in \mathbb{R}^{d_{model} \times d_k}\)</span>，<span class="math inline">\(W_i^V \in \mathbb{R}^{d_{model} \times d_v}\)</span>，<span class="math inline">\(i = 1, \ldots, h\)</span> - 输出投影 <span class="math inline">\(W^O \in \mathbb{R}^{h \cdot d_v \times d_{model}}\)</span></p>
<p><strong>输出</strong>：<span class="math inline">\(\text{MultiHead}(Q, K, V) \in \mathbb{R}^{n \times d_{model}}\)</span></p>
<pre><code>1. 并行计算 h 个注意力头:
   for i = 1 to h (in parallel):
       Q_i ← Q × W_i^Q              # 投影到第 i 个子空间
       K_i ← K × W_i^K
       V_i ← V × W_i^V
       head_i ← Attention(Q_i, K_i, V_i, mask)   # Scaled Dot-Product

2. 拼接所有头的输出:
   Concat ← [head_1; head_2; ...; head_h]    # Concat ∈ ℝ^(n × h·d_v)

3. 最终线性投影:
   Output ← Concat × W^O            # Output ∈ ℝ^(n × d_model)

4. return Output</code></pre>
<p><strong>设计动机</strong>： - 不同的 head 可以关注<strong>不同类型的关系</strong>（语法、语义、位置等） - 投影到低维子空间 (<span class="math inline">\(d_k = d_{model}/h\)</span>) 使计算可行 - 总参数量 = <span class="math inline">\(4 \times d_{model}^2\)</span>，与单头 Attention 相同</p>
<p><strong>典型配置</strong>（Transformer Base）： - <span class="math inline">\(d_{model} = 512\)</span>, <span class="math inline">\(h = 8\)</span>, <span class="math inline">\(d_k = d_v = 64\)</span></p>
<p><em>Source: Vaswani, A. et al.&nbsp;(2017). “Attention Is All You Need”. NeurIPS 2017. <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></em></p>
</div>
</div>
<div id="fig-multi-head" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multi-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/original/fig2-multi-head.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multi-head-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Multi-Head Attention consists of several attention layers running in parallel.
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Vaswani et al.&nbsp;(2017) “Attention Is All You Need”, Figure 2 (right)</em></p>
</div>
</section>
<section id="head数量的选择" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="head数量的选择"><span class="header-section-number">4.2.3</span> Head数量的选择</h4>
<p>Transformer base使用<span class="math inline">\(h=8\)</span>，large使用<span class="math inline">\(h=16\)</span>。选择的考量涉及一个权衡：头太少，表达能力不足；头太多，每个head的维度<span class="math inline">\(d_k = d_{model}/h\)</span>就太小，信息容量不足。经验法则是<span class="math inline">\(d_k \geq 64\)</span>通常是下限。</p>
<p>后续研究发现不同head确实学到了不同的模式。有的head关注局部（相邻词），有的关注全局（长距离依赖）；有的关注语法结构，有的关注语义相似性。这印证了Multi-Head设计的合理性。</p>
</section>
</section>
<section id="位置编码在transformer中的角色" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="位置编码在transformer中的角色"><span class="header-section-number">4.3</span> 位置编码在Transformer中的角色</h3>
<p>第7章已经详细讨论了位置编码的动机（Self-Attention的置换等变性）、正弦编码的数学原理与实现、以及可学习编码等替代方案。这里我们聚焦于位置编码<strong>在Transformer架构中的具体集成方式</strong>。</p>
<p>Transformer采用了正弦位置编码，通过<strong>加法注入</strong>的方式与词嵌入结合：输入表示 = 词嵌入 + 位置编码。这意味着语义信息和位置信息在同一个<span class="math inline">\(d_{model}\)</span>维空间中混合。一个值得注意的工程细节是，词嵌入在相加前需要乘以<span class="math inline">\(\sqrt{d_{model}}\)</span>进行缩放——因为embedding层初始化的值通常很小，直接与PE相加会导致位置信号淹没语义信号。</p>
<div id="fig-positional-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/positional-encoding.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 正弦位置编码可视化：横轴为位置（0-100），纵轴为编码维度（0-128）。低维度区域（顶部）呈现高频振荡，高维度区域（底部）呈现低频变化。这种多尺度频率模式使得模型可以同时感知局部和全局位置关系。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Python生成，基于 Vaswani et al.&nbsp;(2017) 的正弦位置编码公式。<a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></em></p>
</div>
<p>Transformer原论文比较了正弦编码和可学习编码，发现两者效果相当。但后续发展表明，位置编码的选择对模型能力有深远影响——RoPE和ALiBi等新型方案（见第26章）在长度外推方面表现更优。本章”深入理解”一节提供了正弦编码相对位置性质的完整数学证明。</p>
</section>
<section id="feed-forward-network" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="feed-forward-network"><span class="header-section-number">4.4</span> Feed-Forward Network</h3>
<p>每个Transformer层都包含一个逐位置的前馈网络，它的形式很简单：两层线性变换夹一个ReLU激活：</p>
<p><span class="math display">\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p>其中<span class="math inline">\(W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}\)</span>，<span class="math inline">\(W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}\)</span>，通常<span class="math inline">\(d_{ff} = 4 \times d_{model}\)</span>。</p>
<p>FFN是一个被低估的组件。直觉上，Self-Attention负责”信息交流”——决定收集哪些信息，进行不同位置间的加权聚合；FFN负责”信息处理”——决定如何处理信息，对每个位置的表示做非线性变换。可以把这种分工类比为：Attention是”开会讨论”，FFN是”个人思考”。</p>
<p>一个令人惊讶的事实是，在标准Transformer中，FFN的参数量占模型总参数的约三分之二。FFN参数量是<span class="math inline">\(2 \times d_{model} \times d_{ff} = 8d_{model}^2\)</span>，而Attention参数量只有<span class="math inline">\(4 \times d_{model}^2\)</span>。后续研究发现FFN可能起到”记忆”的作用——存储知识，不同层的FFN关注不同类型的信息。稀疏化FFN（如Mixture of Experts）成为了扩展模型规模的重要方向。</p>
</section>
<section id="残差连接与layer-normalization" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="残差连接与layer-normalization"><span class="header-section-number">4.5</span> 残差连接与Layer Normalization</h3>
<section id="残差连接" class="level4" data-number="4.5.1">
<h4 data-number="4.5.1" class="anchored" data-anchor-id="残差连接"><span class="header-section-number">4.5.1</span> 残差连接</h4>
<p>每个子层（Attention或FFN）都被残差连接包裹。残差连接解决的是梯度流动问题：它提供了一条”高速公路”让梯度直接传回前层，使得深层网络训练成为可能。Transformer base有6层，large有12层；后来的模型到96层甚至更多，没有残差连接这是不可想象的。</p>
</section>
<section id="layer-normalization" class="level4" data-number="4.5.2">
<h4 data-number="4.5.2" class="anchored" data-anchor-id="layer-normalization"><span class="header-section-number">4.5.2</span> Layer Normalization</h4>
<p>与Batch Normalization不同，Layer Norm在特征维度而非batch维度上归一化：</p>
<p><span class="math display">\[
\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta
\]</span></p>
<p>其中<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>是<span class="math inline">\(x\)</span>在特征维度上的均值和标准差。</p>
<p>为什么用LayerNorm而非BatchNorm？原因有三。第一，序列长度不固定，不同序列长度不同，batch统计不稳定。第二，LayerNorm不依赖batch统计，训练和推理行为一致。第三，每个样本独立归一化，对并行更友好。</p>
</section>
<section id="pre-norm-vs-post-norm" class="level4" data-number="4.5.3">
<h4 data-number="4.5.3" class="anchored" data-anchor-id="pre-norm-vs-post-norm"><span class="header-section-number">4.5.3</span> Pre-Norm vs Post-Norm</h4>
<p>原始Transformer使用Post-Norm，先子层后归一化：<span class="math inline">\(x_{l+1} = \text{LayerNorm}(x_l + \text{Sublayer}(x_l))\)</span>。后来的模型多采用Pre-Norm，先归一化后子层：<span class="math inline">\(x_{l+1} = x_l + \text{Sublayer}(\text{LayerNorm}(x_l))\)</span>。</p>
<div id="fig-pre-post-norm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pre-post-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/fig-pre-post-norm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pre-post-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Pre-Norm vs Post-Norm：左侧为原始Transformer的Post-Norm结构，右侧为后续模型广泛采用的Pre-Norm结构。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>自绘示意图，基于 Xiong et al.&nbsp;(2020) “On Layer Normalization in the Transformer Architecture”. <a href="https://arxiv.org/abs/2002.04745">arXiv:2002.04745</a></em></p>
</div>
<p>Pre-Norm的优势在于更稳定的训练，尤其是深层模型，甚至不需要warmup也能收敛。GPT系列、LLaMA等都采用了Pre-Norm。</p>
</section>
</section>
<section id="encoder与decoder的结构差异" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="encoder与decoder的结构差异"><span class="header-section-number">4.6</span> Encoder与Decoder的结构差异</h3>
<p>Transformer的Encoder和Decoder有三个关键区别。</p>
<p>第一个区别是Self-Attention的类型。Encoder使用双向Self-Attention，每个位置可以看到整个输入序列：</p>
<p><span class="math display">\[
\text{Attention}_{ij} = \frac{\exp(q_i \cdot k_j / \sqrt{d_k})}{\sum_l \exp(q_i \cdot k_l / \sqrt{d_k})}
\]</span></p>
<p>没有Mask，第<span class="math inline">\(i\)</span>个位置可以关注任意位置<span class="math inline">\(j\)</span>。而Decoder使用因果Self-Attention，第<span class="math inline">\(i\)</span>个位置只能关注位置<span class="math inline">\(1, 2, \ldots, i\)</span>：</p>
<p><span class="math display">\[
\text{Attention}_{ij} = \begin{cases}
\frac{\exp(q_i \cdot k_j / \sqrt{d_k})}{\sum_{l \leq i} \exp(q_i \cdot k_l / \sqrt{d_k})} &amp; j \leq i \\
0 &amp; j &gt; i
\end{cases}
\]</span></p>
<p>这保证了生成时只用到已生成的token，符合自回归的要求。</p>
<p>第二个区别是Decoder有一个额外的Cross-Attention层，通过它连接到Encoder。Query来自Decoder当前层的输出，Key和Value来自Encoder最后一层的输出。这使得生成每个词时都可以参考整个输入序列。</p>
<div id="fig-cross-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cross-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/fig-cross-attention.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Cross-Attention连接Decoder与Encoder：Query来自Decoder，Key和Value来自Encoder的最终输出。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>自绘示意图，基于 Vaswani et al.&nbsp;(2017) “Attention Is All You Need”, Figure 1. <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></em></p>
</div>
<p>第三个区别是用途：Encoder负责理解输入序列，产生上下文表示；Decoder负责基于这些表示生成输出序列。</p>
</section>
<section id="复杂度分析" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="复杂度分析"><span class="header-section-number">4.7</span> 复杂度分析</h3>
<p>设序列长度为<span class="math inline">\(n\)</span>，模型维度为<span class="math inline">\(d\)</span>。Self-Attention的时间复杂度是<span class="math inline">\(O(n^2 d)\)</span>，瓶颈在于<span class="math inline">\(n^2\)</span>的注意力矩阵计算。FFN的时间复杂度是<span class="math inline">\(O(n d^2)\)</span>，瓶颈在于<span class="math inline">\(d^2\)</span>的线性变换。总计是<span class="math inline">\(O(n^2 d + n d^2)\)</span>，当序列很长时<span class="math inline">\(n^2\)</span>项主导。</p>
<p>与RNN的<span class="math inline">\(O(n d^2)\)</span>相比，Transformer在<span class="math inline">\(n &lt; d\)</span>时更快，<span class="math inline">\(n &gt; d\)</span>时更慢。但关键区别在于并行性：RNN需要<span class="math inline">\(O(n)\)</span>个顺序步骤，完全无法并行；Transformer只需要<span class="math inline">\(O(1)\)</span>个顺序步骤（只是层数），计算完全并行。这是Transformer训练速度远超RNN的根本原因。</p>
<p>空间复杂度方面，最大的开销是注意力矩阵，需要<span class="math inline">\(O(n^2)\)</span>的空间。对于长序列，比如处理整本书，这会成为严重问题。这为第9章的高效注意力研究埋下了伏笔。</p>
<hr>
</section>
</section>
<section id="数学推导深入" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="数学推导深入"><span class="header-section-number">5</span> 数学推导深入</h2>
<blockquote class="blockquote">
<p><strong>研究者视角</strong>：这一节提供更详细的数学推导，帮助深入理解Transformer的计算细节</p>
</blockquote>
<section id="矩阵维度的逐步追踪" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="矩阵维度的逐步追踪"><span class="header-section-number">5.1</span> 矩阵维度的逐步追踪</h3>
<p>理解Transformer的一个关键是追踪每一步的矩阵维度。让我们以单个Self-Attention层为例，假设输入序列长度为<span class="math inline">\(n\)</span>，模型维度为<span class="math inline">\(d_{model}\)</span>，使用<span class="math inline">\(h\)</span>个注意力头，每个头的维度为<span class="math inline">\(d_k = d_v = d_{model}/h\)</span>。</p>
<p><strong>输入</strong>：<span class="math inline">\(X \in \mathbb{R}^{n \times d_{model}}\)</span></p>
<p><strong>Step 1：线性投影</strong></p>
<p><span class="math display">\[
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
\]</span></p>
<p>其中<span class="math inline">\(W^Q, W^K \in \mathbb{R}^{d_{model} \times d_k}\)</span>，<span class="math inline">\(W^V \in \mathbb{R}^{d_{model} \times d_v}\)</span>。</p>
<p>得到：<span class="math inline">\(Q, K \in \mathbb{R}^{n \times d_k}\)</span>，<span class="math inline">\(V \in \mathbb{R}^{n \times d_v}\)</span></p>
<p><strong>Step 2：计算注意力分数</strong></p>
<p><span class="math display">\[
S = \frac{QK^\top}{\sqrt{d_k}}
\]</span></p>
<p>矩阵乘法：<span class="math inline">\((n \times d_k) \times (d_k \times n) = (n \times n)\)</span></p>
<p>得到：<span class="math inline">\(S \in \mathbb{R}^{n \times n}\)</span>——这就是<span class="math inline">\(O(n^2)\)</span>复杂度的来源</p>
<p><strong>Step 3：Softmax归一化</strong></p>
<p><span class="math display">\[
A = \text{softmax}(S) \in \mathbb{R}^{n \times n}
\]</span></p>
<p>对每一行做softmax，保持维度不变。第<span class="math inline">\((i,j)\)</span>个元素表示位置<span class="math inline">\(i\)</span>对位置<span class="math inline">\(j\)</span>的注意力权重。</p>
<p><strong>Step 4：加权聚合</strong></p>
<p><span class="math display">\[
\text{Output} = AV
\]</span></p>
<p>矩阵乘法：<span class="math inline">\((n \times n) \times (n \times d_v) = (n \times d_v)\)</span></p>
<p>得到：<span class="math inline">\(\text{Output} \in \mathbb{R}^{n \times d_v}\)</span></p>
<p><strong>Multi-Head的维度变化</strong></p>
<p>对于<span class="math inline">\(h\)</span>个头，每个头独立计算上述过程，得到<span class="math inline">\(h\)</span>个<span class="math inline">\(\mathbb{R}^{n \times d_v}\)</span>的输出。拼接后：</p>
<p><span class="math display">\[
\text{Concat}(\text{head}_1, \ldots, \text{head}_h) \in \mathbb{R}^{n \times (h \cdot d_v)} = \mathbb{R}^{n \times d_{model}}
\]</span></p>
<p>最后通过<span class="math inline">\(W^O \in \mathbb{R}^{d_{model} \times d_{model}}\)</span>投影，输出维度仍为<span class="math inline">\(\mathbb{R}^{n \times d_{model}}\)</span>，与输入相同。</p>
</section>
<section id="参数量的详细计算" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="参数量的详细计算"><span class="header-section-number">5.2</span> 参数量的详细计算</h3>
<p>让我们精确计算Transformer各组件的参数量。</p>
<section id="multi-head-attention-1" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="multi-head-attention-1"><span class="header-section-number">5.2.1</span> Multi-Head Attention</h4>
<p>每个MHA模块包含四个权重矩阵：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>矩阵</th>
<th>维度</th>
<th>参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(W^Q\)</span></td>
<td><span class="math inline">\(d_{model} \times d_{model}\)</span></td>
<td><span class="math inline">\(d_{model}^2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(W^K\)</span></td>
<td><span class="math inline">\(d_{model} \times d_{model}\)</span></td>
<td><span class="math inline">\(d_{model}^2\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(W^V\)</span></td>
<td><span class="math inline">\(d_{model} \times d_{model}\)</span></td>
<td><span class="math inline">\(d_{model}^2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(W^O\)</span></td>
<td><span class="math inline">\(d_{model} \times d_{model}\)</span></td>
<td><span class="math inline">\(d_{model}^2\)</span></td>
</tr>
</tbody>
</table>
<p><strong>MHA总参数量</strong>（不含偏置）：<span class="math inline">\(4d_{model}^2\)</span></p>
<p>注意：虽然我们把<span class="math inline">\(W^Q\)</span>分成<span class="math inline">\(h\)</span>个<span class="math inline">\(W_i^Q\)</span>，但总参数量不变。这是因为<span class="math inline">\(h\)</span>个<span class="math inline">\((d_{model} \times d_k)\)</span>矩阵拼接起来就是一个<span class="math inline">\((d_{model} \times d_{model})\)</span>矩阵。</p>
</section>
<section id="feed-forward-network-1" class="level4" data-number="5.2.2">
<h4 data-number="5.2.2" class="anchored" data-anchor-id="feed-forward-network-1"><span class="header-section-number">5.2.2</span> Feed-Forward Network</h4>
<p><span class="math display">\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>矩阵/向量</th>
<th>维度</th>
<th>参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(W_1\)</span></td>
<td><span class="math inline">\(d_{model} \times d_{ff}\)</span></td>
<td><span class="math inline">\(d_{model} \cdot d_{ff}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(b_1\)</span></td>
<td><span class="math inline">\(d_{ff}\)</span></td>
<td><span class="math inline">\(d_{ff}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(W_2\)</span></td>
<td><span class="math inline">\(d_{ff} \times d_{model}\)</span></td>
<td><span class="math inline">\(d_{ff} \cdot d_{model}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(b_2\)</span></td>
<td><span class="math inline">\(d_{model}\)</span></td>
<td><span class="math inline">\(d_{model}\)</span></td>
</tr>
</tbody>
</table>
<p><strong>FFN总参数量</strong>：<span class="math inline">\(2 \cdot d_{model} \cdot d_{ff} + d_{ff} + d_{model}\)</span></p>
<p>当<span class="math inline">\(d_{ff} = 4 \cdot d_{model}\)</span>时（标准设置）：</p>
<p><span class="math display">\[
\text{FFN参数量} \approx 2 \cdot d_{model} \cdot 4d_{model} = 8d_{model}^2
\]</span></p>
</section>
<section id="单层transformer的参数量比较" class="level4" data-number="5.2.3">
<h4 data-number="5.2.3" class="anchored" data-anchor-id="单层transformer的参数量比较"><span class="header-section-number">5.2.3</span> 单层Transformer的参数量比较</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>组件</th>
<th>参数量</th>
<th>占比</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-Attention</td>
<td><span class="math inline">\(4d_{model}^2\)</span></td>
<td>33%</td>
</tr>
<tr class="even">
<td>FFN</td>
<td><span class="math inline">\(8d_{model}^2\)</span></td>
<td>67%</td>
</tr>
<tr class="odd">
<td>LayerNorm (×2)</td>
<td><span class="math inline">\(4d_{model}\)</span></td>
<td>≈0%</td>
</tr>
<tr class="even">
<td><strong>总计</strong></td>
<td><span class="math inline">\(12d_{model}^2 + 4d_{model}\)</span></td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>这解释了一个有趣的现象：<strong>FFN的参数量是Attention的两倍</strong>。这意味着大模型的大部分参数实际上在FFN中，而非Attention中。</p>
</section>
<section id="完整模型参数量" class="level4" data-number="5.2.4">
<h4 data-number="5.2.4" class="anchored" data-anchor-id="完整模型参数量"><span class="header-section-number">5.2.4</span> 完整模型参数量</h4>
<p>以Transformer Base为例（<span class="math inline">\(d_{model}=512\)</span>, <span class="math inline">\(d_{ff}=2048\)</span>, <span class="math inline">\(L=6\)</span>层, 词汇表<span class="math inline">\(V=32000\)</span>）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>组件</th>
<th>计算</th>
<th>参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Embedding</td>
<td><span class="math inline">\(V \times d_{model}\)</span></td>
<td>16.4M</td>
</tr>
<tr class="even">
<td>Encoder (6层)</td>
<td><span class="math inline">\(6 \times 12 \times d_{model}^2\)</span></td>
<td>18.9M</td>
</tr>
<tr class="odd">
<td>Decoder (6层)</td>
<td><span class="math inline">\(6 \times (12 + 4) \times d_{model}^2\)</span></td>
<td>25.2M</td>
</tr>
<tr class="even">
<td>Output Projection</td>
<td><span class="math inline">\(d_{model} \times V\)</span></td>
<td>16.4M</td>
</tr>
<tr class="odd">
<td><strong>总计</strong></td>
<td></td>
<td><strong>约65M</strong></td>
</tr>
</tbody>
</table>
<p>注：Decoder每层多一个Cross-Attention（<span class="math inline">\(4d_{model}^2\)</span>）</p>
</section>
</section>
<section id="softmax梯度与缩放因子的数学分析" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="softmax梯度与缩放因子的数学分析"><span class="header-section-number">5.3</span> Softmax梯度与缩放因子的数学分析</h3>
<section id="为什么softmax会饱和" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="为什么softmax会饱和"><span class="header-section-number">5.3.1</span> 为什么Softmax会饱和？</h4>
<p>Softmax函数定义为：</p>
<p><span class="math display">\[
\text{softmax}(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
\]</span></p>
<p>考虑输入<span class="math inline">\(z = [a, 0, 0, \ldots, 0]\)</span>，当<span class="math inline">\(a\)</span>很大时：</p>
<p><span class="math display">\[
\text{softmax}(z) \approx [1, 0, 0, \ldots, 0]
\]</span></p>
<p>现在计算Softmax对输入<span class="math inline">\(z_i\)</span>的梯度。设<span class="math inline">\(p_i = \text{softmax}(z)_i\)</span>，则：</p>
<p><span class="math display">\[
\frac{\partial p_i}{\partial z_j} = \begin{cases}
p_i(1 - p_i) &amp; i = j \\
-p_i p_j &amp; i \neq j
\end{cases}
\]</span></p>
<p>当<span class="math inline">\(p_i \approx 1\)</span>时，<span class="math inline">\(\frac{\partial p_i}{\partial z_i} = p_i(1-p_i) \approx 0\)</span>。 当<span class="math inline">\(p_j \approx 0\)</span>时，<span class="math inline">\(\frac{\partial p_i}{\partial z_j} = -p_i p_j \approx 0\)</span>。</p>
<p><strong>结论</strong>：Softmax在饱和区的梯度趋近于零，导致学习停滞。</p>
</section>
<section id="缩放因子的精确推导" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="缩放因子的精确推导"><span class="header-section-number">5.3.2</span> 缩放因子的精确推导</h4>
<p>假设<span class="math inline">\(q, k \in \mathbb{R}^{d_k}\)</span>，每个分量<span class="math inline">\(q_i, k_i \sim \mathcal{N}(0, 1)\)</span>独立同分布。</p>
<p>点积<span class="math inline">\(z = q \cdot k = \sum_{i=1}^{d_k} q_i k_i\)</span>。</p>
<p>由于<span class="math inline">\(q_i\)</span>和<span class="math inline">\(k_i\)</span>独立，<span class="math inline">\(q_i k_i\)</span>的期望和方差为：</p>
<p><span class="math display">\[
\mathbb{E}[q_i k_i] = \mathbb{E}[q_i]\mathbb{E}[k_i] = 0
\]</span></p>
<p><span class="math display">\[
\text{Var}(q_i k_i) = \mathbb{E}[q_i^2 k_i^2] - (\mathbb{E}[q_i k_i])^2 = \mathbb{E}[q_i^2]\mathbb{E}[k_i^2] = 1 \cdot 1 = 1
\]</span></p>
<p>因此：</p>
<p><span class="math display">\[
\mathbb{E}[z] = \sum_{i=1}^{d_k} \mathbb{E}[q_i k_i] = 0
\]</span></p>
<p><span class="math display">\[
\text{Var}(z) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k
\]</span></p>
<p>标准差<span class="math inline">\(\sigma_z = \sqrt{d_k}\)</span>。当<span class="math inline">\(d_k = 512\)</span>时，<span class="math inline">\(\sigma_z \approx 22.6\)</span>。</p>
<p>除以<span class="math inline">\(\sqrt{d_k}\)</span>后：</p>
<p><span class="math display">\[
\text{Var}\left(\frac{z}{\sqrt{d_k}}\right) = \frac{\text{Var}(z)}{d_k} = 1
\]</span></p>
<p><strong>数值示例</strong>： 设<span class="math inline">\(d_k = 64\)</span>，不缩放时点积的95%置信区间约为<span class="math inline">\([-16, 16]\)</span>。 缩放后95%置信区间约为<span class="math inline">\([-2, 2]\)</span>，Softmax输出更平滑，梯度更稳定。</p>
</section>
</section>
<section id="位置编码的相对位置性质证明" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="位置编码的相对位置性质证明"><span class="header-section-number">5.4</span> 位置编码的相对位置性质证明</h3>
<p>正弦位置编码有一个优美的性质：任意固定偏移<span class="math inline">\(k\)</span>的位置关系可以用线性变换表示。</p>
<p><strong>命题</strong>：存在矩阵<span class="math inline">\(M_k\)</span>（只依赖于<span class="math inline">\(k\)</span>），使得对任意位置<span class="math inline">\(pos\)</span>：</p>
<p><span class="math display">\[
PE_{pos+k} = M_k \cdot PE_{pos}
\]</span></p>
<p><strong>证明</strong>：</p>
<p>考虑位置编码的第<span class="math inline">\(2i\)</span>和<span class="math inline">\(2i+1\)</span>维（它们使用相同的频率<span class="math inline">\(\omega_i = 1/10000^{2i/d}\)</span>）：</p>
<p><span class="math display">\[
\begin{bmatrix} PE_{pos, 2i} \\ PE_{pos, 2i+1} \end{bmatrix} = \begin{bmatrix} \sin(\omega_i \cdot pos) \\ \cos(\omega_i \cdot pos) \end{bmatrix}
\]</span></p>
<p>利用三角恒等式：</p>
<p><span class="math display">\[
\sin(\omega_i(pos + k)) = \sin(\omega_i \cdot pos)\cos(\omega_i \cdot k) + \cos(\omega_i \cdot pos)\sin(\omega_i \cdot k)
\]</span></p>
<p><span class="math display">\[
\cos(\omega_i(pos + k)) = \cos(\omega_i \cdot pos)\cos(\omega_i \cdot k) - \sin(\omega_i \cdot pos)\sin(\omega_i \cdot k)
\]</span></p>
<p>写成矩阵形式：</p>
<p><span class="math display">\[
\begin{bmatrix} PE_{pos+k, 2i} \\ PE_{pos+k, 2i+1} \end{bmatrix} = \begin{bmatrix} \cos(\omega_i k) &amp; \sin(\omega_i k) \\ -\sin(\omega_i k) &amp; \cos(\omega_i k) \end{bmatrix} \begin{bmatrix} PE_{pos, 2i} \\ PE_{pos, 2i+1} \end{bmatrix}
\]</span></p>
<p>这是一个旋转矩阵！对于每一对维度<span class="math inline">\((2i, 2i+1)\)</span>，偏移<span class="math inline">\(k\)</span>对应一个旋转角度<span class="math inline">\(\omega_i k\)</span>。</p>
<p>完整的变换矩阵<span class="math inline">\(M_k\)</span>是分块对角矩阵：</p>
<p><span class="math display">\[
M_k = \text{diag}(R_{\omega_0 k}, R_{\omega_1 k}, \ldots, R_{\omega_{d/2-1} k})
\]</span></p>
<p>其中<span class="math inline">\(R_\theta = \begin{bmatrix} \cos\theta &amp; \sin\theta \\ -\sin\theta &amp; \cos\theta \end{bmatrix}\)</span>是2D旋转矩阵。</p>
<p><strong>意义</strong>：这个性质使得模型可以通过学习线性变换来捕获相对位置关系。例如，学习”前一个词”（<span class="math inline">\(k=-1\)</span>）或”后两个词”（<span class="math inline">\(k=2\)</span>）的关系。</p>
</section>
<section id="flops计算训练一个transformer需要多少计算" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="flops计算训练一个transformer需要多少计算"><span class="header-section-number">5.5</span> FLOPs计算：训练一个Transformer需要多少计算？</h3>
<section id="单次前向传播的flops" class="level4" data-number="5.5.1">
<h4 data-number="5.5.1" class="anchored" data-anchor-id="单次前向传播的flops"><span class="header-section-number">5.5.1</span> 单次前向传播的FLOPs</h4>
<p>对于序列长度<span class="math inline">\(n\)</span>，模型维度<span class="math inline">\(d\)</span>，<span class="math inline">\(L\)</span>层Encoder：</p>
<p><strong>Self-Attention（每层）</strong>：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 31%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>操作</th>
<th>计算</th>
<th>FLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Q, K, V\)</span>投影</td>
<td><span class="math inline">\(3 \times (n \times d) \times (d \times d)\)</span></td>
<td><span class="math inline">\(6nd^2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(QK^\top\)</span></td>
<td><span class="math inline">\((n \times d) \times (d \times n)\)</span></td>
<td><span class="math inline">\(2n^2d\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\text{Softmax}(S)V\)</span></td>
<td><span class="math inline">\((n \times n) \times (n \times d)\)</span></td>
<td><span class="math inline">\(2n^2d\)</span></td>
</tr>
<tr class="even">
<td>输出投影</td>
<td><span class="math inline">\((n \times d) \times (d \times d)\)</span></td>
<td><span class="math inline">\(2nd^2\)</span></td>
</tr>
<tr class="odd">
<td><strong>小计</strong></td>
<td></td>
<td><span class="math inline">\(8nd^2 + 4n^2d\)</span></td>
</tr>
</tbody>
</table>
<p><strong>FFN（每层）</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>操作</th>
<th>计算</th>
<th>FLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(W_1\)</span>乘法</td>
<td><span class="math inline">\((n \times d) \times (d \times 4d)\)</span></td>
<td><span class="math inline">\(8nd^2\)</span></td>
</tr>
<tr class="even">
<td>ReLU</td>
<td><span class="math inline">\(4nd\)</span></td>
<td><span class="math inline">\(4nd\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(W_2\)</span>乘法</td>
<td><span class="math inline">\((n \times 4d) \times (4d \times d)\)</span></td>
<td><span class="math inline">\(8nd^2\)</span></td>
</tr>
<tr class="even">
<td><strong>小计</strong></td>
<td></td>
<td><span class="math inline">\(16nd^2 + 4nd\)</span></td>
</tr>
</tbody>
</table>
<p><strong>单层Encoder</strong>：<span class="math inline">\(\approx 24nd^2 + 4n^2d\)</span></p>
<p><strong>L层Encoder前向传播</strong>：<span class="math inline">\(\approx L(24nd^2 + 4n^2d)\)</span></p>
</section>
<section id="训练的总flops估算" class="level4" data-number="5.5.2">
<h4 data-number="5.5.2" class="anchored" data-anchor-id="训练的总flops估算"><span class="header-section-number">5.5.2</span> 训练的总FLOPs估算</h4>
<p>训练时需要前向和反向传播，反向约为前向的2倍，因此单个样本约为前向的3倍。</p>
<p>对于Transformer Base（<span class="math inline">\(d=512\)</span>, <span class="math inline">\(L=6\)</span>, <span class="math inline">\(n=512\)</span>）：</p>
<p><span class="math display">\[
\text{FLOPs/样本} \approx 3 \times 6 \times (24 \times 512 \times 512^2 + 4 \times 512^2 \times 512) \approx 3 \times 10^{10}
\]</span></p>
<p>训练100K步，batch size 64：</p>
<p><span class="math display">\[
\text{总FLOPs} \approx 100000 \times 64 \times 3 \times 10^{10} \approx 2 \times 10^{17}
\]</span></p>
<p>在A100 GPU（312 TFLOPS FP16）上，理论时间约为<span class="math inline">\(2 \times 10^{17} / 3.12 \times 10^{14} \approx 640\)</span>秒，约10分钟。实际由于内存带宽等因素，通常需要数小时。</p>
</section>
</section>
<section id="复杂度对比总结" class="level3" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="复杂度对比总结"><span class="header-section-number">5.6</span> 复杂度对比总结</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>时间复杂度</th>
<th>空间复杂度</th>
<th>顺序操作数</th>
<th>最长路径</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RNN</td>
<td><span class="math inline">\(O(nd^2)\)</span></td>
<td><span class="math inline">\(O(d)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr class="even">
<td>CNN (kernel <span class="math inline">\(k\)</span>)</td>
<td><span class="math inline">\(O(knd^2)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(\log_k n)\)</span></td>
</tr>
<tr class="odd">
<td>Self-Attention</td>
<td><span class="math inline">\(O(n^2d)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="even">
<td>Transformer</td>
<td><span class="math inline">\(O(n^2d + nd^2)\)</span></td>
<td><span class="math inline">\(O(n^2 + nd)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
</tbody>
</table>
<p><strong>解读</strong>：</p>
<ul>
<li><strong>最长路径</strong>：信息从位置1传到位置<span class="math inline">\(n\)</span>需要经过多少层。RNN是<span class="math inline">\(O(n)\)</span>，Transformer是<span class="math inline">\(O(1)\)</span>（一步到位），这是Transformer捕获长距离依赖的关键优势。</li>
<li><strong>顺序操作数</strong>：并行化的程度。RNN必须顺序执行<span class="math inline">\(n\)</span>步，Transformer完全并行。</li>
<li><strong>空间复杂度</strong>：Transformer的<span class="math inline">\(O(n^2)\)</span>是痛点，长序列时显存瓶颈。</li>
</ul>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">6</span> 工程实践</h2>
<section id="从零实现transformer" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="从零实现transformer"><span class="header-section-number">6.1</span> 从零实现Transformer</h3>
<p>以下是Transformer核心模块的PyTorch实现，附详细注释：</p>
<div id="7d083b97" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScaledDotProductAttention(nn.Module):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Scaled Dot-Product Attention</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    核心公式: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, Q, K, V, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> Q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算注意力分数: [batch, heads, seq_len, seq_len]</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用mask（用于因果注意力或padding mask）</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax归一化</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> <span class="va">self</span>.dropout(attention_weights)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加权求和</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.matmul(attention_weights, V)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co">    Multi-Head Attention</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co">    将d_model拆成h个头，每个头独立计算attention，最后拼接</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"d_model必须能被num_heads整除"</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q, K, V的线性投影</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_Q <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_K <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_V <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_O <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> ScaledDotProductAttention(dropout)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, Q, K, V, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 线性投影并reshape为多头格式</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.W_Q(Q).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.W_K(K).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.W_V(V).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算attention</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention(Q, K, V, mask)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接所有头并做最终投影</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> attn_output.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.d_model)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.W_O(attn_output)</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedForward(nn.Module):</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2"""</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(d_model, d_ff)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(d_ff, d_model)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc2(<span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc1(x))))</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""正弦位置编码"""</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :])</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer Encoder层：Self-Attention -&gt; Add &amp; Norm -&gt; FFN -&gt; Add &amp; Norm"""</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> PositionwiseFeedForward(d_model, d_ff, dropout)</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Self-Attention + Residual + LayerNorm</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.self_attn(x, x, x, mask)</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> <span class="va">self</span>.dropout(attn_output))</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN + Residual + LayerNorm</span></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ffn(x)))</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderLayer(nn.Module):</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer Decoder层"""</span></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> PositionwiseFeedForward(d_model, d_ff, dropout)</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, encoder_output, self_attn_mask<span class="op">=</span><span class="va">None</span>, cross_attn_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Masked Self-Attention</span></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.self_attn(x, x, x, self_attn_mask)))</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cross-Attention</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.cross_attn(x, encoder_output, encoder_output, cross_attn_mask)))</span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm3(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ffn(x)))</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""完整的Transformer模型"""</span></span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src_vocab_size, tgt_vocab_size, d_model<span class="op">=</span><span class="dv">512</span>, num_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>                 num_encoder_layers<span class="op">=</span><span class="dv">6</span>, num_decoder_layers<span class="op">=</span><span class="dv">6</span>, d_ff<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>                 max_len<span class="op">=</span><span class="dv">5000</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_embedding <span class="op">=</span> nn.Embedding(src_vocab_size, d_model)</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tgt_embedding <span class="op">=</span> nn.Embedding(tgt_vocab_size, d_model)</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_encoding <span class="op">=</span> PositionalEncoding(d_model, max_len, dropout)</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_encoder_layers)</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_decoder_layers)</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_projection <span class="op">=</span> nn.Linear(d_model, tgt_vocab_size)</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> math.sqrt(d_model)</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, src, src_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_encoding(<span class="va">self</span>.src_embedding(src) <span class="op">*</span> <span class="va">self</span>.scale)</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.encoder_layers:</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, src_mask)</span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, tgt, encoder_output, tgt_mask<span class="op">=</span><span class="va">None</span>, src_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_encoding(<span class="va">self</span>.tgt_embedding(tgt) <span class="op">*</span> <span class="va">self</span>.scale)</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.decoder_layers:</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, encoder_output, tgt_mask, src_mask)</span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>        encoder_output <span class="op">=</span> <span class="va">self</span>.encode(src, src_mask)</span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>        decoder_output <span class="op">=</span> <span class="va">self</span>.decode(tgt, encoder_output, tgt_mask, src_mask)</span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.output_projection(decoder_output)</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_causal_mask(seq_len):</span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""创建因果mask，防止看到未来的token"""</span></span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.triu(torch.ones(seq_len, seq_len), diagonal<span class="op">=</span><span class="dv">1</span>).<span class="bu">bool</span>()</span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">~</span>mask.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="使用hugging-face-transformers" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="使用hugging-face-transformers"><span class="header-section-number">6.2</span> 使用Hugging Face Transformers</h3>
<p>对于实际应用，推荐使用经过优化的库：</p>
<div id="08dea928" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> T5ForConditionalGeneration, T5Tokenizer</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的T5模型（Encoder-Decoder Transformer）</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"t5-small"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained(model_name)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained(model_name)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 翻译任务示例</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"translate English to German: Hello, how are you?"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>translated <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Translation: </span><span class="sc">{</span>translated<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="复现论文的关键细节" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="复现论文的关键细节"><span class="header-section-number">6.3</span> 复现论文的关键细节</h3>
<p>原始Transformer论文中有一些容易忽略但至关重要的细节。</p>
<p>首先是Embedding缩放：输入embedding需要乘以<span class="math inline">\(\sqrt{d_{model}}\)</span>，因为embedding初始化通常较小，缩放后与位置编码的量级匹配。</p>
<p>其次是权重共享的可能性：原论文中，encoder和decoder的embedding可以共享；如果源语言和目标语言相同，output projection也可以与embedding共享，这样可以减少参数量。</p>
<p>Dropout的位置也有讲究：在attention分数计算后、残差加法前、以及embedding之后都要加dropout。</p>
<p>学习率策略使用了一个特殊的warmup schedule：</p>
<pre><code>lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))</code></pre>
<p>前warmup_steps步线性增加，之后按步数平方根衰减。这种设计对训练稳定性至关重要。</p>
<p>最后，论文使用了0.1的label smoothing，这有助于提高泛化性能。</p>
</section>
<section id="实验验证" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="实验验证"><span class="header-section-number">6.4</span> 实验验证</h3>
<p>验证实现是否正确有几个关键步骤。首先检查各模块输入输出维度是否正确。然后在几个样本上尝试过拟合——如果模型正确，应该能完美过拟合小数据集。接着可视化attention pattern，检查是否合理。最后与Hugging Face等权威实现对比中间结果。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">7</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨Transformer的理论基础、边界条件和开放问题</p>
</blockquote>
<section id="为什么有效理论视角" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">7.1</span> 为什么有效？——理论视角</h3>
<section id="表达能力分析" class="level4" data-number="7.1.1">
<h4 data-number="7.1.1" class="anchored" data-anchor-id="表达能力分析"><span class="header-section-number">7.1.1</span> 表达能力分析</h4>
<p>Transformer是通用函数逼近器吗？Yun et al.&nbsp;(2019) 给出了肯定的答案：在合适的条件下，Transformer可以以任意精度逼近任何连续的序列到序列函数。具体地，一个<span class="math inline">\(O(n)\)</span>层的Transformer可以逼近任何Lipschitz连续的permutation equivariant函数。</p>
<p>与RNN相比，RNN也是通用逼近器，但需要无限精度的隐状态。Transformer的优势在于可以用有限宽度实现相同的逼近能力。</p>
</section>
<section id="归纳偏置的视角" class="level4" data-number="7.1.2">
<h4 data-number="7.1.2" class="anchored" data-anchor-id="归纳偏置的视角"><span class="header-section-number">7.1.2</span> 归纳偏置的视角</h4>
<p>不同架构有不同的归纳偏置。RNN假设顺序处理和短程依赖，这符合语言直觉，但长距离依赖困难。CNN假设局部模式和层级结构，擅长捕获n-gram，但感受野有限。Transformer假设全局连接和位置无关，非常灵活且可并行，但需要学习位置关系，计算量也更大。</p>
<p>Transformer的归纳偏置可以说是”更弱”的——它不假设顺序或局部性，而是让模型自己学习这些模式。这意味着需要更多数据来学习显式的位置关系，但也更灵活，可以学到任意模式。当规模够大时，弱归纳偏置反而可能成为优势。</p>
</section>
<section id="与hopfield网络的联系" class="level4" data-number="7.1.3">
<h4 data-number="7.1.3" class="anchored" data-anchor-id="与hopfield网络的联系"><span class="header-section-number">7.1.3</span> 与Hopfield网络的联系</h4>
<p>Ramsauer et al.&nbsp;(2020) 发现了一个有趣的联系：Transformer的attention机制可以理解为现代Hopfield网络的一步更新。这提供了一个全新的视角：Hopfield网络是联想记忆模型，Attention可以看作从”记忆”（Key-Value对）中”检索”与Query相关的内容。这也解释了为什么FFN可能起到存储知识的作用——它就是Transformer的”记忆体”。</p>
</section>
</section>
<section id="为什么有效实证视角" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="为什么有效实证视角"><span class="header-section-number">7.2</span> 为什么有效？——实证视角</h3>
<section id="attention真的在学习有意义的模式吗" class="level4" data-number="7.2.1">
<h4 data-number="7.2.1" class="anchored" data-anchor-id="attention真的在学习有意义的模式吗"><span class="header-section-number">7.2.1</span> Attention真的在学习有意义的模式吗？</h4>
<p>通过可视化不同head的attention pattern，研究者发现确实如此。有些head学会了关注语法依赖，比如主谓关系、动宾关系。有些head学会了关注固定偏移，比如总是关注前一个词或后一个词。还有些head专门关注特殊token，如句号或[SEP]标记。</p>
<div id="fig-attention-visualization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-8/attention-visualization.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Attention可视化：句子”The cat sat on the mat”在不同层、不同head的注意力模式。浅层（Layer 1）倾向于关注相邻词（对角线模式），深层（Layer 3）展现更多语法结构模式（主谓、介宾关系）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Python生成的合成示意图，模式参考 Clark et al.&nbsp;(2019) “What Does BERT Look At?” 和 Voita et al.&nbsp;(2019) “Analyzing Multi-Head Self-Attention” 的实证发现</em></p>
</div>
</section>
<section id="哪些组件是必要的" class="level4" data-number="7.2.2">
<h4 data-number="7.2.2" class="anchored" data-anchor-id="哪些组件是必要的"><span class="header-section-number">7.2.2</span> 哪些组件是必要的？</h4>
<p>通过消融实验，研究者得出了明确的结论。减少Multi-Head的数量会导致性能下降，说明多头是重要的。移除FFN会导致大幅下降，说明它不可或缺。移除Residual会导致模型无法训练。移除LayerNorm会导致训练不稳定。移除Positional Encoding会导致输出几乎随机。</p>
<p>一个有趣的发现是，部分head可以被剪枝而不显著影响性能，说明模型存在一定的冗余。</p>
</section>
</section>
<section id="方法的边界条件" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">7.3</span> 方法的边界条件</h3>
<section id="隐含假设" class="level4" data-number="7.3.1">
<h4 data-number="7.3.1" class="anchored" data-anchor-id="隐含假设"><span class="header-section-number">7.3.1</span> 隐含假设</h4>
<p>Transformer隐含地做了几个假设。第一，位置信息可以通过加法注入——位置编码与内容编码相加，假设它们在同一语义空间可以混合。第二，全局交互是必要的——每个位置都与所有其他位置交互；如果任务只需要局部信息，这其实是一种浪费。第三，数据足够多——弱归纳偏置需要更多数据来补偿。</p>
</section>
<section id="失效条件" class="level4" data-number="7.3.2">
<h4 data-number="7.3.2" class="anchored" data-anchor-id="失效条件"><span class="header-section-number">7.3.2</span> 失效条件</h4>
<p>在某些情况下，Transformer表现并不好。当数据量太小时，带有更强归纳偏置的模型（如CNN、RNN）可能表现更好。当序列太长时，<span class="math inline">\(O(n^2)\)</span>的复杂度会成为瓶颈，超过几千token时需要特殊处理。对于需要精确位置推理的任务，如算术运算、数字排序，Transformer常常失败。对于强顺序依赖的任务，如解析嵌套结构，Transformer也可能不如专门设计的架构。</p>
</section>
<section id="已知的failure-modes" class="level4" data-number="7.3.3">
<h4 data-number="7.3.3" class="anchored" data-anchor-id="已知的failure-modes"><span class="header-section-number">7.3.3</span> 已知的Failure Modes</h4>
<p>研究文献中报告了几种典型的失败模式。长度泛化失败：在短序列上训练的Transformer难以泛化到长序列。位置编码外推困难：超出训练长度后，正弦编码的外推效果不佳。对重复模式敏感：输入中的重复可能导致注意力过度集中。</p>
</section>
</section>
<section id="变体与扩展" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="变体与扩展"><span class="header-section-number">7.4</span> 变体与扩展</h3>
<p>从架构角度看，Transformer衍生出了三大变体。Encoder-only的BERT用于理解任务。Decoder-only的GPT用于生成任务。Encoder-Decoder的T5和BART用于seq2seq任务。后续发展表明，Decoder-only架构在规模化后表现最好，这将在第16章详细讨论。</p>
<p>从注意力机制角度看，Sparse Attention只关注部分位置，Linear Attention将复杂度降到<span class="math inline">\(O(n)\)</span>，相对位置编码如RoPE和ALiBi解决了外推问题。这些改进将在第9章和第26章展开。</p>
</section>
<section id="开放研究问题" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">7.5</span> 开放研究问题</h3>
<p>如果你要在这个方向写一篇论文，可以从哪些问题切入呢？</p>
<p>第一个方向是数据效率：为什么Transformer需要这么多数据？如何设计更好的归纳偏置？数据效率能提升多少？</p>
<p>第二个方向是位置编码：最优设计是什么？正弦、可学习、相对位置…哪种最好？能否设计出完美外推的位置编码？</p>
<p>第三个方向是FFN的作用：它到底在做什么？是在存储知识吗？能否用其他结构替代？</p>
<p>第四个方向是可解释性：Attention权重真的代表”关注”吗？如何更好地解释Transformer的决策？</p>
<p>第五个方向是效率：能否在保持性能的同时降低复杂度？稀疏注意力的极限在哪里？</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">8</span> 局限性与未解决的问题</h2>
<section id="计算复杂度on2的诅咒" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="计算复杂度on2的诅咒"><span class="header-section-number">8.1</span> 计算复杂度：<span class="math inline">\(O(n^2)\)</span>的诅咒</h3>
<p>Transformer最大的局限是Self-Attention的二次复杂度。考虑处理一本10万词的书：</p>
<p><span class="math display">\[
\text{注意力矩阵大小} = 100000^2 = 10^{10} \text{ 元素}
\]</span></p>
<p>即使用FP16，也需要20GB显存仅用于存储注意力矩阵！这直接限制了长文档处理、长程对话、以及需要大上下文的任务如代码生成和法律分析。</p>
</section>
<section id="位置编码的外推问题" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="位置编码的外推问题"><span class="header-section-number">8.2</span> 位置编码的外推问题</h3>
<p>原始正弦位置编码在训练长度之外表现不佳。如果模型在512长度上训练，面对1024长度的输入时，位置512-1023的编码模型从未见过，性能会急剧下降。这限制了模型的实际应用灵活性。</p>
</section>
<section id="缺乏显式的层级结构" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="缺乏显式的层级结构"><span class="header-section-number">8.3</span> 缺乏显式的层级结构</h3>
<p>自然语言有天然的层级结构：词组成短语，短语组成句子，句子组成段落，段落组成文档。Transformer的flat attention没有显式建模这种层级，完全依赖模型自己学习。这可能是某些结构化任务上表现不佳的原因。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">8.4</span> 这些局限导向了什么？</h3>
<p>上述问题推动了后续研究。<span class="math inline">\(O(n^2)\)</span>复杂度的问题催生了第9章的高效注意力机制，包括Longformer、BigBird、Performer等。位置编码外推问题催生了第26章的RoPE、ALiBi等新型位置编码。长上下文需求催生了Flash Attention和KV Cache优化等系统级方案。</p>
<blockquote class="blockquote">
<p>下一章预告：第9章将深入探讨如何打破<span class="math inline">\(O(n^2)\)</span>的魔咒，介绍各种高效注意力机制及其背后的设计思想。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">9</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">9.1</span> 核心要点回顾</h3>
<p>这一章我们探讨了Transformer的完整设计。核心问题是如何构建一个并行高效、能捕获任意距离依赖的序列建模架构。核心洞察是完全抛弃循环结构，用注意力机制本身作为核心计算单元。</p>
<p>具体方法包括Scaled Dot-Product Attention提供高效稳定的注意力计算，Multi-Head Attention在多个子空间捕获多种模式，Positional Encoding注入位置信息，FFN配合Residual和LayerNorm保证表达能力和训练稳定性。</p>
<p>这项工作的意义是开创性的：它建立了完全基于注意力的架构，奠定了预训练大模型的基础。后续的GPT、BERT、LLaMA等划时代模型都是基于Transformer。</p>
</section>
<section id="关键公式速查" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">9.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 42%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>表达式</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Scaled Dot-Product Attention</td>
<td><span class="math inline">\(\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\)</span></td>
</tr>
<tr class="even">
<td>Multi-Head</td>
<td><span class="math inline">\(\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\)</span></td>
</tr>
<tr class="odd">
<td>Positional Encoding</td>
<td><span class="math inline">\(PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)\)</span>, <span class="math inline">\(PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\)</span></td>
</tr>
<tr class="even">
<td>FFN</td>
<td><span class="math inline">\(\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2\)</span></td>
</tr>
<tr class="odd">
<td>LayerNorm</td>
<td><span class="math inline">\(\text{LN}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta\)</span></td>
</tr>
<tr class="even">
<td>点积方差</td>
<td><span class="math inline">\(\text{Var}(q \cdot k) = d_k\)</span> （假设<span class="math inline">\(q_i, k_i \sim \mathcal{N}(0,1)\)</span>独立）</td>
</tr>
<tr class="odd">
<td>Softmax梯度</td>
<td><span class="math inline">\(\frac{\partial p_i}{\partial z_i} = p_i(1-p_i)\)</span>, <span class="math inline">\(\frac{\partial p_i}{\partial z_j} = -p_i p_j\)</span></td>
</tr>
<tr class="even">
<td>位置偏移变换</td>
<td><span class="math inline">\(PE_{pos+k} = M_k \cdot PE_{pos}\)</span>，其中<span class="math inline">\(M_k\)</span>是分块旋转矩阵</td>
</tr>
</tbody>
</table>
</section>
<section id="参数量与复杂度速查" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="参数量与复杂度速查"><span class="header-section-number">9.3</span> 参数量与复杂度速查</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>组件</th>
<th>参数量</th>
<th>时间复杂度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-Attention</td>
<td><span class="math inline">\(4d_{model}^2\)</span></td>
<td><span class="math inline">\(O(n^2d + nd^2)\)</span></td>
</tr>
<tr class="even">
<td>FFN</td>
<td><span class="math inline">\(8d_{model}^2\)</span> (当<span class="math inline">\(d_{ff}=4d\)</span>)</td>
<td><span class="math inline">\(O(nd^2)\)</span></td>
</tr>
<tr class="odd">
<td>单层Encoder</td>
<td><span class="math inline">\(12d_{model}^2\)</span></td>
<td><span class="math inline">\(O(n^2d + nd^2)\)</span></td>
</tr>
<tr class="even">
<td>Transformer Base (65M)</td>
<td><span class="math inline">\(d=512, L=6\)</span></td>
<td>—</td>
</tr>
<tr class="odd">
<td>Transformer Large (213M)</td>
<td><span class="math inline">\(d=1024, L=12\)</span></td>
<td>—</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="思考题"><span class="header-section-number">9.4</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么说Transformer的Self-Attention是”置换等变”的？这与RNN有什么本质区别？如果去掉位置编码，输入”我爱你”和”你爱我”会得到什么样的输出？</p></li>
<li><p><strong>[数学推导-基础]</strong> 证明：当<span class="math inline">\(q\)</span>和<span class="math inline">\(k\)</span>的分量独立同分布且方差为1时，<span class="math inline">\(q \cdot k\)</span>的方差等于<span class="math inline">\(d_k\)</span>。进一步，如果<span class="math inline">\(q_i, k_i \sim \mathcal{N}(0, \sigma^2)\)</span>，方差会是多少？</p></li>
<li><p><strong>[数学推导-进阶]</strong> 证明Multi-Head Attention的参数量与单头Attention相同。具体地，证明<span class="math inline">\(h\)</span>个<span class="math inline">\((d_{model} \times d_k)\)</span>投影矩阵的总参数量等于一个<span class="math inline">\((d_{model} \times d_{model})\)</span>矩阵，其中<span class="math inline">\(d_k = d_{model}/h\)</span>。</p></li>
<li><p><strong>[数学推导-挑战]</strong> 利用三角恒等式证明正弦位置编码的相对位置性质：存在只依赖于偏移<span class="math inline">\(k\)</span>的矩阵<span class="math inline">\(M_k\)</span>，使得<span class="math inline">\(PE_{pos+k} = M_k \cdot PE_{pos}\)</span>。写出<span class="math inline">\(M_k\)</span>在维度<span class="math inline">\((2i, 2i+1)\)</span>上的具体形式。</p></li>
<li><p><strong>[计算题]</strong> 对于一个Transformer模型（<span class="math inline">\(d_{model}=768\)</span>, <span class="math inline">\(d_{ff}=3072\)</span>, <span class="math inline">\(h=12\)</span>头, <span class="math inline">\(L=12\)</span>层），计算：(a) 单层的参数量；(b) 整个Encoder的参数量；(c) 序列长度<span class="math inline">\(n=512\)</span>时，单次前向传播Self-Attention的FLOPs。</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 实现一个简化版Transformer，在IWSLT14德英翻译数据集上训练，达到BLEU &gt; 25。记录训练过程中的loss曲线，并可视化某一层的attention pattern。</p></li>
<li><p><strong>[研究思考]</strong> 有人认为Transformer的成功主要归功于规模（更多参数、更多数据），而非架构本身。你如何看待这个观点？考虑以下证据：(a) 在小数据集上，CNN/RNN有时优于Transformer；(b) Vision Transformer在没有大量数据时表现不佳；(c) Scaling Laws表明性能与规模有幂律关系。</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">10</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">10.1</span> 核心论文（必读）</h3>
<p><strong>Vaswani et al.&nbsp;(2017). “Attention Is All You Need”</strong> 是这一切的起点。重点阅读Section 3的模型架构描述和Section 5.4关于为什么选择Self-Attention的讨论。实验细节可以快速浏览。</p>
</section>
<section id="理论基础" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">10.2</span> 理论基础</h3>
<p><strong>Yun et al.&nbsp;(2019). “Are Transformers universal approximators of sequence-to-sequence functions?”</strong> 证明了Transformer的通用逼近性质，为理解其表达能力提供了理论基础。</p>
<p><strong>Ramsauer et al.&nbsp;(2020). “Hopfield Networks is All You Need”</strong> 揭示了Attention与Hopfield网络的深层联系，提供了一个全新的理解视角。</p>
</section>
<section id="后续发展" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">10.3</span> 后续发展</h3>
<p><strong>Clark et al.&nbsp;(2019). “What Does BERT Look At?”</strong> 深入分析了BERT的attention pattern，帮助我们理解模型到底在关注什么。</p>
<p><strong>Michel et al.&nbsp;(2019). “Are Sixteen Heads Really Better than One?”</strong> 发现多数head可以被剪枝而不显著影响性能，揭示了模型的冗余性。</p>
</section>
<section id="代码资源" class="level3" data-number="10.4">
<h3 data-number="10.4" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">10.4</span> 代码资源</h3>
<p>官方实现在tensor2tensor仓库。PyTorch入门者推荐阅读The Annotated Transformer，它用详细注释的方式解读了完整实现。实际应用推荐使用Hugging Face的transformers库，它提供了经过优化的生产级实现。</p>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">11</span> 历史注脚</h2>
<p>“Attention Is All You Need”这篇论文最初在Google内部也引起了争议。当时RNN仍是主流，有人质疑完全抛弃循环结构是否明智。论文的标题也颇有挑衅意味——“注意力就是你所需要的全部”，仿佛在向整个RNN阵营宣战。</p>
<p>有趣的是，论文的八位作者后来分散到了不同的地方：有的创办了AI公司，有的加入了OpenAI，有的继续在Google做研究。这篇论文成为了整个AI时代的基石。</p>
<p>“Transformer”这个名字的灵感据说来自于变形金刚（Transformers）——既有”变换”的含义，又朗朗上口。另一种说法是它暗示了从RNN到全新架构的”变革”（transformation）。</p>
<p>截至2024年，这篇论文的引用次数超过10万次，是近年来引用最高的AI论文之一。几乎所有现代大语言模型——GPT-4、Claude、Gemini、LLaMA——都是基于Transformer架构。当年那个”激进”的设计决策，已经成为了整个领域的基础设施。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第8章：Transformer——注意力即一切"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Attention Is All You Need"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-22"</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, Transformer, Attention]</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [Multi-Head Attention, FFN, 残差连接, Layer Normalization, Encoder-Decoder, Scaled Dot-Product]</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Attention Is All You Need：Transformer如何用纯注意力替代循环结构，Multi-Head、FFN、残差连接的设计智慧，以及这个架构为何改变了一切。"</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/transformer-banner.png"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：能否用纯注意力机制替代循环结构，构建一个并行高效、能捕获任意距离依赖的序列建模架构？</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2017年 </span><span class="pp">|</span><span class="at"> Vaswani et al. "Attention Is All You Need" </span><span class="pp">|</span><span class="at"> Google Brain/Google Research</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>上一章我们见证了Self-Attention的诞生。这是一个概念性的突破：序列中的每个位置可以直接关注其他任意位置，不再需要通过RNN的逐步传递来建立长距离依赖。Self-Attention彻底打破了循环网络的顺序枷锁，让任意两个位置可以一步到位地直接交互。这不仅解决了长距离依赖的问题，还带来了一个意想不到的副产品：既然不同位置之间的注意力计算彼此独立，它们就可以完全并行——GPU终于能发挥它真正的实力了。更妙的是，注意力权重本身就是可解释的，你可以直接看到模型在处理某个词时"关注"了哪些其他词，这在黑箱横行的深度学习时代尤为珍贵。</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>然而，Self-Attention仍然面临几个关键问题。首先是位置信息的缺失：纯Self-Attention是置换不变的（permutation invariant），打乱输入顺序，输出也只是相应打乱，模型完全不知道"谁在谁前面"。其次，之前的Self-Attention通常只是作为RNN的辅助模块，而非独立架构。最后，如何堆叠多层？如何处理编码器-解码器结构？如何保证训练稳定？这些架构设计问题都还悬而未决。</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>2017年，Google的研究团队在论文"Attention Is All You Need"中给出了一个大胆的回答。</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：完全抛弃循环结构，用精心设计的注意力模块（Scaled Dot-Product Attention + Multi-Head）配合位置编码、残差连接、层归一化，构建一个完整的序列到序列架构——Transformer。</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>这个决定在当时看来相当激进。RNN已经统治序列建模领域多年，放弃它意味着放弃一种直觉上合理的归纳偏置——毕竟，顺序处理符合人类阅读的方式。但实验结果令人震惊：Transformer不仅在机器翻译任务上大幅超越了当时最好的RNN模型，训练速度还快了一个数量级。</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>这一章，我们将深入理解Transformer的每个设计决策：为什么要这样做？还有什么其他选择？这些选择带来了什么后果？</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的精确定义</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>我们要解决的核心问题是序列到序列建模（Sequence-to-Sequence）：给定输入序列 $\mathbf{x} = (x_1, x_2, \ldots, x_n)$，生成输出序列 $\mathbf{y} = (y_1, y_2, \ldots, y_m)$。这个问题为什么重要？因为几乎所有的自然语言处理任务都可以转化为某种形式的序列到序列问题。机器翻译是把英文句子变成中文句子，文本摘要是把长文档变成短摘要，对话系统是把用户输入变成系统回复，代码生成是把自然语言描述变成代码。找到一个通用的序列建模架构，就等于找到了NLP的"万能钥匙"。</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="fu">### RNN-based Seq2Seq的局限</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>在Transformer之前，主流的Seq2Seq架构是基于RNN的Encoder-Decoder加上Attention机制（详见第4-6章）。</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a><span class="al">![RNN-based Seq2Seq with Attention](figures/chapter-8/fig-rnn-seq2seq.png)</span>{#fig-rnn-seq2seq}</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>*自绘示意图，基于 Bahdanau et al. (2015) 和 Sutskever et al. (2014) 的 Seq2Seq + Attention 架构*</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>回顾前几章的讨论，这个架构有三个根本性的问题：**顺序计算瓶颈**（第4章）——RNN必须串行计算 $h_t = f(h_{t-1}, x_t)$，GPU无法并行；**长距离依赖衰减**（第4章）——即使有LSTM/GRU的门控缓解，信息仍需逐步传递，100步的非线性变换不可避免地带来损失；**Attention受限于RNN骨架**（第5-6章）——Attention只是RNN之间的桥梁，Encoder和Decoder内部仍然是顺序处理。</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的架构？</span></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>从上述分析可以看出，理想的序列建模架构应该满足几个关键需求。首先是并行计算能力，这样才能充分利用GPU加速训练。其次是直接的长距离连接，让任意两个位置可以一步到位地交互。同时必须保留位置信息，因为序列顺序对语言理解至关重要。当然，表达能力要足够强，至少能匹配或超越RNN。最后，训练要稳定，深层网络要能收敛。</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>Transformer的设计正是为了同时满足这些需求。</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### 革命性洞察：注意力本身就是计算</span></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>传统观点把注意力看作一种选择机制——从一堆信息中挑选重要的部分。Transformer的洞察更加深刻：注意力可以是计算本身，不只是选择信息，而是通过加权聚合来生成新的表示。</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>让我用一个图书馆的类比来解释这个区别。RNN的方式就像你按顺序阅读书架上的每本书，一本接一本，用笔记记录累积的理解。读到第100本书时，你对第1本的记忆已经模糊了。Transformer的方式则完全不同：你同时把所有书摊开在桌上，对于每个问题（Query），快速扫视所有书（Key），找出相关的（高注意力权重），然后综合这些相关内容（Value的加权和）得到答案。这种方式不仅更高效，而且不会遗忘。</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心架构概览</span></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>Transformer由两个主要部分组成：Encoder负责理解输入序列，Decoder负责生成输出序列。</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a><span class="al">![The Transformer — model architecture.](figures/chapter-8/original/fig1-transformer-architecture.png)</span>{#fig-transformer-overview width=70%}</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>*Source: Vaswani et al. (2017) "Attention Is All You Need", Figure 1*</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>整个架构由五种核心组件构成。Scaled Dot-Product Attention负责高效地计算注意力。Multi-Head Attention让多个注意力"头"关注不同的子空间。Positional Encoding注入位置信息。Feed-Forward Network提供逐位置的非线性变换。Residual Connection和Layer Norm则保证深层训练的稳定性。</span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a><span class="fu">### 设计动机：为什么这些组件？</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>每个组件都是为了解决特定问题而存在的。Scaled Dot-Product用点积计算注意力，比加性注意力更高效，因为可以直接利用矩阵乘法的硬件优化。缩放因子$\sqrt{d_k}$防止softmax饱和导致的梯度消失。Multi-Head让模型能同时捕获多种不同类型的关系模式。Positional Encoding弥补了纯注意力机制丢失的位置信息。FFN提供必要的非线性变换能力，因为单纯的注意力操作本质上是线性的。Residual和LayerNorm则是深层网络训练的标配，没有它们模型根本无法收敛。</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scaled Dot-Product Attention</span></span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 基本公式</span></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>上一章我们建立了Self-Attention的Q-K-V框架和基本计算流程（第7章）。现在让我们看Transformer论文如何将其形式化为一个高效的标准化计算模块。给定Query矩阵 $Q \in \mathbb{R}^{n \times d_k}$，Key矩阵 $K \in \mathbb{R}^{m \times d_k}$，Value矩阵 $V \in \mathbb{R}^{m \times d_v}$，Scaled Dot-Product Attention的计算公式是：</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>这里$n$是Query的数量，$m$是Key/Value的数量，$d_k$是Key的维度，$d_v$是Value的维度。</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Scaled Dot-Product Attention (Vaswani et al., 2017)</span></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>**输入**：</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Query 矩阵 $Q \in \mathbb{R}^{n \times d_k}$</span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Key 矩阵 $K \in \mathbb{R}^{m \times d_k}$</span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Value 矩阵 $V \in \mathbb{R}^{m \times d_v}$</span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>可选的 Mask 矩阵 $M \in \mathbb{R}^{n \times m}$</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>**输出**：注意力输出 $\text{Output} \in \mathbb{R}^{n \times d_v}$</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a><span class="in">1. 计算注意力分数（点积）:</span></span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a><span class="in">   S ← Q × K^T                    # S ∈ ℝ^(n × m)</span></span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a><span class="in">2. 缩放（防止 softmax 饱和）:</span></span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a><span class="in">   S ← S / √d_k</span></span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a><span class="in">3. 应用 Mask（可选，用于因果注意力）:</span></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a><span class="in">   if Mask is provided:</span></span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a><span class="in">       S ← S + M                  # M 中非法位置为 -∞</span></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a><span class="in">4. Softmax 归一化（按行）:</span></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a><span class="in">   A ← softmax(S, dim=-1)         # A ∈ ℝ^(n × m), 每行和为 1</span></span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a><span class="in">5. 加权聚合:</span></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a><span class="in">   Output ← A × V                 # Output ∈ ℝ^(n × d_v)</span></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a><span class="in">6. return Output, A               # 返回输出和注意力权重</span></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>**关键设计**：</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>缩放因子 $\sqrt{d_k}$ 确保点积方差为 1，避免 softmax 梯度消失</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mask 中的 $-\infty$ 经 softmax 变为 0，实现"不可见"效果</span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>时间复杂度：$O(n \cdot m \cdot d_k + n \cdot m \cdot d_v) = O(n \cdot m \cdot d)$</span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>空间复杂度：$O(n \cdot m)$（注意力矩阵）</span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a>*Source: Vaswani, A. et al. (2017). "Attention Is All You Need". NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*</span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a>整个计算流程可以用下图表示：</span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a><span class="al">![Scaled Dot-Product Attention](figures/chapter-8/original/fig2-scaled-attention.png)</span>{#fig-scaled-dot-product width=35%}</span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>*Source: Vaswani et al. (2017) "Attention Is All You Need", Figure 2 (left)*</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 为什么要除以 $\sqrt{d_k}$？</span></span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>这是Transformer论文中最精妙的细节之一，值得我们从数学上仔细理解。</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>问题的根源在于：当$d_k$较大时，点积$q \cdot k$的方差会变大。假设$q$和$k$的每个分量都是独立同分布的，均值为0，方差为1。那么点积的方差是多少呢？</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>\text{Var}(q \cdot k) = \text{Var}\left(\sum_{i=1}^{d_k} q_i k_i\right) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a>当$d_k = 512$时，点积的标准差约为$\sqrt{512} \approx 22.6$。这意味着点积值可能非常大或非常小。</span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a>这会带来什么后果？如果某个点积远大于其他点积，softmax输出会接近one-hot分布，梯度会趋近于零（因为softmax在饱和区的梯度很小），学习就会停滞。</span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>解决方案很优雅：除以$\sqrt{d_k}$，将方差重新标准化为1。</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(\frac{q \cdot k}{\sqrt{d_k}}\right) = \frac{d_k}{d_k} = 1</span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a>这保证了无论$d_k$多大，注意力分数的分布都保持稳定。在$d_k = 64$的典型设置下，不缩放时点积标准差约为8，softmax容易饱和；缩放后标准差为1，softmax梯度稳定。</span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mask的作用</span></span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>在解码器的Self-Attention中，我们需要因果遮罩（Causal Mask）来保证模型只能看到过去，不能看到未来。Mask矩阵的定义是：当$i \geq j$时为0，当$i &lt; j$时为$-\infty$。加上Mask后：</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{Mask}\right)V</span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a>$-\infty$经过softmax变成0，实现了"看不到未来"的效果。这对于自回归生成至关重要——在生成第$t$个词时，模型只能依赖已经生成的前$t-1$个词。</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：从Embedding到Attention输出</span></span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>抽象的公式有时让人难以建立直觉。让我们用一个极简的例子，手把手走完从词嵌入到注意力输出的全过程。</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a>**设定**：句子 "I love NLP"，共3个词。为了便于手算，我们设 $d_{model} = 4$（实际模型用512或更大）。</span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a>**Step 1: 词嵌入（Word Embedding）**</span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a>假设我们的词嵌入矩阵已经训练好，查表得到每个词的向量：</span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a>\text{I} &amp;\rightarrow <span class="co">[</span><span class="ot">1.0, 0.0, 1.0, 0.0</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a>\text{love} &amp;\rightarrow <span class="co">[</span><span class="ot">0.0, 1.0, 0.5, 0.5</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a>\text{NLP} &amp;\rightarrow <span class="co">[</span><span class="ot">1.0, 1.0, 0.0, 0.0</span><span class="co">]</span></span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a>写成矩阵形式 $E \in \mathbb{R}^{3 \times 4}$：</span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a>E = \begin{bmatrix}</span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a>1.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 <span class="sc">\\</span></span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a>0.0 &amp; 1.0 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span></span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a>1.0 &amp; 1.0 &amp; 0.0 &amp; 0.0</span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a>**Step 2: 位置编码（Positional Encoding）**</span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a>使用正弦位置编码公式。对于 $d_{model} = 4$，我们有两对 sin/cos（维度0-1用频率 $\omega_0$，维度2-3用频率 $\omega_1$）：</span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a>\omega_0 = \frac{1}{10000^{0/4}} = 1, \quad \omega_1 = \frac{1}{10000^{2/4}} = 0.01</span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a>计算每个位置的编码：</span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a>PE_0 &amp;= <span class="co">[</span><span class="ot">\sin(0), \cos(0), \sin(0), \cos(0)</span><span class="co">]</span> = <span class="co">[</span><span class="ot">0, 1, 0, 1</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a>PE_1 &amp;= <span class="co">[</span><span class="ot">\sin(1), \cos(1), \sin(0.01), \cos(0.01)</span><span class="co">]</span> \approx <span class="co">[</span><span class="ot">0.84, 0.54, 0.01, 1.00</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a>PE_2 &amp;= <span class="co">[</span><span class="ot">\sin(2), \cos(2), \sin(0.02), \cos(0.02)</span><span class="co">]</span> \approx <span class="co">[</span><span class="ot">0.91, -0.42, 0.02, 1.00</span><span class="co">]</span></span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a>位置编码矩阵 $PE \in \mathbb{R}^{3 \times 4}$：</span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a>PE = \begin{bmatrix}</span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a>0.00 &amp; 1.00 &amp; 0.00 &amp; 1.00 <span class="sc">\\</span></span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a>0.84 &amp; 0.54 &amp; 0.01 &amp; 1.00 <span class="sc">\\</span></span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a>0.91 &amp; -0.42 &amp; 0.02 &amp; 1.00</span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a>**Step 3: 输入表示 = 词嵌入 + 位置编码**</span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a>X = E + PE = \begin{bmatrix}</span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 1.00 &amp; 1.00 &amp; 1.00 <span class="sc">\\</span></span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a>0.84 &amp; 1.54 &amp; 0.51 &amp; 1.50 <span class="sc">\\</span></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a>1.91 &amp; 0.58 &amp; 0.02 &amp; 1.00</span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a>现在每个词的表示既包含语义信息（词嵌入），也包含位置信息（位置编码）。</span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a>**Step 4: 线性投影得到 Q, K, V**</span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a>为简化演示，假设投影矩阵 $W^Q, W^K, W^V$ 都是单位矩阵（实际中是可学习参数）：</span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a>Q = XW^Q = X, \quad K = XW^K = X, \quad V = XW^V = X</span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a>在 Self-Attention 中，Q、K、V 都来自同一个输入 $X$。</span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a>**Step 5: 计算注意力分数 $QK^\top$**</span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a>这一步计算每对位置之间的"相似度"：</span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a>QK^\top = XX^\top = \begin{bmatrix}</span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 1.00 &amp; 1.00 &amp; 1.00 <span class="sc">\\</span></span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a>0.84 &amp; 1.54 &amp; 0.51 &amp; 1.50 <span class="sc">\\</span></span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a>1.91 &amp; 0.58 &amp; 0.02 &amp; 1.00</span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 0.84 &amp; 1.91 <span class="sc">\\</span></span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 1.54 &amp; 0.58 <span class="sc">\\</span></span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 0.51 &amp; 0.02 <span class="sc">\\</span></span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a>1.00 &amp; 1.50 &amp; 1.00</span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a>逐元素计算（以第(1,2)个元素为例）：</span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a>(QK^\top)_{12} = 1.0 \times 0.84 + 1.0 \times 1.54 + 1.0 \times 0.51 + 1.0 \times 1.50 = 4.39</span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a>完整的注意力分数矩阵：</span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a>QK^\top = \begin{bmatrix}</span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a>4.00 &amp; 4.39 &amp; 3.51 <span class="sc">\\</span></span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a>4.39 &amp; 5.06 &amp; 3.64 <span class="sc">\\</span></span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a>3.51 &amp; 3.64 &amp; 4.70</span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a>观察：对角线元素（自己与自己的相似度）通常较高，但不一定最高。</span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a>**Step 6: 缩放（除以 $\sqrt{d_k}$）**</span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a>$d_k = 4$，所以 $\sqrt{d_k} = 2$：</span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a>\frac{QK^\top}{\sqrt{d_k}} = \begin{bmatrix}</span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a>2.00 &amp; 2.20 &amp; 1.76 <span class="sc">\\</span></span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a>2.20 &amp; 2.53 &amp; 1.82 <span class="sc">\\</span></span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a>1.76 &amp; 1.82 &amp; 2.35</span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a>缩放使得数值更加温和，避免 softmax 饱和。</span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a>**Step 7: Softmax（按行归一化）**</span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a>对每一行应用 softmax，得到注意力权重 $A$：</span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a>\text{softmax}(<span class="co">[</span><span class="ot">2.00, 2.20, 1.76</span><span class="co">]</span>) = \frac{<span class="co">[</span><span class="ot">e^{2.00}, e^{2.20}, e^{1.76}</span><span class="co">]</span>}{\sum} = \frac{<span class="co">[</span><span class="ot">7.39, 9.03, 5.81</span><span class="co">]</span>}{22.23} = <span class="co">[</span><span class="ot">0.33, 0.41, 0.26</span><span class="co">]</span></span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a>完整的注意力权重矩阵：</span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a>A = \begin{bmatrix}</span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a>0.33 &amp; 0.41 &amp; 0.26 <span class="sc">\\</span></span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a>0.30 &amp; 0.42 &amp; 0.28 <span class="sc">\\</span></span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a>0.27 &amp; 0.29 &amp; 0.44</span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a>**解读注意力权重**：</span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>第1行：处理 "I" 时，模型最关注 "love"（0.41），其次是自己（0.33），最后是 "NLP"（0.26）</span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>第2行：处理 "love" 时，也最关注自己（0.42），但同时也在看 "I" 和 "NLP"</span>
<span id="cb6-340"><a href="#cb6-340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>第3行：处理 "NLP" 时，最关注自己（0.44），这符合直觉——专有名词更依赖自身语义</span>
<span id="cb6-341"><a href="#cb6-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a>**Step 8: 加权求和得到输出**</span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a>最终输出是 Value 向量的加权和：</span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a>\text{Output} = AV = A \cdot X</span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a>以第一行（"I" 的新表示）为例：</span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a>\text{Output}_1 = 0.33 \times <span class="co">[</span><span class="ot">1.00, 1.00, 1.00, 1.00</span><span class="co">]</span> + 0.41 \times <span class="co">[</span><span class="ot">0.84, 1.54, 0.51, 1.50</span><span class="co">]</span> + 0.26 \times <span class="co">[</span><span class="ot">1.91, 0.58, 0.02, 1.00</span><span class="co">]</span></span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.33, 0.33, 0.33, 0.33</span><span class="co">]</span> + <span class="co">[</span><span class="ot">0.34, 0.63, 0.21, 0.62</span><span class="co">]</span> + <span class="co">[</span><span class="ot">0.50, 0.15, 0.01, 0.26</span><span class="co">]</span> = <span class="co">[</span><span class="ot">1.17, 1.11, 0.55, 1.21</span><span class="co">]</span></span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a>完整输出矩阵：</span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a>\text{Output} = \begin{bmatrix}</span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a>1.17 &amp; 1.11 &amp; 0.55 &amp; 1.21 <span class="sc">\\</span></span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a>1.15 &amp; 1.15 &amp; 0.54 &amp; 1.22 <span class="sc">\\</span></span>
<span id="cb6-366"><a href="#cb6-366" aria-hidden="true" tabindex="-1"></a>1.31 &amp; 0.99 &amp; 0.44 &amp; 1.13</span>
<span id="cb6-367"><a href="#cb6-367" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a>**关键洞察**：</span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a>注意力机制做了什么？比较输入 $X$ 和输出：</span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**信息融合**：每个词的输出不再只是自己的表示，而是融合了其他词的信息。"I" 的新表示包含了 "love" 和 "NLP" 的成分。</span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**上下文感知**：同一个词 "I" 在不同句子中会有不同的输出，因为它会融合不同的上下文。</span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**软选择**：通过 softmax 权重实现"软"的信息选择，而非硬性地只看某一个词。</span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a>这就是 Self-Attention 的核心：让每个词都能"看到"整个句子，并根据相关性选择性地融合信息。</span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a><span class="fu">## 实际模型的规模</span></span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a>上面的例子用 $d_{model} = 4$ 是为了手算方便。实际的 Transformer Base 用 $d_{model} = 512$，序列长度可达数百甚至数千。注意力矩阵的大小是 $n \times n$，这就是 $O(n^2)$ 复杂度的来源。</span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Head Attention</span></span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 动机：为什么需要多头？</span></span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a>单头注意力有一个根本性的局限：每个位置只能有一种"关注模式"。但在自然语言中，一个词可能同时需要关注多种不同类型的信息。比如动词需要关注它的主语（语法关系），代词需要关注它的先行词（指代关系），同义词之间也需要相互关注（语义关系）。单头难以同时捕获这些不同类型的依赖，这就是Multi-Head的动机。</span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数学形式</span></span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a>Multi-Head Attention的核心思想是将$d_{model}$维的Q、K、V分别投影到$h$个子空间，在每个子空间独立计算注意力，最后拼接起来：</span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a>\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O</span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a>其中每个head的计算是：</span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a>\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a>参数矩阵的维度设计很讲究：$W_i^Q$和$W_i^K$都是$d_{model} \times d_k$，$W_i^V$是$d_{model} \times d_v$，输出投影$W^O$是$hd_v \times d_{model}$。通常设置$d_k = d_v = d_{model}/h$，这样保证了总参数量不变。</span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Multi-Head Attention (Vaswani et al., 2017)</span></span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a>**输入**：</span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Query 矩阵 $Q \in \mathbb{R}^{n \times d_{model}}$</span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Key 矩阵 $K \in \mathbb{R}^{m \times d_{model}}$</span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Value 矩阵 $V \in \mathbb{R}^{m \times d_{model}}$</span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>头数 $h$，每头维度 $d_k = d_v = d_{model} / h$</span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a>**参数**：</span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>投影矩阵 $W_i^Q, W_i^K \in \mathbb{R}^{d_{model} \times d_k}$，$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$，$i = 1, \ldots, h$</span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输出投影 $W^O \in \mathbb{R}^{h \cdot d_v \times d_{model}}$</span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a>**输出**：$\text{MultiHead}(Q, K, V) \in \mathbb{R}^{n \times d_{model}}$</span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a><span class="in">1. 并行计算 h 个注意力头:</span></span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a><span class="in">   for i = 1 to h (in parallel):</span></span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a><span class="in">       Q_i ← Q × W_i^Q              # 投影到第 i 个子空间</span></span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a><span class="in">       K_i ← K × W_i^K</span></span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a><span class="in">       V_i ← V × W_i^V</span></span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a><span class="in">       head_i ← Attention(Q_i, K_i, V_i, mask)   # Scaled Dot-Product</span></span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a><span class="in">2. 拼接所有头的输出:</span></span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a><span class="in">   Concat ← [head_1; head_2; ...; head_h]    # Concat ∈ ℝ^(n × h·d_v)</span></span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-434"><a href="#cb6-434" aria-hidden="true" tabindex="-1"></a><span class="in">3. 最终线性投影:</span></span>
<span id="cb6-435"><a href="#cb6-435" aria-hidden="true" tabindex="-1"></a><span class="in">   Output ← Concat × W^O            # Output ∈ ℝ^(n × d_model)</span></span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a><span class="in">4. return Output</span></span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a>**设计动机**：</span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>不同的 head 可以关注**不同类型的关系**（语法、语义、位置等）</span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>投影到低维子空间 ($d_k = d_{model}/h$) 使计算可行</span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>总参数量 = $4 \times d_{model}^2$，与单头 Attention 相同</span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a>**典型配置**（Transformer Base）：</span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$d_{model} = 512$, $h = 8$, $d_k = d_v = 64$</span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a>*Source: Vaswani, A. et al. (2017). "Attention Is All You Need". NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*</span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a><span class="al">![Multi-Head Attention consists of several attention layers running in parallel.](figures/chapter-8/original/fig2-multi-head.png)</span>{#fig-multi-head width=50%}</span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a>*Source: Vaswani et al. (2017) "Attention Is All You Need", Figure 2 (right)*</span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Head数量的选择</span></span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a>Transformer base使用$h=8$，large使用$h=16$。选择的考量涉及一个权衡：头太少，表达能力不足；头太多，每个head的维度$d_k = d_{model}/h$就太小，信息容量不足。经验法则是$d_k \geq 64$通常是下限。</span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a>后续研究发现不同head确实学到了不同的模式。有的head关注局部（相邻词），有的关注全局（长距离依赖）；有的关注语法结构，有的关注语义相似性。这印证了Multi-Head设计的合理性。</span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a><span class="fu">### 位置编码在Transformer中的角色</span></span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a>第7章已经详细讨论了位置编码的动机（Self-Attention的置换等变性）、正弦编码的数学原理与实现、以及可学习编码等替代方案。这里我们聚焦于位置编码**在Transformer架构中的具体集成方式**。</span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a>Transformer采用了正弦位置编码，通过**加法注入**的方式与词嵌入结合：输入表示 = 词嵌入 + 位置编码。这意味着语义信息和位置信息在同一个$d_{model}$维空间中混合。一个值得注意的工程细节是，词嵌入在相加前需要乘以$\sqrt{d_{model}}$进行缩放——因为embedding层初始化的值通常很小，直接与PE相加会导致位置信号淹没语义信号。</span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a><span class="al">![正弦位置编码可视化：横轴为位置（0-100），纵轴为编码维度（0-128）。低维度区域（顶部）呈现高频振荡，高维度区域（底部）呈现低频变化。这种多尺度频率模式使得模型可以同时感知局部和全局位置关系。](figures/chapter-8/positional-encoding.png)</span>{#fig-positional-encoding}</span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a>*Python生成，基于 Vaswani et al. (2017) 的正弦位置编码公式。[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*</span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a>Transformer原论文比较了正弦编码和可学习编码，发现两者效果相当。但后续发展表明，位置编码的选择对模型能力有深远影响——RoPE和ALiBi等新型方案（见第26章）在长度外推方面表现更优。本章"深入理解"一节提供了正弦编码相对位置性质的完整数学证明。</span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feed-Forward Network</span></span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a>每个Transformer层都包含一个逐位置的前馈网络，它的形式很简单：两层线性变换夹一个ReLU激活：</span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a>\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2</span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a>其中$W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$，$W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$，通常$d_{ff} = 4 \times d_{model}$。</span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a>FFN是一个被低估的组件。直觉上，Self-Attention负责"信息交流"——决定收集哪些信息，进行不同位置间的加权聚合；FFN负责"信息处理"——决定如何处理信息，对每个位置的表示做非线性变换。可以把这种分工类比为：Attention是"开会讨论"，FFN是"个人思考"。</span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a>一个令人惊讶的事实是，在标准Transformer中，FFN的参数量占模型总参数的约三分之二。FFN参数量是$2 \times d_{model} \times d_{ff} = 8d_{model}^2$，而Attention参数量只有$4 \times d_{model}^2$。后续研究发现FFN可能起到"记忆"的作用——存储知识，不同层的FFN关注不同类型的信息。稀疏化FFN（如Mixture of Experts）成为了扩展模型规模的重要方向。</span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a><span class="fu">### 残差连接与Layer Normalization</span></span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 残差连接</span></span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a>每个子层（Attention或FFN）都被残差连接包裹。残差连接解决的是梯度流动问题：它提供了一条"高速公路"让梯度直接传回前层，使得深层网络训练成为可能。Transformer base有6层，large有12层；后来的模型到96层甚至更多，没有残差连接这是不可想象的。</span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Layer Normalization</span></span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a>与Batch Normalization不同，Layer Norm在特征维度而非batch维度上归一化：</span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-502"><a href="#cb6-502" aria-hidden="true" tabindex="-1"></a>\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta</span>
<span id="cb6-503"><a href="#cb6-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a>其中$\mu$和$\sigma$是$x$在特征维度上的均值和标准差。</span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a>为什么用LayerNorm而非BatchNorm？原因有三。第一，序列长度不固定，不同序列长度不同，batch统计不稳定。第二，LayerNorm不依赖batch统计，训练和推理行为一致。第三，每个样本独立归一化，对并行更友好。</span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pre-Norm vs Post-Norm</span></span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a>原始Transformer使用Post-Norm，先子层后归一化：$x_{l+1} = \text{LayerNorm}(x_l + \text{Sublayer}(x_l))$。后来的模型多采用Pre-Norm，先归一化后子层：$x_{l+1} = x_l + \text{Sublayer}(\text{LayerNorm}(x_l))$。</span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a><span class="al">![Pre-Norm vs Post-Norm：左侧为原始Transformer的Post-Norm结构，右侧为后续模型广泛采用的Pre-Norm结构。](figures/chapter-8/fig-pre-post-norm.png)</span>{#fig-pre-post-norm}</span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a>*自绘示意图，基于 Xiong et al. (2020) "On Layer Normalization in the Transformer Architecture". [arXiv:2002.04745](https://arxiv.org/abs/2002.04745)*</span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a>Pre-Norm的优势在于更稳定的训练，尤其是深层模型，甚至不需要warmup也能收敛。GPT系列、LLaMA等都采用了Pre-Norm。</span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a><span class="fu">### Encoder与Decoder的结构差异</span></span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a>Transformer的Encoder和Decoder有三个关键区别。</span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-525"><a href="#cb6-525" aria-hidden="true" tabindex="-1"></a>第一个区别是Self-Attention的类型。Encoder使用双向Self-Attention，每个位置可以看到整个输入序列：</span>
<span id="cb6-526"><a href="#cb6-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a>\text{Attention}_{ij} = \frac{\exp(q_i \cdot k_j / \sqrt{d_k})}{\sum_l \exp(q_i \cdot k_l / \sqrt{d_k})}</span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a>没有Mask，第$i$个位置可以关注任意位置$j$。而Decoder使用因果Self-Attention，第$i$个位置只能关注位置$1, 2, \ldots, i$：</span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a>\text{Attention}_{ij} = \begin{cases}</span>
<span id="cb6-535"><a href="#cb6-535" aria-hidden="true" tabindex="-1"></a>\frac{\exp(q_i \cdot k_j / \sqrt{d_k})}{\sum_{l \leq i} \exp(q_i \cdot k_l / \sqrt{d_k})} &amp; j \leq i <span class="sc">\\</span></span>
<span id="cb6-536"><a href="#cb6-536" aria-hidden="true" tabindex="-1"></a>0 &amp; j &gt; i</span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a>这保证了生成时只用到已生成的token，符合自回归的要求。</span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a>第二个区别是Decoder有一个额外的Cross-Attention层，通过它连接到Encoder。Query来自Decoder当前层的输出，Key和Value来自Encoder最后一层的输出。这使得生成每个词时都可以参考整个输入序列。</span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a><span class="al">![Cross-Attention连接Decoder与Encoder：Query来自Decoder，Key和Value来自Encoder的最终输出。](figures/chapter-8/fig-cross-attention.png)</span>{#fig-cross-attention}</span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a>*自绘示意图，基于 Vaswani et al. (2017) "Attention Is All You Need", Figure 1. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*</span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a>第三个区别是用途：Encoder负责理解输入序列，产生上下文表示；Decoder负责基于这些表示生成输出序列。</span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度分析</span></span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a>设序列长度为$n$，模型维度为$d$。Self-Attention的时间复杂度是$O(n^2 d)$，瓶颈在于$n^2$的注意力矩阵计算。FFN的时间复杂度是$O(n d^2)$，瓶颈在于$d^2$的线性变换。总计是$O(n^2 d + n d^2)$，当序列很长时$n^2$项主导。</span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a>与RNN的$O(n d^2)$相比，Transformer在$n &lt; d$时更快，$n &gt; d$时更慢。但关键区别在于并行性：RNN需要$O(n)$个顺序步骤，完全无法并行；Transformer只需要$O(1)$个顺序步骤（只是层数），计算完全并行。这是Transformer训练速度远超RNN的根本原因。</span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-558"><a href="#cb6-558" aria-hidden="true" tabindex="-1"></a>空间复杂度方面，最大的开销是注意力矩阵，需要$O(n^2)$的空间。对于长序列，比如处理整本书，这会成为严重问题。这为第9章的高效注意力研究埋下了伏笔。</span>
<span id="cb6-559"><a href="#cb6-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-560"><a href="#cb6-560" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-561"><a href="#cb6-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-562"><a href="#cb6-562" aria-hidden="true" tabindex="-1"></a><span class="fu">## 数学推导深入</span></span>
<span id="cb6-563"><a href="#cb6-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-564"><a href="#cb6-564" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者视角**：这一节提供更详细的数学推导，帮助深入理解Transformer的计算细节</span></span>
<span id="cb6-565"><a href="#cb6-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-566"><a href="#cb6-566" aria-hidden="true" tabindex="-1"></a><span class="fu">### 矩阵维度的逐步追踪</span></span>
<span id="cb6-567"><a href="#cb6-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-568"><a href="#cb6-568" aria-hidden="true" tabindex="-1"></a>理解Transformer的一个关键是追踪每一步的矩阵维度。让我们以单个Self-Attention层为例，假设输入序列长度为$n$，模型维度为$d_{model}$，使用$h$个注意力头，每个头的维度为$d_k = d_v = d_{model}/h$。</span>
<span id="cb6-569"><a href="#cb6-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-570"><a href="#cb6-570" aria-hidden="true" tabindex="-1"></a>**输入**：$X \in \mathbb{R}^{n \times d_{model}}$</span>
<span id="cb6-571"><a href="#cb6-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-572"><a href="#cb6-572" aria-hidden="true" tabindex="-1"></a>**Step 1：线性投影**</span>
<span id="cb6-573"><a href="#cb6-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-574"><a href="#cb6-574" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-575"><a href="#cb6-575" aria-hidden="true" tabindex="-1"></a>Q = XW^Q, \quad K = XW^K, \quad V = XW^V</span>
<span id="cb6-576"><a href="#cb6-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-577"><a href="#cb6-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-578"><a href="#cb6-578" aria-hidden="true" tabindex="-1"></a>其中$W^Q, W^K \in \mathbb{R}^{d_{model} \times d_k}$，$W^V \in \mathbb{R}^{d_{model} \times d_v}$。</span>
<span id="cb6-579"><a href="#cb6-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-580"><a href="#cb6-580" aria-hidden="true" tabindex="-1"></a>得到：$Q, K \in \mathbb{R}^{n \times d_k}$，$V \in \mathbb{R}^{n \times d_v}$</span>
<span id="cb6-581"><a href="#cb6-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-582"><a href="#cb6-582" aria-hidden="true" tabindex="-1"></a>**Step 2：计算注意力分数**</span>
<span id="cb6-583"><a href="#cb6-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-584"><a href="#cb6-584" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-585"><a href="#cb6-585" aria-hidden="true" tabindex="-1"></a>S = \frac{QK^\top}{\sqrt{d_k}}</span>
<span id="cb6-586"><a href="#cb6-586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-587"><a href="#cb6-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-588"><a href="#cb6-588" aria-hidden="true" tabindex="-1"></a>矩阵乘法：$(n \times d_k) \times (d_k \times n) = (n \times n)$</span>
<span id="cb6-589"><a href="#cb6-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-590"><a href="#cb6-590" aria-hidden="true" tabindex="-1"></a>得到：$S \in \mathbb{R}^{n \times n}$——这就是$O(n^2)$复杂度的来源</span>
<span id="cb6-591"><a href="#cb6-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-592"><a href="#cb6-592" aria-hidden="true" tabindex="-1"></a>**Step 3：Softmax归一化**</span>
<span id="cb6-593"><a href="#cb6-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-594"><a href="#cb6-594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-595"><a href="#cb6-595" aria-hidden="true" tabindex="-1"></a>A = \text{softmax}(S) \in \mathbb{R}^{n \times n}</span>
<span id="cb6-596"><a href="#cb6-596" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-597"><a href="#cb6-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-598"><a href="#cb6-598" aria-hidden="true" tabindex="-1"></a>对每一行做softmax，保持维度不变。第$(i,j)$个元素表示位置$i$对位置$j$的注意力权重。</span>
<span id="cb6-599"><a href="#cb6-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-600"><a href="#cb6-600" aria-hidden="true" tabindex="-1"></a>**Step 4：加权聚合**</span>
<span id="cb6-601"><a href="#cb6-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-602"><a href="#cb6-602" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-603"><a href="#cb6-603" aria-hidden="true" tabindex="-1"></a>\text{Output} = AV</span>
<span id="cb6-604"><a href="#cb6-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-605"><a href="#cb6-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-606"><a href="#cb6-606" aria-hidden="true" tabindex="-1"></a>矩阵乘法：$(n \times n) \times (n \times d_v) = (n \times d_v)$</span>
<span id="cb6-607"><a href="#cb6-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-608"><a href="#cb6-608" aria-hidden="true" tabindex="-1"></a>得到：$\text{Output} \in \mathbb{R}^{n \times d_v}$</span>
<span id="cb6-609"><a href="#cb6-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-610"><a href="#cb6-610" aria-hidden="true" tabindex="-1"></a>**Multi-Head的维度变化**</span>
<span id="cb6-611"><a href="#cb6-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-612"><a href="#cb6-612" aria-hidden="true" tabindex="-1"></a>对于$h$个头，每个头独立计算上述过程，得到$h$个$\mathbb{R}^{n \times d_v}$的输出。拼接后：</span>
<span id="cb6-613"><a href="#cb6-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-614"><a href="#cb6-614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-615"><a href="#cb6-615" aria-hidden="true" tabindex="-1"></a>\text{Concat}(\text{head}_1, \ldots, \text{head}_h) \in \mathbb{R}^{n \times (h \cdot d_v)} = \mathbb{R}^{n \times d_{model}}</span>
<span id="cb6-616"><a href="#cb6-616" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-617"><a href="#cb6-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-618"><a href="#cb6-618" aria-hidden="true" tabindex="-1"></a>最后通过$W^O \in \mathbb{R}^{d_{model} \times d_{model}}$投影，输出维度仍为$\mathbb{R}^{n \times d_{model}}$，与输入相同。</span>
<span id="cb6-619"><a href="#cb6-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-620"><a href="#cb6-620" aria-hidden="true" tabindex="-1"></a><span class="fu">### 参数量的详细计算</span></span>
<span id="cb6-621"><a href="#cb6-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-622"><a href="#cb6-622" aria-hidden="true" tabindex="-1"></a>让我们精确计算Transformer各组件的参数量。</span>
<span id="cb6-623"><a href="#cb6-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-624"><a href="#cb6-624" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multi-Head Attention</span></span>
<span id="cb6-625"><a href="#cb6-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-626"><a href="#cb6-626" aria-hidden="true" tabindex="-1"></a>每个MHA模块包含四个权重矩阵：</span>
<span id="cb6-627"><a href="#cb6-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-628"><a href="#cb6-628" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 矩阵 <span class="pp">|</span> 维度 <span class="pp">|</span> 参数量 <span class="pp">|</span></span>
<span id="cb6-629"><a href="#cb6-629" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|--------|</span></span>
<span id="cb6-630"><a href="#cb6-630" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W^Q$ <span class="pp">|</span> $d_{model} \times d_{model}$ <span class="pp">|</span> $d_{model}^2$ <span class="pp">|</span></span>
<span id="cb6-631"><a href="#cb6-631" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W^K$ <span class="pp">|</span> $d_{model} \times d_{model}$ <span class="pp">|</span> $d_{model}^2$ <span class="pp">|</span></span>
<span id="cb6-632"><a href="#cb6-632" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W^V$ <span class="pp">|</span> $d_{model} \times d_{model}$ <span class="pp">|</span> $d_{model}^2$ <span class="pp">|</span></span>
<span id="cb6-633"><a href="#cb6-633" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W^O$ <span class="pp">|</span> $d_{model} \times d_{model}$ <span class="pp">|</span> $d_{model}^2$ <span class="pp">|</span></span>
<span id="cb6-634"><a href="#cb6-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-635"><a href="#cb6-635" aria-hidden="true" tabindex="-1"></a>**MHA总参数量**（不含偏置）：$4d_{model}^2$</span>
<span id="cb6-636"><a href="#cb6-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-637"><a href="#cb6-637" aria-hidden="true" tabindex="-1"></a>注意：虽然我们把$W^Q$分成$h$个$W_i^Q$，但总参数量不变。这是因为$h$个$(d_{model} \times d_k)$矩阵拼接起来就是一个$(d_{model} \times d_{model})$矩阵。</span>
<span id="cb6-638"><a href="#cb6-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-639"><a href="#cb6-639" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Feed-Forward Network</span></span>
<span id="cb6-640"><a href="#cb6-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-641"><a href="#cb6-641" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-642"><a href="#cb6-642" aria-hidden="true" tabindex="-1"></a>\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2</span>
<span id="cb6-643"><a href="#cb6-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-644"><a href="#cb6-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-645"><a href="#cb6-645" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 矩阵/向量 <span class="pp">|</span> 维度 <span class="pp">|</span> 参数量 <span class="pp">|</span></span>
<span id="cb6-646"><a href="#cb6-646" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|------|--------|</span></span>
<span id="cb6-647"><a href="#cb6-647" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W_1$ <span class="pp">|</span> $d_{model} \times d_{ff}$ <span class="pp">|</span> $d_{model} \cdot d_{ff}$ <span class="pp">|</span></span>
<span id="cb6-648"><a href="#cb6-648" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $b_1$ <span class="pp">|</span> $d_{ff}$ <span class="pp">|</span> $d_{ff}$ <span class="pp">|</span></span>
<span id="cb6-649"><a href="#cb6-649" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W_2$ <span class="pp">|</span> $d_{ff} \times d_{model}$ <span class="pp">|</span> $d_{ff} \cdot d_{model}$ <span class="pp">|</span></span>
<span id="cb6-650"><a href="#cb6-650" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $b_2$ <span class="pp">|</span> $d_{model}$ <span class="pp">|</span> $d_{model}$ <span class="pp">|</span></span>
<span id="cb6-651"><a href="#cb6-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-652"><a href="#cb6-652" aria-hidden="true" tabindex="-1"></a>**FFN总参数量**：$2 \cdot d_{model} \cdot d_{ff} + d_{ff} + d_{model}$</span>
<span id="cb6-653"><a href="#cb6-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-654"><a href="#cb6-654" aria-hidden="true" tabindex="-1"></a>当$d_{ff} = 4 \cdot d_{model}$时（标准设置）：</span>
<span id="cb6-655"><a href="#cb6-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-656"><a href="#cb6-656" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-657"><a href="#cb6-657" aria-hidden="true" tabindex="-1"></a>\text{FFN参数量} \approx 2 \cdot d_{model} \cdot 4d_{model} = 8d_{model}^2</span>
<span id="cb6-658"><a href="#cb6-658" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-659"><a href="#cb6-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-660"><a href="#cb6-660" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 单层Transformer的参数量比较</span></span>
<span id="cb6-661"><a href="#cb6-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-662"><a href="#cb6-662" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 组件 <span class="pp">|</span> 参数量 <span class="pp">|</span> 占比 <span class="pp">|</span></span>
<span id="cb6-663"><a href="#cb6-663" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|------|</span></span>
<span id="cb6-664"><a href="#cb6-664" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Self-Attention <span class="pp">|</span> $4d_{model}^2$ <span class="pp">|</span> 33% <span class="pp">|</span></span>
<span id="cb6-665"><a href="#cb6-665" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FFN <span class="pp">|</span> $8d_{model}^2$ <span class="pp">|</span> 67% <span class="pp">|</span></span>
<span id="cb6-666"><a href="#cb6-666" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LayerNorm (×2) <span class="pp">|</span> $4d_{model}$ <span class="pp">|</span> ≈0% <span class="pp">|</span></span>
<span id="cb6-667"><a href="#cb6-667" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **总计** <span class="pp">|</span> $12d_{model}^2 + 4d_{model}$ <span class="pp">|</span> 100% <span class="pp">|</span></span>
<span id="cb6-668"><a href="#cb6-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-669"><a href="#cb6-669" aria-hidden="true" tabindex="-1"></a>这解释了一个有趣的现象：**FFN的参数量是Attention的两倍**。这意味着大模型的大部分参数实际上在FFN中，而非Attention中。</span>
<span id="cb6-670"><a href="#cb6-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-671"><a href="#cb6-671" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整模型参数量</span></span>
<span id="cb6-672"><a href="#cb6-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-673"><a href="#cb6-673" aria-hidden="true" tabindex="-1"></a>以Transformer Base为例（$d_{model}=512$, $d_{ff}=2048$, $L=6$层, 词汇表$V=32000$）：</span>
<span id="cb6-674"><a href="#cb6-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-675"><a href="#cb6-675" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 组件 <span class="pp">|</span> 计算 <span class="pp">|</span> 参数量 <span class="pp">|</span></span>
<span id="cb6-676"><a href="#cb6-676" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|--------|</span></span>
<span id="cb6-677"><a href="#cb6-677" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Embedding <span class="pp">|</span> $V \times d_{model}$ <span class="pp">|</span> 16.4M <span class="pp">|</span></span>
<span id="cb6-678"><a href="#cb6-678" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Encoder (6层) <span class="pp">|</span> $6 \times 12 \times d_{model}^2$ <span class="pp">|</span> 18.9M <span class="pp">|</span></span>
<span id="cb6-679"><a href="#cb6-679" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Decoder (6层) <span class="pp">|</span> $6 \times (12 + 4) \times d_{model}^2$ <span class="pp">|</span> 25.2M <span class="pp">|</span></span>
<span id="cb6-680"><a href="#cb6-680" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Output Projection <span class="pp">|</span> $d_{model} \times V$ <span class="pp">|</span> 16.4M <span class="pp">|</span></span>
<span id="cb6-681"><a href="#cb6-681" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **总计** | | **约65M** <span class="pp">|</span></span>
<span id="cb6-682"><a href="#cb6-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-683"><a href="#cb6-683" aria-hidden="true" tabindex="-1"></a>注：Decoder每层多一个Cross-Attention（$4d_{model}^2$）</span>
<span id="cb6-684"><a href="#cb6-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-685"><a href="#cb6-685" aria-hidden="true" tabindex="-1"></a><span class="fu">### Softmax梯度与缩放因子的数学分析</span></span>
<span id="cb6-686"><a href="#cb6-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-687"><a href="#cb6-687" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 为什么Softmax会饱和？</span></span>
<span id="cb6-688"><a href="#cb6-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-689"><a href="#cb6-689" aria-hidden="true" tabindex="-1"></a>Softmax函数定义为：</span>
<span id="cb6-690"><a href="#cb6-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-691"><a href="#cb6-691" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-692"><a href="#cb6-692" aria-hidden="true" tabindex="-1"></a>\text{softmax}(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}</span>
<span id="cb6-693"><a href="#cb6-693" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-694"><a href="#cb6-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-695"><a href="#cb6-695" aria-hidden="true" tabindex="-1"></a>考虑输入$z = <span class="co">[</span><span class="ot">a, 0, 0, \ldots, 0</span><span class="co">]</span>$，当$a$很大时：</span>
<span id="cb6-696"><a href="#cb6-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-697"><a href="#cb6-697" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-698"><a href="#cb6-698" aria-hidden="true" tabindex="-1"></a>\text{softmax}(z) \approx <span class="co">[</span><span class="ot">1, 0, 0, \ldots, 0</span><span class="co">]</span></span>
<span id="cb6-699"><a href="#cb6-699" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-700"><a href="#cb6-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-701"><a href="#cb6-701" aria-hidden="true" tabindex="-1"></a>现在计算Softmax对输入$z_i$的梯度。设$p_i = \text{softmax}(z)_i$，则：</span>
<span id="cb6-702"><a href="#cb6-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-703"><a href="#cb6-703" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-704"><a href="#cb6-704" aria-hidden="true" tabindex="-1"></a>\frac{\partial p_i}{\partial z_j} = \begin{cases}</span>
<span id="cb6-705"><a href="#cb6-705" aria-hidden="true" tabindex="-1"></a>p_i(1 - p_i) &amp; i = j <span class="sc">\\</span></span>
<span id="cb6-706"><a href="#cb6-706" aria-hidden="true" tabindex="-1"></a>-p_i p_j &amp; i \neq j</span>
<span id="cb6-707"><a href="#cb6-707" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb6-708"><a href="#cb6-708" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-709"><a href="#cb6-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-710"><a href="#cb6-710" aria-hidden="true" tabindex="-1"></a>当$p_i \approx 1$时，$\frac{\partial p_i}{\partial z_i} = p_i(1-p_i) \approx 0$。</span>
<span id="cb6-711"><a href="#cb6-711" aria-hidden="true" tabindex="-1"></a>当$p_j \approx 0$时，$\frac{\partial p_i}{\partial z_j} = -p_i p_j \approx 0$。</span>
<span id="cb6-712"><a href="#cb6-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-713"><a href="#cb6-713" aria-hidden="true" tabindex="-1"></a>**结论**：Softmax在饱和区的梯度趋近于零，导致学习停滞。</span>
<span id="cb6-714"><a href="#cb6-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-715"><a href="#cb6-715" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 缩放因子的精确推导</span></span>
<span id="cb6-716"><a href="#cb6-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-717"><a href="#cb6-717" aria-hidden="true" tabindex="-1"></a>假设$q, k \in \mathbb{R}^{d_k}$，每个分量$q_i, k_i \sim \mathcal{N}(0, 1)$独立同分布。</span>
<span id="cb6-718"><a href="#cb6-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-719"><a href="#cb6-719" aria-hidden="true" tabindex="-1"></a>点积$z = q \cdot k = \sum_{i=1}^{d_k} q_i k_i$。</span>
<span id="cb6-720"><a href="#cb6-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-721"><a href="#cb6-721" aria-hidden="true" tabindex="-1"></a>由于$q_i$和$k_i$独立，$q_i k_i$的期望和方差为：</span>
<span id="cb6-722"><a href="#cb6-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-723"><a href="#cb6-723" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-724"><a href="#cb6-724" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">q_i k_i</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">q_i</span><span class="co">]</span>\mathbb{E}<span class="co">[</span><span class="ot">k_i</span><span class="co">]</span> = 0</span>
<span id="cb6-725"><a href="#cb6-725" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-726"><a href="#cb6-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-727"><a href="#cb6-727" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-728"><a href="#cb6-728" aria-hidden="true" tabindex="-1"></a>\text{Var}(q_i k_i) = \mathbb{E}<span class="co">[</span><span class="ot">q_i^2 k_i^2</span><span class="co">]</span> - (\mathbb{E}<span class="co">[</span><span class="ot">q_i k_i</span><span class="co">]</span>)^2 = \mathbb{E}<span class="co">[</span><span class="ot">q_i^2</span><span class="co">]</span>\mathbb{E}<span class="co">[</span><span class="ot">k_i^2</span><span class="co">]</span> = 1 \cdot 1 = 1</span>
<span id="cb6-729"><a href="#cb6-729" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-730"><a href="#cb6-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-731"><a href="#cb6-731" aria-hidden="true" tabindex="-1"></a>因此：</span>
<span id="cb6-732"><a href="#cb6-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-733"><a href="#cb6-733" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-734"><a href="#cb6-734" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">z</span><span class="co">]</span> = \sum_{i=1}^{d_k} \mathbb{E}<span class="co">[</span><span class="ot">q_i k_i</span><span class="co">]</span> = 0</span>
<span id="cb6-735"><a href="#cb6-735" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-736"><a href="#cb6-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-737"><a href="#cb6-737" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-738"><a href="#cb6-738" aria-hidden="true" tabindex="-1"></a>\text{Var}(z) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k</span>
<span id="cb6-739"><a href="#cb6-739" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-740"><a href="#cb6-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-741"><a href="#cb6-741" aria-hidden="true" tabindex="-1"></a>标准差$\sigma_z = \sqrt{d_k}$。当$d_k = 512$时，$\sigma_z \approx 22.6$。</span>
<span id="cb6-742"><a href="#cb6-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-743"><a href="#cb6-743" aria-hidden="true" tabindex="-1"></a>除以$\sqrt{d_k}$后：</span>
<span id="cb6-744"><a href="#cb6-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-745"><a href="#cb6-745" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-746"><a href="#cb6-746" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(\frac{z}{\sqrt{d_k}}\right) = \frac{\text{Var}(z)}{d_k} = 1</span>
<span id="cb6-747"><a href="#cb6-747" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-748"><a href="#cb6-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-749"><a href="#cb6-749" aria-hidden="true" tabindex="-1"></a>**数值示例**：</span>
<span id="cb6-750"><a href="#cb6-750" aria-hidden="true" tabindex="-1"></a>设$d_k = 64$，不缩放时点积的95%置信区间约为$<span class="co">[</span><span class="ot">-16, 16</span><span class="co">]</span>$。</span>
<span id="cb6-751"><a href="#cb6-751" aria-hidden="true" tabindex="-1"></a>缩放后95%置信区间约为$<span class="co">[</span><span class="ot">-2, 2</span><span class="co">]</span>$，Softmax输出更平滑，梯度更稳定。</span>
<span id="cb6-752"><a href="#cb6-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-753"><a href="#cb6-753" aria-hidden="true" tabindex="-1"></a><span class="fu">### 位置编码的相对位置性质证明</span></span>
<span id="cb6-754"><a href="#cb6-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-755"><a href="#cb6-755" aria-hidden="true" tabindex="-1"></a>正弦位置编码有一个优美的性质：任意固定偏移$k$的位置关系可以用线性变换表示。</span>
<span id="cb6-756"><a href="#cb6-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-757"><a href="#cb6-757" aria-hidden="true" tabindex="-1"></a>**命题**：存在矩阵$M_k$（只依赖于$k$），使得对任意位置$pos$：</span>
<span id="cb6-758"><a href="#cb6-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-759"><a href="#cb6-759" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-760"><a href="#cb6-760" aria-hidden="true" tabindex="-1"></a>PE_{pos+k} = M_k \cdot PE_{pos}</span>
<span id="cb6-761"><a href="#cb6-761" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-762"><a href="#cb6-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-763"><a href="#cb6-763" aria-hidden="true" tabindex="-1"></a>**证明**：</span>
<span id="cb6-764"><a href="#cb6-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-765"><a href="#cb6-765" aria-hidden="true" tabindex="-1"></a>考虑位置编码的第$2i$和$2i+1$维（它们使用相同的频率$\omega_i = 1/10000^{2i/d}$）：</span>
<span id="cb6-766"><a href="#cb6-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-767"><a href="#cb6-767" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-768"><a href="#cb6-768" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix} PE_{pos, 2i} <span class="sc">\\</span> PE_{pos, 2i+1} \end{bmatrix} = \begin{bmatrix} \sin(\omega_i \cdot pos) <span class="sc">\\</span> \cos(\omega_i \cdot pos) \end{bmatrix}</span>
<span id="cb6-769"><a href="#cb6-769" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-770"><a href="#cb6-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-771"><a href="#cb6-771" aria-hidden="true" tabindex="-1"></a>利用三角恒等式：</span>
<span id="cb6-772"><a href="#cb6-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-773"><a href="#cb6-773" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-774"><a href="#cb6-774" aria-hidden="true" tabindex="-1"></a>\sin(\omega_i(pos + k)) = \sin(\omega_i \cdot pos)\cos(\omega_i \cdot k) + \cos(\omega_i \cdot pos)\sin(\omega_i \cdot k)</span>
<span id="cb6-775"><a href="#cb6-775" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-776"><a href="#cb6-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-777"><a href="#cb6-777" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-778"><a href="#cb6-778" aria-hidden="true" tabindex="-1"></a>\cos(\omega_i(pos + k)) = \cos(\omega_i \cdot pos)\cos(\omega_i \cdot k) - \sin(\omega_i \cdot pos)\sin(\omega_i \cdot k)</span>
<span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a>写成矩阵形式：</span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix} PE_{pos+k, 2i} <span class="sc">\\</span> PE_{pos+k, 2i+1} \end{bmatrix} = \begin{bmatrix} \cos(\omega_i k) &amp; \sin(\omega_i k) <span class="sc">\\</span> -\sin(\omega_i k) &amp; \cos(\omega_i k) \end{bmatrix} \begin{bmatrix} PE_{pos, 2i} <span class="sc">\\</span> PE_{pos, 2i+1} \end{bmatrix}</span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a>这是一个旋转矩阵！对于每一对维度$(2i, 2i+1)$，偏移$k$对应一个旋转角度$\omega_i k$。</span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a>完整的变换矩阵$M_k$是分块对角矩阵：</span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a>M_k = \text{diag}(R_{\omega_0 k}, R_{\omega_1 k}, \ldots, R_{\omega_{d/2-1} k})</span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a>其中$R_\theta = \begin{bmatrix} \cos\theta &amp; \sin\theta <span class="sc">\\</span> -\sin\theta &amp; \cos\theta \end{bmatrix}$是2D旋转矩阵。</span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a>**意义**：这个性质使得模型可以通过学习线性变换来捕获相对位置关系。例如，学习"前一个词"（$k=-1$）或"后两个词"（$k=2$）的关系。</span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a><span class="fu">### FLOPs计算：训练一个Transformer需要多少计算？</span></span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 单次前向传播的FLOPs</span></span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a>对于序列长度$n$，模型维度$d$，$L$层Encoder：</span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a>**Self-Attention（每层）**：</span>
<span id="cb6-806"><a href="#cb6-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-807"><a href="#cb6-807" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 操作 <span class="pp">|</span> 计算 <span class="pp">|</span> FLOPs <span class="pp">|</span></span>
<span id="cb6-808"><a href="#cb6-808" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|-------|</span></span>
<span id="cb6-809"><a href="#cb6-809" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $Q, K, V$投影 <span class="pp">|</span> $3 \times (n \times d) \times (d \times d)$ <span class="pp">|</span> $6nd^2$ <span class="pp">|</span></span>
<span id="cb6-810"><a href="#cb6-810" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $QK^\top$ <span class="pp">|</span> $(n \times d) \times (d \times n)$ <span class="pp">|</span> $2n^2d$ <span class="pp">|</span></span>
<span id="cb6-811"><a href="#cb6-811" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\text{Softmax}(S)V$ <span class="pp">|</span> $(n \times n) \times (n \times d)$ <span class="pp">|</span> $2n^2d$ <span class="pp">|</span></span>
<span id="cb6-812"><a href="#cb6-812" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 输出投影 <span class="pp">|</span> $(n \times d) \times (d \times d)$ <span class="pp">|</span> $2nd^2$ <span class="pp">|</span></span>
<span id="cb6-813"><a href="#cb6-813" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **小计** <span class="pp">|</span> <span class="pp">|</span> $8nd^2 + 4n^2d$ <span class="pp">|</span></span>
<span id="cb6-814"><a href="#cb6-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-815"><a href="#cb6-815" aria-hidden="true" tabindex="-1"></a>**FFN（每层）**：</span>
<span id="cb6-816"><a href="#cb6-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-817"><a href="#cb6-817" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 操作 <span class="pp">|</span> 计算 <span class="pp">|</span> FLOPs <span class="pp">|</span></span>
<span id="cb6-818"><a href="#cb6-818" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|-------|</span></span>
<span id="cb6-819"><a href="#cb6-819" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W_1$乘法 <span class="pp">|</span> $(n \times d) \times (d \times 4d)$ <span class="pp">|</span> $8nd^2$ <span class="pp">|</span></span>
<span id="cb6-820"><a href="#cb6-820" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ReLU <span class="pp">|</span> $4nd$ <span class="pp">|</span> $4nd$ <span class="pp">|</span></span>
<span id="cb6-821"><a href="#cb6-821" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $W_2$乘法 <span class="pp">|</span> $(n \times 4d) \times (4d \times d)$ <span class="pp">|</span> $8nd^2$ <span class="pp">|</span></span>
<span id="cb6-822"><a href="#cb6-822" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **小计** <span class="pp">|</span> <span class="pp">|</span> $16nd^2 + 4nd$ <span class="pp">|</span></span>
<span id="cb6-823"><a href="#cb6-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-824"><a href="#cb6-824" aria-hidden="true" tabindex="-1"></a>**单层Encoder**：$\approx 24nd^2 + 4n^2d$</span>
<span id="cb6-825"><a href="#cb6-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-826"><a href="#cb6-826" aria-hidden="true" tabindex="-1"></a>**L层Encoder前向传播**：$\approx L(24nd^2 + 4n^2d)$</span>
<span id="cb6-827"><a href="#cb6-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-828"><a href="#cb6-828" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 训练的总FLOPs估算</span></span>
<span id="cb6-829"><a href="#cb6-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-830"><a href="#cb6-830" aria-hidden="true" tabindex="-1"></a>训练时需要前向和反向传播，反向约为前向的2倍，因此单个样本约为前向的3倍。</span>
<span id="cb6-831"><a href="#cb6-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-832"><a href="#cb6-832" aria-hidden="true" tabindex="-1"></a>对于Transformer Base（$d=512$, $L=6$, $n=512$）：</span>
<span id="cb6-833"><a href="#cb6-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-834"><a href="#cb6-834" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-835"><a href="#cb6-835" aria-hidden="true" tabindex="-1"></a>\text{FLOPs/样本} \approx 3 \times 6 \times (24 \times 512 \times 512^2 + 4 \times 512^2 \times 512) \approx 3 \times 10^{10}</span>
<span id="cb6-836"><a href="#cb6-836" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-837"><a href="#cb6-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-838"><a href="#cb6-838" aria-hidden="true" tabindex="-1"></a>训练100K步，batch size 64：</span>
<span id="cb6-839"><a href="#cb6-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-840"><a href="#cb6-840" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-841"><a href="#cb6-841" aria-hidden="true" tabindex="-1"></a>\text{总FLOPs} \approx 100000 \times 64 \times 3 \times 10^{10} \approx 2 \times 10^{17}</span>
<span id="cb6-842"><a href="#cb6-842" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-843"><a href="#cb6-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-844"><a href="#cb6-844" aria-hidden="true" tabindex="-1"></a>在A100 GPU（312 TFLOPS FP16）上，理论时间约为$2 \times 10^{17} / 3.12 \times 10^{14} \approx 640$秒，约10分钟。实际由于内存带宽等因素，通常需要数小时。</span>
<span id="cb6-845"><a href="#cb6-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-846"><a href="#cb6-846" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复杂度对比总结</span></span>
<span id="cb6-847"><a href="#cb6-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-848"><a href="#cb6-848" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 时间复杂度 <span class="pp">|</span> 空间复杂度 <span class="pp">|</span> 顺序操作数 <span class="pp">|</span> 最长路径 <span class="pp">|</span></span>
<span id="cb6-849"><a href="#cb6-849" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|-----------|-----------|----------|</span></span>
<span id="cb6-850"><a href="#cb6-850" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RNN <span class="pp">|</span> $O(nd^2)$ <span class="pp">|</span> $O(d)$ <span class="pp">|</span> $O(n)$ <span class="pp">|</span> $O(n)$ <span class="pp">|</span></span>
<span id="cb6-851"><a href="#cb6-851" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> CNN (kernel $k$) <span class="pp">|</span> $O(knd^2)$ <span class="pp">|</span> $O(n)$ <span class="pp">|</span> $O(1)$ <span class="pp">|</span> $O(\log_k n)$ <span class="pp">|</span></span>
<span id="cb6-852"><a href="#cb6-852" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Self-Attention <span class="pp">|</span> $O(n^2d)$ <span class="pp">|</span> $O(n^2)$ <span class="pp">|</span> $O(1)$ <span class="pp">|</span> $O(1)$ <span class="pp">|</span></span>
<span id="cb6-853"><a href="#cb6-853" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Transformer <span class="pp">|</span> $O(n^2d + nd^2)$ <span class="pp">|</span> $O(n^2 + nd)$ <span class="pp">|</span> $O(1)$ <span class="pp">|</span> $O(1)$ <span class="pp">|</span></span>
<span id="cb6-854"><a href="#cb6-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-855"><a href="#cb6-855" aria-hidden="true" tabindex="-1"></a>**解读**：</span>
<span id="cb6-856"><a href="#cb6-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-857"><a href="#cb6-857" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**最长路径**：信息从位置1传到位置$n$需要经过多少层。RNN是$O(n)$，Transformer是$O(1)$（一步到位），这是Transformer捕获长距离依赖的关键优势。</span>
<span id="cb6-858"><a href="#cb6-858" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**顺序操作数**：并行化的程度。RNN必须顺序执行$n$步，Transformer完全并行。</span>
<span id="cb6-859"><a href="#cb6-859" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**空间复杂度**：Transformer的$O(n^2)$是痛点，长序列时显存瓶颈。</span>
<span id="cb6-860"><a href="#cb6-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-861"><a href="#cb6-861" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-862"><a href="#cb6-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-863"><a href="#cb6-863" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb6-864"><a href="#cb6-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-865"><a href="#cb6-865" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从零实现Transformer</span></span>
<span id="cb6-866"><a href="#cb6-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-867"><a href="#cb6-867" aria-hidden="true" tabindex="-1"></a>以下是Transformer核心模块的PyTorch实现，附详细注释：</span>
<span id="cb6-868"><a href="#cb6-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-871"><a href="#cb6-871" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-872"><a href="#cb6-872" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb6-873"><a href="#cb6-873" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-874"><a href="#cb6-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-875"><a href="#cb6-875" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-876"><a href="#cb6-876" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-877"><a href="#cb6-877" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb6-878"><a href="#cb6-878" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb6-879"><a href="#cb6-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-880"><a href="#cb6-880" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScaledDotProductAttention(nn.Module):</span>
<span id="cb6-881"><a href="#cb6-881" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-882"><a href="#cb6-882" aria-hidden="true" tabindex="-1"></a><span class="co">    Scaled Dot-Product Attention</span></span>
<span id="cb6-883"><a href="#cb6-883" aria-hidden="true" tabindex="-1"></a><span class="co">    核心公式: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V</span></span>
<span id="cb6-884"><a href="#cb6-884" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-885"><a href="#cb6-885" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-886"><a href="#cb6-886" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-887"><a href="#cb6-887" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb6-888"><a href="#cb6-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-889"><a href="#cb6-889" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, Q, K, V, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-890"><a href="#cb6-890" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> Q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-891"><a href="#cb6-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-892"><a href="#cb6-892" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算注意力分数: [batch, heads, seq_len, seq_len]</span></span>
<span id="cb6-893"><a href="#cb6-893" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb6-894"><a href="#cb6-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-895"><a href="#cb6-895" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 应用mask（用于因果注意力或padding mask）</span></span>
<span id="cb6-896"><a href="#cb6-896" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-897"><a href="#cb6-897" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb6-898"><a href="#cb6-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-899"><a href="#cb6-899" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Softmax归一化</span></span>
<span id="cb6-900"><a href="#cb6-900" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb6-901"><a href="#cb6-901" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> <span class="va">self</span>.dropout(attention_weights)</span>
<span id="cb6-902"><a href="#cb6-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-903"><a href="#cb6-903" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 加权求和</span></span>
<span id="cb6-904"><a href="#cb6-904" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.matmul(attention_weights, V)</span>
<span id="cb6-905"><a href="#cb6-905" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb6-906"><a href="#cb6-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-907"><a href="#cb6-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-908"><a href="#cb6-908" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb6-909"><a href="#cb6-909" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-910"><a href="#cb6-910" aria-hidden="true" tabindex="-1"></a><span class="co">    Multi-Head Attention</span></span>
<span id="cb6-911"><a href="#cb6-911" aria-hidden="true" tabindex="-1"></a><span class="co">    将d_model拆成h个头，每个头独立计算attention，最后拼接</span></span>
<span id="cb6-912"><a href="#cb6-912" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-913"><a href="#cb6-913" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-914"><a href="#cb6-914" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-915"><a href="#cb6-915" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"d_model必须能被num_heads整除"</span></span>
<span id="cb6-916"><a href="#cb6-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-917"><a href="#cb6-917" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb6-918"><a href="#cb6-918" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb6-919"><a href="#cb6-919" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb6-920"><a href="#cb6-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-921"><a href="#cb6-921" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q, K, V的线性投影</span></span>
<span id="cb6-922"><a href="#cb6-922" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_Q <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb6-923"><a href="#cb6-923" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_K <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb6-924"><a href="#cb6-924" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_V <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb6-925"><a href="#cb6-925" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_O <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb6-926"><a href="#cb6-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-927"><a href="#cb6-927" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> ScaledDotProductAttention(dropout)</span>
<span id="cb6-928"><a href="#cb6-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-929"><a href="#cb6-929" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, Q, K, V, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-930"><a href="#cb6-930" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> Q.size(<span class="dv">0</span>)</span>
<span id="cb6-931"><a href="#cb6-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-932"><a href="#cb6-932" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 线性投影并reshape为多头格式</span></span>
<span id="cb6-933"><a href="#cb6-933" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.W_Q(Q).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-934"><a href="#cb6-934" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.W_K(K).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-935"><a href="#cb6-935" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.W_V(V).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-936"><a href="#cb6-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-937"><a href="#cb6-937" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 计算attention</span></span>
<span id="cb6-938"><a href="#cb6-938" aria-hidden="true" tabindex="-1"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention(Q, K, V, mask)</span>
<span id="cb6-939"><a href="#cb6-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-940"><a href="#cb6-940" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 拼接所有头并做最终投影</span></span>
<span id="cb6-941"><a href="#cb6-941" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> attn_output.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.d_model)</span>
<span id="cb6-942"><a href="#cb6-942" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.W_O(attn_output)</span>
<span id="cb6-943"><a href="#cb6-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-944"><a href="#cb6-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-945"><a href="#cb6-945" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedForward(nn.Module):</span>
<span id="cb6-946"><a href="#cb6-946" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2"""</span></span>
<span id="cb6-947"><a href="#cb6-947" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-948"><a href="#cb6-948" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-949"><a href="#cb6-949" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(d_model, d_ff)</span>
<span id="cb6-950"><a href="#cb6-950" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(d_ff, d_model)</span>
<span id="cb6-951"><a href="#cb6-951" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb6-952"><a href="#cb6-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-953"><a href="#cb6-953" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-954"><a href="#cb6-954" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc2(<span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc1(x))))</span>
<span id="cb6-955"><a href="#cb6-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-956"><a href="#cb6-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-957"><a href="#cb6-957" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb6-958"><a href="#cb6-958" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""正弦位置编码"""</span></span>
<span id="cb6-959"><a href="#cb6-959" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-960"><a href="#cb6-960" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-961"><a href="#cb6-961" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb6-962"><a href="#cb6-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-963"><a href="#cb6-963" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb6-964"><a href="#cb6-964" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb6-965"><a href="#cb6-965" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb6-966"><a href="#cb6-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-967"><a href="#cb6-967" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb6-968"><a href="#cb6-968" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb6-969"><a href="#cb6-969" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb6-970"><a href="#cb6-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-971"><a href="#cb6-971" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-972"><a href="#cb6-972" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :])</span>
<span id="cb6-973"><a href="#cb6-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-974"><a href="#cb6-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-975"><a href="#cb6-975" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb6-976"><a href="#cb6-976" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer Encoder层：Self-Attention -&gt; Add &amp; Norm -&gt; FFN -&gt; Add &amp; Norm"""</span></span>
<span id="cb6-977"><a href="#cb6-977" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-978"><a href="#cb6-978" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-979"><a href="#cb6-979" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb6-980"><a href="#cb6-980" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> PositionwiseFeedForward(d_model, d_ff, dropout)</span>
<span id="cb6-981"><a href="#cb6-981" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb6-982"><a href="#cb6-982" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb6-983"><a href="#cb6-983" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb6-984"><a href="#cb6-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-985"><a href="#cb6-985" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-986"><a href="#cb6-986" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Self-Attention + Residual + LayerNorm</span></span>
<span id="cb6-987"><a href="#cb6-987" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.self_attn(x, x, x, mask)</span>
<span id="cb6-988"><a href="#cb6-988" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> <span class="va">self</span>.dropout(attn_output))</span>
<span id="cb6-989"><a href="#cb6-989" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN + Residual + LayerNorm</span></span>
<span id="cb6-990"><a href="#cb6-990" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ffn(x)))</span>
<span id="cb6-991"><a href="#cb6-991" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-992"><a href="#cb6-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-993"><a href="#cb6-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-994"><a href="#cb6-994" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderLayer(nn.Module):</span>
<span id="cb6-995"><a href="#cb6-995" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer Decoder层"""</span></span>
<span id="cb6-996"><a href="#cb6-996" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-997"><a href="#cb6-997" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-998"><a href="#cb6-998" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb6-999"><a href="#cb6-999" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(d_model, num_heads, dropout)</span>
<span id="cb6-1000"><a href="#cb6-1000" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> PositionwiseFeedForward(d_model, d_ff, dropout)</span>
<span id="cb6-1001"><a href="#cb6-1001" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb6-1002"><a href="#cb6-1002" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb6-1003"><a href="#cb6-1003" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb6-1004"><a href="#cb6-1004" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb6-1005"><a href="#cb6-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1006"><a href="#cb6-1006" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, encoder_output, self_attn_mask<span class="op">=</span><span class="va">None</span>, cross_attn_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-1007"><a href="#cb6-1007" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Masked Self-Attention</span></span>
<span id="cb6-1008"><a href="#cb6-1008" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.self_attn(x, x, x, self_attn_mask)))</span>
<span id="cb6-1009"><a href="#cb6-1009" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cross-Attention</span></span>
<span id="cb6-1010"><a href="#cb6-1010" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.cross_attn(x, encoder_output, encoder_output, cross_attn_mask)))</span>
<span id="cb6-1011"><a href="#cb6-1011" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN</span></span>
<span id="cb6-1012"><a href="#cb6-1012" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm3(x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ffn(x)))</span>
<span id="cb6-1013"><a href="#cb6-1013" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-1014"><a href="#cb6-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1015"><a href="#cb6-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1016"><a href="#cb6-1016" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb6-1017"><a href="#cb6-1017" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""完整的Transformer模型"""</span></span>
<span id="cb6-1018"><a href="#cb6-1018" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, src_vocab_size, tgt_vocab_size, d_model<span class="op">=</span><span class="dv">512</span>, num_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb6-1019"><a href="#cb6-1019" aria-hidden="true" tabindex="-1"></a>                 num_encoder_layers<span class="op">=</span><span class="dv">6</span>, num_decoder_layers<span class="op">=</span><span class="dv">6</span>, d_ff<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb6-1020"><a href="#cb6-1020" aria-hidden="true" tabindex="-1"></a>                 max_len<span class="op">=</span><span class="dv">5000</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-1021"><a href="#cb6-1021" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-1022"><a href="#cb6-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1023"><a href="#cb6-1023" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.src_embedding <span class="op">=</span> nn.Embedding(src_vocab_size, d_model)</span>
<span id="cb6-1024"><a href="#cb6-1024" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tgt_embedding <span class="op">=</span> nn.Embedding(tgt_vocab_size, d_model)</span>
<span id="cb6-1025"><a href="#cb6-1025" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_encoding <span class="op">=</span> PositionalEncoding(d_model, max_len, dropout)</span>
<span id="cb6-1026"><a href="#cb6-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1027"><a href="#cb6-1027" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb6-1028"><a href="#cb6-1028" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)</span>
<span id="cb6-1029"><a href="#cb6-1029" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_encoder_layers)</span>
<span id="cb6-1030"><a href="#cb6-1030" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb6-1031"><a href="#cb6-1031" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb6-1032"><a href="#cb6-1032" aria-hidden="true" tabindex="-1"></a>            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)</span>
<span id="cb6-1033"><a href="#cb6-1033" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_decoder_layers)</span>
<span id="cb6-1034"><a href="#cb6-1034" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb6-1035"><a href="#cb6-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1036"><a href="#cb6-1036" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_projection <span class="op">=</span> nn.Linear(d_model, tgt_vocab_size)</span>
<span id="cb6-1037"><a href="#cb6-1037" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> math.sqrt(d_model)</span>
<span id="cb6-1038"><a href="#cb6-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1039"><a href="#cb6-1039" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, src, src_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-1040"><a href="#cb6-1040" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_encoding(<span class="va">self</span>.src_embedding(src) <span class="op">*</span> <span class="va">self</span>.scale)</span>
<span id="cb6-1041"><a href="#cb6-1041" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.encoder_layers:</span>
<span id="cb6-1042"><a href="#cb6-1042" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, src_mask)</span>
<span id="cb6-1043"><a href="#cb6-1043" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-1044"><a href="#cb6-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1045"><a href="#cb6-1045" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, tgt, encoder_output, tgt_mask<span class="op">=</span><span class="va">None</span>, src_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-1046"><a href="#cb6-1046" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_encoding(<span class="va">self</span>.tgt_embedding(tgt) <span class="op">*</span> <span class="va">self</span>.scale)</span>
<span id="cb6-1047"><a href="#cb6-1047" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.decoder_layers:</span>
<span id="cb6-1048"><a href="#cb6-1048" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, encoder_output, tgt_mask, src_mask)</span>
<span id="cb6-1049"><a href="#cb6-1049" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-1050"><a href="#cb6-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1051"><a href="#cb6-1051" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-1052"><a href="#cb6-1052" aria-hidden="true" tabindex="-1"></a>        encoder_output <span class="op">=</span> <span class="va">self</span>.encode(src, src_mask)</span>
<span id="cb6-1053"><a href="#cb6-1053" aria-hidden="true" tabindex="-1"></a>        decoder_output <span class="op">=</span> <span class="va">self</span>.decode(tgt, encoder_output, tgt_mask, src_mask)</span>
<span id="cb6-1054"><a href="#cb6-1054" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.output_projection(decoder_output)</span>
<span id="cb6-1055"><a href="#cb6-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1056"><a href="#cb6-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1057"><a href="#cb6-1057" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_causal_mask(seq_len):</span>
<span id="cb6-1058"><a href="#cb6-1058" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""创建因果mask，防止看到未来的token"""</span></span>
<span id="cb6-1059"><a href="#cb6-1059" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.triu(torch.ones(seq_len, seq_len), diagonal<span class="op">=</span><span class="dv">1</span>).<span class="bu">bool</span>()</span>
<span id="cb6-1060"><a href="#cb6-1060" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">~</span>mask.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb6-1061"><a href="#cb6-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-1062"><a href="#cb6-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1063"><a href="#cb6-1063" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Hugging Face Transformers</span></span>
<span id="cb6-1064"><a href="#cb6-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1065"><a href="#cb6-1065" aria-hidden="true" tabindex="-1"></a>对于实际应用，推荐使用经过优化的库：</span>
<span id="cb6-1066"><a href="#cb6-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1069"><a href="#cb6-1069" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-1070"><a href="#cb6-1070" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb6-1071"><a href="#cb6-1071" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb6-1072"><a href="#cb6-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1073"><a href="#cb6-1073" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> T5ForConditionalGeneration, T5Tokenizer</span>
<span id="cb6-1074"><a href="#cb6-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1075"><a href="#cb6-1075" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载预训练的T5模型（Encoder-Decoder Transformer）</span></span>
<span id="cb6-1076"><a href="#cb6-1076" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"t5-small"</span></span>
<span id="cb6-1077"><a href="#cb6-1077" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained(model_name)</span>
<span id="cb6-1078"><a href="#cb6-1078" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained(model_name)</span>
<span id="cb6-1079"><a href="#cb6-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1080"><a href="#cb6-1080" aria-hidden="true" tabindex="-1"></a><span class="co"># 翻译任务示例</span></span>
<span id="cb6-1081"><a href="#cb6-1081" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"translate English to German: Hello, how are you?"</span></span>
<span id="cb6-1082"><a href="#cb6-1082" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb6-1083"><a href="#cb6-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1084"><a href="#cb6-1084" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb6-1085"><a href="#cb6-1085" aria-hidden="true" tabindex="-1"></a>translated <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-1086"><a href="#cb6-1086" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Translation: </span><span class="sc">{</span>translated<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-1087"><a href="#cb6-1087" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-1088"><a href="#cb6-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1089"><a href="#cb6-1089" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复现论文的关键细节</span></span>
<span id="cb6-1090"><a href="#cb6-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1091"><a href="#cb6-1091" aria-hidden="true" tabindex="-1"></a>原始Transformer论文中有一些容易忽略但至关重要的细节。</span>
<span id="cb6-1092"><a href="#cb6-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1093"><a href="#cb6-1093" aria-hidden="true" tabindex="-1"></a>首先是Embedding缩放：输入embedding需要乘以$\sqrt{d_{model}}$，因为embedding初始化通常较小，缩放后与位置编码的量级匹配。</span>
<span id="cb6-1094"><a href="#cb6-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1095"><a href="#cb6-1095" aria-hidden="true" tabindex="-1"></a>其次是权重共享的可能性：原论文中，encoder和decoder的embedding可以共享；如果源语言和目标语言相同，output projection也可以与embedding共享，这样可以减少参数量。</span>
<span id="cb6-1096"><a href="#cb6-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1097"><a href="#cb6-1097" aria-hidden="true" tabindex="-1"></a>Dropout的位置也有讲究：在attention分数计算后、残差加法前、以及embedding之后都要加dropout。</span>
<span id="cb6-1098"><a href="#cb6-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1099"><a href="#cb6-1099" aria-hidden="true" tabindex="-1"></a>学习率策略使用了一个特殊的warmup schedule：</span>
<span id="cb6-1100"><a href="#cb6-1100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-1101"><a href="#cb6-1101" aria-hidden="true" tabindex="-1"></a><span class="in">lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))</span></span>
<span id="cb6-1102"><a href="#cb6-1102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-1103"><a href="#cb6-1103" aria-hidden="true" tabindex="-1"></a>前warmup_steps步线性增加，之后按步数平方根衰减。这种设计对训练稳定性至关重要。</span>
<span id="cb6-1104"><a href="#cb6-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1105"><a href="#cb6-1105" aria-hidden="true" tabindex="-1"></a>最后，论文使用了0.1的label smoothing，这有助于提高泛化性能。</span>
<span id="cb6-1106"><a href="#cb6-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1107"><a href="#cb6-1107" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实验验证</span></span>
<span id="cb6-1108"><a href="#cb6-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1109"><a href="#cb6-1109" aria-hidden="true" tabindex="-1"></a>验证实现是否正确有几个关键步骤。首先检查各模块输入输出维度是否正确。然后在几个样本上尝试过拟合——如果模型正确，应该能完美过拟合小数据集。接着可视化attention pattern，检查是否合理。最后与Hugging Face等权威实现对比中间结果。</span>
<span id="cb6-1110"><a href="#cb6-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1111"><a href="#cb6-1111" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-1112"><a href="#cb6-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1113"><a href="#cb6-1113" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb6-1114"><a href="#cb6-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1115"><a href="#cb6-1115" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨Transformer的理论基础、边界条件和开放问题</span></span>
<span id="cb6-1116"><a href="#cb6-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1117"><a href="#cb6-1117" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb6-1118"><a href="#cb6-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1119"><a href="#cb6-1119" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 表达能力分析</span></span>
<span id="cb6-1120"><a href="#cb6-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1121"><a href="#cb6-1121" aria-hidden="true" tabindex="-1"></a>Transformer是通用函数逼近器吗？Yun et al. (2019) 给出了肯定的答案：在合适的条件下，Transformer可以以任意精度逼近任何连续的序列到序列函数。具体地，一个$O(n)$层的Transformer可以逼近任何Lipschitz连续的permutation equivariant函数。</span>
<span id="cb6-1122"><a href="#cb6-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1123"><a href="#cb6-1123" aria-hidden="true" tabindex="-1"></a>与RNN相比，RNN也是通用逼近器，但需要无限精度的隐状态。Transformer的优势在于可以用有限宽度实现相同的逼近能力。</span>
<span id="cb6-1124"><a href="#cb6-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1125"><a href="#cb6-1125" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 归纳偏置的视角</span></span>
<span id="cb6-1126"><a href="#cb6-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1127"><a href="#cb6-1127" aria-hidden="true" tabindex="-1"></a>不同架构有不同的归纳偏置。RNN假设顺序处理和短程依赖，这符合语言直觉，但长距离依赖困难。CNN假设局部模式和层级结构，擅长捕获n-gram，但感受野有限。Transformer假设全局连接和位置无关，非常灵活且可并行，但需要学习位置关系，计算量也更大。</span>
<span id="cb6-1128"><a href="#cb6-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1129"><a href="#cb6-1129" aria-hidden="true" tabindex="-1"></a>Transformer的归纳偏置可以说是"更弱"的——它不假设顺序或局部性，而是让模型自己学习这些模式。这意味着需要更多数据来学习显式的位置关系，但也更灵活，可以学到任意模式。当规模够大时，弱归纳偏置反而可能成为优势。</span>
<span id="cb6-1130"><a href="#cb6-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1131"><a href="#cb6-1131" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 与Hopfield网络的联系</span></span>
<span id="cb6-1132"><a href="#cb6-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1133"><a href="#cb6-1133" aria-hidden="true" tabindex="-1"></a>Ramsauer et al. (2020) 发现了一个有趣的联系：Transformer的attention机制可以理解为现代Hopfield网络的一步更新。这提供了一个全新的视角：Hopfield网络是联想记忆模型，Attention可以看作从"记忆"（Key-Value对）中"检索"与Query相关的内容。这也解释了为什么FFN可能起到存储知识的作用——它就是Transformer的"记忆体"。</span>
<span id="cb6-1134"><a href="#cb6-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1135"><a href="#cb6-1135" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——实证视角</span></span>
<span id="cb6-1136"><a href="#cb6-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1137"><a href="#cb6-1137" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Attention真的在学习有意义的模式吗？</span></span>
<span id="cb6-1138"><a href="#cb6-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1139"><a href="#cb6-1139" aria-hidden="true" tabindex="-1"></a>通过可视化不同head的attention pattern，研究者发现确实如此。有些head学会了关注语法依赖，比如主谓关系、动宾关系。有些head学会了关注固定偏移，比如总是关注前一个词或后一个词。还有些head专门关注特殊token，如句号或<span class="co">[</span><span class="ot">SEP</span><span class="co">]</span>标记。</span>
<span id="cb6-1140"><a href="#cb6-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1141"><a href="#cb6-1141" aria-hidden="true" tabindex="-1"></a><span class="al">![Attention可视化：句子"The cat sat on the mat"在不同层、不同head的注意力模式。浅层（Layer 1）倾向于关注相邻词（对角线模式），深层（Layer 3）展现更多语法结构模式（主谓、介宾关系）。](figures/chapter-8/attention-visualization.png)</span>{#fig-attention-visualization}</span>
<span id="cb6-1142"><a href="#cb6-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1143"><a href="#cb6-1143" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb6-1144"><a href="#cb6-1144" aria-hidden="true" tabindex="-1"></a>*Python生成的合成示意图，模式参考 Clark et al. (2019) "What Does BERT Look At?" 和 Voita et al. (2019) "Analyzing Multi-Head Self-Attention" 的实证发现*</span>
<span id="cb6-1145"><a href="#cb6-1145" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-1146"><a href="#cb6-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1147"><a href="#cb6-1147" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 哪些组件是必要的？</span></span>
<span id="cb6-1148"><a href="#cb6-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1149"><a href="#cb6-1149" aria-hidden="true" tabindex="-1"></a>通过消融实验，研究者得出了明确的结论。减少Multi-Head的数量会导致性能下降，说明多头是重要的。移除FFN会导致大幅下降，说明它不可或缺。移除Residual会导致模型无法训练。移除LayerNorm会导致训练不稳定。移除Positional Encoding会导致输出几乎随机。</span>
<span id="cb6-1150"><a href="#cb6-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1151"><a href="#cb6-1151" aria-hidden="true" tabindex="-1"></a>一个有趣的发现是，部分head可以被剪枝而不显著影响性能，说明模型存在一定的冗余。</span>
<span id="cb6-1152"><a href="#cb6-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1153"><a href="#cb6-1153" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb6-1154"><a href="#cb6-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1155"><a href="#cb6-1155" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 隐含假设</span></span>
<span id="cb6-1156"><a href="#cb6-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1157"><a href="#cb6-1157" aria-hidden="true" tabindex="-1"></a>Transformer隐含地做了几个假设。第一，位置信息可以通过加法注入——位置编码与内容编码相加，假设它们在同一语义空间可以混合。第二，全局交互是必要的——每个位置都与所有其他位置交互；如果任务只需要局部信息，这其实是一种浪费。第三，数据足够多——弱归纳偏置需要更多数据来补偿。</span>
<span id="cb6-1158"><a href="#cb6-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1159"><a href="#cb6-1159" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 失效条件</span></span>
<span id="cb6-1160"><a href="#cb6-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1161"><a href="#cb6-1161" aria-hidden="true" tabindex="-1"></a>在某些情况下，Transformer表现并不好。当数据量太小时，带有更强归纳偏置的模型（如CNN、RNN）可能表现更好。当序列太长时，$O(n^2)$的复杂度会成为瓶颈，超过几千token时需要特殊处理。对于需要精确位置推理的任务，如算术运算、数字排序，Transformer常常失败。对于强顺序依赖的任务，如解析嵌套结构，Transformer也可能不如专门设计的架构。</span>
<span id="cb6-1162"><a href="#cb6-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1163"><a href="#cb6-1163" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 已知的Failure Modes</span></span>
<span id="cb6-1164"><a href="#cb6-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1165"><a href="#cb6-1165" aria-hidden="true" tabindex="-1"></a>研究文献中报告了几种典型的失败模式。长度泛化失败：在短序列上训练的Transformer难以泛化到长序列。位置编码外推困难：超出训练长度后，正弦编码的外推效果不佳。对重复模式敏感：输入中的重复可能导致注意力过度集中。</span>
<span id="cb6-1166"><a href="#cb6-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1167"><a href="#cb6-1167" aria-hidden="true" tabindex="-1"></a><span class="fu">### 变体与扩展</span></span>
<span id="cb6-1168"><a href="#cb6-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1169"><a href="#cb6-1169" aria-hidden="true" tabindex="-1"></a>从架构角度看，Transformer衍生出了三大变体。Encoder-only的BERT用于理解任务。Decoder-only的GPT用于生成任务。Encoder-Decoder的T5和BART用于seq2seq任务。后续发展表明，Decoder-only架构在规模化后表现最好，这将在第16章详细讨论。</span>
<span id="cb6-1170"><a href="#cb6-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1171"><a href="#cb6-1171" aria-hidden="true" tabindex="-1"></a>从注意力机制角度看，Sparse Attention只关注部分位置，Linear Attention将复杂度降到$O(n)$，相对位置编码如RoPE和ALiBi解决了外推问题。这些改进将在第9章和第26章展开。</span>
<span id="cb6-1172"><a href="#cb6-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1173"><a href="#cb6-1173" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb6-1174"><a href="#cb6-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1175"><a href="#cb6-1175" aria-hidden="true" tabindex="-1"></a>如果你要在这个方向写一篇论文，可以从哪些问题切入呢？</span>
<span id="cb6-1176"><a href="#cb6-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1177"><a href="#cb6-1177" aria-hidden="true" tabindex="-1"></a>第一个方向是数据效率：为什么Transformer需要这么多数据？如何设计更好的归纳偏置？数据效率能提升多少？</span>
<span id="cb6-1178"><a href="#cb6-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1179"><a href="#cb6-1179" aria-hidden="true" tabindex="-1"></a>第二个方向是位置编码：最优设计是什么？正弦、可学习、相对位置...哪种最好？能否设计出完美外推的位置编码？</span>
<span id="cb6-1180"><a href="#cb6-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1181"><a href="#cb6-1181" aria-hidden="true" tabindex="-1"></a>第三个方向是FFN的作用：它到底在做什么？是在存储知识吗？能否用其他结构替代？</span>
<span id="cb6-1182"><a href="#cb6-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1183"><a href="#cb6-1183" aria-hidden="true" tabindex="-1"></a>第四个方向是可解释性：Attention权重真的代表"关注"吗？如何更好地解释Transformer的决策？</span>
<span id="cb6-1184"><a href="#cb6-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1185"><a href="#cb6-1185" aria-hidden="true" tabindex="-1"></a>第五个方向是效率：能否在保持性能的同时降低复杂度？稀疏注意力的极限在哪里？</span>
<span id="cb6-1186"><a href="#cb6-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1187"><a href="#cb6-1187" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-1188"><a href="#cb6-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1189"><a href="#cb6-1189" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb6-1190"><a href="#cb6-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1191"><a href="#cb6-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">### 计算复杂度：$O(n^2)$的诅咒</span></span>
<span id="cb6-1192"><a href="#cb6-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1193"><a href="#cb6-1193" aria-hidden="true" tabindex="-1"></a>Transformer最大的局限是Self-Attention的二次复杂度。考虑处理一本10万词的书：</span>
<span id="cb6-1194"><a href="#cb6-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1195"><a href="#cb6-1195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-1196"><a href="#cb6-1196" aria-hidden="true" tabindex="-1"></a>\text{注意力矩阵大小} = 100000^2 = 10^{10} \text{ 元素}</span>
<span id="cb6-1197"><a href="#cb6-1197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-1198"><a href="#cb6-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1199"><a href="#cb6-1199" aria-hidden="true" tabindex="-1"></a>即使用FP16，也需要20GB显存仅用于存储注意力矩阵！这直接限制了长文档处理、长程对话、以及需要大上下文的任务如代码生成和法律分析。</span>
<span id="cb6-1200"><a href="#cb6-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1201"><a href="#cb6-1201" aria-hidden="true" tabindex="-1"></a><span class="fu">### 位置编码的外推问题</span></span>
<span id="cb6-1202"><a href="#cb6-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1203"><a href="#cb6-1203" aria-hidden="true" tabindex="-1"></a>原始正弦位置编码在训练长度之外表现不佳。如果模型在512长度上训练，面对1024长度的输入时，位置512-1023的编码模型从未见过，性能会急剧下降。这限制了模型的实际应用灵活性。</span>
<span id="cb6-1204"><a href="#cb6-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1205"><a href="#cb6-1205" aria-hidden="true" tabindex="-1"></a><span class="fu">### 缺乏显式的层级结构</span></span>
<span id="cb6-1206"><a href="#cb6-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1207"><a href="#cb6-1207" aria-hidden="true" tabindex="-1"></a>自然语言有天然的层级结构：词组成短语，短语组成句子，句子组成段落，段落组成文档。Transformer的flat attention没有显式建模这种层级，完全依赖模型自己学习。这可能是某些结构化任务上表现不佳的原因。</span>
<span id="cb6-1208"><a href="#cb6-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1209"><a href="#cb6-1209" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb6-1210"><a href="#cb6-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1211"><a href="#cb6-1211" aria-hidden="true" tabindex="-1"></a>上述问题推动了后续研究。$O(n^2)$复杂度的问题催生了第9章的高效注意力机制，包括Longformer、BigBird、Performer等。位置编码外推问题催生了第26章的RoPE、ALiBi等新型位置编码。长上下文需求催生了Flash Attention和KV Cache优化等系统级方案。</span>
<span id="cb6-1212"><a href="#cb6-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1213"><a href="#cb6-1213" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第9章将深入探讨如何打破$O(n^2)$的魔咒，介绍各种高效注意力机制及其背后的设计思想。</span></span>
<span id="cb6-1214"><a href="#cb6-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1215"><a href="#cb6-1215" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-1216"><a href="#cb6-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1217"><a href="#cb6-1217" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb6-1218"><a href="#cb6-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1219"><a href="#cb6-1219" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb6-1220"><a href="#cb6-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1221"><a href="#cb6-1221" aria-hidden="true" tabindex="-1"></a>这一章我们探讨了Transformer的完整设计。核心问题是如何构建一个并行高效、能捕获任意距离依赖的序列建模架构。核心洞察是完全抛弃循环结构，用注意力机制本身作为核心计算单元。</span>
<span id="cb6-1222"><a href="#cb6-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1223"><a href="#cb6-1223" aria-hidden="true" tabindex="-1"></a>具体方法包括Scaled Dot-Product Attention提供高效稳定的注意力计算，Multi-Head Attention在多个子空间捕获多种模式，Positional Encoding注入位置信息，FFN配合Residual和LayerNorm保证表达能力和训练稳定性。</span>
<span id="cb6-1224"><a href="#cb6-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1225"><a href="#cb6-1225" aria-hidden="true" tabindex="-1"></a>这项工作的意义是开创性的：它建立了完全基于注意力的架构，奠定了预训练大模型的基础。后续的GPT、BERT、LLaMA等划时代模型都是基于Transformer。</span>
<span id="cb6-1226"><a href="#cb6-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1227"><a href="#cb6-1227" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb6-1228"><a href="#cb6-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1229"><a href="#cb6-1229" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 表达式 <span class="pp">|</span></span>
<span id="cb6-1230"><a href="#cb6-1230" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|</span></span>
<span id="cb6-1231"><a href="#cb6-1231" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Scaled Dot-Product Attention <span class="pp">|</span> $\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$ <span class="pp">|</span></span>
<span id="cb6-1232"><a href="#cb6-1232" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Multi-Head <span class="pp">|</span> $\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$ <span class="pp">|</span></span>
<span id="cb6-1233"><a href="#cb6-1233" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Positional Encoding <span class="pp">|</span> $PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$, $PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$ <span class="pp">|</span></span>
<span id="cb6-1234"><a href="#cb6-1234" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FFN <span class="pp">|</span> $\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$ <span class="pp">|</span></span>
<span id="cb6-1235"><a href="#cb6-1235" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> LayerNorm <span class="pp">|</span> $\text{LN}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta$ <span class="pp">|</span></span>
<span id="cb6-1236"><a href="#cb6-1236" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 点积方差 <span class="pp">|</span> $\text{Var}(q \cdot k) = d_k$ （假设$q_i, k_i \sim \mathcal{N}(0,1)$独立） <span class="pp">|</span></span>
<span id="cb6-1237"><a href="#cb6-1237" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Softmax梯度 <span class="pp">|</span> $\frac{\partial p_i}{\partial z_i} = p_i(1-p_i)$, $\frac{\partial p_i}{\partial z_j} = -p_i p_j$ <span class="pp">|</span></span>
<span id="cb6-1238"><a href="#cb6-1238" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 位置偏移变换 <span class="pp">|</span> $PE_{pos+k} = M_k \cdot PE_{pos}$，其中$M_k$是分块旋转矩阵 <span class="pp">|</span></span>
<span id="cb6-1239"><a href="#cb6-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1240"><a href="#cb6-1240" aria-hidden="true" tabindex="-1"></a><span class="fu">### 参数量与复杂度速查</span></span>
<span id="cb6-1241"><a href="#cb6-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1242"><a href="#cb6-1242" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 组件 <span class="pp">|</span> 参数量 <span class="pp">|</span> 时间复杂度 <span class="pp">|</span></span>
<span id="cb6-1243"><a href="#cb6-1243" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|-----------|</span></span>
<span id="cb6-1244"><a href="#cb6-1244" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Self-Attention <span class="pp">|</span> $4d_{model}^2$ <span class="pp">|</span> $O(n^2d + nd^2)$ <span class="pp">|</span></span>
<span id="cb6-1245"><a href="#cb6-1245" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> FFN <span class="pp">|</span> $8d_{model}^2$ (当$d_{ff}=4d$) <span class="pp">|</span> $O(nd^2)$ <span class="pp">|</span></span>
<span id="cb6-1246"><a href="#cb6-1246" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 单层Encoder <span class="pp">|</span> $12d_{model}^2$ <span class="pp">|</span> $O(n^2d + nd^2)$ <span class="pp">|</span></span>
<span id="cb6-1247"><a href="#cb6-1247" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Transformer Base (65M) <span class="pp">|</span> $d=512, L=6$ <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb6-1248"><a href="#cb6-1248" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Transformer Large (213M) <span class="pp">|</span> $d=1024, L=12$ <span class="pp">|</span> — <span class="pp">|</span></span>
<span id="cb6-1249"><a href="#cb6-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1250"><a href="#cb6-1250" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb6-1251"><a href="#cb6-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1252"><a href="#cb6-1252" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么说Transformer的Self-Attention是"置换等变"的？这与RNN有什么本质区别？如果去掉位置编码，输入"我爱你"和"你爱我"会得到什么样的输出？</span>
<span id="cb6-1253"><a href="#cb6-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1254"><a href="#cb6-1254" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导-基础]** 证明：当$q$和$k$的分量独立同分布且方差为1时，$q \cdot k$的方差等于$d_k$。进一步，如果$q_i, k_i \sim \mathcal{N}(0, \sigma^2)$，方差会是多少？</span>
<span id="cb6-1255"><a href="#cb6-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1256"><a href="#cb6-1256" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[数学推导-进阶]** 证明Multi-Head Attention的参数量与单头Attention相同。具体地，证明$h$个$(d_{model} \times d_k)$投影矩阵的总参数量等于一个$(d_{model} \times d_{model})$矩阵，其中$d_k = d_{model}/h$。</span>
<span id="cb6-1257"><a href="#cb6-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1258"><a href="#cb6-1258" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[数学推导-挑战]** 利用三角恒等式证明正弦位置编码的相对位置性质：存在只依赖于偏移$k$的矩阵$M_k$，使得$PE_{pos+k} = M_k \cdot PE_{pos}$。写出$M_k$在维度$(2i, 2i+1)$上的具体形式。</span>
<span id="cb6-1259"><a href="#cb6-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1260"><a href="#cb6-1260" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[计算题]** 对于一个Transformer模型（$d_{model}=768$, $d_{ff}=3072$, $h=12$头, $L=12$层），计算：(a) 单层的参数量；(b) 整个Encoder的参数量；(c) 序列长度$n=512$时，单次前向传播Self-Attention的FLOPs。</span>
<span id="cb6-1261"><a href="#cb6-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1262"><a href="#cb6-1262" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**[工程实践]** 实现一个简化版Transformer，在IWSLT14德英翻译数据集上训练，达到BLEU &gt; 25。记录训练过程中的loss曲线，并可视化某一层的attention pattern。</span>
<span id="cb6-1263"><a href="#cb6-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1264"><a href="#cb6-1264" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**[研究思考]** 有人认为Transformer的成功主要归功于规模（更多参数、更多数据），而非架构本身。你如何看待这个观点？考虑以下证据：(a) 在小数据集上，CNN/RNN有时优于Transformer；(b) Vision Transformer在没有大量数据时表现不佳；(c) Scaling Laws表明性能与规模有幂律关系。</span>
<span id="cb6-1265"><a href="#cb6-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1266"><a href="#cb6-1266" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-1267"><a href="#cb6-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1268"><a href="#cb6-1268" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb6-1269"><a href="#cb6-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1270"><a href="#cb6-1270" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb6-1271"><a href="#cb6-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1272"><a href="#cb6-1272" aria-hidden="true" tabindex="-1"></a>**Vaswani et al. (2017). "Attention Is All You Need"** 是这一切的起点。重点阅读Section 3的模型架构描述和Section 5.4关于为什么选择Self-Attention的讨论。实验细节可以快速浏览。</span>
<span id="cb6-1273"><a href="#cb6-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1274"><a href="#cb6-1274" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb6-1275"><a href="#cb6-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1276"><a href="#cb6-1276" aria-hidden="true" tabindex="-1"></a>**Yun et al. (2019). "Are Transformers universal approximators of sequence-to-sequence functions?"** 证明了Transformer的通用逼近性质，为理解其表达能力提供了理论基础。</span>
<span id="cb6-1277"><a href="#cb6-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1278"><a href="#cb6-1278" aria-hidden="true" tabindex="-1"></a>**Ramsauer et al. (2020). "Hopfield Networks is All You Need"** 揭示了Attention与Hopfield网络的深层联系，提供了一个全新的理解视角。</span>
<span id="cb6-1279"><a href="#cb6-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1280"><a href="#cb6-1280" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb6-1281"><a href="#cb6-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1282"><a href="#cb6-1282" aria-hidden="true" tabindex="-1"></a>**Clark et al. (2019). "What Does BERT Look At?"** 深入分析了BERT的attention pattern，帮助我们理解模型到底在关注什么。</span>
<span id="cb6-1283"><a href="#cb6-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1284"><a href="#cb6-1284" aria-hidden="true" tabindex="-1"></a>**Michel et al. (2019). "Are Sixteen Heads Really Better than One?"** 发现多数head可以被剪枝而不显著影响性能，揭示了模型的冗余性。</span>
<span id="cb6-1285"><a href="#cb6-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1286"><a href="#cb6-1286" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb6-1287"><a href="#cb6-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1288"><a href="#cb6-1288" aria-hidden="true" tabindex="-1"></a>官方实现在tensor2tensor仓库。PyTorch入门者推荐阅读The Annotated Transformer，它用详细注释的方式解读了完整实现。实际应用推荐使用Hugging Face的transformers库，它提供了经过优化的生产级实现。</span>
<span id="cb6-1289"><a href="#cb6-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1290"><a href="#cb6-1290" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-1291"><a href="#cb6-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1292"><a href="#cb6-1292" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb6-1293"><a href="#cb6-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1294"><a href="#cb6-1294" aria-hidden="true" tabindex="-1"></a>"Attention Is All You Need"这篇论文最初在Google内部也引起了争议。当时RNN仍是主流，有人质疑完全抛弃循环结构是否明智。论文的标题也颇有挑衅意味——"注意力就是你所需要的全部"，仿佛在向整个RNN阵营宣战。</span>
<span id="cb6-1295"><a href="#cb6-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1296"><a href="#cb6-1296" aria-hidden="true" tabindex="-1"></a>有趣的是，论文的八位作者后来分散到了不同的地方：有的创办了AI公司，有的加入了OpenAI，有的继续在Google做研究。这篇论文成为了整个AI时代的基石。</span>
<span id="cb6-1297"><a href="#cb6-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1298"><a href="#cb6-1298" aria-hidden="true" tabindex="-1"></a>"Transformer"这个名字的灵感据说来自于变形金刚（Transformers）——既有"变换"的含义，又朗朗上口。另一种说法是它暗示了从RNN到全新架构的"变革"（transformation）。</span>
<span id="cb6-1299"><a href="#cb6-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-1300"><a href="#cb6-1300" aria-hidden="true" tabindex="-1"></a>截至2024年，这篇论文的引用次数超过10万次，是近年来引用最高的AI论文之一。几乎所有现代大语言模型——GPT-4、Claude、Gemini、LLaMA——都是基于Transformer架构。当年那个"激进"的设计决策，已经成为了整个领域的基础设施。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>