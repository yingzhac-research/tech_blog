<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="前三章（Ch17–Ch19）回答了’如何训练大模型’的理论和工程问题——Scaling Laws告诉我们规模与性能的数学关系，训练稳定性技术让百亿参数的训练不崩溃，分布式系统让千亿参数模型在万卡集群上跑起来。但一个更本质的问题悬而未决：这些大模型到底能做什么’小模型做不到的事’？2020年，OpenAI的GPT-3用175B参数给出了一个令人震惊的答案——In-Context Learning：不需要任何梯度更新，仅通过在输入中提供几个示例，模型就能学会新任务。这种能力不是被训练出来的，而是在规模达到一定阈值后自发涌现的。本章系统讲述GPT-3的架构、训练、ICL的现象与机制，以及由此诞生的Prompt Engineering范式。">

<title>第20章：GPT-3与In-Context Learning – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#从微调范式的局限说起" id="toc-从微调范式的局限说起" class="nav-link" data-scroll-target="#从微调范式的局限说起"><span class="header-section-number">2.1</span> 从”微调范式”的局限说起</a></li>
  <li><a href="#一个理想中的万能模型" id="toc-一个理想中的万能模型" class="nav-link" data-scroll-target="#一个理想中的万能模型"><span class="header-section-number">2.2</span> 一个理想中的”万能模型”</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#关键洞察规模带来的不仅是更好而是不同" id="toc-关键洞察规模带来的不仅是更好而是不同" class="nav-link" data-scroll-target="#关键洞察规模带来的不仅是更好而是不同"><span class="header-section-number">3.1</span> 关键洞察：规模带来的不仅是”更好”，而是”不同”</a></li>
  <li><a href="#in-context-learning-的三种模式" id="toc-in-context-learning-的三种模式" class="nav-link" data-scroll-target="#in-context-learning-的三种模式"><span class="header-section-number">3.2</span> In-Context Learning 的三种模式</a></li>
  <li><a href="#一个类比从培训员工到给指令" id="toc-一个类比从培训员工到给指令" class="nav-link" data-scroll-target="#一个类比从培训员工到给指令"><span class="header-section-number">3.3</span> 一个类比：从”培训员工”到”给指令”</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#gpt-3的模型架构" id="toc-gpt-3的模型架构" class="nav-link" data-scroll-target="#gpt-3的模型架构"><span class="header-section-number">4.1</span> GPT-3的模型架构</a></li>
  <li><a href="#训练数据与计算量" id="toc-训练数据与计算量" class="nav-link" data-scroll-target="#训练数据与计算量"><span class="header-section-number">4.2</span> 训练数据与计算量</a></li>
  <li><a href="#in-context-learning-的实验结果" id="toc-in-context-learning-的实验结果" class="nav-link" data-scroll-target="#in-context-learning-的实验结果"><span class="header-section-number">4.3</span> In-Context Learning 的实验结果</a></li>
  <li><a href="#标志性实验结果" id="toc-标志性实验结果" class="nav-link" data-scroll-target="#标志性实验结果"><span class="header-section-number">4.4</span> 标志性实验结果</a></li>
  <li><a href="#icl的规模效应量化越大越好" id="toc-icl的规模效应量化越大越好" class="nav-link" data-scroll-target="#icl的规模效应量化越大越好"><span class="header-section-number">4.5</span> ICL的规模效应：量化”越大越好”</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#gpt-3-apillm即服务的诞生" id="toc-gpt-3-apillm即服务的诞生" class="nav-link" data-scroll-target="#gpt-3-apillm即服务的诞生"><span class="header-section-number">5.1</span> GPT-3 API：LLM即服务的诞生</a></li>
  <li><a href="#icl推理流程" id="toc-icl推理流程" class="nav-link" data-scroll-target="#icl推理流程"><span class="header-section-number">5.2</span> ICL推理流程</a></li>
  <li><a href="#prompt设计的核心原则" id="toc-prompt设计的核心原则" class="nav-link" data-scroll-target="#prompt设计的核心原则"><span class="header-section-number">5.3</span> Prompt设计的核心原则</a></li>
  <li><a href="#一个完整的few-shot实现" id="toc-一个完整的few-shot实现" class="nav-link" data-scroll-target="#一个完整的few-shot实现"><span class="header-section-number">5.4</span> 一个完整的Few-shot实现</a></li>
  <li><a href="#复现细节与工程注意事项" id="toc-复现细节与工程注意事项" class="nav-link" data-scroll-target="#复现细节与工程注意事项"><span class="header-section-number">5.5</span> 复现细节与工程注意事项</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#为什么有效实证视角" id="toc-为什么有效实证视角" class="nav-link" data-scroll-target="#为什么有效实证视角"><span class="header-section-number">6.2</span> 为什么有效？——实证视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</a></li>
  <li><a href="#icl的不稳定性问题" id="toc-icl的不稳定性问题" class="nav-link" data-scroll-target="#icl的不稳定性问题"><span class="header-section-number">6.4</span> ICL的不稳定性问题</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.5</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#本方法的局限" id="toc-本方法的局限" class="nav-link" data-scroll-target="#本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">9.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.3</span> 后续发展</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">9.4</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第20章：GPT-3与In-Context Learning</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">When Scale Brings Emergence: The Birth of In-Context Learning</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">GPT-3</div>
    <div class="quarto-category">In-Context Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    前三章（Ch17–Ch19）回答了’如何训练大模型’的理论和工程问题——Scaling Laws告诉我们规模与性能的数学关系，训练稳定性技术让百亿参数的训练不崩溃，分布式系统让千亿参数模型在万卡集群上跑起来。但一个更本质的问题悬而未决：这些大模型到底能做什么’小模型做不到的事’？2020年，OpenAI的GPT-3用175B参数给出了一个令人震惊的答案——In-Context Learning：不需要任何梯度更新，仅通过在输入中提供几个示例，模型就能学会新任务。这种能力不是被训练出来的，而是在规模达到一定阈值后自发涌现的。本章系统讲述GPT-3的架构、训练、ICL的现象与机制，以及由此诞生的Prompt Engineering范式。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：当语言模型的规模从十亿增长到千亿参数时，是否会涌现出小模型不具备的全新能力？In-Context Learning——不经过梯度更新就能从少量示例中学会新任务——是如何发生的？</p>
<p><strong>历史坐标</strong>：2020 | Brown et al.&nbsp;“Language Models are Few-Shot Learners” (GPT-3) | 从规模到涌现、从微调到提示</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Brown et al.&nbsp;(2020)</strong> “Language Models are Few-Shot Learners” (arXiv:2005.14165, GPT-3) — 参考了 Section 2（Approach: zero/one/few-shot 定义）、Section 3（全部42个benchmark实验结果）、Figure 2.1（ICL范式对比图）、Figure 1.2（ICL学习曲线）、Figure 1.3（聚合性能随规模的变化）、Table 2.1（8个模型规模配置）；提取了 5 张论文原图</li>
<li><strong>Xie et al.&nbsp;(2022)</strong> “An Explanation of In-context Learning as Implicit Bayesian Inference” (arXiv:2111.02080) — 参考了 ICL 作为隐式贝叶斯推断的理论框架</li>
<li><strong>Dai et al.&nbsp;(2023)</strong> “Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers” (arXiv:2212.10559) — 参考了 ICL 与隐式梯度下降的等价性分析</li>
<li><strong>Olsson et al.&nbsp;(2022)</strong> “In-context Learning and Induction Heads” (arXiv:2209.11895, Anthropic) — 参考了归纳头（Induction Heads）作为 ICL 机制单元的发现</li>
<li><strong>Min et al.&nbsp;(2022)</strong> “Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?” (arXiv:2202.12837) — 参考了”标签可能不重要，格式才重要”的反直觉发现</li>
<li><strong>Zhao et al.&nbsp;(2021)</strong> “Calibrate Before Use: Improving Few-Shot Performance of Language Models” (arXiv:2102.09690) — 参考了 ICL 的不稳定性分析（对示例选择、顺序、格式的敏感性）</li>
<li><strong>Lu et al.&nbsp;(2022)</strong> “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity” (arXiv:2104.08786) — 参考了示例顺序敏感性的量化分析</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li>SLP3 Chapter 7 (Large Language Models) — 参考了 GPT-3 架构、In-Context Learning 定义和 scaling 讨论</li>
<li>D2L Chapter 15 — 参考了预训练语言模型的教学组织</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li>Stanford CS224N Lecture 10-11 (Winter 2025) — 参考了 GPT-3 和 In-Context Learning 的教学框架</li>
<li>CMU 11-711 ANLP Lecture “Prompting &amp; In-context Learning” (Fall 2024) — 参考了 Prompting 技术的系统化讲解</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>前三章我们系统地解决了大语言模型训练的三大基础问题。第17章揭示了规模与性能之间的幂律关系，Scaling Laws 将大模型训练从炼金术变成了可预测的工程科学。第18章讨论了如何在百亿参数的规模下保持训练稳定——从 Adam 到 BF16，从 warmup 到梯度裁剪，这些数值工程技巧构成了大模型训练的”安全网”。第19章则解决了最根本的硬件约束：通过数据并行、张量并行、流水线并行和 ZeRO 内存优化的组合，让千亿参数的模型能在成千上万张 GPU 上训练起来。</p>
<p>至此，“如何训练一个超大模型”的问题已经有了完整的答案。但正如第19章结尾所提到的：分布式训练解决了工程问题，但训练出来的大模型是否真的比小模型更有用？规模的增长带来的是量变还是质变？</p>
<p>这个问题的答案比任何人预期的都更加戏剧性。2020年5月，OpenAI发表了GPT-3论文——一个拥有1750亿参数的语言模型。GPT-3不仅在困惑度上符合Scaling Laws的预测（量变），它还展示了一种小模型完全不具备的全新能力：<strong>In-Context Learning</strong>（上下文学习）。你不需要对模型做任何微调，不需要更新一个参数，只需要在输入中给出几个任务示例，GPT-3就能”学会”执行这个任务——翻译、摘要、问答、甚至简单的算术。</p>
<p>这种能力不是被特意训练出来的。OpenAI只是训练了一个更大的语言模型来预测下一个词——和GPT-2做的事情完全一样。但当模型规模跨过某个阈值时，一种全新的能力似乎”自发涌现”了。这个发现震动了整个AI领域，因为它暗示着一种可能性：或许通过不断扩大规模，语言模型能够获得越来越多我们事先未预料到的能力。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：GPT-3的核心贡献不是”更大的语言模型”（那只是Scaling Laws的延续），而是发现了 <strong>In-Context Learning</strong>——一种无需梯度更新、仅通过输入示例就能完成新任务的能力。这种能力随规模增长而增强，改变了NLP的使用范式：从”预训练+微调”转向”预训练+提示”（prompting）。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="从微调范式的局限说起" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="从微调范式的局限说起"><span class="header-section-number">2.1</span> 从”微调范式”的局限说起</h3>
<p>在GPT-3出现之前，使用预训练语言模型的标准流程是<strong>预训练+微调</strong>（pre-train then fine-tune）。第13章讲述的BERT就是这个范式的代表：先在大规模语料上做掩码语言模型预训练，然后为每个下游任务收集标注数据、添加任务特定的分类头、更新全部参数。</p>
<p>这个范式虽然有效，但存在三个根本性的限制。</p>
<p>第一个限制是<strong>任务专用性</strong>。每个任务都需要一个独立微调的模型。如果你有10个不同的NLP任务，就需要训练、部署和维护10个不同的模型。当模型规模达到百亿或千亿参数时，这意味着你需要存储数十个TB级别的模型权重——光是存储成本就令人望而却步。</p>
<p>第二个限制是<strong>对标注数据的依赖</strong>。微调需要下游任务的标注数据，通常至少数千到数万条。对于很多实际任务（如特定领域的信息抽取、新语言的翻译），获取高质量标注数据既昂贵又耗时。更尴尬的是，标注数据的获取速度远远跟不上人们对AI应用的需求。</p>
<p>第三个限制更加微妙：<strong>微调可能损害模型的通用能力</strong>。当你在一个小数据集上微调一个大模型时，模型往往会”忘记”预训练阶段学到的很多知识——这被称为灾难性遗忘（catastrophic forgetting）。微调后的BERT在目标任务上表现很好，但它不再是一个”通用的语言理解系统”。</p>
<p>这些限制共同指向一个更深层的问题：<strong>我们能否找到一种使用方式，让一个模型不经修改就能适应各种任务？</strong></p>
</section>
<section id="一个理想中的万能模型" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="一个理想中的万能模型"><span class="header-section-number">2.2</span> 一个理想中的”万能模型”</h3>
<p>让我们想象一下理想的场景：你有一个经过大规模预训练的语言模型，你想用它来做情感分析。在微调范式下，你需要收集情感标注数据、设计分类头、训练几个epoch、调参、评估。但如果模型足够”聪明”，你能不能直接告诉它：“这是一个情感分类任务。‘这部电影太棒了’ → 正面。‘演技很烂’ → 负面。现在请分类：‘剧情引人入胜’”——然后模型直接给出答案？</p>
<p>这正是In-Context Learning的核心思想：<strong>将任务说明和示例直接放在模型的输入中，让模型通过”阅读”这些信息来完成任务，而不是通过梯度更新来”学习”任务。</strong></p>
<p>但这听起来像是天方夜谭。一个仅仅被训练来预测”下一个词”的模型，怎么可能仅凭输入中的几个示例就”学会”一个全新的任务？要回答这个问题，我们首先需要理解GPT-3本身。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="关键洞察规模带来的不仅是更好而是不同" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="关键洞察规模带来的不仅是更好而是不同"><span class="header-section-number">3.1</span> 关键洞察：规模带来的不仅是”更好”，而是”不同”</h3>
<p>GPT-3的核心洞察可以用一句话概括：<strong>当语言模型足够大时，它不仅能更好地预测下一个词，还能从输入的上下文中”推断”出你想让它做什么任务，并直接执行。</strong></p>
<p>这不是一个连续的量变过程。GPT-2（1.5B参数）只能勉强做一些零样本任务，效果远不如微调模型。但GPT-3（175B参数）在很多任务上的few-shot表现已经接近甚至超过了微调的BERT。从1.5B到175B，参数量增加了约100倍，但ICL能力的提升远超100倍——这更像是一种相变（phase transition），而非线性增长。</p>
<p>为什么规模会带来这种质变？一个直觉性的解释是：语言模型在预训练时看到的文本中，天然地包含了大量”从示例中学习”的模式。互联网上充满了这样的文本结构：</p>
<pre><code>问：法国的首都是哪里？
答：巴黎。

问：日本的首都是哪里？
答：东京。

问：巴西的首都是哪里？
答：</code></pre>
<p>当模型在海量文本上训练时，它不仅学会了”东京是日本的首都”这样的知识，还学会了”如果前面有几个问答对，下一个应该按同样的格式回答”这种<strong>元模式</strong>（meta-pattern）。小模型可能只学到了浅层的文本模式（续写一段通顺的句子），而足够大的模型则学到了更深层的结构——包括”根据示例推断任务规则”的能力。</p>
</section>
<section id="in-context-learning-的三种模式" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="in-context-learning-的三种模式"><span class="header-section-number">3.2</span> In-Context Learning 的三种模式</h3>
<p>GPT-3论文定义了三种使用模式，从简单到复杂：</p>
<p><strong>Zero-shot</strong>（零样本）：只给任务描述，不给任何示例。</p>
<pre><code>Translate English to French:
cheese =&gt;</code></pre>
<p>模型需要仅凭”Translate English to French”这个指令来理解任务并执行。这要求模型在预训练中就已经”知道”翻译是什么，以及如何从英文翻译成法文。</p>
<p><strong>One-shot</strong>（单样本）：给一个示例。</p>
<pre><code>Translate English to French:
sea otter =&gt; loutre de mer
cheese =&gt;</code></pre>
<p>一个示例帮助模型”锁定”了任务的格式（英文 =&gt; 法文）和风格。</p>
<p><strong>Few-shot</strong>（少样本）：给几个到几十个示例。</p>
<pre><code>Translate English to French:
sea otter =&gt; loutre de mer
peppermint =&gt; menthe poivrée
plush giraffe =&gt; girafe en peluche
cheese =&gt;</code></pre>
<p>更多的示例让模型更准确地理解任务边界和预期输出格式。</p>
<div id="fig-gpt3-paradigms" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt3-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-20/original/fig1-gpt3-zero-one-few-shot.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpt3-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: GPT-3的四种任务范式对比：传统微调（需要梯度更新）vs Zero-shot / One-shot / Few-shot（仅需前向推理）。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Brown et al.&nbsp;(2020) “Language Models are Few-Shot Learners”, Figure 2.1</em></p>
</div>
<p><a href="#fig-gpt3-paradigms" class="quarto-xref">Figure&nbsp;1</a> 是GPT-3论文中最具标志性的图之一。它清晰地展示了传统微调与ICL三种模式的根本区别：微调需要大量标注数据和梯度更新来调整模型参数，而零样本、单样本和少样本学习完全不修改模型——所有”学习”都发生在前向推理的输入中。这不仅是技术方法的差异，更是<strong>使用范式</strong>的根本转变。</p>
</section>
<section id="一个类比从培训员工到给指令" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="一个类比从培训员工到给指令"><span class="header-section-number">3.3</span> 一个类比：从”培训员工”到”给指令”</h3>
<p>传统的微调范式就像是<strong>培训一个新员工</strong>：你给他教材（标注数据），让他练习（训练），经过几周的训练（epoch），他终于能胜任这个特定岗位。但如果你需要他做另一项工作，就得重新培训。</p>
<p>In-Context Learning则像是<strong>给一个经验丰富的通才下指令</strong>：你不需要培训他，只需要告诉他”这是任务要求，这是几个例子”，他就能立刻理解并执行。他之所以能做到这一点，是因为他在过去的工作中已经积累了足够丰富的经验和能力——你不是在教他新技能，而是在<strong>激活</strong>他已有的能力。</p>
<p>GPT-3正是这样一个”通才”：它在预训练阶段看过了互联网上几乎所有类型的文本，积累了海量的”经验”。当你给它few-shot示例时，你不是在教它学翻译、学分类——这些能力它已经”知道”了。你做的只是<strong>告诉它你想用哪种能力</strong>。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="gpt-3的模型架构" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="gpt-3的模型架构"><span class="header-section-number">4.1</span> GPT-3的模型架构</h3>
<p>GPT-3的架构本身并无太大创新——它基本上就是一个更大的GPT-2。核心架构仍然是Decoder-only Transformer，使用因果（causal）自注意力和自回归语言建模目标。与GPT-2的主要区别在于：GPT-3采用了交替使用 dense 和 locally banded sparse attention pattern（类似 Sparse Transformer），但论文中并没有详细讨论这一细节的影响。</p>
<p>GPT-3论文训练了8个不同规模的模型，从1.25亿参数到1750亿参数：</p>
<div id="tbl-gpt3-models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-gpt3-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: GPT-3 系列模型的架构配置。数据源自 Brown et al.&nbsp;(2020) Table 2.1。
</figcaption>
<div aria-describedby="tbl-gpt3-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th>模型名</th>
<th>参数量 <span class="math inline">\(N\)</span></th>
<th>层数 <span class="math inline">\(n_{\text{layers}}\)</span></th>
<th>隐藏维度 <span class="math inline">\(d_{\text{model}}\)</span></th>
<th>注意力头数 <span class="math inline">\(n_{\text{heads}}\)</span></th>
<th>头维度 <span class="math inline">\(d_{\text{head}}\)</span></th>
<th>Batch Size</th>
<th>学习率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3 Small</td>
<td>125M</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>64</td>
<td>0.5M</td>
<td>6.0×10⁻⁴</td>
</tr>
<tr class="even">
<td>GPT-3 Medium</td>
<td>350M</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>64</td>
<td>0.5M</td>
<td>3.0×10⁻⁴</td>
</tr>
<tr class="odd">
<td>GPT-3 Large</td>
<td>760M</td>
<td>24</td>
<td>1536</td>
<td>16</td>
<td>96</td>
<td>0.5M</td>
<td>2.5×10⁻⁴</td>
</tr>
<tr class="even">
<td>GPT-3 XL</td>
<td>1.3B</td>
<td>24</td>
<td>2048</td>
<td>24</td>
<td>128</td>
<td>1M</td>
<td>2.0×10⁻⁴</td>
</tr>
<tr class="odd">
<td>GPT-3 2.7B</td>
<td>2.7B</td>
<td>32</td>
<td>2560</td>
<td>32</td>
<td>80</td>
<td>1M</td>
<td>1.6×10⁻⁴</td>
</tr>
<tr class="even">
<td>GPT-3 6.7B</td>
<td>6.7B</td>
<td>32</td>
<td>4096</td>
<td>32</td>
<td>128</td>
<td>2M</td>
<td>1.2×10⁻⁴</td>
</tr>
<tr class="odd">
<td>GPT-3 13B</td>
<td>13B</td>
<td>40</td>
<td>5140</td>
<td>40</td>
<td>128</td>
<td>2M</td>
<td>1.0×10⁻⁴</td>
</tr>
<tr class="even">
<td><strong>GPT-3 175B</strong></td>
<td><strong>175B</strong></td>
<td><strong>96</strong></td>
<td><strong>12288</strong></td>
<td><strong>96</strong></td>
<td><strong>128</strong></td>
<td><strong>3.2M</strong></td>
<td><strong>0.6×10⁻⁴</strong></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>这个表格包含了几个值得关注的设计细节。首先，所有模型的上下文窗口长度都是 <span class="math inline">\(n_{\text{ctx}} = 2048\)</span> 个token（与GPT-2相同）。其次，学习率随模型规模增大而减小——这与第18章讨论的训练稳定性直觉一致：更大的模型需要更温和的学习率来避免训练不稳定。第三，batch size随模型增大而增大（从0.5M到3.2M tokens），这是因为更大的模型能更高效地利用更大的batch（第17章的Scaling Laws预测了这一点）。</p>
</section>
<section id="训练数据与计算量" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="训练数据与计算量"><span class="header-section-number">4.2</span> 训练数据与计算量</h3>
<p>GPT-3的训练数据由五个来源混合而成：</p>
<div id="tbl-gpt3-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-gpt3-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: GPT-3 训练数据构成。注意训练权重并不正比于数据大小——高质量数据(WebText2, Wikipedia)被过采样。
</figcaption>
<div aria-describedby="tbl-gpt3-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>数据集</th>
<th>Token数量</th>
<th>训练中权重</th>
<th>训练中的Epoch数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Common Crawl (过滤后)</td>
<td>410B</td>
<td>60%</td>
<td>0.44</td>
</tr>
<tr class="even">
<td>WebText2</td>
<td>19B</td>
<td>22%</td>
<td>2.9</td>
</tr>
<tr class="odd">
<td>Books1</td>
<td>12B</td>
<td>8%</td>
<td>1.9</td>
</tr>
<tr class="even">
<td>Books2</td>
<td>55B</td>
<td>8%</td>
<td>0.43</td>
</tr>
<tr class="odd">
<td>Wikipedia</td>
<td>3B</td>
<td>3%</td>
<td>3.4</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>一个有趣的观察是：虽然Common Crawl占据了绝大多数token（410B），但高质量数据集被显著过采样——WebText2只有19B token却占了22%的训练权重，Wikipedia只有3B token却被训练了3.4个epoch。这反映了一个现在被广泛接受的观点：<strong>数据质量比数据数量更重要</strong>，这在后来LLaMA等模型的训练中被进一步强化。</p>
<p>GPT-3 175B的总计算量约为 <span class="math inline">\(3.14 \times 10^{23}\)</span> FLOPs。按照第17章介绍的 <span class="math inline">\(C \approx 6ND\)</span> 公式，<span class="math inline">\(6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.15 \times 10^{23}\)</span>，与论文报告一致。然而，按照Chinchilla法则（<span class="math inline">\(D^* \approx 20N\)</span>），175B参数应配 <span class="math inline">\(3.5 \times 10^{12}\)</span> tokens，而GPT-3只用了 <span class="math inline">\(300 \times 10^9\)</span> tokens——训练数据不到”最优值”的十分之一。换言之，<strong>GPT-3是一个严重欠训练的模型</strong>。但即便如此，它仍然展示了惊人的ICL能力——这让人不禁想象，如果GPT-3按照Chinchilla法则充分训练，效果会有多好。</p>
<div id="fig-gpt3-compute" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt3-compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-20/original/fig5-gpt3-scaling-compute.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpt3-compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: GPT-3系列模型的训练计算量对比。尽管GPT-3 175B比RoBERTa-Large大近500倍，但由于在远少于典型规模的token数上训练，其计算量的增长幅度远小于参数量的增长。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Brown et al.&nbsp;(2020) “Language Models are Few-Shot Learners”, Figure 2.2</em></p>
</div>
</section>
<section id="in-context-learning-的实验结果" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="in-context-learning-的实验结果"><span class="header-section-number">4.3</span> In-Context Learning 的实验结果</h3>
<p>GPT-3论文在42个benchmark上进行了系统评估，覆盖了传统NLP任务、翻译、常识推理、阅读理解等多个领域。</p>
<div id="fig-gpt3-aggregate" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt3-aggregate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-20/original/fig3-gpt3-aggregate-performance.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpt3-aggregate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: GPT-3在42个accuracy-denominated benchmarks上的聚合性能。Zero-shot性能随规模稳步提升，但few-shot性能提升更快，表明更大的模型更擅长In-Context Learning。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Brown et al.&nbsp;(2020) “Language Models are Few-Shot Learners”, Figure 1.3</em></p>
</div>
<p><a href="#fig-gpt3-aggregate" class="quarto-xref">Figure&nbsp;3</a> 传达了本章最重要的实验发现：<strong>ICL能力与模型规模之间存在超线性的关系</strong>。随着模型从125M增长到175B：</p>
<ul>
<li><strong>Zero-shot</strong> 性能稳步提升——这并不太令人惊讶，因为更大的模型存储了更多知识。</li>
<li><strong>Few-shot</strong> 性能提升得<strong>更快</strong>——这才是关键发现。小模型即使给了示例也无法有效利用，而大模型能够从少量示例中快速”学会”任务。</li>
</ul>
<p>这个差异的含义是深远的：ICL不仅仅是”模型知道更多知识”的体现，它是一种随规模涌现的<strong>元能力</strong>——利用上下文信息来适应新任务的能力。</p>
<section id="完整数值示例in-context-learning的工作过程" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="完整数值示例in-context-learning的工作过程"><span class="header-section-number">4.3.1</span> 完整数值示例：In-Context Learning的工作过程</h4>
<p><strong>设定</strong>：使用GPT-3对一段文本做情感分类。模型的输入就是一个文本序列，我们通过精心构造输入来”告诉”模型做什么。</p>
<p><strong>Step 1: 构造 Few-shot 输入</strong></p>
<pre class="text"><code>Classify the sentiment of the review.

Review: "The food was amazing and the service was excellent!"
Sentiment: Positive

Review: "Terrible experience. The room was dirty and staff was rude."
Sentiment: Negative

Review: "The hotel was okay, nothing special but not bad either."
Sentiment: Neutral

Review: "I absolutely loved this movie, the acting was superb!"
Sentiment:</code></pre>
<p><strong>Step 2: 模型处理输入</strong></p>
<p>GPT-3将整个输入视为一个token序列，通过自回归方式逐token处理。当它到达最后一行的”Sentiment:“时，它需要预测下一个token。</p>
<p><strong>Step 3: 模型内部发生了什么？</strong></p>
<p>在传统的语言模型视角下，模型只是在”续写”一段文本。但在这个特定的上下文中，模型观察到了一个清晰的模式：</p>
<ul>
<li>每段”Review: … Sentiment: …“构成一个样本</li>
<li>前三个样本建立了”正面评价→Positive，负面→Negative，中性→Neutral”的映射</li>
<li>最后一个Review的内容（“absolutely loved”, “superb”）强烈暗示正面情感</li>
</ul>
<p><strong>Step 4: 模型输出</strong></p>
<p>模型生成的下一个token（以最高概率）是”Positive”。</p>
<p><strong>解读</strong>：从外部观察，GPT-3”学会了”做情感分类——它正确地将”I absolutely loved this movie”分类为Positive。但在模型内部，<strong>没有发生任何参数更新</strong>。模型所做的一切就是：阅读输入文本，识别出其中的任务模式，然后按照这个模式生成最合理的续写。</p>
<p>这个例子揭示了ICL的一个深刻特性：<strong>它模糊了”学习”和”推理”的边界</strong>。模型到底是在”学习”一个新任务，还是在”推理”出给定上下文中最合理的输出？这个问题至今仍是活跃的研究话题。</p>
</section>
</section>
<section id="标志性实验结果" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="标志性实验结果"><span class="header-section-number">4.4</span> 标志性实验结果</h3>
<p>让我们看几个具体的benchmark结果，以量化ICL的效果。</p>
<p><strong>LAMBADA</strong>（最后一词预测任务）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>设定</th>
<th>准确率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3 175B</td>
<td>Zero-shot</td>
<td>76.2%</td>
</tr>
<tr class="even">
<td>GPT-3 175B</td>
<td>Few-shot</td>
<td><strong>86.4%</strong></td>
</tr>
<tr class="odd">
<td>之前SOTA（微调模型）</td>
<td>有监督</td>
<td>68.0%</td>
</tr>
</tbody>
</table>
<p>GPT-3的few-shot结果不仅超越了之前所有微调模型的SOTA，甚至zero-shot性能就已经大幅领先。这是ICL最令人印象深刻的展示之一。</p>
<p><strong>TriviaQA</strong>（开放域问答）：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>设定</th>
<th>准确率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3 175B</td>
<td>Zero-shot</td>
<td>64.3%</td>
</tr>
<tr class="even">
<td>GPT-3 175B</td>
<td>One-shot</td>
<td>68.0%</td>
</tr>
<tr class="odd">
<td>GPT-3 175B</td>
<td>Few-shot</td>
<td><strong>71.2%</strong></td>
</tr>
<tr class="even">
<td>微调的T5-11B+SSM</td>
<td>有监督</td>
<td>60.5%</td>
</tr>
</tbody>
</table>
<p>在TriviaQA上，GPT-3的few-shot结果也超越了精心微调的T5-11B。这里需要注意的是，GPT-3使用的是闭卷设置（不访问外部知识库），完全依靠参数中存储的知识来回答问题。</p>
<p><strong>翻译</strong>（En→Fr, En→De, En→Ro）：</p>
<p>GPT-3在英法翻译上达到了接近有监督SOTA的水平（few-shot BLEU 32.6 vs 有监督SOTA 35.0），这一点格外值得注意——GPT-3的训练数据中非英语文本只占约7%，但它仍然”学会了”翻译。</p>
<p>然而，GPT-3在某些任务上的表现并不令人满意。在自然语言推理（NLI）任务如 SuperGLUE 的 RTE 上，GPT-3 few-shot 只达到约 72%，而微调的BERT已经超过 85%。在需要精确理解句子间语义关系的任务上，ICL的局限性开始显现。</p>
<div id="fig-gpt3-icl-curves" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt3-icl-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-20/original/fig2-gpt3-icl-learning-curves.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpt3-icl-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: GPT-3在不同规模下的ICL学习曲线。更大的模型不仅绝对性能更高，还展示出更陡峭的”学习曲线”——即从上下文示例中获益更多。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Brown et al.&nbsp;(2020) “Language Models are Few-Shot Learners”, Figure 1.2</em></p>
</div>
</section>
<section id="icl的规模效应量化越大越好" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="icl的规模效应量化越大越好"><span class="header-section-number">4.5</span> ICL的规模效应：量化”越大越好”</h3>
<p><a href="#fig-gpt3-icl-curves" class="quarto-xref">Figure&nbsp;4</a> 展示了一个精巧的实验：在一个简单的符号操作任务上（从单词中去除随机符号），不同规模的模型如何利用上下文示例。图中最引人注目的发现是：<strong>大模型的ICL学习曲线显著更陡</strong>。</p>
<p>这意味着什么？小模型即使给了100个示例，性能提升也很有限——它们缺乏从示例中”提取规则”的能力。而175B模型只需要10-15个示例就能接近完美表现。这不是因为大模型”记住了更多”，而是因为它们具备了一种更高层次的能力：<strong>从少量示例中归纳规则</strong>。</p>
<p>我们可以将不同规模下的ICL能力做一个定性总结：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>规模</th>
<th>Zero-shot</th>
<th>Few-shot</th>
<th>ICL行为</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>~125M</td>
<td>接近随机</td>
<td>几乎无提升</td>
<td>无法有效利用上下文</td>
</tr>
<tr class="even">
<td>~1.3B</td>
<td>略好于随机</td>
<td>有一定提升</td>
<td>能识别简单模式</td>
</tr>
<tr class="odd">
<td>~13B</td>
<td>有意义的性能</td>
<td>显著提升</td>
<td>能执行中等复杂度任务</td>
</tr>
<tr class="even">
<td>~175B</td>
<td>接近SOTA</td>
<td>经常超越微调</td>
<td>能从示例推断复杂规则</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="gpt-3-apillm即服务的诞生" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="gpt-3-apillm即服务的诞生"><span class="header-section-number">5.1</span> GPT-3 API：LLM即服务的诞生</h3>
<p>2020年6月，在论文发表后不久，OpenAI发布了GPT-3的API——这是历史上第一个大语言模型的商业化接口。用户不需要下载模型、不需要GPU，只需要发送一个HTTP请求，就能使用175B参数的语言模型。</p>
<p>这个商业决策的影响远超技术层面。它催生了一种全新的软件开发范式：<strong>Prompt Programming</strong>（提示编程）。开发者不再需要收集数据、训练模型、部署模型，而是通过设计合适的prompt来完成任务。一个人、一台笔记本、一个API key，就能构建之前需要整个ML团队才能开发的功能。</p>
</section>
<section id="icl推理流程" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="icl推理流程"><span class="header-section-number">5.2</span> ICL推理流程</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: Few-Shot In-Context Learning Inference（改编自 Brown et al., 2020）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Input:  预训练语言模型 LM，任务描述 T，
        K 个示例 {(x₁,y₁), ..., (xₖ,yₖ)}，查询输入 x_query
Output: 预测标签 ŷ

1. 构造 prompt：
     prompt ← T + "\n\n"
     for k = 1 to K:
         prompt ← prompt + format(xₖ, yₖ) + "\n\n"
     prompt ← prompt + format_query(x_query)

2. 前向推理（无梯度更新）：
     token_probs ← LM.forward(prompt)

3. 提取预测：
     ŷ ← argmax token_probs  # 或采样

4. return ŷ</code></pre>
<p><em>改编自 Brown et al.&nbsp;(2020) “Language Models are Few-Shot Learners”, Section 2.1。注意：整个过程没有任何参数更新，所有”学习”都编码在 prompt 的构造中。</em></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 2: Contextual Calibration（Zhao et al., 2021）
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Input:  语言模型 LM，prompt 模板 P，
        标签空间 Y = {y₁, ..., yₘ}，查询输入 x_query
Output: 校准后的预测 ŷ

1. 测量先验偏差：
     p̂_bias ← LM(P + "N/A")   # 用空内容输入测量偏差
     W ← diag(p̂_bias)⁻¹        # 构造校准矩阵

2. 对查询进行推理：
     p̂_raw ← LM(P + x_query)   # 原始预测概率

3. 校准：
     p̂_cal ← normalize(W · p̂_raw)  # 减去先验偏差

4. return ŷ ← argmax p̂_cal</code></pre>
<p><em>Source: Zhao et al.&nbsp;(2021) “Calibrate Before Use: Improving Few-Shot Performance of Language Models”, Section 3. <a href="https://arxiv.org/abs/2102.09690">arXiv:2102.09690</a></em></p>
</div>
</div>
</section>
<section id="prompt设计的核心原则" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="prompt设计的核心原则"><span class="header-section-number">5.3</span> Prompt设计的核心原则</h3>
<p>ICL的效果高度依赖于prompt的设计。以下是从GPT-3论文和后续研究中总结的关键原则：</p>
<p><strong>原则1：格式一致性</strong></p>
<p>Few-shot示例应该保持一致的格式。模型从示例中学习的不仅是任务内容，更重要的是<strong>输入-输出的映射格式</strong>。</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 好的 prompt：格式一致</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of France?</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">A: Paris</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of Japan?</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="st">A: Tokyo</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of Brazil?</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="st">A: """</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 差的 prompt：格式不一致</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="st">The capital of France is Paris.</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of Japan?</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="st">Answer: Tokyo</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="st">What about Brazil's capital?</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>原则2：示例的代表性</strong></p>
<p>选择的示例应该覆盖任务的不同方面：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 情感分类：覆盖正面、负面、中性</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Great product!"</span>, <span class="st">"Positive"</span>),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Terrible quality."</span>, <span class="st">"Negative"</span>),</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"It's okay."</span>, <span class="st">"Neutral"</span>),</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>原则3：任务描述的清晰性</strong></p>
<p>一个清晰的任务描述（task instruction）可以显著提升zero-shot性能：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 有明确任务描述</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Classify the following movie review as Positive or Negative.</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 无任务描述（仅靠示例推断）</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">""</span>  <span class="co"># 直接给示例</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="一个完整的few-shot实现" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="一个完整的few-shot实现"><span class="header-section-number">5.4</span> 一个完整的Few-shot实现</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> few_shot_classify(text, examples, task_desc, model<span class="op">=</span><span class="st">"gpt-3"</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    使用 Few-shot ICL 进行文本分类。</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">        text: 待分类的文本</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">        examples: list of (input, label) 示例对</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">        task_desc: 任务描述字符串</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">        model: 模型名称</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">        模型预测的标签</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构造 prompt</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> task_desc <span class="op">+</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 添加 few-shot 示例</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inp, label <span class="kw">in</span> examples:</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"Input: </span><span class="sc">{</span>inp<span class="sc">}</span><span class="ch">\n</span><span class="ss">Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 添加待分类样本</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Input: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ch">\n</span><span class="ss">Label:"</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 调用模型（生成 1 个 token 即可）</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.Completion.create(</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        prompt<span class="op">=</span>prompt,</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,  <span class="co"># 贪心解码，确保确定性</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        stop<span class="op">=</span>[<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>]      <span class="co"># 遇到换行停止</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].text.strip()</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"The food was delicious!"</span>, <span class="st">"Positive"</span>),</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Worst service ever."</span>, <span class="st">"Negative"</span>),</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"It was fine, nothing special."</span>, <span class="st">"Neutral"</span>),</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> few_shot_classify(</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span><span class="st">"I absolutely loved the atmosphere!"</span>,</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>    examples<span class="op">=</span>examples,</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    task_desc<span class="op">=</span><span class="st">"Classify the sentiment of the following review."</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prediction: </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Prediction: Positive</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>这段代码虽然简单，但它包含了ICL的全部核心逻辑：构造一个包含任务描述和示例的prompt，将其送入模型，从生成的文本中提取答案。没有训练循环、没有损失函数、没有梯度计算——“学习”完全发生在输入构造阶段。</p>
</section>
<section id="复现细节与工程注意事项" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="复现细节与工程注意事项"><span class="header-section-number">5.5</span> 复现细节与工程注意事项</h3>
<p><strong>Token预算管理</strong>：GPT-3的上下文窗口只有2048个token，few-shot示例占据的token越多，留给输入文本和输出的空间就越少。在实践中需要在”示例数量”和”输入长度”之间权衡：</p>
<p><span class="math display">\[
n_{\text{ctx}} = n_{\text{instruction}} + K \cdot n_{\text{example}} + n_{\text{input}} + n_{\text{output}}
\]</span></p>
<p>其中 <span class="math inline">\(K\)</span> 是示例数量。如果每个示例平均占50个token，任务描述占20个token，那么在2048的窗口下，即使留200个token给输入和输出，也最多只能放约36个示例。这解释了为什么GPT-3论文中的few-shot通常只用10-100个示例。</p>
<p><strong>Temperature设置</strong>：对于分类等确定性任务，应使用 <span class="math inline">\(\text{temperature} = 0\)</span>（等价于贪心解码）；对于创意生成任务，可以使用 <span class="math inline">\(0.7 - 1.0\)</span> 的温度。</p>
<p><strong>Stop tokens</strong>：在分类任务中，设置合适的停止token（如换行符<code>\n</code>）可以防止模型生成多余的文本。</p>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨In-Context Learning的理论机制、边界条件和开放问题。</p>
</blockquote>
<section id="为什么有效理论视角" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</h3>
<p>In-Context Learning为什么能work？这是一个看似简单、实则深刻的问题。一个仅被训练来预测下一个词的模型，为什么能从输入中的几个示例”学会”新任务？自2020年GPT-3论文发表以来，多个研究团队从不同角度提出了理论解释。</p>
<section id="理论1icl作为隐式贝叶斯推断" class="level4" data-number="6.1.1">
<h4 data-number="6.1.1" class="anchored" data-anchor-id="理论1icl作为隐式贝叶斯推断"><span class="header-section-number">6.1.1</span> 理论1：ICL作为隐式贝叶斯推断</h4>
<p>Xie et al.&nbsp;(2022) 提出了一个优雅的理论框架：In-Context Learning本质上是在做<strong>隐式贝叶斯推断</strong>（Implicit Bayesian Inference）。</p>
<p>核心思想是这样的：假设预训练数据来自多个”概念”（concept）的混合分布。每个概念决定了文本的生成规则——比如”情感分类”是一个概念，“翻译”是另一个概念。当模型在预训练中学习了这个混合分布后，给它few-shot示例等价于给它观测数据，让它推断当前的”概念”是什么。</p>
<p>形式化地，设 <span class="math inline">\(\theta\)</span> 表示潜在概念，<span class="math inline">\(x_{1:K}\)</span> 是few-shot示例，<span class="math inline">\(x_{\text{query}}\)</span> 是查询输入。模型的预测可以理解为：</p>
<p><span class="math display">\[
p(y \mid x_{\text{query}}, x_{1:K}) = \int p(y \mid x_{\text{query}}, \theta) \cdot p(\theta \mid x_{1:K}) \, d\theta
\]</span></p>
<p>其中 <span class="math inline">\(p(\theta \mid x_{1:K})\)</span> 是模型从示例中推断出的概念后验分布。更多的示例提供更多”证据”，使后验更加集中，从而提高预测准确性。</p>
<p>这个理论的美妙之处在于它解释了两个实验现象：第一，为什么更多示例能提高性能（更多证据→更好的后验）；第二，为什么示例的标签可能不那么重要（示例的主要作用是帮助推断概念/任务，而非学习输入-输出映射）。</p>
</section>
<section id="理论2icl作为隐式梯度下降" class="level4" data-number="6.1.2">
<h4 data-number="6.1.2" class="anchored" data-anchor-id="理论2icl作为隐式梯度下降"><span class="header-section-number">6.1.2</span> 理论2：ICL作为隐式梯度下降</h4>
<p>Dai et al.&nbsp;(2023) 提出了另一个令人惊讶的视角：Transformer的注意力机制在处理few-shot示例时，其行为<strong>类似于梯度下降</strong>。</p>
<p>他们的核心观察是：考虑一个线性注意力层 <span class="math inline">\(f(q) = W_V X^\top \cdot \text{softmax}(X W_K^\top q)\)</span>。当few-shot示例被加入上下文时，这等价于对一个线性模型做了一步梯度更新。具体地，他们证明了在某些简化假设下：</p>
<p><span class="math display">\[
\text{ICL}_{\text{Attention}}(x_{\text{query}}) \approx \text{ICL}_{\text{GD}}(x_{\text{query}}) = W_0 x_{\text{query}} - \eta \nabla_{W} \mathcal{L}(W_0) \cdot x_{\text{query}}
\]</span></p>
<p>其中 <span class="math inline">\(W_0\)</span> 是预训练权重，<span class="math inline">\(\mathcal{L}\)</span> 是基于few-shot示例的损失函数，<span class="math inline">\(\eta\)</span> 是隐式”学习率”。</p>
<p>换言之，Transformer的一层注意力计算等价于一步梯度下降——前向传播中隐含了一个微型的”训练过程”。这个理论的力量在于它将ICL和传统的基于梯度的学习统一到了同一个框架下。</p>
</section>
<section id="理论3归纳头induction-heads" class="level4" data-number="6.1.3">
<h4 data-number="6.1.3" class="anchored" data-anchor-id="理论3归纳头induction-heads"><span class="header-section-number">6.1.3</span> 理论3：归纳头（Induction Heads）</h4>
<p>Olsson et al.&nbsp;(2022, Anthropic) 从更微观的层面发现了ICL的一种具体机制：<strong>归纳头</strong>（Induction Heads）。归纳头是一种由两个注意力头协作形成的电路，其功能是：如果在上下文中出现了模式 “[A][B] … [A]”，归纳头会预测下一个token是 “[B]”。</p>
<p>这看起来只是一个简单的”复制”模式，但它是ICL的基础。当few-shot示例中包含 “positive → Positive” 的映射，而查询也是一个positive的文本时，归纳头帮助模型”复制”对应的标签。</p>
<p>Olsson et al.&nbsp;的关键发现是：</p>
<ol type="1">
<li>归纳头在训练过程中会突然出现（phase transition），这与ICL能力的突然涌现一致。</li>
<li>消除归纳头会导致ICL能力大幅下降。</li>
<li>归纳头的形成与模型规模有关——更大的模型能形成更复杂的归纳头。</li>
</ol>
</section>
</section>
<section id="为什么有效实证视角" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="为什么有效实证视角"><span class="header-section-number">6.2</span> 为什么有效？——实证视角</h3>
<p>Min et al.&nbsp;(2022) 的工作提供了关于ICL的最反直觉的实证发现之一。他们系统地测试了few-shot示例中各个成分的重要性：</p>
<p><strong>实验</strong>：在情感分类任务中，将few-shot示例的标签<strong>随机替换</strong>（正面文本标注为”Negative”，反面文本标注为”Positive”），然后观察模型性能的变化。</p>
<p><strong>结果</strong>：令人震惊的是，<strong>使用随机标签的few-shot性能与使用正确标签几乎相同</strong>。</p>
<p>这意味着什么？模型从few-shot示例中”学到”的主要不是输入-输出的映射（这是我们直觉上认为的），而是：</p>
<ol type="1">
<li><strong>输入的分布</strong>：什么样的文本是有效输入</li>
<li><strong>输出的格式和空间</strong>：标签应该是”Positive”或”Negative”（而不是自由文本）</li>
<li><strong>输入-输出的映射模式</strong>：一个输入对应一个标签（而不是一段解释）</li>
</ol>
<p>这个发现与Xie et al.&nbsp;的贝叶斯推断理论高度一致：few-shot示例的主要作用是帮助模型<strong>锁定任务类型</strong>（推断概念 <span class="math inline">\(\theta\)</span>），而非学习具体的映射规则。</p>
<p>然而，需要注意的是，这个结论并非在所有任务和规模上都成立。在某些需要学习新映射的任务上（如将非标准标签映射到类别），正确标签确实重要。这个发现的价值在于它迫使我们重新思考ICL”学到了什么”。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</h3>
<p>ICL并非万能的。以下是其已知的失效条件和局限：</p>
<p><strong>失效条件1：任务复杂度超过模型容量</strong>。ICL在需要多步推理的任务上表现不佳。例如，多位数加法（如 “1234 + 5678 = ?”）即使给了很多示例，GPT-3也经常出错。这表明ICL能做的”学习”有其复杂度上限。这个局限直接催生了下一章将讨论的Chain-of-Thought。</p>
<p><strong>失效条件2：任务需要精确的格式对齐</strong>。如果目标任务的输入-输出格式与预训练数据中常见的模式差异很大，ICL效果会显著下降。</p>
<p><strong>失效条件3：示例数量受限于上下文窗口</strong>。GPT-3只有2048个token的窗口，这严重限制了可以提供的示例数量。对于需要大量示例才能学会的复杂任务，ICL的表现会受到上下文长度的瓶颈限制。</p>
<p><strong>失效条件4：对示例选择和顺序高度敏感</strong>。这一点值得特别展开。</p>
</section>
<section id="icl的不稳定性问题" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="icl的不稳定性问题"><span class="header-section-number">6.4</span> ICL的不稳定性问题</h3>
<p>Zhao et al.&nbsp;(2021) 和 Lu et al.&nbsp;(2022) 系统地量化了ICL的不稳定性，发现了几个令人不安的现象。</p>
<p><strong>示例顺序敏感性</strong>：Zhao et al.&nbsp;(2021) 在 SST-2 情感分类任务上用 GPT-3 2.7B 做了一个触目惊心的实验——使用相同的4个few-shot示例，仅仅改变它们的<strong>排列顺序</strong>，准确率就从 <strong>54.3%（接近随机猜测）波动到 93.4%（接近SOTA）</strong>。更极端的是，在2-shot设置下，仅仅将两个示例的顺序<strong>对调</strong>，准确率就从 88.5% 骤降到 51.3%。Lu et al.&nbsp;(2022) 进一步证实，这种顺序敏感性在<strong>所有模型规模</strong>上都存在，增加模型大小或示例数量都<strong>无法消除</strong>这一问题。</p>
<p><strong>三种系统性偏差</strong>：Zhao et al.&nbsp;识别出了ICL中的三种系统性偏差来源。<strong>多数标签偏差</strong>（Majority Label Bias）：如果few-shot示例中某个标签出现得更频繁，模型会倾向于预测多数标签。<strong>近因偏差</strong>（Recency Bias）：模型倾向于预测与最后几个示例相同的标签。<strong>常见token偏差</strong>（Common Token Bias）：模型更倾向于预测在预训练数据中频繁出现的token（如预测”America”而非”Saint Lucia”）。</p>
<p>Zhao et al.&nbsp;(2021) 提出了一种名为<strong>Contextual Calibration</strong>的缓解方法：先用一个”空输入”（content-free input，如 “N/A”、“[MASK]” 或空字符串）来测量模型的先验偏差，然后将其从预测中减去。这种方法简单但效果显著——在某些任务上可以带来高达 <strong>30个百分点</strong> 的绝对提升。</p>
</section>
<section id="开放研究问题" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.5</span> 开放研究问题</h3>
<p>如果你要在这个方向写一篇论文，以下几个问题仍然值得深入探索：</p>
<p>第一个方向是<strong>ICL的机制理解</strong>。尽管已有贝叶斯推断、隐式梯度下降、归纳头等多种理论解释，但这些理论之间的关系尚不清楚。它们是同一现象的不同视角，还是描述了不同层次的机制？一个统一的理论框架仍然缺失。</p>
<p>第二个方向是<strong>ICL能力的涌现条件</strong>。为什么ICL需要如此大的模型规模才能出现？存在某个最小规模阈值吗？通过改进训练方法或架构设计，能否让更小的模型也获得强ICL能力？</p>
<p>第三个方向是<strong>ICL与微调的关系</strong>。ICL和微调是两种完全不同的”学习”吗？还是它们是同一连续体上的两个端点？如果ICL确实等价于隐式梯度下降，那它与显式微调的精确差异是什么？</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="本方法的局限" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="本方法的局限"><span class="header-section-number">7.1</span> 本方法的局限</h3>
<p>GPT-3和In-Context Learning虽然代表了NLP范式的重大转变，但它们面临着几个根本性的局限。</p>
<p>第一个局限是<strong>推理能力的缺失</strong>。ICL擅长的是模式匹配和知识检索，但在需要多步推理、逻辑推导或数学计算的任务上表现很差。给GPT-3一个需要四步推理的数学题，即使提供了类似问题的示例，它也经常在中间步骤出错。这不是因为模型”不够大”——即使是175B参数，直接的ICL也无法可靠地执行复杂推理。</p>
<p>第二个局限是<strong>不可控性</strong>。ICL的输出高度依赖prompt的措辞、示例的选择和顺序，而这些因素的影响难以预测和控制。在需要稳定、可靠输出的生产环境中，这种不可控性是一个严重问题。</p>
<p>第三个局限是<strong>计算成本</strong>。每次推理都需要处理整个prompt（包括所有few-shot示例），这意味着ICL的推理成本远高于微调模型。175B参数模型的单次推理就需要大量计算，再加上几十个示例的长prompt，成本进一步攀升。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.2</span> 这些局限导向了什么？</h3>
<p>ICL的推理缺陷引出了一个自然的问题：<strong>如果模型无法直接”跳到”答案，能否让它像人类一样展示中间推理步骤？</strong>如果给模型的示例中不仅包含最终答案，还包含详细的推理过程（“首先…，然后…，因此…”），模型能否学会这种”展示工作过程”的模式？</p>
<p>这正是下一章将讨论的 <strong>Chain-of-Thought (CoT) Prompting</strong> 的核心思想——它不是一种全新的能力，而是对ICL的一种巧妙利用：通过在few-shot示例中加入推理链条，引导模型也生成类似的中间推理步骤。CoT的出现进一步释放了大模型的潜力，但也引发了更深层的问题：LLM真的在”推理”吗？</p>
<blockquote class="blockquote">
<p>下一章预告：第21章将聚焦<strong>涌现能力与思维链推理</strong>。Chain-of-Thought如何让GPT-3级别的模型在数学和逻辑推理上获得飞跃式提升？涌现能力是真实的相变还是度量方式的假象？Zero-shot CoT的神奇咒语”Let’s think step by step”为什么有效？——这些问题将引导我们更深入地理解大语言模型的能力边界。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>本章围绕GPT-3和In-Context Learning，讲述了大语言模型从”基础设施”到”涌现能力”的关键跨越。</p>
<p>GPT-3（2020）以175B参数——96层Transformer，12288维隐藏状态，96个注意力头——成为当时最大的语言模型。它的架构本身并无太大创新（仍然是Decoder-only Transformer + 自回归语言建模），真正的突破在于规模带来的涌现：In-Context Learning。</p>
<p>In-Context Learning的核心发现是：足够大的语言模型不需要梯度更新就能从输入中的少量示例”学会”新任务。这种能力有三种模式——zero-shot（仅描述任务）、one-shot（一个示例）和few-shot（多个示例）——它们的效果与模型规模呈超线性关系：大模型不仅绝对性能更高，还更擅长利用上下文示例。</p>
<p>关于ICL为什么work，目前有三种主要的理论解释：隐式贝叶斯推断（Xie et al.，模型从示例推断潜在概念）、隐式梯度下降（Dai et al.，注意力计算等价于一步梯度更新）、以及归纳头机制（Olsson et al.，特定的注意力头模式实现了模式匹配）。Min et al.的实验进一步表明，ICL的主要作用可能是帮助模型”锁定任务类型”，而非学习具体的输入-输出映射。</p>
<p>然而，ICL也有明显的局限：对prompt设计高度敏感（示例顺序可导致30%到90%的准确率波动）、无法可靠执行多步推理、计算成本高昂。</p>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 62%">
</colgroup>
<thead>
<tr class="header">
<th>内容</th>
<th>公式/数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3 参数量</td>
<td><span class="math inline">\(N = 175 \times 10^9\)</span></td>
</tr>
<tr class="even">
<td>总计算量</td>
<td><span class="math inline">\(C \approx 3.14 \times 10^{23}\)</span> FLOPs</td>
</tr>
<tr class="odd">
<td>上下文窗口</td>
<td><span class="math inline">\(n_{\text{ctx}} = 2048\)</span> tokens</td>
</tr>
<tr class="even">
<td>Token预算约束</td>
<td><span class="math inline">\(n_{\text{ctx}} = n_{\text{instr}} + K \cdot n_{\text{example}} + n_{\text{input}} + n_{\text{output}}\)</span></td>
</tr>
<tr class="odd">
<td>ICL贝叶斯推断</td>
<td><span class="math inline">\(p(y \mid x_q, x_{1:K}) = \int p(y \mid x_q, \theta) \cdot p(\theta \mid x_{1:K}) d\theta\)</span></td>
</tr>
<tr class="even">
<td>ICL隐式梯度下降</td>
<td><span class="math inline">\(\text{ICL}(x_q) \approx W_0 x_q - \eta \nabla_W \mathcal{L}(W_0) \cdot x_q\)</span></td>
</tr>
<tr class="odd">
<td>按Chinchilla法则的最优数据量</td>
<td><span class="math inline">\(D^* = 20 \times 175B = 3.5T\)</span> tokens（而GPT-3仅用300B）</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么GPT-3的few-shot性能与模型规模之间是”超线性”关系（即大模型从示例中获益更多），而非简单的线性关系？提示：考虑ICL需要什么样的”元能力”，以及这种能力与模型容量的关系。</p></li>
<li><p><strong>[数学推导]</strong> 根据Dai et al.的理论，线性注意力层处理few-shot示例等价于一步梯度下降。请推导：对于一个线性模型 <span class="math inline">\(y = Wx\)</span>，如果使用MSE损失 <span class="math inline">\(\mathcal{L} = \frac{1}{K}\sum_{k=1}^K \|Wx_k - y_k\|^2\)</span>，一步梯度下降后的权重更新 <span class="math inline">\(W' = W - \eta \nabla_W \mathcal{L}\)</span> 的显式形式是什么？它如何对应到注意力机制的Key-Value结构？</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用OpenAI API（或其他LLM API），在SST-2情感分类数据集上比较以下设置的准确率：(a) zero-shot，(b) 4-shot随机示例，(c) 4-shot但标签随机打乱，(d) 4-shot相同示例但不同顺序（至少5种排列）。你的结果是否验证了Min et al.和Lu et al.的发现？</p></li>
<li><p><strong>[研究思考]</strong> ICL的三种理论解释（贝叶斯推断、隐式梯度下降、归纳头）各自有什么假设和局限？它们是互斥的还是互补的？如果你要设计实验来区分这三种理论，你会怎么做？</p></li>
<li><p><strong>[开放思考]</strong> GPT-3展示了”规模带来涌现”的现象，但这是否意味着”只要足够大就能解决一切”？考虑以下反例：(a) GPT-3在简单算术上的失败，(b) ICL的不稳定性不随规模完全消失，(c) 某些任务（如精确推理）可能需要根本不同的方法。你认为ICL的能力边界在哪里？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Brown, T. et al.&nbsp;(2020). “Language Models are Few-Shot Learners”</strong>。GPT-3原始论文，本章最核心的参考。重点阅读：Section 2（zero/one/few-shot的精确定义）、Section 3.1-3.5（关键benchmark结果）、Figure 1.2-1.3（ICL学习曲线和聚合性能）、Figure 2.1（ICL范式图）。可快速浏览：Section 4（数据污染分析）、Appendix中的详细实验表格。注意：论文长达75页，其中大量是实验结果，核心内容集中在前20页。<a href="https://arxiv.org/abs/2005.14165">arXiv:2005.14165</a></p>
</section>
<section id="理论基础" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">9.2</span> 理论基础</h3>
<p><strong>Xie, S. et al.&nbsp;(2022). “An Explanation of In-context Learning as Implicit Bayesian Inference”</strong>。将ICL理解为贝叶斯推断的优雅理论框架。重点阅读：Section 3（理论推导）。<a href="https://arxiv.org/abs/2111.02080">arXiv:2111.02080</a></p>
<p><strong>Dai, D. et al.&nbsp;(2023). “Why Can GPT Learn In-Context?”</strong>。证明ICL等价于隐式梯度下降。重点阅读：Section 3（双重形式对比）。<a href="https://arxiv.org/abs/2212.10559">arXiv:2212.10559</a></p>
<p><strong>Olsson, C. et al.&nbsp;(2022). “In-context Learning and Induction Heads”</strong>。Anthropic的研究，发现了ICL的微观机制。重点阅读：Section 2-3（归纳头的定义和实验验证）。<a href="https://arxiv.org/abs/2209.11895">arXiv:2209.11895</a></p>
<p><strong>von Oswald, J. et al.&nbsp;(2023). “Transformers Learn In-Context by Gradient Descent”</strong>。严格证明单层线性自注意力可以实现一步梯度下降，N层可近似N步GD。与Dai et al.互补，提供了更形式化的理论保证。ICML 2023。<a href="https://arxiv.org/abs/2212.07677">arXiv:2212.07677</a></p>
</section>
<section id="后续发展" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.3</span> 后续发展</h3>
<p><strong>Min, S. et al.&nbsp;(2022). “Rethinking the Role of Demonstrations”</strong>。反直觉发现：ICL中标签可能不如格式重要。<a href="https://arxiv.org/abs/2202.12837">arXiv:2202.12837</a></p>
<p><strong>Zhao, Z. et al.&nbsp;(2021). “Calibrate Before Use”</strong>。系统分析ICL不稳定性并提出校准方法。<a href="https://arxiv.org/abs/2102.09690">arXiv:2102.09690</a></p>
<p><strong>Lu, Y. et al.&nbsp;(2022). “Fantastically Ordered Prompts and Where to Find Them”</strong>。量化了示例顺序对ICL的影响。<a href="https://arxiv.org/abs/2104.08786">arXiv:2104.08786</a></p>
<p><strong>Wei, J. et al.&nbsp;(2022). “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”</strong>。下一章的核心论文，展示了如何通过ICL的变体解决推理问题。<a href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a></p>
</section>
<section id="综述与教程" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">9.4</span> 综述与教程</h3>
<p><strong>Dong, Q. et al.&nbsp;(2023). “A Survey on In-Context Learning”</strong>。ICL领域最全面的综述。<a href="https://arxiv.org/abs/2301.00234">arXiv:2301.00234</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.5</span> 代码资源</h3>
<ul>
<li><strong>OpenAI API文档</strong>：<a href="https://platform.openai.com/docs">platform.openai.com/docs</a>（ICL的主要使用方式）</li>
<li><strong>HELM</strong>（Holistic Evaluation of Language Models）：<a href="https://crfm.stanford.edu/helm/">crfm.stanford.edu/helm</a>（系统性评估LLM的ICL能力）</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>GPT-3的发布是AI历史上一个标志性的时刻，其影响远超技术领域。</p>
<p>2020年5月28日，Brown et al.&nbsp;的论文在arXiv上发布，迅速引发了学术界的轰动。仅仅两周后的6月11日，OpenAI就宣布了GPT-3的API——这个速度本身就说明了论文和商业化是同步推进的。训练一次GPT-3 175B估计需要约355个GPU年的V100计算时间，成本约为<strong>460万美元</strong>。在此之前，使用大语言模型需要这种级别的投入和深厚的ML工程能力；API的出现让任何一个会写Python的人都能以每1000个token几美分的价格使用175B参数的模型。这催生了一波”prompt programming”的浪潮——Twitter上充斥着人们用GPT-3做各种令人惊叹（或令人担忧）的事情的演示：写代码、写诗、模拟对话、生成法律文书……</p>
<p>一个有趣的细节是：GPT-3论文有31位作者，但只有一小部分人参与了核心的模型训练和实验。论文中的许多benchmark实验实际上是由不同的小组分别完成的，这反映了大模型研究日益工业化的趋势——不再是一两个研究者的工作，而是需要整个团队的协作。</p>
<p>GPT-3也标志着一个概念的诞生：<strong>基础模型</strong>（Foundation Model）。2021年，Stanford HAI发布了著名的”基础模型”报告，其核心论点正是基于GPT-3的示范：一个足够大的预训练模型可以通过ICL适配几乎任何下游任务，因此它不再是”一个模型解决一个问题”，而是”一个基础模型支撑一整个应用生态”。这个概念深刻地重塑了AI行业的商业模式和研究方向。</p>
<p>讽刺的是，GPT-3虽然以”Few-Shot Learners”为标题强调了ICL，但后来最成功的GPT系列产品（ChatGPT、GPT-4）反而不是主要依靠ICL来使用的——它们通过指令微调（Instruction Tuning）和RLHF让模型学会了直接理解用户意图，不再需要few-shot示例。从这个角度看，GPT-3发现的ICL能力更像是一个<strong>过渡性的里程碑</strong>：它证明了大模型具有惊人的潜力，但释放这种潜力的最终方式不是ICL，而是更精细的对齐技术——这正是第23-25章将要讲述的故事。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第20章：GPT-3与In-Context Learning"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "When Scale Brings Emergence: The Birth of In-Context Learning"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, GPT-3, In-Context Learning]</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [GPT-3, In-Context Learning, Few-Shot, Zero-Shot, Prompt Engineering, 涌现能力, 大语言模型, Brown, OpenAI]</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "前三章（Ch17–Ch19）回答了'如何训练大模型'的理论和工程问题——Scaling Laws告诉我们规模与性能的数学关系，训练稳定性技术让百亿参数的训练不崩溃，分布式系统让千亿参数模型在万卡集群上跑起来。但一个更本质的问题悬而未决：这些大模型到底能做什么'小模型做不到的事'？2020年，OpenAI的GPT-3用175B参数给出了一个令人震惊的答案——In-Context Learning：不需要任何梯度更新，仅通过在输入中提供几个示例，模型就能学会新任务。这种能力不是被训练出来的，而是在规模达到一定阈值后自发涌现的。本章系统讲述GPT-3的架构、训练、ICL的现象与机制，以及由此诞生的Prompt Engineering范式。"</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-20/original/fig1-gpt3-zero-one-few-shot.png"</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：当语言模型的规模从十亿增长到千亿参数时，是否会涌现出小模型不具备的全新能力？In-Context Learning——不经过梯度更新就能从少量示例中学会新任务——是如何发生的？</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2020 </span><span class="pp">|</span><span class="at"> Brown et al. "Language Models are Few-Shot Learners" (GPT-3) </span><span class="pp">|</span><span class="at"> 从规模到涌现、从微调到提示</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Brown et al. (2020)** "Language Models are Few-Shot Learners" (arXiv:2005.14165, GPT-3) — 参考了 Section 2（Approach: zero/one/few-shot 定义）、Section 3（全部42个benchmark实验结果）、Figure 2.1（ICL范式对比图）、Figure 1.2（ICL学习曲线）、Figure 1.3（聚合性能随规模的变化）、Table 2.1（8个模型规模配置）；提取了 5 张论文原图</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Xie et al. (2022)** "An Explanation of In-context Learning as Implicit Bayesian Inference" (arXiv:2111.02080) — 参考了 ICL 作为隐式贝叶斯推断的理论框架</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dai et al. (2023)** "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers" (arXiv:2212.10559) — 参考了 ICL 与隐式梯度下降的等价性分析</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Olsson et al. (2022)** "In-context Learning and Induction Heads" (arXiv:2209.11895, Anthropic) — 参考了归纳头（Induction Heads）作为 ICL 机制单元的发现</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Min et al. (2022)** "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?" (arXiv:2202.12837) — 参考了"标签可能不重要，格式才重要"的反直觉发现</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zhao et al. (2021)** "Calibrate Before Use: Improving Few-Shot Performance of Language Models" (arXiv:2102.09690) — 参考了 ICL 的不稳定性分析（对示例选择、顺序、格式的敏感性）</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lu et al. (2022)** "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity" (arXiv:2104.08786) — 参考了示例顺序敏感性的量化分析</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SLP3 Chapter 7 (Large Language Models) — 参考了 GPT-3 架构、In-Context Learning 定义和 scaling 讨论</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>D2L Chapter 15 — 参考了预训练语言模型的教学组织</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stanford CS224N Lecture 10-11 (Winter 2025) — 参考了 GPT-3 和 In-Context Learning 的教学框架</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CMU 11-711 ANLP Lecture "Prompting &amp; In-context Learning" (Fall 2024) — 参考了 Prompting 技术的系统化讲解</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>前三章我们系统地解决了大语言模型训练的三大基础问题。第17章揭示了规模与性能之间的幂律关系，Scaling Laws 将大模型训练从炼金术变成了可预测的工程科学。第18章讨论了如何在百亿参数的规模下保持训练稳定——从 Adam 到 BF16，从 warmup 到梯度裁剪，这些数值工程技巧构成了大模型训练的"安全网"。第19章则解决了最根本的硬件约束：通过数据并行、张量并行、流水线并行和 ZeRO 内存优化的组合，让千亿参数的模型能在成千上万张 GPU 上训练起来。</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>至此，"如何训练一个超大模型"的问题已经有了完整的答案。但正如第19章结尾所提到的：分布式训练解决了工程问题，但训练出来的大模型是否真的比小模型更有用？规模的增长带来的是量变还是质变？</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>这个问题的答案比任何人预期的都更加戏剧性。2020年5月，OpenAI发表了GPT-3论文——一个拥有1750亿参数的语言模型。GPT-3不仅在困惑度上符合Scaling Laws的预测（量变），它还展示了一种小模型完全不具备的全新能力：**In-Context Learning**（上下文学习）。你不需要对模型做任何微调，不需要更新一个参数，只需要在输入中给出几个任务示例，GPT-3就能"学会"执行这个任务——翻译、摘要、问答、甚至简单的算术。</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>这种能力不是被特意训练出来的。OpenAI只是训练了一个更大的语言模型来预测下一个词——和GPT-2做的事情完全一样。但当模型规模跨过某个阈值时，一种全新的能力似乎"自发涌现"了。这个发现震动了整个AI领域，因为它暗示着一种可能性：或许通过不断扩大规模，语言模型能够获得越来越多我们事先未预料到的能力。</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：GPT-3的核心贡献不是"更大的语言模型"（那只是Scaling Laws的延续），而是发现了 **In-Context Learning**——一种无需梯度更新、仅通过输入示例就能完成新任务的能力。这种能力随规模增长而增强，改变了NLP的使用范式：从"预训练+微调"转向"预训练+提示"（prompting）。</span></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从"微调范式"的局限说起</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>在GPT-3出现之前，使用预训练语言模型的标准流程是**预训练+微调**（pre-train then fine-tune）。第13章讲述的BERT就是这个范式的代表：先在大规模语料上做掩码语言模型预训练，然后为每个下游任务收集标注数据、添加任务特定的分类头、更新全部参数。</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>这个范式虽然有效，但存在三个根本性的限制。</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>第一个限制是**任务专用性**。每个任务都需要一个独立微调的模型。如果你有10个不同的NLP任务，就需要训练、部署和维护10个不同的模型。当模型规模达到百亿或千亿参数时，这意味着你需要存储数十个TB级别的模型权重——光是存储成本就令人望而却步。</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>第二个限制是**对标注数据的依赖**。微调需要下游任务的标注数据，通常至少数千到数万条。对于很多实际任务（如特定领域的信息抽取、新语言的翻译），获取高质量标注数据既昂贵又耗时。更尴尬的是，标注数据的获取速度远远跟不上人们对AI应用的需求。</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>第三个限制更加微妙：**微调可能损害模型的通用能力**。当你在一个小数据集上微调一个大模型时，模型往往会"忘记"预训练阶段学到的很多知识——这被称为灾难性遗忘（catastrophic forgetting）。微调后的BERT在目标任务上表现很好，但它不再是一个"通用的语言理解系统"。</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>这些限制共同指向一个更深层的问题：**我们能否找到一种使用方式，让一个模型不经修改就能适应各种任务？**</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个理想中的"万能模型"</span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>让我们想象一下理想的场景：你有一个经过大规模预训练的语言模型，你想用它来做情感分析。在微调范式下，你需要收集情感标注数据、设计分类头、训练几个epoch、调参、评估。但如果模型足够"聪明"，你能不能直接告诉它："这是一个情感分类任务。'这部电影太棒了' → 正面。'演技很烂' → 负面。现在请分类：'剧情引人入胜'"——然后模型直接给出答案？</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>这正是In-Context Learning的核心思想：**将任务说明和示例直接放在模型的输入中，让模型通过"阅读"这些信息来完成任务，而不是通过梯度更新来"学习"任务。**</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>但这听起来像是天方夜谭。一个仅仅被训练来预测"下一个词"的模型，怎么可能仅凭输入中的几个示例就"学会"一个全新的任务？要回答这个问题，我们首先需要理解GPT-3本身。</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键洞察：规模带来的不仅是"更好"，而是"不同"</span></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>GPT-3的核心洞察可以用一句话概括：**当语言模型足够大时，它不仅能更好地预测下一个词，还能从输入的上下文中"推断"出你想让它做什么任务，并直接执行。**</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>这不是一个连续的量变过程。GPT-2（1.5B参数）只能勉强做一些零样本任务，效果远不如微调模型。但GPT-3（175B参数）在很多任务上的few-shot表现已经接近甚至超过了微调的BERT。从1.5B到175B，参数量增加了约100倍，但ICL能力的提升远超100倍——这更像是一种相变（phase transition），而非线性增长。</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>为什么规模会带来这种质变？一个直觉性的解释是：语言模型在预训练时看到的文本中，天然地包含了大量"从示例中学习"的模式。互联网上充满了这样的文本结构：</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="in">问：法国的首都是哪里？</span></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="in">答：巴黎。</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="in">问：日本的首都是哪里？</span></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="in">答：东京。</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a><span class="in">问：巴西的首都是哪里？</span></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="in">答：</span></span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>当模型在海量文本上训练时，它不仅学会了"东京是日本的首都"这样的知识，还学会了"如果前面有几个问答对，下一个应该按同样的格式回答"这种**元模式**（meta-pattern）。小模型可能只学到了浅层的文本模式（续写一段通顺的句子），而足够大的模型则学到了更深层的结构——包括"根据示例推断任务规则"的能力。</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### In-Context Learning 的三种模式</span></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>GPT-3论文定义了三种使用模式，从简单到复杂：</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>**Zero-shot**（零样本）：只给任务描述，不给任何示例。</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a><span class="in">Translate English to French:</span></span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a><span class="in">cheese =&gt;</span></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>模型需要仅凭"Translate English to French"这个指令来理解任务并执行。这要求模型在预训练中就已经"知道"翻译是什么，以及如何从英文翻译成法文。</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>**One-shot**（单样本）：给一个示例。</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a><span class="in">Translate English to French:</span></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a><span class="in">sea otter =&gt; loutre de mer</span></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a><span class="in">cheese =&gt;</span></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>一个示例帮助模型"锁定"了任务的格式（英文 =&gt; 法文）和风格。</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>**Few-shot**（少样本）：给几个到几十个示例。</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a><span class="in">Translate English to French:</span></span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a><span class="in">sea otter =&gt; loutre de mer</span></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a><span class="in">peppermint =&gt; menthe poivrée</span></span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a><span class="in">plush giraffe =&gt; girafe en peluche</span></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="in">cheese =&gt;</span></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a>更多的示例让模型更准确地理解任务边界和预期输出格式。</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a><span class="al">![GPT-3的四种任务范式对比：传统微调（需要梯度更新）vs Zero-shot / One-shot / Few-shot（仅需前向推理）。](figures/chapter-20/original/fig1-gpt3-zero-one-few-shot.png)</span>{#fig-gpt3-paradigms width=85%}</span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 2.1*</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a>@fig-gpt3-paradigms 是GPT-3论文中最具标志性的图之一。它清晰地展示了传统微调与ICL三种模式的根本区别：微调需要大量标注数据和梯度更新来调整模型参数，而零样本、单样本和少样本学习完全不修改模型——所有"学习"都发生在前向推理的输入中。这不仅是技术方法的差异，更是**使用范式**的根本转变。</span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个类比：从"培训员工"到"给指令"</span></span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>传统的微调范式就像是**培训一个新员工**：你给他教材（标注数据），让他练习（训练），经过几周的训练（epoch），他终于能胜任这个特定岗位。但如果你需要他做另一项工作，就得重新培训。</span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>In-Context Learning则像是**给一个经验丰富的通才下指令**：你不需要培训他，只需要告诉他"这是任务要求，这是几个例子"，他就能立刻理解并执行。他之所以能做到这一点，是因为他在过去的工作中已经积累了足够丰富的经验和能力——你不是在教他新技能，而是在**激活**他已有的能力。</span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>GPT-3正是这样一个"通才"：它在预训练阶段看过了互联网上几乎所有类型的文本，积累了海量的"经验"。当你给它few-shot示例时，你不是在教它学翻译、学分类——这些能力它已经"知道"了。你做的只是**告诉它你想用哪种能力**。</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPT-3的模型架构</span></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a>GPT-3的架构本身并无太大创新——它基本上就是一个更大的GPT-2。核心架构仍然是Decoder-only Transformer，使用因果（causal）自注意力和自回归语言建模目标。与GPT-2的主要区别在于：GPT-3采用了交替使用 dense 和 locally banded sparse attention pattern（类似 Sparse Transformer），但论文中并没有详细讨论这一细节的影响。</span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>GPT-3论文训练了8个不同规模的模型，从1.25亿参数到1750亿参数：</span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型名 <span class="pp">|</span> 参数量 $N$ <span class="pp">|</span> 层数 $n_{\text{layers}}$ <span class="pp">|</span> 隐藏维度 $d_{\text{model}}$ <span class="pp">|</span> 注意力头数 $n_{\text{heads}}$ <span class="pp">|</span> 头维度 $d_{\text{head}}$ <span class="pp">|</span> Batch Size <span class="pp">|</span> 学习率 <span class="pp">|</span></span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----------|----------|-----------|-----------|---------|-----------|--------|</span></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 Small <span class="pp">|</span> 125M <span class="pp">|</span> 12 <span class="pp">|</span> 768 <span class="pp">|</span> 12 <span class="pp">|</span> 64 <span class="pp">|</span> 0.5M <span class="pp">|</span> 6.0×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 Medium <span class="pp">|</span> 350M <span class="pp">|</span> 24 <span class="pp">|</span> 1024 <span class="pp">|</span> 16 <span class="pp">|</span> 64 <span class="pp">|</span> 0.5M <span class="pp">|</span> 3.0×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 Large <span class="pp">|</span> 760M <span class="pp">|</span> 24 <span class="pp">|</span> 1536 <span class="pp">|</span> 16 <span class="pp">|</span> 96 <span class="pp">|</span> 0.5M <span class="pp">|</span> 2.5×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 XL <span class="pp">|</span> 1.3B <span class="pp">|</span> 24 <span class="pp">|</span> 2048 <span class="pp">|</span> 24 <span class="pp">|</span> 128 <span class="pp">|</span> 1M <span class="pp">|</span> 2.0×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 2.7B <span class="pp">|</span> 2.7B <span class="pp">|</span> 32 <span class="pp">|</span> 2560 <span class="pp">|</span> 32 <span class="pp">|</span> 80 <span class="pp">|</span> 1M <span class="pp">|</span> 1.6×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 6.7B <span class="pp">|</span> 6.7B <span class="pp">|</span> 32 <span class="pp">|</span> 4096 <span class="pp">|</span> 32 <span class="pp">|</span> 128 <span class="pp">|</span> 2M <span class="pp">|</span> 1.2×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 13B <span class="pp">|</span> 13B <span class="pp">|</span> 40 <span class="pp">|</span> 5140 <span class="pp">|</span> 40 <span class="pp">|</span> 128 <span class="pp">|</span> 2M <span class="pp">|</span> 1.0×10⁻⁴ <span class="pp">|</span></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **GPT-3 175B** | **175B** | **96** | **12288** | **96** | **128** | **3.2M** | **0.6×10⁻⁴** <span class="pp">|</span></span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>: GPT-3 系列模型的架构配置。数据源自 Brown et al. (2020) Table 2.1。 {#tbl-gpt3-models}</span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a>这个表格包含了几个值得关注的设计细节。首先，所有模型的上下文窗口长度都是 $n_{\text{ctx}} = 2048$ 个token（与GPT-2相同）。其次，学习率随模型规模增大而减小——这与第18章讨论的训练稳定性直觉一致：更大的模型需要更温和的学习率来避免训练不稳定。第三，batch size随模型增大而增大（从0.5M到3.2M tokens），这是因为更大的模型能更高效地利用更大的batch（第17章的Scaling Laws预测了这一点）。</span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练数据与计算量</span></span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>GPT-3的训练数据由五个来源混合而成：</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 数据集 <span class="pp">|</span> Token数量 <span class="pp">|</span> 训练中权重 <span class="pp">|</span> 训练中的Epoch数 <span class="pp">|</span></span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----------|-----------|----------------|</span></span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Common Crawl (过滤后) <span class="pp">|</span> 410B <span class="pp">|</span> 60% <span class="pp">|</span> 0.44 <span class="pp">|</span></span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> WebText2 <span class="pp">|</span> 19B <span class="pp">|</span> 22% <span class="pp">|</span> 2.9 <span class="pp">|</span></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Books1 <span class="pp">|</span> 12B <span class="pp">|</span> 8% <span class="pp">|</span> 1.9 <span class="pp">|</span></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Books2 <span class="pp">|</span> 55B <span class="pp">|</span> 8% <span class="pp">|</span> 0.43 <span class="pp">|</span></span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Wikipedia <span class="pp">|</span> 3B <span class="pp">|</span> 3% <span class="pp">|</span> 3.4 <span class="pp">|</span></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>: GPT-3 训练数据构成。注意训练权重并不正比于数据大小——高质量数据(WebText2, Wikipedia)被过采样。 {#tbl-gpt3-data}</span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a>一个有趣的观察是：虽然Common Crawl占据了绝大多数token（410B），但高质量数据集被显著过采样——WebText2只有19B token却占了22%的训练权重，Wikipedia只有3B token却被训练了3.4个epoch。这反映了一个现在被广泛接受的观点：**数据质量比数据数量更重要**，这在后来LLaMA等模型的训练中被进一步强化。</span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a>GPT-3 175B的总计算量约为 $3.14 \times 10^{23}$ FLOPs。按照第17章介绍的 $C \approx 6ND$ 公式，$6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.15 \times 10^{23}$，与论文报告一致。然而，按照Chinchilla法则（$D^* \approx 20N$），175B参数应配 $3.5 \times 10^{12}$ tokens，而GPT-3只用了 $300 \times 10^9$ tokens——训练数据不到"最优值"的十分之一。换言之，**GPT-3是一个严重欠训练的模型**。但即便如此，它仍然展示了惊人的ICL能力——这让人不禁想象，如果GPT-3按照Chinchilla法则充分训练，效果会有多好。</span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a><span class="al">![GPT-3系列模型的训练计算量对比。尽管GPT-3 175B比RoBERTa-Large大近500倍，但由于在远少于典型规模的token数上训练，其计算量的增长幅度远小于参数量的增长。](figures/chapter-20/original/fig5-gpt3-scaling-compute.png)</span>{#fig-gpt3-compute width=85%}</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a>*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 2.2*</span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="fu">### In-Context Learning 的实验结果</span></span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a>GPT-3论文在42个benchmark上进行了系统评估，覆盖了传统NLP任务、翻译、常识推理、阅读理解等多个领域。</span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a><span class="al">![GPT-3在42个accuracy-denominated benchmarks上的聚合性能。Zero-shot性能随规模稳步提升，但few-shot性能提升更快，表明更大的模型更擅长In-Context Learning。](figures/chapter-20/original/fig3-gpt3-aggregate-performance.png)</span>{#fig-gpt3-aggregate width=85%}</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a>*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 1.3*</span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a>@fig-gpt3-aggregate 传达了本章最重要的实验发现：**ICL能力与模型规模之间存在超线性的关系**。随着模型从125M增长到175B：</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zero-shot** 性能稳步提升——这并不太令人惊讶，因为更大的模型存储了更多知识。</span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Few-shot** 性能提升得**更快**——这才是关键发现。小模型即使给了示例也无法有效利用，而大模型能够从少量示例中快速"学会"任务。</span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>这个差异的含义是深远的：ICL不仅仅是"模型知道更多知识"的体现，它是一种随规模涌现的**元能力**——利用上下文信息来适应新任务的能力。</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 完整数值示例：In-Context Learning的工作过程</span></span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a>**设定**：使用GPT-3对一段文本做情感分类。模型的输入就是一个文本序列，我们通过精心构造输入来"告诉"模型做什么。</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a>**Step 1: 构造 Few-shot 输入**</span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a><span class="in">```text</span></span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a><span class="in">Classify the sentiment of the review.</span></span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a><span class="in">Review: "The food was amazing and the service was excellent!"</span></span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a><span class="in">Sentiment: Positive</span></span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a><span class="in">Review: "Terrible experience. The room was dirty and staff was rude."</span></span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a><span class="in">Sentiment: Negative</span></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a><span class="in">Review: "The hotel was okay, nothing special but not bad either."</span></span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a><span class="in">Sentiment: Neutral</span></span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a><span class="in">Review: "I absolutely loved this movie, the acting was superb!"</span></span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a><span class="in">Sentiment:</span></span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>**Step 2: 模型处理输入**</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>GPT-3将整个输入视为一个token序列，通过自回归方式逐token处理。当它到达最后一行的"Sentiment:"时，它需要预测下一个token。</span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a>**Step 3: 模型内部发生了什么？**</span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a>在传统的语言模型视角下，模型只是在"续写"一段文本。但在这个特定的上下文中，模型观察到了一个清晰的模式：</span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>每段"Review: ... Sentiment: ..."构成一个样本</span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>前三个样本建立了"正面评价→Positive，负面→Negative，中性→Neutral"的映射</span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>最后一个Review的内容（"absolutely loved", "superb"）强烈暗示正面情感</span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>**Step 4: 模型输出**</span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>模型生成的下一个token（以最高概率）是"Positive"。</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a>**解读**：从外部观察，GPT-3"学会了"做情感分类——它正确地将"I absolutely loved this movie"分类为Positive。但在模型内部，**没有发生任何参数更新**。模型所做的一切就是：阅读输入文本，识别出其中的任务模式，然后按照这个模式生成最合理的续写。</span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>这个例子揭示了ICL的一个深刻特性：**它模糊了"学习"和"推理"的边界**。模型到底是在"学习"一个新任务，还是在"推理"出给定上下文中最合理的输出？这个问题至今仍是活跃的研究话题。</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a><span class="fu">### 标志性实验结果</span></span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a>让我们看几个具体的benchmark结果，以量化ICL的效果。</span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a>**LAMBADA**（最后一词预测任务）：</span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 设定 <span class="pp">|</span> 准确率 <span class="pp">|</span></span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|--------|</span></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 175B <span class="pp">|</span> Zero-shot <span class="pp">|</span> 76.2% <span class="pp">|</span></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 175B <span class="pp">|</span> Few-shot <span class="pp">|</span> **86.4%** <span class="pp">|</span></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 之前SOTA（微调模型） <span class="pp">|</span> 有监督 <span class="pp">|</span> 68.0% <span class="pp">|</span></span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a>GPT-3的few-shot结果不仅超越了之前所有微调模型的SOTA，甚至zero-shot性能就已经大幅领先。这是ICL最令人印象深刻的展示之一。</span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a>**TriviaQA**（开放域问答）：</span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 模型 <span class="pp">|</span> 设定 <span class="pp">|</span> 准确率 <span class="pp">|</span></span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|--------|</span></span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 175B <span class="pp">|</span> Zero-shot <span class="pp">|</span> 64.3% <span class="pp">|</span></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 175B <span class="pp">|</span> One-shot <span class="pp">|</span> 68.0% <span class="pp">|</span></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 175B <span class="pp">|</span> Few-shot <span class="pp">|</span> **71.2%** <span class="pp">|</span></span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 微调的T5-11B+SSM <span class="pp">|</span> 有监督 <span class="pp">|</span> 60.5% <span class="pp">|</span></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a>在TriviaQA上，GPT-3的few-shot结果也超越了精心微调的T5-11B。这里需要注意的是，GPT-3使用的是闭卷设置（不访问外部知识库），完全依靠参数中存储的知识来回答问题。</span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a>**翻译**（En→Fr, En→De, En→Ro）：</span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a>GPT-3在英法翻译上达到了接近有监督SOTA的水平（few-shot BLEU 32.6 vs 有监督SOTA 35.0），这一点格外值得注意——GPT-3的训练数据中非英语文本只占约7%，但它仍然"学会了"翻译。</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>然而，GPT-3在某些任务上的表现并不令人满意。在自然语言推理（NLI）任务如 SuperGLUE 的 RTE 上，GPT-3 few-shot 只达到约 72%，而微调的BERT已经超过 85%。在需要精确理解句子间语义关系的任务上，ICL的局限性开始显现。</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a><span class="al">![GPT-3在不同规模下的ICL学习曲线。更大的模型不仅绝对性能更高，还展示出更陡峭的"学习曲线"——即从上下文示例中获益更多。](figures/chapter-20/original/fig2-gpt3-icl-learning-curves.png)</span>{#fig-gpt3-icl-curves width=85%}</span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a>*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 1.2*</span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a><span class="fu">### ICL的规模效应：量化"越大越好"</span></span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>@fig-gpt3-icl-curves 展示了一个精巧的实验：在一个简单的符号操作任务上（从单词中去除随机符号），不同规模的模型如何利用上下文示例。图中最引人注目的发现是：**大模型的ICL学习曲线显著更陡**。</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a>这意味着什么？小模型即使给了100个示例，性能提升也很有限——它们缺乏从示例中"提取规则"的能力。而175B模型只需要10-15个示例就能接近完美表现。这不是因为大模型"记住了更多"，而是因为它们具备了一种更高层次的能力：**从少量示例中归纳规则**。</span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>我们可以将不同规模下的ICL能力做一个定性总结：</span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 规模 <span class="pp">|</span> Zero-shot <span class="pp">|</span> Few-shot <span class="pp">|</span> ICL行为 <span class="pp">|</span></span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|----------|---------|</span></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~125M <span class="pp">|</span> 接近随机 <span class="pp">|</span> 几乎无提升 <span class="pp">|</span> 无法有效利用上下文 <span class="pp">|</span></span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~1.3B <span class="pp">|</span> 略好于随机 <span class="pp">|</span> 有一定提升 <span class="pp">|</span> 能识别简单模式 <span class="pp">|</span></span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~13B <span class="pp">|</span> 有意义的性能 <span class="pp">|</span> 显著提升 <span class="pp">|</span> 能执行中等复杂度任务 <span class="pp">|</span></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ~175B <span class="pp">|</span> 接近SOTA <span class="pp">|</span> 经常超越微调 <span class="pp">|</span> 能从示例推断复杂规则 <span class="pp">|</span></span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPT-3 API：LLM即服务的诞生</span></span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a>2020年6月，在论文发表后不久，OpenAI发布了GPT-3的API——这是历史上第一个大语言模型的商业化接口。用户不需要下载模型、不需要GPU，只需要发送一个HTTP请求，就能使用175B参数的语言模型。</span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>这个商业决策的影响远超技术层面。它催生了一种全新的软件开发范式：**Prompt Programming**（提示编程）。开发者不再需要收集数据、训练模型、部署模型，而是通过设计合适的prompt来完成任务。一个人、一台笔记本、一个API key，就能构建之前需要整个ML团队才能开发的功能。</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### ICL推理流程</span></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: Few-Shot In-Context Learning Inference（改编自 Brown et al., 2020）</span></span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a><span class="in">Input:  预训练语言模型 LM，任务描述 T，</span></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a><span class="in">        K 个示例 {(x₁,y₁), ..., (xₖ,yₖ)}，查询输入 x_query</span></span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a><span class="in">Output: 预测标签 ŷ</span></span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a><span class="in">1. 构造 prompt：</span></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a><span class="in">     prompt ← T + "\n\n"</span></span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a><span class="in">     for k = 1 to K:</span></span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a><span class="in">         prompt ← prompt + format(xₖ, yₖ) + "\n\n"</span></span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a><span class="in">     prompt ← prompt + format_query(x_query)</span></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a><span class="in">2. 前向推理（无梯度更新）：</span></span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a><span class="in">     token_probs ← LM.forward(prompt)</span></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a><span class="in">3. 提取预测：</span></span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a><span class="in">     ŷ ← argmax token_probs  # 或采样</span></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a><span class="in">4. return ŷ</span></span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a>*改编自 Brown et al. (2020) "Language Models are Few-Shot Learners", Section 2.1。注意：整个过程没有任何参数更新，所有"学习"都编码在 prompt 的构造中。*</span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 2: Contextual Calibration（Zhao et al., 2021）</span></span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a><span class="in">Input:  语言模型 LM，prompt 模板 P，</span></span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a><span class="in">        标签空间 Y = {y₁, ..., yₘ}，查询输入 x_query</span></span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a><span class="in">Output: 校准后的预测 ŷ</span></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a><span class="in">1. 测量先验偏差：</span></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="in">     p̂_bias ← LM(P + "N/A")   # 用空内容输入测量偏差</span></span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a><span class="in">     W ← diag(p̂_bias)⁻¹        # 构造校准矩阵</span></span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a><span class="in">2. 对查询进行推理：</span></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a><span class="in">     p̂_raw ← LM(P + x_query)   # 原始预测概率</span></span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a><span class="in">3. 校准：</span></span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a><span class="in">     p̂_cal ← normalize(W · p̂_raw)  # 减去先验偏差</span></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a><span class="in">4. return ŷ ← argmax p̂_cal</span></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a>*Source: Zhao et al. (2021) "Calibrate Before Use: Improving Few-Shot Performance of Language Models", Section 3. [arXiv:2102.09690](https://arxiv.org/abs/2102.09690)*</span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prompt设计的核心原则</span></span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a>ICL的效果高度依赖于prompt的设计。以下是从GPT-3论文和后续研究中总结的关键原则：</span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a>**原则1：格式一致性**</span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a>Few-shot示例应该保持一致的格式。模型从示例中学习的不仅是任务内容，更重要的是**输入-输出的映射格式**。</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a><span class="co"># 好的 prompt：格式一致</span></span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of France?</span></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a><span class="st">A: Paris</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of Japan?</span></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a><span class="st">A: Tokyo</span></span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of Brazil?</span></span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a><span class="st">A: """</span></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a><span class="co"># 差的 prompt：格式不一致</span></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a><span class="st">The capital of France is Paris.</span></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a><span class="st">Q: What is the capital of Japan?</span></span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a><span class="st">Answer: Tokyo</span></span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a><span class="st">What about Brazil's capital?</span></span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>**原则2：示例的代表性**</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a>选择的示例应该覆盖任务的不同方面：</span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a><span class="co"># 情感分类：覆盖正面、负面、中性</span></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Great product!"</span>, <span class="st">"Positive"</span>),</span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Terrible quality."</span>, <span class="st">"Negative"</span>),</span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"It's okay."</span>, <span class="st">"Neutral"</span>),</span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a>**原则3：任务描述的清晰性**</span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>一个清晰的任务描述（task instruction）可以显著提升zero-shot性能：</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a><span class="co"># 有明确任务描述</span></span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Classify the following movie review as Positive or Negative.</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a><span class="co"># 无任务描述（仅靠示例推断）</span></span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">""</span>  <span class="co"># 直接给示例</span></span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个完整的Few-shot实现</span></span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> few_shot_classify(text, examples, task_desc, model<span class="op">=</span><span class="st">"gpt-3"</span>):</span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a><span class="co">    使用 Few-shot ICL 进行文本分类。</span></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a><span class="co">        text: 待分类的文本</span></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a><span class="co">        examples: list of (input, label) 示例对</span></span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a><span class="co">        task_desc: 任务描述字符串</span></span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a><span class="co">        model: 模型名称</span></span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a><span class="co">        模型预测的标签</span></span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构造 prompt</span></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> task_desc <span class="op">+</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span></span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 添加 few-shot 示例</span></span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inp, label <span class="kw">in</span> examples:</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">+=</span> <span class="ss">f"Input: </span><span class="sc">{</span>inp<span class="sc">}</span><span class="ch">\n</span><span class="ss">Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">"</span></span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 添加待分类样本</span></span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">+=</span> <span class="ss">f"Input: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ch">\n</span><span class="ss">Label:"</span></span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 调用模型（生成 1 个 token 即可）</span></span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> openai.Completion.create(</span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model,</span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a>        prompt<span class="op">=</span>prompt,</span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a>        max_tokens<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="dv">0</span>,  <span class="co"># 贪心解码，确保确定性</span></span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a>        stop<span class="op">=</span>[<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>]      <span class="co"># 遇到换行停止</span></span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.choices[<span class="dv">0</span>].text.strip()</span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"The food was delicious!"</span>, <span class="st">"Positive"</span>),</span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Worst service ever."</span>, <span class="st">"Negative"</span>),</span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"It was fine, nothing special."</span>, <span class="st">"Neutral"</span>),</span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> few_shot_classify(</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span><span class="st">"I absolutely loved the atmosphere!"</span>,</span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>    examples<span class="op">=</span>examples,</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a>    task_desc<span class="op">=</span><span class="st">"Classify the sentiment of the following review."</span></span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prediction: </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Prediction: Positive</span></span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>这段代码虽然简单，但它包含了ICL的全部核心逻辑：构造一个包含任务描述和示例的prompt，将其送入模型，从生成的文本中提取答案。没有训练循环、没有损失函数、没有梯度计算——"学习"完全发生在输入构造阶段。</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a><span class="fu">### 复现细节与工程注意事项</span></span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>**Token预算管理**：GPT-3的上下文窗口只有2048个token，few-shot示例占据的token越多，留给输入文本和输出的空间就越少。在实践中需要在"示例数量"和"输入长度"之间权衡：</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a>n_{\text{ctx}} = n_{\text{instruction}} + K \cdot n_{\text{example}} + n_{\text{input}} + n_{\text{output}}</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a>其中 $K$ 是示例数量。如果每个示例平均占50个token，任务描述占20个token，那么在2048的窗口下，即使留200个token给输入和输出，也最多只能放约36个示例。这解释了为什么GPT-3论文中的few-shot通常只用10-100个示例。</span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a>**Temperature设置**：对于分类等确定性任务，应使用 $\text{temperature} = 0$（等价于贪心解码）；对于创意生成任务，可以使用 $0.7 - 1.0$ 的温度。</span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a>**Stop tokens**：在分类任务中，设置合适的停止token（如换行符<span class="in">`\n`</span>）可以防止模型生成多余的文本。</span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨In-Context Learning的理论机制、边界条件和开放问题。</span></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a>In-Context Learning为什么能work？这是一个看似简单、实则深刻的问题。一个仅被训练来预测下一个词的模型，为什么能从输入中的几个示例"学会"新任务？自2020年GPT-3论文发表以来，多个研究团队从不同角度提出了理论解释。</span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 理论1：ICL作为隐式贝叶斯推断</span></span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a>Xie et al. (2022) 提出了一个优雅的理论框架：In-Context Learning本质上是在做**隐式贝叶斯推断**（Implicit Bayesian Inference）。</span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>核心思想是这样的：假设预训练数据来自多个"概念"（concept）的混合分布。每个概念决定了文本的生成规则——比如"情感分类"是一个概念，"翻译"是另一个概念。当模型在预训练中学习了这个混合分布后，给它few-shot示例等价于给它观测数据，让它推断当前的"概念"是什么。</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a>形式化地，设 $\theta$ 表示潜在概念，$x_{1:K}$ 是few-shot示例，$x_{\text{query}}$ 是查询输入。模型的预测可以理解为：</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a>p(y \mid x_{\text{query}}, x_{1:K}) = \int p(y \mid x_{\text{query}}, \theta) \cdot p(\theta \mid x_{1:K}) \, d\theta</span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a>其中 $p(\theta \mid x_{1:K})$ 是模型从示例中推断出的概念后验分布。更多的示例提供更多"证据"，使后验更加集中，从而提高预测准确性。</span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a>这个理论的美妙之处在于它解释了两个实验现象：第一，为什么更多示例能提高性能（更多证据→更好的后验）；第二，为什么示例的标签可能不那么重要（示例的主要作用是帮助推断概念/任务，而非学习输入-输出映射）。</span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 理论2：ICL作为隐式梯度下降</span></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a>Dai et al. (2023) 提出了另一个令人惊讶的视角：Transformer的注意力机制在处理few-shot示例时，其行为**类似于梯度下降**。</span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a>他们的核心观察是：考虑一个线性注意力层 $f(q) = W_V X^\top \cdot \text{softmax}(X W_K^\top q)$。当few-shot示例被加入上下文时，这等价于对一个线性模型做了一步梯度更新。具体地，他们证明了在某些简化假设下：</span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>\text{ICL}_{\text{Attention}}(x_{\text{query}}) \approx \text{ICL}_{\text{GD}}(x_{\text{query}}) = W_0 x_{\text{query}} - \eta \nabla_{W} \mathcal{L}(W_0) \cdot x_{\text{query}}</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a>其中 $W_0$ 是预训练权重，$\mathcal{L}$ 是基于few-shot示例的损失函数，$\eta$ 是隐式"学习率"。</span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a>换言之，Transformer的一层注意力计算等价于一步梯度下降——前向传播中隐含了一个微型的"训练过程"。这个理论的力量在于它将ICL和传统的基于梯度的学习统一到了同一个框架下。</span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 理论3：归纳头（Induction Heads）</span></span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>Olsson et al. (2022, Anthropic) 从更微观的层面发现了ICL的一种具体机制：**归纳头**（Induction Heads）。归纳头是一种由两个注意力头协作形成的电路，其功能是：如果在上下文中出现了模式 "<span class="co">[</span><span class="ot">A</span><span class="co">][B]</span> ... <span class="co">[</span><span class="ot">A</span><span class="co">]</span>"，归纳头会预测下一个token是 "<span class="co">[</span><span class="ot">B</span><span class="co">]</span>"。</span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a>这看起来只是一个简单的"复制"模式，但它是ICL的基础。当few-shot示例中包含 "positive → Positive" 的映射，而查询也是一个positive的文本时，归纳头帮助模型"复制"对应的标签。</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a>Olsson et al. 的关键发现是：</span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>归纳头在训练过程中会突然出现（phase transition），这与ICL能力的突然涌现一致。</span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>消除归纳头会导致ICL能力大幅下降。</span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>归纳头的形成与模型规模有关——更大的模型能形成更复杂的归纳头。</span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——实证视角</span></span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a>Min et al. (2022) 的工作提供了关于ICL的最反直觉的实证发现之一。他们系统地测试了few-shot示例中各个成分的重要性：</span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a>**实验**：在情感分类任务中，将few-shot示例的标签**随机替换**（正面文本标注为"Negative"，反面文本标注为"Positive"），然后观察模型性能的变化。</span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a>**结果**：令人震惊的是，**使用随机标签的few-shot性能与使用正确标签几乎相同**。</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>这意味着什么？模型从few-shot示例中"学到"的主要不是输入-输出的映射（这是我们直觉上认为的），而是：</span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**输入的分布**：什么样的文本是有效输入</span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**输出的格式和空间**：标签应该是"Positive"或"Negative"（而不是自由文本）</span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**输入-输出的映射模式**：一个输入对应一个标签（而不是一段解释）</span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a>这个发现与Xie et al. 的贝叶斯推断理论高度一致：few-shot示例的主要作用是帮助模型**锁定任务类型**（推断概念 $\theta$），而非学习具体的映射规则。</span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a>然而，需要注意的是，这个结论并非在所有任务和规模上都成立。在某些需要学习新映射的任务上（如将非标准标签映射到类别），正确标签确实重要。这个发现的价值在于它迫使我们重新思考ICL"学到了什么"。</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a>ICL并非万能的。以下是其已知的失效条件和局限：</span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a>**失效条件1：任务复杂度超过模型容量**。ICL在需要多步推理的任务上表现不佳。例如，多位数加法（如 "1234 + 5678 = ?"）即使给了很多示例，GPT-3也经常出错。这表明ICL能做的"学习"有其复杂度上限。这个局限直接催生了下一章将讨论的Chain-of-Thought。</span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a>**失效条件2：任务需要精确的格式对齐**。如果目标任务的输入-输出格式与预训练数据中常见的模式差异很大，ICL效果会显著下降。</span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a>**失效条件3：示例数量受限于上下文窗口**。GPT-3只有2048个token的窗口，这严重限制了可以提供的示例数量。对于需要大量示例才能学会的复杂任务，ICL的表现会受到上下文长度的瓶颈限制。</span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a>**失效条件4：对示例选择和顺序高度敏感**。这一点值得特别展开。</span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a><span class="fu">### ICL的不稳定性问题</span></span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a>Zhao et al. (2021) 和 Lu et al. (2022) 系统地量化了ICL的不稳定性，发现了几个令人不安的现象。</span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>**示例顺序敏感性**：Zhao et al. (2021) 在 SST-2 情感分类任务上用 GPT-3 2.7B 做了一个触目惊心的实验——使用相同的4个few-shot示例，仅仅改变它们的**排列顺序**，准确率就从 **54.3%（接近随机猜测）波动到 93.4%（接近SOTA）**。更极端的是，在2-shot设置下，仅仅将两个示例的顺序**对调**，准确率就从 88.5% 骤降到 51.3%。Lu et al. (2022) 进一步证实，这种顺序敏感性在**所有模型规模**上都存在，增加模型大小或示例数量都**无法消除**这一问题。</span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>**三种系统性偏差**：Zhao et al. 识别出了ICL中的三种系统性偏差来源。**多数标签偏差**（Majority Label Bias）：如果few-shot示例中某个标签出现得更频繁，模型会倾向于预测多数标签。**近因偏差**（Recency Bias）：模型倾向于预测与最后几个示例相同的标签。**常见token偏差**（Common Token Bias）：模型更倾向于预测在预训练数据中频繁出现的token（如预测"America"而非"Saint Lucia"）。</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a>Zhao et al. (2021) 提出了一种名为**Contextual Calibration**的缓解方法：先用一个"空输入"（content-free input，如 "N/A"、"[MASK]" 或空字符串）来测量模型的先验偏差，然后将其从预测中减去。这种方法简单但效果显著——在某些任务上可以带来高达 **30个百分点** 的绝对提升。</span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a>如果你要在这个方向写一篇论文，以下几个问题仍然值得深入探索：</span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a>第一个方向是**ICL的机制理解**。尽管已有贝叶斯推断、隐式梯度下降、归纳头等多种理论解释，但这些理论之间的关系尚不清楚。它们是同一现象的不同视角，还是描述了不同层次的机制？一个统一的理论框架仍然缺失。</span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a>第二个方向是**ICL能力的涌现条件**。为什么ICL需要如此大的模型规模才能出现？存在某个最小规模阈值吗？通过改进训练方法或架构设计，能否让更小的模型也获得强ICL能力？</span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a>第三个方向是**ICL与微调的关系**。ICL和微调是两种完全不同的"学习"吗？还是它们是同一连续体上的两个端点？如果ICL确实等价于隐式梯度下降，那它与显式微调的精确差异是什么？</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本方法的局限</span></span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a>GPT-3和In-Context Learning虽然代表了NLP范式的重大转变，但它们面临着几个根本性的局限。</span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>第一个局限是**推理能力的缺失**。ICL擅长的是模式匹配和知识检索，但在需要多步推理、逻辑推导或数学计算的任务上表现很差。给GPT-3一个需要四步推理的数学题，即使提供了类似问题的示例，它也经常在中间步骤出错。这不是因为模型"不够大"——即使是175B参数，直接的ICL也无法可靠地执行复杂推理。</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a>第二个局限是**不可控性**。ICL的输出高度依赖prompt的措辞、示例的选择和顺序，而这些因素的影响难以预测和控制。在需要稳定、可靠输出的生产环境中，这种不可控性是一个严重问题。</span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a>第三个局限是**计算成本**。每次推理都需要处理整个prompt（包括所有few-shot示例），这意味着ICL的推理成本远高于微调模型。175B参数模型的单次推理就需要大量计算，再加上几十个示例的长prompt，成本进一步攀升。</span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a>ICL的推理缺陷引出了一个自然的问题：**如果模型无法直接"跳到"答案，能否让它像人类一样展示中间推理步骤？**如果给模型的示例中不仅包含最终答案，还包含详细的推理过程（"首先...，然后...，因此..."），模型能否学会这种"展示工作过程"的模式？</span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a>这正是下一章将讨论的 **Chain-of-Thought (CoT) Prompting** 的核心思想——它不是一种全新的能力，而是对ICL的一种巧妙利用：通过在few-shot示例中加入推理链条，引导模型也生成类似的中间推理步骤。CoT的出现进一步释放了大模型的潜力，但也引发了更深层的问题：LLM真的在"推理"吗？</span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第21章将聚焦**涌现能力与思维链推理**。Chain-of-Thought如何让GPT-3级别的模型在数学和逻辑推理上获得飞跃式提升？涌现能力是真实的相变还是度量方式的假象？Zero-shot CoT的神奇咒语"Let's think step by step"为什么有效？——这些问题将引导我们更深入地理解大语言模型的能力边界。</span></span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-636"><a href="#cb12-636" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb12-637"><a href="#cb12-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a>本章围绕GPT-3和In-Context Learning，讲述了大语言模型从"基础设施"到"涌现能力"的关键跨越。</span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a>GPT-3（2020）以175B参数——96层Transformer，12288维隐藏状态，96个注意力头——成为当时最大的语言模型。它的架构本身并无太大创新（仍然是Decoder-only Transformer + 自回归语言建模），真正的突破在于规模带来的涌现：In-Context Learning。</span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a>In-Context Learning的核心发现是：足够大的语言模型不需要梯度更新就能从输入中的少量示例"学会"新任务。这种能力有三种模式——zero-shot（仅描述任务）、one-shot（一个示例）和few-shot（多个示例）——它们的效果与模型规模呈超线性关系：大模型不仅绝对性能更高，还更擅长利用上下文示例。</span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-644"><a href="#cb12-644" aria-hidden="true" tabindex="-1"></a>关于ICL为什么work，目前有三种主要的理论解释：隐式贝叶斯推断（Xie et al.，模型从示例推断潜在概念）、隐式梯度下降（Dai et al.，注意力计算等价于一步梯度更新）、以及归纳头机制（Olsson et al.，特定的注意力头模式实现了模式匹配）。Min et al.的实验进一步表明，ICL的主要作用可能是帮助模型"锁定任务类型"，而非学习具体的输入-输出映射。</span>
<span id="cb12-645"><a href="#cb12-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-646"><a href="#cb12-646" aria-hidden="true" tabindex="-1"></a>然而，ICL也有明显的局限：对prompt设计高度敏感（示例顺序可导致30%到90%的准确率波动）、无法可靠执行多步推理、计算成本高昂。</span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 内容 <span class="pp">|</span> 公式/数值 <span class="pp">|</span></span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|</span></span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> GPT-3 参数量 <span class="pp">|</span> $N = 175 \times 10^9$ <span class="pp">|</span></span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 总计算量 <span class="pp">|</span> $C \approx 3.14 \times 10^{23}$ FLOPs <span class="pp">|</span></span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 上下文窗口 <span class="pp">|</span> $n_{\text{ctx}} = 2048$ tokens <span class="pp">|</span></span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Token预算约束 <span class="pp">|</span> $n_{\text{ctx}} = n_{\text{instr}} + K \cdot n_{\text{example}} + n_{\text{input}} + n_{\text{output}}$ <span class="pp">|</span></span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ICL贝叶斯推断 <span class="pp">|</span> $p(y \mid x_q, x_{1:K}) = \int p(y \mid x_q, \theta) \cdot p(\theta \mid x_{1:K}) d\theta$ <span class="pp">|</span></span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ICL隐式梯度下降 <span class="pp">|</span> $\text{ICL}(x_q) \approx W_0 x_q - \eta \nabla_W \mathcal{L}(W_0) \cdot x_q$ <span class="pp">|</span></span>
<span id="cb12-658"><a href="#cb12-658" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 按Chinchilla法则的最优数据量 <span class="pp">|</span> $D^* = 20 \times 175B = 3.5T$ tokens（而GPT-3仅用300B） <span class="pp">|</span></span>
<span id="cb12-659"><a href="#cb12-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么GPT-3的few-shot性能与模型规模之间是"超线性"关系（即大模型从示例中获益更多），而非简单的线性关系？提示：考虑ICL需要什么样的"元能力"，以及这种能力与模型容量的关系。</span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 根据Dai et al.的理论，线性注意力层处理few-shot示例等价于一步梯度下降。请推导：对于一个线性模型 $y = Wx$，如果使用MSE损失 $\mathcal{L} = \frac{1}{K}\sum_{k=1}^K <span class="sc">\|</span>Wx_k - y_k<span class="sc">\|</span>^2$，一步梯度下降后的权重更新 $W' = W - \eta \nabla_W \mathcal{L}$ 的显式形式是什么？它如何对应到注意力机制的Key-Value结构？</span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用OpenAI API（或其他LLM API），在SST-2情感分类数据集上比较以下设置的准确率：(a) zero-shot，(b) 4-shot随机示例，(c) 4-shot但标签随机打乱，(d) 4-shot相同示例但不同顺序（至少5种排列）。你的结果是否验证了Min et al.和Lu et al.的发现？</span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[研究思考]** ICL的三种理论解释（贝叶斯推断、隐式梯度下降、归纳头）各自有什么假设和局限？它们是互斥的还是互补的？如果你要设计实验来区分这三种理论，你会怎么做？</span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-670"><a href="#cb12-670" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放思考]** GPT-3展示了"规模带来涌现"的现象，但这是否意味着"只要足够大就能解决一切"？考虑以下反例：(a) GPT-3在简单算术上的失败，(b) ICL的不稳定性不随规模完全消失，(c) 某些任务（如精确推理）可能需要根本不同的方法。你认为ICL的能力边界在哪里？</span>
<span id="cb12-671"><a href="#cb12-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a>**Brown, T. et al. (2020). "Language Models are Few-Shot Learners"**。GPT-3原始论文，本章最核心的参考。重点阅读：Section 2（zero/one/few-shot的精确定义）、Section 3.1-3.5（关键benchmark结果）、Figure 1.2-1.3（ICL学习曲线和聚合性能）、Figure 2.1（ICL范式图）。可快速浏览：Section 4（数据污染分析）、Appendix中的详细实验表格。注意：论文长达75页，其中大量是实验结果，核心内容集中在前20页。<span class="co">[</span><span class="ot">arXiv:2005.14165</span><span class="co">](https://arxiv.org/abs/2005.14165)</span></span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb12-681"><a href="#cb12-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-682"><a href="#cb12-682" aria-hidden="true" tabindex="-1"></a>**Xie, S. et al. (2022). "An Explanation of In-context Learning as Implicit Bayesian Inference"**。将ICL理解为贝叶斯推断的优雅理论框架。重点阅读：Section 3（理论推导）。<span class="co">[</span><span class="ot">arXiv:2111.02080</span><span class="co">](https://arxiv.org/abs/2111.02080)</span></span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a>**Dai, D. et al. (2023). "Why Can GPT Learn In-Context?"**。证明ICL等价于隐式梯度下降。重点阅读：Section 3（双重形式对比）。<span class="co">[</span><span class="ot">arXiv:2212.10559</span><span class="co">](https://arxiv.org/abs/2212.10559)</span></span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a>**Olsson, C. et al. (2022). "In-context Learning and Induction Heads"**。Anthropic的研究，发现了ICL的微观机制。重点阅读：Section 2-3（归纳头的定义和实验验证）。<span class="co">[</span><span class="ot">arXiv:2209.11895</span><span class="co">](https://arxiv.org/abs/2209.11895)</span></span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>**von Oswald, J. et al. (2023). "Transformers Learn In-Context by Gradient Descent"**。严格证明单层线性自注意力可以实现一步梯度下降，N层可近似N步GD。与Dai et al.互补，提供了更形式化的理论保证。ICML 2023。<span class="co">[</span><span class="ot">arXiv:2212.07677</span><span class="co">](https://arxiv.org/abs/2212.07677)</span></span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a>**Min, S. et al. (2022). "Rethinking the Role of Demonstrations"**。反直觉发现：ICL中标签可能不如格式重要。<span class="co">[</span><span class="ot">arXiv:2202.12837</span><span class="co">](https://arxiv.org/abs/2202.12837)</span></span>
<span id="cb12-693"><a href="#cb12-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-694"><a href="#cb12-694" aria-hidden="true" tabindex="-1"></a>**Zhao, Z. et al. (2021). "Calibrate Before Use"**。系统分析ICL不稳定性并提出校准方法。<span class="co">[</span><span class="ot">arXiv:2102.09690</span><span class="co">](https://arxiv.org/abs/2102.09690)</span></span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-696"><a href="#cb12-696" aria-hidden="true" tabindex="-1"></a>**Lu, Y. et al. (2022). "Fantastically Ordered Prompts and Where to Find Them"**。量化了示例顺序对ICL的影响。<span class="co">[</span><span class="ot">arXiv:2104.08786</span><span class="co">](https://arxiv.org/abs/2104.08786)</span></span>
<span id="cb12-697"><a href="#cb12-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-698"><a href="#cb12-698" aria-hidden="true" tabindex="-1"></a>**Wei, J. et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"**。下一章的核心论文，展示了如何通过ICL的变体解决推理问题。<span class="co">[</span><span class="ot">arXiv:2201.11903</span><span class="co">](https://arxiv.org/abs/2201.11903)</span></span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a>**Dong, Q. et al. (2023). "A Survey on In-Context Learning"**。ICL领域最全面的综述。<span class="co">[</span><span class="ot">arXiv:2301.00234</span><span class="co">](https://arxiv.org/abs/2301.00234)</span></span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-704"><a href="#cb12-704" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb12-705"><a href="#cb12-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**OpenAI API文档**：<span class="co">[</span><span class="ot">platform.openai.com/docs</span><span class="co">](https://platform.openai.com/docs)</span>（ICL的主要使用方式）</span>
<span id="cb12-707"><a href="#cb12-707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**HELM**（Holistic Evaluation of Language Models）：<span class="co">[</span><span class="ot">crfm.stanford.edu/helm</span><span class="co">](https://crfm.stanford.edu/helm/)</span>（系统性评估LLM的ICL能力）</span>
<span id="cb12-708"><a href="#cb12-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a>GPT-3的发布是AI历史上一个标志性的时刻，其影响远超技术领域。</span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a>2020年5月28日，Brown et al. 的论文在arXiv上发布，迅速引发了学术界的轰动。仅仅两周后的6月11日，OpenAI就宣布了GPT-3的API——这个速度本身就说明了论文和商业化是同步推进的。训练一次GPT-3 175B估计需要约355个GPU年的V100计算时间，成本约为**460万美元**。在此之前，使用大语言模型需要这种级别的投入和深厚的ML工程能力；API的出现让任何一个会写Python的人都能以每1000个token几美分的价格使用175B参数的模型。这催生了一波"prompt programming"的浪潮——Twitter上充斥着人们用GPT-3做各种令人惊叹（或令人担忧）的事情的演示：写代码、写诗、模拟对话、生成法律文书……</span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a>一个有趣的细节是：GPT-3论文有31位作者，但只有一小部分人参与了核心的模型训练和实验。论文中的许多benchmark实验实际上是由不同的小组分别完成的，这反映了大模型研究日益工业化的趋势——不再是一两个研究者的工作，而是需要整个团队的协作。</span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a>GPT-3也标志着一个概念的诞生：**基础模型**（Foundation Model）。2021年，Stanford HAI发布了著名的"基础模型"报告，其核心论点正是基于GPT-3的示范：一个足够大的预训练模型可以通过ICL适配几乎任何下游任务，因此它不再是"一个模型解决一个问题"，而是"一个基础模型支撑一整个应用生态"。这个概念深刻地重塑了AI行业的商业模式和研究方向。</span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a>讽刺的是，GPT-3虽然以"Few-Shot Learners"为标题强调了ICL，但后来最成功的GPT系列产品（ChatGPT、GPT-4）反而不是主要依靠ICL来使用的——它们通过指令微调（Instruction Tuning）和RLHF让模型学会了直接理解用户意图，不再需要few-shot示例。从这个角度看，GPT-3发现的ICL能力更像是一个**过渡性的里程碑**：它证明了大模型具有惊人的潜力，但释放这种潜力的最终方式不是ICL，而是更精细的对齐技术——这正是第23-25章将要讲述的故事。</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>