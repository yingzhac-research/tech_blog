<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-25">
<meta name="description" content="从跨序列注意力到自注意力：序列如何审视自己，Q-K-V框架的建立，以及位置信息缺失的挑战与解决方案。">

<title>第7章：Self-Attention的突破 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#问题的精确定义" id="toc-问题的精确定义" class="nav-link" data-scroll-target="#问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</a></li>
  <li><a href="#一个关键的思想实验" id="toc-一个关键的思想实验" class="nav-link" data-scroll-target="#一个关键的思想实验"><span class="header-section-number">2.2</span> 一个关键的思想实验</a></li>
  <li><a href="#我们需要什么样的解决方案" id="toc-我们需要什么样的解决方案" class="nav-link" data-scroll-target="#我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#从cross-attention到self-attention" id="toc-从cross-attention到self-attention" class="nav-link" data-scroll-target="#从cross-attention到self-attention"><span class="header-section-number">3.1</span> 从Cross-Attention到Self-Attention</a></li>
  <li><a href="#query-key-value的三元组视角" id="toc-query-key-value的三元组视角" class="nav-link" data-scroll-target="#query-key-value的三元组视角"><span class="header-section-number">3.2</span> Query-Key-Value的三元组视角</a></li>
  <li><a href="#一个具体的例子" id="toc-一个具体的例子" class="nav-link" data-scroll-target="#一个具体的例子"><span class="header-section-number">3.3</span> 一个具体的例子</a></li>
  </ul></li>
  <li><a href="#memory-networksself-attention的思想先驱" id="toc-memory-networksself-attention的思想先驱" class="nav-link" data-scroll-target="#memory-networksself-attention的思想先驱"><span class="header-section-number">4</span> Memory Networks：Self-Attention的思想先驱</a>
  <ul class="collapse">
  <li><a href="#问答系统的挑战" id="toc-问答系统的挑战" class="nav-link" data-scroll-target="#问答系统的挑战"><span class="header-section-number">4.1</span> 问答系统的挑战</a></li>
  <li><a href="#memory-networks的设计" id="toc-memory-networks的设计" class="nav-link" data-scroll-target="#memory-networks的设计"><span class="header-section-number">4.2</span> Memory Networks的设计</a></li>
  <li><a href="#memory-networks与self-attention的联系" id="toc-memory-networks与self-attention的联系" class="nav-link" data-scroll-target="#memory-networks与self-attention的联系"><span class="header-section-number">4.3</span> Memory Networks与Self-Attention的联系</a></li>
  <li><a href="#从memory-networks到transformer" id="toc-从memory-networks到transformer" class="nav-link" data-scroll-target="#从memory-networks到transformer"><span class="header-section-number">4.4</span> 从Memory Networks到Transformer</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">5</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#self-attention的数学形式" id="toc-self-attention的数学形式" class="nav-link" data-scroll-target="#self-attention的数学形式"><span class="header-section-number">5.1</span> Self-Attention的数学形式</a></li>
  <li><a href="#完整数值示例从输入到输出" id="toc-完整数值示例从输入到输出" class="nav-link" data-scroll-target="#完整数值示例从输入到输出"><span class="header-section-number">5.2</span> 完整数值示例：从输入到输出</a></li>
  <li><a href="#self-attention的复杂度分析" id="toc-self-attention的复杂度分析" class="nav-link" data-scroll-target="#self-attention的复杂度分析"><span class="header-section-number">5.3</span> Self-Attention的复杂度分析</a></li>
  </ul></li>
  <li><a href="#self-attention的致命缺陷位置信息丢失" id="toc-self-attention的致命缺陷位置信息丢失" class="nav-link" data-scroll-target="#self-attention的致命缺陷位置信息丢失"><span class="header-section-number">6</span> Self-Attention的致命缺陷：位置信息丢失</a>
  <ul class="collapse">
  <li><a href="#问题的本质" id="toc-问题的本质" class="nav-link" data-scroll-target="#问题的本质"><span class="header-section-number">6.1</span> 问题的本质</a></li>
  <li><a href="#为什么这是致命的" id="toc-为什么这是致命的" class="nav-link" data-scroll-target="#为什么这是致命的"><span class="header-section-number">6.2</span> 为什么这是致命的？</a></li>
  <li><a href="#直觉演示位置信息为什么重要" id="toc-直觉演示位置信息为什么重要" class="nav-link" data-scroll-target="#直觉演示位置信息为什么重要"><span class="header-section-number">6.3</span> 直觉演示：位置信息为什么重要</a></li>
  </ul></li>
  <li><a href="#位置编码弥补缺失的顺序信息" id="toc-位置编码弥补缺失的顺序信息" class="nav-link" data-scroll-target="#位置编码弥补缺失的顺序信息"><span class="header-section-number">7</span> 位置编码：弥补缺失的顺序信息</a>
  <ul class="collapse">
  <li><a href="#设计目标" id="toc-设计目标" class="nav-link" data-scroll-target="#设计目标"><span class="header-section-number">7.1</span> 设计目标</a></li>
  <li><a href="#方案一可学习的位置嵌入" id="toc-方案一可学习的位置嵌入" class="nav-link" data-scroll-target="#方案一可学习的位置嵌入"><span class="header-section-number">7.2</span> 方案一：可学习的位置嵌入</a></li>
  <li><a href="#方案二正弦位置编码" id="toc-方案二正弦位置编码" class="nav-link" data-scroll-target="#方案二正弦位置编码"><span class="header-section-number">7.3</span> 方案二：正弦位置编码</a></li>
  <li><a href="#正弦编码的几何直觉" id="toc-正弦编码的几何直觉" class="nav-link" data-scroll-target="#正弦编码的几何直觉"><span class="header-section-number">7.4</span> 正弦编码的几何直觉</a></li>
  <li><a href="#方案三相对位置编码" id="toc-方案三相对位置编码" class="nav-link" data-scroll-target="#方案三相对位置编码"><span class="header-section-number">7.5</span> 方案三：相对位置编码</a></li>
  <li><a href="#位置编码的注入方式" id="toc-位置编码的注入方式" class="nav-link" data-scroll-target="#位置编码的注入方式"><span class="header-section-number">7.6</span> 位置编码的注入方式</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">8</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#实现self-attention" id="toc-实现self-attention" class="nav-link" data-scroll-target="#实现self-attention"><span class="header-section-number">8.1</span> 实现Self-Attention</a></li>
  <li><a href="#实现位置编码" id="toc-实现位置编码" class="nav-link" data-scroll-target="#实现位置编码"><span class="header-section-number">8.2</span> 实现位置编码</a></li>
  <li><a href="#完整演示self-attention的效果" id="toc-完整演示self-attention的效果" class="nav-link" data-scroll-target="#完整演示self-attention的效果"><span class="header-section-number">8.3</span> 完整演示：Self-Attention的效果</a></li>
  <li><a href="#可视化注意力模式" id="toc-可视化注意力模式" class="nav-link" data-scroll-target="#可视化注意力模式"><span class="header-section-number">8.4</span> 可视化注意力模式</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">9</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么self-attention有效理论视角" id="toc-为什么self-attention有效理论视角" class="nav-link" data-scroll-target="#为什么self-attention有效理论视角"><span class="header-section-number">9.1</span> 为什么Self-Attention有效？——理论视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">9.2</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">9.3</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与展望" id="toc-局限性与展望" class="nav-link" data-scroll-target="#局限性与展望"><span class="header-section-number">10</span> 局限性与展望</a>
  <ul class="collapse">
  <li><a href="#self-attention的核心局限" id="toc-self-attention的核心局限" class="nav-link" data-scroll-target="#self-attention的核心局限"><span class="header-section-number">10.1</span> Self-Attention的核心局限</a></li>
  <li><a href="#这些局限指向什么" id="toc-这些局限指向什么" class="nav-link" data-scroll-target="#这些局限指向什么"><span class="header-section-number">10.2</span> 这些局限指向什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">11</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">11.1</span> 关键公式速查</a></li>
  </ul></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">12</span> 思考题</a></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">13</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">13.1</span> 核心论文（必读）</a></li>
  <li><a href="#理论基础" id="toc-理论基础" class="nav-link" data-scroll-target="#理论基础"><span class="header-section-number">13.2</span> 理论基础</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">13.3</span> 后续发展</a></li>
  <li><a href="#可视化资源" id="toc-可视化资源" class="nav-link" data-scroll-target="#可视化资源"><span class="header-section-number">13.4</span> 可视化资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">14</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第7章：Self-Attention的突破</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">当序列开始审视自己</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Attention</div>
    <div class="quarto-category">Self-Attention</div>
    <div class="quarto-category">Memory Networks</div>
    <div class="quarto-category">位置编码</div>
  </div>
  </div>

<div>
  <div class="description">
    从跨序列注意力到自注意力：序列如何审视自己，Q-K-V框架的建立，以及位置信息缺失的挑战与解决方案。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：序列中的每个位置能否直接关注同一序列中的其他位置？如果可以，这种”自我审视”的机制会带来什么突破，又会丢失什么？</p>
<p><strong>历史坐标</strong>：2015-2017 | Memory Networks, Decomposable Attention | 从辅助机制到核心架构的转变</p>
</blockquote>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们系统探索了Attention机制的设计空间：加性与乘性对齐函数的权衡、全局与局部范围的取舍、软注意力与硬注意力的差异。Luong的工作确立了点积注意力的效率优势，为后来的发展奠定了基础。但无论是Bahdanau还是Luong，他们的Attention都有一个共同的特点——它是<strong>跨序列</strong>的，让解码器关注编码器。</p>
<p>然而，上一章的结尾留下了两个悬而未决的问题。</p>
<p>第一个问题是：<strong>Attention能否独立于RNN？</strong> 当前的Attention只是RNN架构的增强组件。编码器是RNN，解码器也是RNN，Attention只是在它们之间架起了一座桥梁。顺序计算的瓶颈、长距离依赖的梯度问题，这些RNN的固有缺陷并没有被Attention解决。</p>
<p>第二个问题更加根本：<strong>同一序列内部的位置能否相互关注？</strong> 考虑这句话：“The animal didn’t cross the street because it was too tired.” 要理解”it”指代什么，需要建立”it”与”animal”之间的联系。在当前的RNN编码器中，这种联系只能通过逐步传递来隐式建立。但如果”it”能够直接”看到”句子中的其他词，直接计算与”animal”和”street”的相关性，理解不是会更加直接吗？</p>
<p>这就引出了本章的主角：<strong>Self-Attention</strong>——让序列”审视自己”的机制。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：Self-Attention让序列中的每个位置可以直接关注同一序列的所有其他位置，一步到位地建立任意距离的依赖关系。这不仅突破了RNN的顺序瓶颈，还为完全并行的计算打开了大门。但这种能力是有代价的——Self-Attention天生丢失了位置信息，如何弥补这一缺陷将成为关键的设计挑战。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="问题的精确定义" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="问题的精确定义"><span class="header-section-number">2.1</span> 问题的精确定义</h3>
<p>我们要解决的核心问题是：<strong>如何让序列中的每个位置都能获得关于整个序列的上下文信息？</strong></p>
<p>更正式地说，给定输入序列 <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)\)</span>，我们希望为每个位置 <span class="math inline">\(i\)</span> 生成一个上下文感知的表示 <span class="math inline">\(\mathbf{z}_i\)</span>，使得 <span class="math inline">\(\mathbf{z}_i\)</span> 能够编码来自整个序列的相关信息。</p>
<p>在RNN中，这是通过顺序计算实现的：</p>
<p><span class="math display">\[
\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)
\]</span></p>
<p>隐藏状态 <span class="math inline">\(\mathbf{h}_t\)</span> 隐式地”压缩”了位置 <span class="math inline">\(1\)</span> 到 <span class="math inline">\(t\)</span> 的所有信息。双向RNN则同时考虑前向和后向的信息。但这种方式有两个根本问题：</p>
<p><strong>顺序依赖</strong>：<span class="math inline">\(\mathbf{h}_t\)</span> 必须等 <span class="math inline">\(\mathbf{h}_{t-1}\)</span> 计算完成，无法并行。</p>
<p><strong>间接连接</strong>：位置 <span class="math inline">\(1\)</span> 的信息要到达位置 <span class="math inline">\(n\)</span>，必须经过 <span class="math inline">\(n-1\)</span> 次非线性变换。每次变换都会带来信息损失。</p>
</section>
<section id="一个关键的思想实验" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="一个关键的思想实验"><span class="header-section-number">2.2</span> 一个关键的思想实验</h3>
<p>让我们做一个思想实验。假设我们有一个句子 “I love natural language processing”，包含5个词。现在考虑两种方式来建立词与词之间的联系：</p>
<p><strong>RNN方式</strong>：</p>
<pre><code>I → love → natural → language → processing</code></pre>
<p>“I” 的信息要到达 “processing”，需要经过 4 步传递。每一步，信息都与新的输入混合，经过非线性变换。最终到达 “processing” 时，关于 “I” 的信息已经被大量稀释。</p>
<p><strong>理想方式</strong>：</p>
<pre><code>     I ←───────────────────────→ processing
       ↖                       ↗
         love ←─────────→ language
               ↖       ↗
                 natural</code></pre>
<p>每个词都可以直接”看到”其他所有词。“processing” 想要知道 “I” 的信息？直接查询，一步到位。</p>
<p>这就是Self-Attention的核心思想：<strong>把 <span class="math inline">\(O(n)\)</span> 的间接路径变成 <span class="math inline">\(O(1)\)</span> 的直接路径</strong>。</p>
</section>
<section id="我们需要什么样的解决方案" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="我们需要什么样的解决方案"><span class="header-section-number">2.3</span> 我们需要什么样的解决方案？</h3>
<p>从上述分析可以看出，理想的序列建模机制应该满足以下特性：</p>
<ol type="1">
<li><strong>直接连接</strong>：任意两个位置之间的信息传递是一步到位的，而非逐步传递</li>
<li><strong>并行计算</strong>：不同位置的表示可以同时计算，不需要等待前序计算完成</li>
<li><strong>动态权重</strong>：关注的强度应该取决于内容的相关性，而非固定的位置关系</li>
<li><strong>全局视野</strong>：每个位置都能看到整个序列，而非只有局部窗口</li>
</ol>
<p>Self-Attention正是为了同时满足这四个需求而设计的。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="从cross-attention到self-attention" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="从cross-attention到self-attention"><span class="header-section-number">3.1</span> 从Cross-Attention到Self-Attention</h3>
<p>回顾一下Seq2Seq中的Attention机制：</p>
<p><span class="math display">\[
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j^{(enc)}
\]</span></p>
<p>其中 <span class="math inline">\(\alpha_{ij}\)</span> 衡量解码器位置 <span class="math inline">\(i\)</span> 对编码器位置 <span class="math inline">\(j\)</span> 的关注程度。这里的关键是：Query来自解码器，Key和Value来自编码器——这是<strong>跨序列</strong>的注意力（Cross-Attention）。</p>
<p>Self-Attention的想法极其简单：<strong>让Query、Key、Value都来自同一个序列</strong>。</p>
<p><span class="math display">\[
\mathbf{z}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{x}_j
\]</span></p>
<p>其中 <span class="math inline">\(\alpha_{ij}\)</span> 衡量位置 <span class="math inline">\(i\)</span> 对位置 <span class="math inline">\(j\)</span> 的关注程度。现在，序列在”审视自己”——每个位置都在问：“我应该关注序列中的哪些其他位置？”</p>
</section>
<section id="query-key-value的三元组视角" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="query-key-value的三元组视角"><span class="header-section-number">3.2</span> Query-Key-Value的三元组视角</h3>
<p>为了让Self-Attention更加灵活，我们不直接用原始表示计算注意力，而是先进行线性变换：</p>
<p><span class="math display">\[
\mathbf{q}_i = \mathbf{W}_Q \mathbf{x}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{x}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{x}_i
\]</span></p>
<p>然后：</p>
<p><span class="math display">\[
\alpha_{ij} = \text{softmax}_j\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{z}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{v}_j
\]</span></p>
<p>这种Query-Key-Value的三元组结构有深刻的直觉意义：</p>
<ul>
<li><strong>Query（查询）</strong>：代表”我在找什么？“——位置 <span class="math inline">\(i\)</span> 想要获取什么样的信息</li>
<li><strong>Key（键）</strong>：代表”我是什么？“——位置 <span class="math inline">\(j\)</span> 的身份标识，用于匹配查询</li>
<li><strong>Value（值）</strong>：代表”我能提供什么？“——位置 <span class="math inline">\(j\)</span> 实际贡献的内容</li>
</ul>
<p>这个设计允许同一个位置以不同的”身份”参与计算：当它作为查询者时，它在寻找相关信息；当它作为被查询者时，它的Key决定是否被选中，它的Value决定贡献什么内容。</p>
</section>
<section id="一个具体的例子" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="一个具体的例子"><span class="header-section-number">3.3</span> 一个具体的例子</h3>
<p>考虑句子 “The cat sat on the mat”，我们想理解词 “sat” 的上下文表示。</p>
<p>在Self-Attention中，“sat” 会：</p>
<ol type="1">
<li>生成一个Query向量，编码”动词’sat’在寻找它的主语和宾语”</li>
<li>与所有词的Key向量计算相似度</li>
<li>与 “cat” 的Key高度匹配（“cat”是主语），与 “mat” 也有一定匹配（“mat”是介词宾语的对象）</li>
<li>最终的表示是所有词的Value的加权和，“cat” 和 “mat” 贡献较多</li>
</ol>
<p>这个过程完全是基于内容的——模型学习到”动词通常需要关注它的主语和宾语”，而这种学习是从数据中自动获得的，无需人工定义语法规则。</p>
<hr>
</section>
</section>
<section id="memory-networksself-attention的思想先驱" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="memory-networksself-attention的思想先驱"><span class="header-section-number">4</span> Memory Networks：Self-Attention的思想先驱</h2>
<section id="问答系统的挑战" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="问答系统的挑战"><span class="header-section-number">4.1</span> 问答系统的挑战</h3>
<p>在Self-Attention被明确提出之前，有一条平行的研究脉络在探索类似的想法：<strong>Memory Networks</strong>。</p>
<p>2014-2015年，Facebook AI Research（Sukhbaatar, Szegedy, Weston等人）提出了Memory Networks来解决需要多跳推理的问答任务。考虑以下例子：</p>
<p><strong>故事</strong>： &gt; Mary moved to the bathroom. &gt; John went to the hallway. &gt; Mary traveled to the office.</p>
<p><strong>问题</strong>：Where is Mary?</p>
<p><strong>答案</strong>：office</p>
<p>要回答这个问题，模型需要： 1. 从记忆中找到与”Mary”相关的信息 2. 识别出最新的位置信息 3. 综合这些信息得出答案</p>
<p>这比简单的文本匹配复杂得多——需要在多条信息之间建立联系，进行推理。</p>
</section>
<section id="memory-networks的设计" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="memory-networks的设计"><span class="header-section-number">4.2</span> Memory Networks的设计</h3>
<p>Memory Networks的核心思想是：将信息存储在一个外部记忆中，然后通过注意力机制来读取相关信息。</p>
<div id="fig-memory-network" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-memory-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-7/original/fig-memory-network.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-memory-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: End-To-End Memory Networks的架构。输入句子被编码为记忆向量（红色），问题被编码为查询向量（蓝色），通过注意力机制从记忆中检索相关信息。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Sukhbaatar et al.&nbsp;(2015) “End-To-End Memory Networks”, Figure 1. <a href="https://arxiv.org/abs/1503.08895">arXiv:1503.08895</a></em></p>
</div>
<p>具体来说，给定一组记忆 <span class="math inline">\(\{\mathbf{m}_1, \mathbf{m}_2, \ldots, \mathbf{m}_n\}\)</span> 和一个查询 <span class="math inline">\(\mathbf{q}\)</span>：</p>
<p><strong>Step 1: 计算注意力权重</strong></p>
<p><span class="math display">\[
p_i = \text{softmax}(\mathbf{q}^\top \mathbf{m}_i)
\]</span></p>
<p><strong>Step 2: 读取记忆</strong></p>
<p><span class="math display">\[
\mathbf{o} = \sum_{i=1}^{n} p_i \mathbf{c}_i
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{c}_i\)</span> 是记忆 <span class="math inline">\(i\)</span> 的输出表示（可以与 <span class="math inline">\(\mathbf{m}_i\)</span> 不同）。</p>
<p><strong>Step 3: 更新查询并迭代</strong></p>
<p><span class="math display">\[
\mathbf{q}' = \mathbf{q} + \mathbf{o}
\]</span></p>
<p>然后用 <span class="math inline">\(\mathbf{q}'\)</span> 再次查询记忆，进行多跳推理。</p>
</section>
<section id="memory-networks与self-attention的联系" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="memory-networks与self-attention的联系"><span class="header-section-number">4.3</span> Memory Networks与Self-Attention的联系</h3>
<p>看出来了吗？Memory Networks中的注意力机制与Self-Attention惊人地相似：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 51%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Memory Networks</th>
<th>Self-Attention</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>查询 <span class="math inline">\(\mathbf{q}\)</span></td>
<td>Query <span class="math inline">\(\mathbf{Q}\)</span></td>
</tr>
<tr class="even">
<td>记忆 <span class="math inline">\(\mathbf{m}_i\)</span></td>
<td>Key <span class="math inline">\(\mathbf{K}\)</span></td>
</tr>
<tr class="odd">
<td>输出 <span class="math inline">\(\mathbf{c}_i\)</span></td>
<td>Value <span class="math inline">\(\mathbf{V}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_i = \text{softmax}(\mathbf{q}^\top \mathbf{m}_i)\)</span></td>
<td><span class="math inline">\(\alpha = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d})\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{o} = \sum_i p_i \mathbf{c}_i\)</span></td>
<td><span class="math inline">\(\mathbf{Z} = \alpha \mathbf{V}\)</span></td>
</tr>
</tbody>
</table>
<p>关键的联系在于：<strong>Self-Attention可以看作是序列对自己进行Memory Network式的查询</strong>。</p>
<p>每个位置都是一个”记忆槽”，每个位置也都是一个”查询者”。当位置 <span class="math inline">\(i\)</span> 计算它的输出时，它就像在用自己的Query去检索其他位置（包括自己）存储的信息。</p>
<p>这个视角解释了为什么Self-Attention如此强大：它本质上是在做<strong>基于内容的关联记忆检索</strong>，而这正是推理和理解的核心能力。</p>
</section>
<section id="从memory-networks到transformer" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="从memory-networks到transformer"><span class="header-section-number">4.4</span> 从Memory Networks到Transformer</h3>
<p>Memory Networks的工作对后来的Transformer有深远影响：</p>
<ol type="1">
<li><p><strong>端到端可微分</strong>：早期的Memory Networks需要强监督（告诉模型应该关注哪些记忆），后来的End-to-End Memory Networks通过软注意力实现了端到端训练——这正是Transformer所采用的方式。</p></li>
<li><p><strong>多跳推理</strong>：Memory Networks通过多次查询记忆来实现复杂推理。Transformer中的多层堆叠也可以理解为多跳推理——每一层的Self-Attention都是一次”查询”。</p></li>
<li><p><strong>Key-Value分离</strong>：Memory Networks中 <span class="math inline">\(\mathbf{m}_i\)</span>（用于计算注意力）和 <span class="math inline">\(\mathbf{c}_i\)</span>（用于输出）可以不同。这启发了Self-Attention中Key和Value的分离设计。</p></li>
</ol>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">5</span> 技术细节</h2>
<section id="self-attention的数学形式" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="self-attention的数学形式"><span class="header-section-number">5.1</span> Self-Attention的数学形式</h3>
<p>让我们正式定义Self-Attention的计算过程。</p>
<p>给定输入序列 <span class="math inline">\(\mathbf{X} = [\mathbf{x}_1; \mathbf{x}_2; \ldots; \mathbf{x}_n]^\top \in \mathbb{R}^{n \times d}\)</span>，Self-Attention的计算如下：</p>
<p><strong>Step 1: 线性投影</strong></p>
<p><span class="math display">\[
\mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_V
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d \times d_k}\)</span>，<span class="math inline">\(\mathbf{W}_V \in \mathbb{R}^{d \times d_v}\)</span>。</p>
<p><strong>Step 2: 计算注意力分数</strong></p>
<p><span class="math display">\[
\mathbf{S} = \mathbf{Q} \mathbf{K}^\top \in \mathbb{R}^{n \times n}
\]</span></p>
<p>矩阵 <span class="math inline">\(\mathbf{S}\)</span> 的元素 <span class="math inline">\(S_{ij} = \mathbf{q}_i^\top \mathbf{k}_j\)</span> 衡量位置 <span class="math inline">\(i\)</span> 对位置 <span class="math inline">\(j\)</span> 的”关注程度”。</p>
<p><strong>Step 3: 缩放和归一化</strong></p>
<p><span class="math display">\[
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{S}}{\sqrt{d_k}}\right) \in \mathbb{R}^{n \times n}
\]</span></p>
<p>每一行是一个概率分布，表示该位置对所有位置的注意力权重。</p>
<p><strong>Step 4: 加权聚合</strong></p>
<p><span class="math display">\[
\mathbf{Z} = \mathbf{A} \mathbf{V} \in \mathbb{R}^{n \times d_v}
\]</span></p>
<p>输出 <span class="math inline">\(\mathbf{z}_i = \sum_{j=1}^{n} A_{ij} \mathbf{v}_j\)</span> 是所有Value向量的加权和。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm: Scaled Dot-Product Self-Attention (Vaswani et al., 2017)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention(X, W_Q, W_K, W_V):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Self-Attention 计算</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X: 输入序列 [batch, seq_len, d_model]</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        W_Q, W_K: 投影矩阵 [d_model, d_k]</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">        W_V: 投影矩阵 [d_model, d_v]</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Z: 输出序列 [batch, seq_len, d_v]</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        A: 注意力权重 [batch, seq_len, seq_len]</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 线性投影</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> X <span class="op">@</span> W_Q  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> X <span class="op">@</span> W_K  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> X <span class="op">@</span> W_V  <span class="co"># [batch, seq_len, d_v]</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 计算注意力分数</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> Q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch, seq_len, seq_len]</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 缩放 + Softmax</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: 加权聚合</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> A <span class="op">@</span> V  <span class="co"># [batch, seq_len, d_v]</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z, A</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Adapted from: Vaswani, A. et al.&nbsp;(2017). “Attention Is All You Need”. NeurIPS 2017. <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></em></p>
</div>
</div>
</section>
<section id="完整数值示例从输入到输出" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="完整数值示例从输入到输出"><span class="header-section-number">5.2</span> 完整数值示例：从输入到输出</h3>
<p>让我们用一个小规模的例子完整演示Self-Attention的计算过程。</p>
<p><strong>设定</strong>：句子 “I love NLP”，共3个词。<span class="math inline">\(d_{model} = 4\)</span>，<span class="math inline">\(d_k = d_v = 4\)</span>。</p>
<p><strong>Step 1: 输入表示</strong>（假设已经通过embedding获得）</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
\mathbf{x}_I \\
\mathbf{x}_{love} \\
\mathbf{x}_{NLP}
\end{bmatrix} = \begin{bmatrix}
1.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\
0.0 &amp; 1.0 &amp; 0.5 &amp; 0.5 \\
1.0 &amp; 1.0 &amp; 0.0 &amp; 0.0
\end{bmatrix}
\]</span></p>
<p><strong>Step 2: 投影矩阵</strong>（为简化，使用接近单位矩阵的值）</p>
<p><span class="math display">\[
\mathbf{W}_Q = \mathbf{W}_K = \mathbf{W}_V = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>因此 <span class="math inline">\(\mathbf{Q} = \mathbf{K} = \mathbf{V} = \mathbf{X}\)</span>。</p>
<p><strong>Step 3: 计算注意力分数 <span class="math inline">\(\mathbf{Q}\mathbf{K}^\top\)</span></strong></p>
<p><span class="math display">\[
\mathbf{S} = \mathbf{X} \mathbf{X}^\top = \begin{bmatrix}
\mathbf{x}_I^\top \mathbf{x}_I &amp; \mathbf{x}_I^\top \mathbf{x}_{love} &amp; \mathbf{x}_I^\top \mathbf{x}_{NLP} \\
\mathbf{x}_{love}^\top \mathbf{x}_I &amp; \mathbf{x}_{love}^\top \mathbf{x}_{love} &amp; \mathbf{x}_{love}^\top \mathbf{x}_{NLP} \\
\mathbf{x}_{NLP}^\top \mathbf{x}_I &amp; \mathbf{x}_{NLP}^\top \mathbf{x}_{love} &amp; \mathbf{x}_{NLP}^\top \mathbf{x}_{NLP}
\end{bmatrix}
\]</span></p>
<p>逐个计算： - <span class="math inline">\(\mathbf{x}_I^\top \mathbf{x}_I = 1^2 + 0^2 + 1^2 + 0^2 = 2.0\)</span> - <span class="math inline">\(\mathbf{x}_I^\top \mathbf{x}_{love} = 1 \times 0 + 0 \times 1 + 1 \times 0.5 + 0 \times 0.5 = 0.5\)</span> - <span class="math inline">\(\mathbf{x}_I^\top \mathbf{x}_{NLP} = 1 \times 1 + 0 \times 1 + 1 \times 0 + 0 \times 0 = 1.0\)</span> - <span class="math inline">\(\mathbf{x}_{love}^\top \mathbf{x}_{love} = 0^2 + 1^2 + 0.5^2 + 0.5^2 = 1.5\)</span> - <span class="math inline">\(\mathbf{x}_{love}^\top \mathbf{x}_{NLP} = 0 \times 1 + 1 \times 1 + 0.5 \times 0 + 0.5 \times 0 = 1.0\)</span> - <span class="math inline">\(\mathbf{x}_{NLP}^\top \mathbf{x}_{NLP} = 1^2 + 1^2 + 0^2 + 0^2 = 2.0\)</span></p>
<p><span class="math display">\[
\mathbf{S} = \begin{bmatrix}
2.0 &amp; 0.5 &amp; 1.0 \\
0.5 &amp; 1.5 &amp; 1.0 \\
1.0 &amp; 1.0 &amp; 2.0
\end{bmatrix}
\]</span></p>
<p><strong>Step 4: 缩放（除以 <span class="math inline">\(\sqrt{d_k} = 2\)</span>）</strong></p>
<p><span class="math display">\[
\frac{\mathbf{S}}{\sqrt{4}} = \begin{bmatrix}
1.0 &amp; 0.25 &amp; 0.5 \\
0.25 &amp; 0.75 &amp; 0.5 \\
0.5 &amp; 0.5 &amp; 1.0
\end{bmatrix}
\]</span></p>
<p><strong>Step 5: Softmax（按行）</strong></p>
<p>第一行：<span class="math inline">\([e^{1.0}, e^{0.25}, e^{0.5}] = [2.72, 1.28, 1.65]\)</span>，和为 <span class="math inline">\(5.65\)</span></p>
<p><span class="math display">\[
\alpha_{I} = [2.72/5.65, 1.28/5.65, 1.65/5.65] = [0.48, 0.23, 0.29]
\]</span></p>
<p>类似计算其他行：</p>
<p><span class="math display">\[
\mathbf{A} = \begin{bmatrix}
0.48 &amp; 0.23 &amp; 0.29 \\
0.26 &amp; 0.43 &amp; 0.31 \\
0.31 &amp; 0.31 &amp; 0.38
\end{bmatrix}
\]</span></p>
<p><strong>Step 6: 加权聚合 <span class="math inline">\(\mathbf{Z} = \mathbf{A}\mathbf{V}\)</span></strong></p>
<p><span class="math display">\[
\mathbf{z}_I = 0.48 \times \mathbf{x}_I + 0.23 \times \mathbf{x}_{love} + 0.29 \times \mathbf{x}_{NLP}
\]</span></p>
<p><span class="math display">\[
= 0.48 \times [1, 0, 1, 0] + 0.23 \times [0, 1, 0.5, 0.5] + 0.29 \times [1, 1, 0, 0]
\]</span></p>
<p><span class="math display">\[
= [0.48 + 0 + 0.29, 0 + 0.23 + 0.29, 0.48 + 0.115 + 0, 0 + 0.115 + 0]
\]</span></p>
<p><span class="math display">\[
= [0.77, 0.52, 0.60, 0.12]
\]</span></p>
<p><strong>解读</strong>：</p>
<p>“I” 的Self-Attention输出不再只是自己的表示，而是融合了整个句子的信息。注意力分布 <span class="math inline">\([0.48, 0.23, 0.29]\)</span> 显示： - “I” 最关注自己（0.48）——这是合理的，自己的信息最相关 - 对 “NLP”（0.29）的关注略高于 “love”（0.23）——在这个简化例子中，这来自于向量相似度</p>
<p>在真实的训练模型中，这些权重会学习到语言的结构：主语会关注动词，动词会关注宾语，代词会关注它的指代对象。</p>
</section>
<section id="self-attention的复杂度分析" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="self-attention的复杂度分析"><span class="header-section-number">5.3</span> Self-Attention的复杂度分析</h3>
<p><strong>时间复杂度</strong>：</p>
<ul>
<li>投影：<span class="math inline">\(O(n \cdot d \cdot d_k)\)</span>，三次投影共 <span class="math inline">\(O(n \cdot d^2)\)</span></li>
<li>计算 <span class="math inline">\(\mathbf{Q}\mathbf{K}^\top\)</span>：<span class="math inline">\(O(n^2 \cdot d_k)\)</span></li>
<li>Softmax：<span class="math inline">\(O(n^2)\)</span></li>
<li>计算 <span class="math inline">\(\mathbf{A}\mathbf{V}\)</span>：<span class="math inline">\(O(n^2 \cdot d_v)\)</span></li>
</ul>
<p>总体：<span class="math inline">\(O(n^2 \cdot d + n \cdot d^2)\)</span></p>
<p>当 <span class="math inline">\(n\)</span> 很大时，<span class="math inline">\(O(n^2)\)</span> 是主导项——这是Self-Attention的主要瓶颈。</p>
<p><strong>空间复杂度</strong>：</p>
<p>需要存储 <span class="math inline">\(n \times n\)</span> 的注意力矩阵，因此空间复杂度为 <span class="math inline">\(O(n^2)\)</span>。</p>
<p><strong>与RNN的对比</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>指标</th>
<th>RNN</th>
<th>Self-Attention</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>时间复杂度</td>
<td><span class="math inline">\(O(n \cdot d^2)\)</span></td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span></td>
</tr>
<tr class="even">
<td>并行度</td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr class="odd">
<td>最长路径</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="even">
<td>空间复杂度</td>
<td><span class="math inline">\(O(d)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
</tbody>
</table>
<p>Self-Attention用 <span class="math inline">\(O(n^2)\)</span> 的复杂度换来了： 1. 完全并行化（从 <span class="math inline">\(O(1)\)</span> 到 <span class="math inline">\(O(n)\)</span>） 2. 任意位置之间的直接路径（从 <span class="math inline">\(O(n)\)</span> 到 <span class="math inline">\(O(1)\)</span>）</p>
<p>这个权衡在大多数NLP任务中是值得的，因为句子长度通常不超过几百个词。但对于超长序列（如文档、代码），<span class="math inline">\(O(n^2)\)</span> 会成为严重瓶颈——这催生了后来的高效注意力变体（Sparse Attention、Linear Attention等）。</p>
<hr>
</section>
</section>
<section id="self-attention的致命缺陷位置信息丢失" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="self-attention的致命缺陷位置信息丢失"><span class="header-section-number">6</span> Self-Attention的致命缺陷：位置信息丢失</h2>
<section id="问题的本质" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="问题的本质"><span class="header-section-number">6.1</span> 问题的本质</h3>
<p>仔细观察Self-Attention的计算公式：</p>
<p><span class="math display">\[
\mathbf{z}_i = \sum_{j=1}^{n} \text{softmax}\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right) \mathbf{v}_j
\]</span></p>
<p>这里有一个致命的问题：<strong>计算完全与位置无关</strong>。</p>
<p>如果我们打乱输入序列的顺序，比如把 “I love NLP” 变成 “NLP I love”，每个词的注意力权重只是相应地打乱，最终输出只是重新排列，与原来一一对应。</p>
<p>更正式地说，Self-Attention是<strong>置换等变的（permutation equivariant）</strong>：</p>
<p><span class="math display">\[
\text{SelfAttn}(\mathbf{P}\mathbf{X}) = \mathbf{P} \cdot \text{SelfAttn}(\mathbf{X})
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{P}\)</span> 是任意置换矩阵。</p>
<p>这意味着：Self-Attention完全不知道”谁在谁前面”。</p>
</section>
<section id="为什么这是致命的" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="为什么这是致命的"><span class="header-section-number">6.2</span> 为什么这是致命的？</h3>
<p>在自然语言中，顺序携带着关键信息：</p>
<ol type="1">
<li><p><strong>语法角色</strong>：“The dog bit the man” vs “The man bit the dog” 意思完全不同</p></li>
<li><p><strong>时态</strong>：“I will go” vs “I go” vs “I went” 依赖于词的相对位置</p></li>
<li><p><strong>指代消解</strong>：“John told Bill that he…” 中 “he” 通常指代较近的名词</p></li>
<li><p><strong>否定范围</strong>：“I didn’t go to school” vs “I went to not school” 否定词的位置决定否定的范围</p></li>
</ol>
<p>如果模型不知道词的顺序，它怎么可能理解语言？</p>
</section>
<section id="直觉演示位置信息为什么重要" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="直觉演示位置信息为什么重要"><span class="header-section-number">6.3</span> 直觉演示：位置信息为什么重要</h3>
<p>让我们用一个简单的例子来直观感受位置信息的重要性。</p>
<p>考虑两个句子： - A: “The cat chased the mouse” - B: “The mouse chased the cat”</p>
<p>在纯Self-Attention中（没有位置编码），两个句子的表示是<strong>完全相同的</strong>，因为包含的词集合相同，只是顺序不同。</p>
<p>但这两个句子的意思完全相反！如果模型不能区分它们，就无法正确理解语言。</p>
<hr>
</section>
</section>
<section id="位置编码弥补缺失的顺序信息" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="位置编码弥补缺失的顺序信息"><span class="header-section-number">7</span> 位置编码：弥补缺失的顺序信息</h2>
<section id="设计目标" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="设计目标"><span class="header-section-number">7.1</span> 设计目标</h3>
<p>我们需要一种方法，将位置信息注入到Self-Attention中。这种方法应该满足：</p>
<ol type="1">
<li><strong>唯一性</strong>：不同位置的编码应该不同</li>
<li><strong>可泛化</strong>：模型应该能够处理训练时没见过的长度</li>
<li><strong>相对关系</strong>：编码应该能表达位置之间的相对关系，而非仅仅是绝对位置</li>
<li><strong>有界性</strong>：编码值不应随位置无限增长</li>
</ol>
</section>
<section id="方案一可学习的位置嵌入" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="方案一可学习的位置嵌入"><span class="header-section-number">7.2</span> 方案一：可学习的位置嵌入</h3>
<p>最简单的想法：为每个位置学习一个向量。</p>
<p><span class="math display">\[
\mathbf{x}_i' = \mathbf{x}_i + \mathbf{p}_i
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{p}_i \in \mathbb{R}^d\)</span> 是位置 <span class="math inline">\(i\)</span> 的可学习嵌入。</p>
<p><strong>优点</strong>： - 简单直观 - 模型可以自由学习位置表示</p>
<p><strong>缺点</strong>： - 无法泛化到训练时没见过的长度 - 如果最长训练序列是512，就无法处理513长度的输入 - 不能表达相对位置关系</p>
<p>这种方式被BERT等模型采用，但需要配合固定的最大长度限制。</p>
</section>
<section id="方案二正弦位置编码" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="方案二正弦位置编码"><span class="header-section-number">7.3</span> 方案二：正弦位置编码</h3>
<p>Transformer论文提出了一种优雅的方案：使用不同频率的正弦和余弦函数。</p>
<p><span class="math display">\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
\]</span></p>
<p><span class="math display">\[
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]</span></p>
<p>其中 <span class="math inline">\(pos\)</span> 是位置，<span class="math inline">\(i\)</span> 是维度索引。</p>
<p>这个看起来复杂的公式实际上有着深刻的设计思想：</p>
<p><strong>1. 唯一性</strong>：每个位置有唯一的编码向量</p>
<p><strong>2. 有界性</strong>：正弦和余弦函数的值域是 <span class="math inline">\([-1, 1]\)</span>，不会随位置爆炸</p>
<p><strong>3. 长度泛化</strong>：函数是连续的，任何长度都可以计算</p>
<p><strong>4. 相对位置可表达</strong>：这是最精妙的部分。可以证明，位置 <span class="math inline">\(pos + k\)</span> 的编码可以表示为位置 <span class="math inline">\(pos\)</span> 的编码的线性变换：</p>
<p><span class="math display">\[
PE_{pos+k} = T_k \cdot PE_{pos}
\]</span></p>
<p>其中 <span class="math inline">\(T_k\)</span> 是只依赖于 <span class="math inline">\(k\)</span>（偏移量）的矩阵。这意味着模型有可能学习到相对位置关系。</p>
</section>
<section id="正弦编码的几何直觉" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="正弦编码的几何直觉"><span class="header-section-number">7.4</span> 正弦编码的几何直觉</h3>
<p>想象一个时钟：</p>
<ul>
<li>秒针转一圈是60秒（高频）</li>
<li>分针转一圈是60分钟（中频）</li>
<li>时针转一圈是12小时（低频）</li>
</ul>
<p>不同的指针在不同的”频率”上运动，组合起来可以唯一地表示时间。</p>
<p>正弦位置编码的原理类似： - 低维度（小 <span class="math inline">\(i\)</span>）的正弦波频率高，变化快，编码<strong>局部位置信息</strong> - 高维度（大 <span class="math inline">\(i\)</span>）的正弦波频率低，变化慢，编码<strong>全局位置信息</strong></p>
<p>组合所有维度，就能唯一标识每个位置。</p>
<div id="baffb8fe" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sinusoidal_position_encoding(max_len, d_model):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""计算正弦位置编码"""</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((max_len, d_model))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.arange(max_len)[:, np.newaxis]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> np.exp(np.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> sinusoidal_position_encoding(<span class="dv">100</span>, <span class="dv">64</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 左图：热力图</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> axes[<span class="dv">0</span>].imshow(pe, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Dimension'</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Position'</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Sinusoidal Position Encoding'</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 右图：不同维度的波形</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dim <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>]:</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(pe[:, dim], label<span class="op">=</span><span class="ss">f'dim </span><span class="sc">{</span>dim<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Position'</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Value'</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Different Dimensions = Different Frequencies'</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ch07-self-attention_files/figure-html/cell-2-output-1.png" width="1334" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="方案三相对位置编码" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="方案三相对位置编码"><span class="header-section-number">7.5</span> 方案三：相对位置编码</h3>
<p>正弦编码虽然精巧，但它编码的是<strong>绝对位置</strong>。一些研究者认为，对于语言理解来说，<strong>相对位置</strong>（两个词之间的距离）可能比绝对位置更重要。</p>
<p>相对位置编码的核心思想是修改注意力计算，让它直接考虑位置差：</p>
<p><span class="math display">\[
\alpha_{ij} \propto \exp\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{r}_{j-i}}{\sqrt{d_k}}\right)
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{r}_{j-i}\)</span> 是表示相对位置 <span class="math inline">\(j-i\)</span> 的向量。</p>
<p>这种方式的优点是： - 直接建模相对关系 - 不同层可以有不同的相对位置偏好 - 理论上可以泛化到任意长度</p>
<p>后来的RoPE（Rotary Position Embedding）进一步发展了这个思想，成为现代大语言模型的标配——我们将在第26章详细讨论。</p>
</section>
<section id="位置编码的注入方式" class="level3" data-number="7.6">
<h3 data-number="7.6" class="anchored" data-anchor-id="位置编码的注入方式"><span class="header-section-number">7.6</span> 位置编码的注入方式</h3>
<p>有了位置编码 <span class="math inline">\(\mathbf{PE}\)</span>，如何将其与输入结合？主要有两种方式：</p>
<p><strong>加法注入（Transformer采用）</strong>：</p>
<p><span class="math display">\[
\mathbf{X}' = \mathbf{X} + \mathbf{PE}
\]</span></p>
<p>将位置编码直接加到词嵌入上。这假设位置信息和语义信息可以在同一空间中表达和混合。</p>
<p><strong>拼接注入</strong>：</p>
<p><span class="math display">\[
\mathbf{X}' = [\mathbf{X}; \mathbf{PE}]
\]</span></p>
<p>将位置编码与词嵌入拼接，保持两者分离。这需要更多参数，但避免了信息混淆。</p>
<p>实践中，加法注入更常用，因为它更简洁且效果良好。</p>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">8</span> 工程实践</h2>
<section id="实现self-attention" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="实现self-attention"><span class="header-section-number">8.1</span> 实现Self-Attention</h3>
<div id="b6127518" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Self-Attention 模块</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_k<span class="op">=</span><span class="va">None</span>, d_v<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_k <span class="cf">if</span> d_k <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> d_model</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_v <span class="op">=</span> d_v <span class="cf">if</span> d_v <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> d_model</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q, K, V 投影矩阵</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_Q <span class="op">=</span> nn.Linear(d_model, <span class="va">self</span>.d_k, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_K <span class="op">=</span> nn.Linear(d_model, <span class="va">self</span>.d_k, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_V <span class="op">=</span> nn.Linear(d_model, <span class="va">self</span>.d_v, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 缩放因子</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> math.sqrt(<span class="va">self</span>.d_k)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">        X: [batch, seq_len, d_model]</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, seq_len, seq_len], True 表示需要 mask 的位置</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co">        返回:</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co">            output: [batch, seq_len, d_v]</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weights: [batch, seq_len, seq_len]</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 线性投影</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.W_Q(X)  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.W_K(X)  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.W_V(X)  <span class="co"># [batch, seq_len, d_v]</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 计算注意力分数</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(Q, K.transpose(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="op">/</span> <span class="va">self</span>.scale  <span class="co"># [batch, seq_len, seq_len]</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 应用 mask</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Softmax</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: 加权聚合</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.bmm(attention_weights, V)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="实现位置编码" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="实现位置编码"><span class="header-section-number">8.2</span> 实现位置编码</h3>
<div id="2274baf1" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    正弦位置编码</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 预计算位置编码</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)  <span class="co"># [1, max_len, d_model]</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch, seq_len, d_model]</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LearnablePositionalEmbedding(nn.Module):</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="co">    可学习的位置嵌入</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> nn.Embedding(max_len, d_model)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch, seq_len, d_model]</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>x.device).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.position_embedding(positions)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="完整演示self-attention的效果" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="完整演示self-attention的效果"><span class="header-section-number">8.3</span> 完整演示：Self-Attention的效果</h3>
<div id="ad5ab18e" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建测试数据</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 随机输入</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(batch_size, seq_len, d_model)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建模型</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> SelfAttention(d_model)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>pos_enc <span class="op">=</span> PositionalEncoding(d_model)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 添加位置编码</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>X_with_pos <span class="op">=</span> pos_enc(X)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算 Self-Attention</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>output, attention_weights <span class="op">=</span> self_attn(X_with_pos)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"输入形状: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"输出形状: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"注意力权重形状: </span><span class="sc">{</span>attention_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">第一个样本的注意力矩阵:</span><span class="ch">\n</span><span class="sc">{</span>attention_weights[<span class="dv">0</span>]<span class="sc">.</span>detach()<span class="sc">.</span>numpy()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>输入形状: torch.Size([2, 5, 64])
输出形状: torch.Size([2, 5, 64])
注意力权重形状: torch.Size([2, 5, 5])

第一个样本的注意力矩阵:
[[0.11 0.33 0.32 0.09 0.15]
 [0.13 0.31 0.16 0.19 0.22]
 [0.14 0.22 0.21 0.22 0.22]
 [0.14 0.27 0.21 0.11 0.27]
 [0.18 0.19 0.44 0.1  0.09]]</code></pre>
</div>
</div>
</section>
<section id="可视化注意力模式" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="可视化注意力模式"><span class="header-section-number">8.4</span> 可视化注意力模式</h3>
<div id="473322c5" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建一个有意义的例子</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"The"</span>, <span class="st">"cat"</span>, <span class="st">"sat"</span>, <span class="st">"on"</span>, <span class="st">"mat"</span>]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="bu">len</span>(words)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 模拟一个训练好的注意力矩阵</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 这个矩阵展示了一些语言学上合理的模式</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>attention_pattern <span class="op">=</span> torch.tensor([</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>],  <span class="co"># "The" 主要关注自己和 "cat"</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>],  <span class="co"># "cat" 关注 "sat"（动词）</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>],    <span class="co"># "sat" 关注 "cat"（主语）和 "on"</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.25</span>],  <span class="co"># "on" 关注 "sat" 和 "mat"</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>],  <span class="co"># "mat" 关注 "on" 和自己</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax.imshow(attention_pattern, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(<span class="bu">range</span>(seq_len))</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(<span class="bu">range</span>(seq_len))</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(words)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(words)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Key (attended to)'</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Query (attending from)'</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Self-Attention Weights: "The cat sat on mat"'</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 添加数值标注</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        ax.text(j, i, <span class="ss">f'</span><span class="sc">{</span>attention_pattern[i, j]<span class="sc">:.2f}</span><span class="ss">'</span>,</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'black'</span> <span class="cf">if</span> attention_pattern[i, j] <span class="op">&lt;</span> <span class="fl">0.3</span> <span class="cf">else</span> <span class="st">'white'</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ch07-self-attention_files/figure-html/cell-6-output-1.png" width="659" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">9</span> 深入理解</h2>
<section id="为什么self-attention有效理论视角" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="为什么self-attention有效理论视角"><span class="header-section-number">9.1</span> 为什么Self-Attention有效？——理论视角</h3>
<p><strong>表达能力分析</strong></p>
<p>Self-Attention可以看作一种特殊的图神经网络，其中图是完全连接的，边权重由注意力决定。Yun et al.&nbsp;(2020) 证明了：在适当条件下，Self-Attention是<strong>通用函数逼近器</strong>——它可以逼近任何连续的置换等变函数。</p>
<p><strong>与卷积的对比</strong></p>
<p>卷积神经网络（CNN）也可以处理序列，但它有固定的感受野。一个3-gram卷积只能看到相邻的3个词；要看到更远的词，需要堆叠多层。Self-Attention则一步就能看到全局。</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>特性</th>
<th>CNN</th>
<th>Self-Attention</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>感受野</td>
<td>局部（逐层扩大）</td>
<td>全局（一层到位）</td>
</tr>
<tr class="even">
<td>计算复杂度</td>
<td><span class="math inline">\(O(n \cdot k^2 \cdot d)\)</span></td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span></td>
</tr>
<tr class="odd">
<td>位置偏置</td>
<td>强（相对位置关系固定）</td>
<td>弱（需要位置编码）</td>
</tr>
<tr class="even">
<td>参数共享</td>
<td>跨位置共享</td>
<td>Q/K/V投影共享</td>
</tr>
</tbody>
</table>
<p><strong>Memory Network视角</strong></p>
<p>如前所述，Self-Attention可以理解为序列对自己进行Memory Network式的查询。每一层Self-Attention都是一次”多跳推理”——前一层的输出成为下一层的”记忆”。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">9.2</span> 方法的边界条件</h3>
<p><strong>Self-Attention的隐含假设</strong>：</p>
<ol type="1">
<li><p><strong>全局相关性假设</strong>：每个位置都可能与任何其他位置相关。但对于某些任务（如时间序列预测），局部相关性可能更重要。</p></li>
<li><p><strong>均匀计算假设</strong>：所有位置之间的交互用相同的计算量。但直觉上，“重要”的交互可能需要更多计算。</p></li>
<li><p><strong>线性可分假设</strong>：相关性可以通过向量点积来衡量。但有些复杂的关系可能无法用点积捕获。</p></li>
</ol>
<p><strong>失效条件</strong>：</p>
<ol type="1">
<li><strong>超长序列</strong>：当 <span class="math inline">\(n &gt; 10000\)</span> 时，<span class="math inline">\(O(n^2)\)</span> 复杂度变得不可接受。</li>
<li><strong>强位置依赖任务</strong>：对于需要精确位置推理的任务（如算术），Self-Attention常常失败。</li>
<li><strong>数据量不足</strong>：Self-Attention的弱归纳偏置需要大量数据来补偿。</li>
</ol>
</section>
<section id="开放研究问题" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">9.3</span> 开放研究问题</h3>
<ol type="1">
<li><p><strong>位置编码的最优设计</strong>：正弦编码、可学习编码、相对位置编码——哪种最好？是否存在”最优”的位置编码？</p></li>
<li><p><strong>Self-Attention的可解释性</strong>：注意力权重真的代表了”重要性”吗？还是只是计算的副产品？</p></li>
<li><p><strong>效率与表达能力的权衡</strong>：能否设计出 <span class="math inline">\(O(n)\)</span> 复杂度但保持 <span class="math inline">\(O(n^2)\)</span> 表达能力的注意力机制？</p></li>
<li><p><strong>归纳偏置的设计</strong>：如何在Self-Attention中引入合适的归纳偏置，减少对数据量的依赖？</p></li>
</ol>
<hr>
</section>
</section>
<section id="局限性与展望" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="局限性与展望"><span class="header-section-number">10</span> 局限性与展望</h2>
<section id="self-attention的核心局限" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="self-attention的核心局限"><span class="header-section-number">10.1</span> Self-Attention的核心局限</h3>
<p><strong>1. 位置信息仍是”补丁”</strong></p>
<p>虽然位置编码解决了问题，但它是一种事后补救。Self-Attention本身不具备位置感知能力，位置信息是外部注入的。这种设计是否最优？是否有更优雅的方式让注意力机制天生具有位置感知能力？</p>
<p><strong>2. <span class="math inline">\(O(n^2)\)</span> 复杂度</strong></p>
<p>全局注意力的代价是 <span class="math inline">\(n \times n\)</span> 的注意力矩阵。当序列长度达到数万甚至数十万时，这变得不可行。如何在保持全局视野的同时降低复杂度？</p>
<p><strong>3. 单一的注意力模式</strong></p>
<p>在一层Self-Attention中，每个位置只生成一组Query、Key、Value。但直觉上，一个位置可能需要同时关注多种不同类型的信息——比如主语需要关注动词（语法），也需要关注语境中的相关实体（语义）。能否让模型同时捕获多种关注模式？</p>
</section>
<section id="这些局限指向什么" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="这些局限指向什么"><span class="header-section-number">10.2</span> 这些局限指向什么？</h3>
<p>Self-Attention为序列建模提供了一个强大的基础，但要构建完整的架构，还需要回答几个关键问题：</p>
<p><strong>如何堆叠多层？</strong> 简单地堆叠Self-Attention层会导致什么问题？需要什么额外的组件来保证训练稳定性？</p>
<p><strong>如何处理编码器-解码器结构？</strong> 机器翻译需要编码器理解源语言，解码器生成目标语言。Self-Attention如何在这个框架中使用？解码器如何既关注自己已生成的内容，又关注编码器的输出？</p>
<p><strong>如何实现”多头”注意力？</strong> 能否让多个注意力”头”同时工作，每个头关注不同的子空间，最后综合它们的输出？</p>
<p>这些问题的答案，将在下一章揭晓。2017年，Google的研究团队在论文”Attention Is All You Need”中提出了<strong>Transformer</strong>架构——一个完全基于注意力的序列到序列模型。它不仅回答了上述所有问题，还以惊人的效果证明了一个大胆的论断：<strong>注意力，就是你所需要的一切</strong>。</p>
<blockquote class="blockquote">
<p>Self-Attention让序列学会了”审视自己”，打破了RNN的顺序枷锁。但它只是革命的序曲。当研究者意识到可以完全抛弃循环结构，只用注意力来构建整个模型时，深度学习的历史翻开了新的一页。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">11</span> 本章小结</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>核心要点
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>问题</strong>：如何让序列中的每个位置获得全局上下文信息，同时避免RNN的顺序计算瓶颈？</li>
<li><strong>洞察</strong>：让序列中的每个位置直接关注所有其他位置（Self-Attention），将 <span class="math inline">\(O(n)\)</span> 的间接路径变成 <span class="math inline">\(O(1)\)</span> 的直接路径</li>
<li><strong>方法</strong>：Query-Key-Value三元组结构，通过点积计算注意力权重，加权求和得到输出</li>
<li><strong>代价</strong>：Self-Attention丢失位置信息，需要位置编码来弥补；复杂度为 <span class="math inline">\(O(n^2)\)</span></li>
<li><strong>意义</strong>：Self-Attention为完全抛弃RNN、构建纯注意力架构奠定了基础</li>
</ul>
</div>
</div>
<section id="关键公式速查" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">11.1</span> 关键公式速查</h3>
<p><strong>Self-Attention</strong>：</p>
<p><span class="math display">\[
\text{SelfAttn}(\mathbf{X}) = \text{softmax}\left(\frac{\mathbf{X}\mathbf{W}_Q (\mathbf{X}\mathbf{W}_K)^\top}{\sqrt{d_k}}\right) \mathbf{X}\mathbf{W}_V
\]</span></p>
<p><strong>正弦位置编码</strong>：</p>
<p><span class="math display">\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]</span></p>
<p><strong>与Cross-Attention的对比</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>类型</th>
<th>Query来源</th>
<th>Key/Value来源</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cross-Attention</td>
<td>解码器</td>
<td>编码器</td>
<td>跨序列关注</td>
</tr>
<tr class="even">
<td>Self-Attention</td>
<td>同一序列</td>
<td>同一序列</td>
<td>序列内部建模</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="思考题" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="思考题"><span class="header-section-number">12</span> 思考题</h2>
<ol type="1">
<li><p><strong>[概念理解]</strong> Self-Attention为什么是”置换等变”的？设计一个简单的实验来验证这一点。如果我们想让模型区分 “dog bites man” 和 “man bites dog”，仅靠Self-Attention（不加位置编码）能做到吗？为什么？</p></li>
<li><p><strong>[数学推导]</strong> 证明正弦位置编码的相对位置性质：<span class="math inline">\(PE_{pos+k}\)</span> 可以表示为 <span class="math inline">\(PE_{pos}\)</span> 的线性变换。具体写出变换矩阵的形式。（提示：利用三角函数的和差公式）</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 实现一个简单的Self-Attention文本分类器（不使用RNN）：</p>
<ul>
<li>输入：IMDB电影评论</li>
<li>架构：Embedding → Self-Attention → 平均池化 → 分类头</li>
<li>对比有无位置编码的效果差异</li>
</ul></li>
<li><p><strong>[批判思考]</strong> Memory Networks和Self-Attention在形式上非常相似。它们的本质区别是什么？Memory Networks的”外部记忆”和Self-Attention的”序列作为记忆”有什么不同的设计考量？</p></li>
<li><p><strong>[开放问题]</strong> Self-Attention的 <span class="math inline">\(O(n^2)\)</span> 复杂度是一个根本限制吗？有没有可能设计出 <span class="math inline">\(O(n)\)</span> 复杂度的注意力机制，同时保持 <span class="math inline">\(O(1)\)</span> 的最长路径？现有的线性注意力（Linear Attention）为什么会有性能损失？</p></li>
</ol>
<hr>
</section>
<section id="延伸阅读" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">13</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="13.1">
<h3 data-number="13.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">13.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>[Vaswani et al., 2017] Attention Is All You Need</strong>
<ul>
<li>提出Transformer和Scaled Dot-Product Attention</li>
<li>重点读：Section 3.2（Attention）、Section 3.5（位置编码）</li>
<li>arXiv: <a href="https://arxiv.org/abs/1706.03762">1706.03762</a></li>
</ul></li>
<li><strong>[Sukhbaatar et al., 2015] End-To-End Memory Networks</strong>
<ul>
<li>Self-Attention的思想先驱</li>
<li>重点读：Section 2（模型架构）</li>
<li>arXiv: <a href="https://arxiv.org/abs/1503.08895">1503.08895</a></li>
</ul></li>
</ul>
</section>
<section id="理论基础" class="level3" data-number="13.2">
<h3 data-number="13.2" class="anchored" data-anchor-id="理论基础"><span class="header-section-number">13.2</span> 理论基础</h3>
<ul>
<li><strong>[Yun et al., 2020] Are Transformers universal approximators of sequence-to-sequence functions?</strong>
<ul>
<li>证明Transformer的通用逼近性质</li>
<li>arXiv: <a href="https://arxiv.org/abs/1912.10077">1912.10077</a></li>
</ul></li>
<li><strong>[Ramsauer et al., 2020] Hopfield Networks is All You Need</strong>
<ul>
<li>将Attention解释为现代Hopfield网络</li>
<li>arXiv: <a href="https://arxiv.org/abs/2008.02217">2008.02217</a></li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="13.3">
<h3 data-number="13.3" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">13.3</span> 后续发展</h3>
<ul>
<li><strong>[Shaw et al., 2018] Self-Attention with Relative Position Representations</strong>
<ul>
<li>相对位置编码的早期工作</li>
<li>arXiv: <a href="https://arxiv.org/abs/1803.02155">1803.02155</a></li>
</ul></li>
<li><strong>[Su et al., 2021] RoFormer: Enhanced Transformer with Rotary Position Embedding</strong>
<ul>
<li>提出RoPE，成为现代LLM的标配</li>
<li>arXiv: <a href="https://arxiv.org/abs/2104.09864">2104.09864</a></li>
</ul></li>
</ul>
</section>
<section id="可视化资源" class="level3" data-number="13.4">
<h3 data-number="13.4" class="anchored" data-anchor-id="可视化资源"><span class="header-section-number">13.4</span> 可视化资源</h3>
<ul>
<li><strong>[Jay Alammar] The Illustrated Transformer</strong>
<ul>
<li>最佳可视化教程</li>
<li>网址: <a href="https://jalammar.github.io/illustrated-transformer/">jalammar.github.io/illustrated-transformer</a></li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">14</span> 历史注脚</h2>
<p>Self-Attention的想法并非凭空出现。在2017年Transformer论文之前，已经有多条研究脉络在探索类似的思想：</p>
<p><strong>Memory Networks (2014-2015)</strong>：Facebook AI Research的团队提出了用注意力机制从外部记忆中检索信息的想法。虽然他们的目标是问答系统而非序列建模，但Key-Value分离、软注意力检索等设计深刻影响了后来的发展。</p>
<p><strong>Neural Turing Machines (2014)</strong>：DeepMind提出的神经图灵机也使用了类似注意力的机制来读写外部记忆。虽然更加复杂，但它展示了神经网络可以学习类似”查找表”的操作。</p>
<p><strong>Decomposable Attention (2016)</strong>：Parikh等人提出了一个完全基于注意力的自然语言推理模型，不使用任何RNN。这是”纯注意力”模型的早期成功案例。</p>
<p><strong>Transformer的贡献</strong>：不是发明Self-Attention，而是将其系统化并证明它可以完全替代RNN。“Attention Is All You Need”这个大胆的标题，既是技术声明，也是研究宣言。</p>
<p>有趣的是，Transformer论文的作者之一Ashish Vaswani后来回忆说，他们最初并不确定完全抛弃RNN是否可行。实验结果的惊人效果超出了所有人的预期——不仅效果更好，训练还快了一个数量级。这个结果改变了整个领域的方向。</p>
<p>从Memory Networks到Transformer，从辅助机制到核心架构，Self-Attention完成了从”配角”到”主角”的华丽转身。下一章，我们将见证这场革命的高潮：Transformer——“Attention Is All You Need”。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第7章：Self-Attention的突破"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "当序列开始审视自己"</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-25"</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Attention, Self-Attention, Memory Networks, 位置编码]</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [自注意力, Q-K-V, 位置编码, 正弦编码, 置换不变性, Decomposable Attention]</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "从跨序列注意力到自注意力：序列如何审视自己，Q-K-V框架的建立，以及位置信息缺失的挑战与解决方案。"</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：序列中的每个位置能否直接关注同一序列中的其他位置？如果可以，这种"自我审视"的机制会带来什么突破，又会丢失什么？</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2015-2017 </span><span class="pp">|</span><span class="at"> Memory Networks, Decomposable Attention </span><span class="pp">|</span><span class="at"> 从辅助机制到核心架构的转变</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>上一章我们系统探索了Attention机制的设计空间：加性与乘性对齐函数的权衡、全局与局部范围的取舍、软注意力与硬注意力的差异。Luong的工作确立了点积注意力的效率优势，为后来的发展奠定了基础。但无论是Bahdanau还是Luong，他们的Attention都有一个共同的特点——它是**跨序列**的，让解码器关注编码器。</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>然而，上一章的结尾留下了两个悬而未决的问题。</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>第一个问题是：**Attention能否独立于RNN？** 当前的Attention只是RNN架构的增强组件。编码器是RNN，解码器也是RNN，Attention只是在它们之间架起了一座桥梁。顺序计算的瓶颈、长距离依赖的梯度问题，这些RNN的固有缺陷并没有被Attention解决。</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>第二个问题更加根本：**同一序列内部的位置能否相互关注？** 考虑这句话："The animal didn't cross the street because it was too tired." 要理解"it"指代什么，需要建立"it"与"animal"之间的联系。在当前的RNN编码器中，这种联系只能通过逐步传递来隐式建立。但如果"it"能够直接"看到"句子中的其他词，直接计算与"animal"和"street"的相关性，理解不是会更加直接吗？</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>这就引出了本章的主角：**Self-Attention**——让序列"审视自己"的机制。</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：Self-Attention让序列中的每个位置可以直接关注同一序列的所有其他位置，一步到位地建立任意距离的依赖关系。这不仅突破了RNN的顺序瓶颈，还为完全并行的计算打开了大门。但这种能力是有代价的——Self-Attention天生丢失了位置信息，如何弥补这一缺陷将成为关键的设计挑战。</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的精确定义</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>我们要解决的核心问题是：**如何让序列中的每个位置都能获得关于整个序列的上下文信息？**</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>更正式地说，给定输入序列 $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)$，我们希望为每个位置 $i$ 生成一个上下文感知的表示 $\mathbf{z}_i$，使得 $\mathbf{z}_i$ 能够编码来自整个序列的相关信息。</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>在RNN中，这是通过顺序计算实现的：</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>隐藏状态 $\mathbf{h}_t$ 隐式地"压缩"了位置 $1$ 到 $t$ 的所有信息。双向RNN则同时考虑前向和后向的信息。但这种方式有两个根本问题：</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>**顺序依赖**：$\mathbf{h}_t$ 必须等 $\mathbf{h}_{t-1}$ 计算完成，无法并行。</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>**间接连接**：位置 $1$ 的信息要到达位置 $n$，必须经过 $n-1$ 次非线性变换。每次变换都会带来信息损失。</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个关键的思想实验</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>让我们做一个思想实验。假设我们有一个句子 "I love natural language processing"，包含5个词。现在考虑两种方式来建立词与词之间的联系：</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>**RNN方式**：</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a><span class="in">I → love → natural → language → processing</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>"I" 的信息要到达 "processing"，需要经过 4 步传递。每一步，信息都与新的输入混合，经过非线性变换。最终到达 "processing" 时，关于 "I" 的信息已经被大量稀释。</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>**理想方式**：</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a><span class="in">     I ←───────────────────────→ processing</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a><span class="in">       ↖                       ↗</span></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="in">         love ←─────────→ language</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="in">               ↖       ↗</span></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a><span class="in">                 natural</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>每个词都可以直接"看到"其他所有词。"processing" 想要知道 "I" 的信息？直接查询，一步到位。</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>这就是Self-Attention的核心思想：**把 $O(n)$ 的间接路径变成 $O(1)$ 的直接路径**。</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要什么样的解决方案？</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>从上述分析可以看出，理想的序列建模机制应该满足以下特性：</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**直接连接**：任意两个位置之间的信息传递是一步到位的，而非逐步传递</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**并行计算**：不同位置的表示可以同时计算，不需要等待前序计算完成</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**动态权重**：关注的强度应该取决于内容的相关性，而非固定的位置关系</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**全局视野**：每个位置都能看到整个序列，而非只有局部窗口</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>Self-Attention正是为了同时满足这四个需求而设计的。</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从Cross-Attention到Self-Attention</span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>回顾一下Seq2Seq中的Attention机制：</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j^{(enc)}</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>其中 $\alpha_{ij}$ 衡量解码器位置 $i$ 对编码器位置 $j$ 的关注程度。这里的关键是：Query来自解码器，Key和Value来自编码器——这是**跨序列**的注意力（Cross-Attention）。</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>Self-Attention的想法极其简单：**让Query、Key、Value都来自同一个序列**。</span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{x}_j</span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a>其中 $\alpha_{ij}$ 衡量位置 $i$ 对位置 $j$ 的关注程度。现在，序列在"审视自己"——每个位置都在问："我应该关注序列中的哪些其他位置？"</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a><span class="fu">### Query-Key-Value的三元组视角</span></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>为了让Self-Attention更加灵活，我们不直接用原始表示计算注意力，而是先进行线性变换：</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>\mathbf{q}_i = \mathbf{W}_Q \mathbf{x}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{x}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{x}_i</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>然后：</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \text{softmax}_j\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right)</span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{v}_j</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>这种Query-Key-Value的三元组结构有深刻的直觉意义：</span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Query（查询）**：代表"我在找什么？"——位置 $i$ 想要获取什么样的信息</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key（键）**：代表"我是什么？"——位置 $j$ 的身份标识，用于匹配查询</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Value（值）**：代表"我能提供什么？"——位置 $j$ 实际贡献的内容</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>这个设计允许同一个位置以不同的"身份"参与计算：当它作为查询者时，它在寻找相关信息；当它作为被查询者时，它的Key决定是否被选中，它的Value决定贡献什么内容。</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个具体的例子</span></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>考虑句子 "The cat sat on the mat"，我们想理解词 "sat" 的上下文表示。</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>在Self-Attention中，"sat" 会：</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>生成一个Query向量，编码"动词'sat'在寻找它的主语和宾语"</span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>与所有词的Key向量计算相似度</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>与 "cat" 的Key高度匹配（"cat"是主语），与 "mat" 也有一定匹配（"mat"是介词宾语的对象）</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>最终的表示是所有词的Value的加权和，"cat" 和 "mat" 贡献较多</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>这个过程完全是基于内容的——模型学习到"动词通常需要关注它的主语和宾语"，而这种学习是从数据中自动获得的，无需人工定义语法规则。</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a><span class="fu">## Memory Networks：Self-Attention的思想先驱</span></span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问答系统的挑战</span></span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>在Self-Attention被明确提出之前，有一条平行的研究脉络在探索类似的想法：**Memory Networks**。</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>2014-2015年，Facebook AI Research（Sukhbaatar, Szegedy, Weston等人）提出了Memory Networks来解决需要多跳推理的问答任务。考虑以下例子：</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>**故事**：</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Mary moved to the bathroom.</span></span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; John went to the hallway.</span></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Mary traveled to the office.</span></span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a>**问题**：Where is Mary?</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>**答案**：office</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a>要回答这个问题，模型需要：</span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>从记忆中找到与"Mary"相关的信息</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>识别出最新的位置信息</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>综合这些信息得出答案</span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a>这比简单的文本匹配复杂得多——需要在多条信息之间建立联系，进行推理。</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a><span class="fu">### Memory Networks的设计</span></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a>Memory Networks的核心思想是：将信息存储在一个外部记忆中，然后通过注意力机制来读取相关信息。</span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a><span class="al">![End-To-End Memory Networks的架构。输入句子被编码为记忆向量（红色），问题被编码为查询向量（蓝色），通过注意力机制从记忆中检索相关信息。](figures/chapter-7/original/fig-memory-network.png)</span>{#fig-memory-network width=70%}</span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>*Source: Sukhbaatar et al. (2015) "End-To-End Memory Networks", Figure 1. [arXiv:1503.08895](https://arxiv.org/abs/1503.08895)*</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a>具体来说，给定一组记忆 $<span class="sc">\{</span>\mathbf{m}_1, \mathbf{m}_2, \ldots, \mathbf{m}_n<span class="sc">\}</span>$ 和一个查询 $\mathbf{q}$：</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a>**Step 1: 计算注意力权重**</span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>p_i = \text{softmax}(\mathbf{q}^\top \mathbf{m}_i)</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>**Step 2: 读取记忆**</span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a>\mathbf{o} = \sum_{i=1}^{n} p_i \mathbf{c}_i</span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{c}_i$ 是记忆 $i$ 的输出表示（可以与 $\mathbf{m}_i$ 不同）。</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a>**Step 3: 更新查询并迭代**</span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>\mathbf{q}' = \mathbf{q} + \mathbf{o}</span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>然后用 $\mathbf{q}'$ 再次查询记忆，进行多跳推理。</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a><span class="fu">### Memory Networks与Self-Attention的联系</span></span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>看出来了吗？Memory Networks中的注意力机制与Self-Attention惊人地相似：</span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Memory Networks <span class="pp">|</span> Self-Attention <span class="pp">|</span></span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------------|----------------|</span></span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 查询 $\mathbf{q}$ <span class="pp">|</span> Query $\mathbf{Q}$ <span class="pp">|</span></span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 记忆 $\mathbf{m}_i$ <span class="pp">|</span> Key $\mathbf{K}$ <span class="pp">|</span></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 输出 $\mathbf{c}_i$ <span class="pp">|</span> Value $\mathbf{V}$ <span class="pp">|</span></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $p_i = \text{softmax}(\mathbf{q}^\top \mathbf{m}_i)$ <span class="pp">|</span> $\alpha = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d})$ <span class="pp">|</span></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathbf{o} = \sum_i p_i \mathbf{c}_i$ <span class="pp">|</span> $\mathbf{Z} = \alpha \mathbf{V}$ <span class="pp">|</span></span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a>关键的联系在于：**Self-Attention可以看作是序列对自己进行Memory Network式的查询**。</span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>每个位置都是一个"记忆槽"，每个位置也都是一个"查询者"。当位置 $i$ 计算它的输出时，它就像在用自己的Query去检索其他位置（包括自己）存储的信息。</span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>这个视角解释了为什么Self-Attention如此强大：它本质上是在做**基于内容的关联记忆检索**，而这正是推理和理解的核心能力。</span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从Memory Networks到Transformer</span></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a>Memory Networks的工作对后来的Transformer有深远影响：</span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**端到端可微分**：早期的Memory Networks需要强监督（告诉模型应该关注哪些记忆），后来的End-to-End Memory Networks通过软注意力实现了端到端训练——这正是Transformer所采用的方式。</span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**多跳推理**：Memory Networks通过多次查询记忆来实现复杂推理。Transformer中的多层堆叠也可以理解为多跳推理——每一层的Self-Attention都是一次"查询"。</span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Key-Value分离**：Memory Networks中 $\mathbf{m}_i$（用于计算注意力）和 $\mathbf{c}_i$（用于输出）可以不同。这启发了Self-Attention中Key和Value的分离设计。</span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Attention的数学形式</span></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a>让我们正式定义Self-Attention的计算过程。</span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a>给定输入序列 $\mathbf{X} = <span class="co">[</span><span class="ot">\mathbf{x}_1; \mathbf{x}_2; \ldots; \mathbf{x}_n</span><span class="co">]</span>^\top \in \mathbb{R}^{n \times d}$，Self-Attention的计算如下：</span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a>**Step 1: 线性投影**</span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>\mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_V</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d \times d_k}$，$\mathbf{W}_V \in \mathbb{R}^{d \times d_v}$。</span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a>**Step 2: 计算注意力分数**</span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a>\mathbf{S} = \mathbf{Q} \mathbf{K}^\top \in \mathbb{R}^{n \times n}</span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a>矩阵 $\mathbf{S}$ 的元素 $S_{ij} = \mathbf{q}_i^\top \mathbf{k}_j$ 衡量位置 $i$ 对位置 $j$ 的"关注程度"。</span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a>**Step 3: 缩放和归一化**</span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a>\mathbf{A} = \text{softmax}\left(\frac{\mathbf{S}}{\sqrt{d_k}}\right) \in \mathbb{R}^{n \times n}</span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a>每一行是一个概率分布，表示该位置对所有位置的注意力权重。</span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a>**Step 4: 加权聚合**</span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a>\mathbf{Z} = \mathbf{A} \mathbf{V} \in \mathbb{R}^{n \times d_v}</span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a>输出 $\mathbf{z}_i = \sum_{j=1}^{n} A_{ij} \mathbf{v}_j$ 是所有Value向量的加权和。</span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm: Scaled Dot-Product Self-Attention (Vaswani et al., 2017)</span></span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention(X, W_Q, W_K, W_V):</span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a><span class="co">    Self-Attention 计算</span></span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a><span class="co">    参数:</span></span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a><span class="co">        X: 输入序列 [batch, seq_len, d_model]</span></span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a><span class="co">        W_Q, W_K: 投影矩阵 [d_model, d_k]</span></span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a><span class="co">        W_V: 投影矩阵 [d_model, d_v]</span></span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a><span class="co">    返回:</span></span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="co">        Z: 输出序列 [batch, seq_len, d_v]</span></span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a><span class="co">        A: 注意力权重 [batch, seq_len, seq_len]</span></span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 线性投影</span></span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> X <span class="op">@</span> W_Q  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> X <span class="op">@</span> W_K  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> X <span class="op">@</span> W_V  <span class="co"># [batch, seq_len, d_v]</span></span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: 计算注意力分数</span></span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> Q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># [batch, seq_len, seq_len]</span></span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 缩放 + Softmax</span></span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: 加权聚合</span></span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> A <span class="op">@</span> V  <span class="co"># [batch, seq_len, d_v]</span></span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z, A</span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a>*Adapted from: Vaswani, A. et al. (2017). "Attention Is All You Need". NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*</span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整数值示例：从输入到输出</span></span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-331"><a href="#cb10-331" aria-hidden="true" tabindex="-1"></a>让我们用一个小规模的例子完整演示Self-Attention的计算过程。</span>
<span id="cb10-332"><a href="#cb10-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-333"><a href="#cb10-333" aria-hidden="true" tabindex="-1"></a>**设定**：句子 "I love NLP"，共3个词。$d_{model} = 4$，$d_k = d_v = 4$。</span>
<span id="cb10-334"><a href="#cb10-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-335"><a href="#cb10-335" aria-hidden="true" tabindex="-1"></a>**Step 1: 输入表示**（假设已经通过embedding获得）</span>
<span id="cb10-336"><a href="#cb10-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-337"><a href="#cb10-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-338"><a href="#cb10-338" aria-hidden="true" tabindex="-1"></a>\mathbf{X} = \begin{bmatrix}</span>
<span id="cb10-339"><a href="#cb10-339" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_I <span class="sc">\\</span></span>
<span id="cb10-340"><a href="#cb10-340" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_{love} <span class="sc">\\</span></span>
<span id="cb10-341"><a href="#cb10-341" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_{NLP}</span>
<span id="cb10-342"><a href="#cb10-342" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} = \begin{bmatrix}</span>
<span id="cb10-343"><a href="#cb10-343" aria-hidden="true" tabindex="-1"></a>1.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 <span class="sc">\\</span></span>
<span id="cb10-344"><a href="#cb10-344" aria-hidden="true" tabindex="-1"></a>0.0 &amp; 1.0 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span></span>
<span id="cb10-345"><a href="#cb10-345" aria-hidden="true" tabindex="-1"></a>1.0 &amp; 1.0 &amp; 0.0 &amp; 0.0</span>
<span id="cb10-346"><a href="#cb10-346" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb10-347"><a href="#cb10-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-348"><a href="#cb10-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-349"><a href="#cb10-349" aria-hidden="true" tabindex="-1"></a>**Step 2: 投影矩阵**（为简化，使用接近单位矩阵的值）</span>
<span id="cb10-350"><a href="#cb10-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-351"><a href="#cb10-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-352"><a href="#cb10-352" aria-hidden="true" tabindex="-1"></a>\mathbf{W}_Q = \mathbf{W}_K = \mathbf{W}_V = \begin{bmatrix}</span>
<span id="cb10-353"><a href="#cb10-353" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb10-354"><a href="#cb10-354" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb10-355"><a href="#cb10-355" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb10-356"><a href="#cb10-356" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; 1</span>
<span id="cb10-357"><a href="#cb10-357" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb10-358"><a href="#cb10-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-359"><a href="#cb10-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-360"><a href="#cb10-360" aria-hidden="true" tabindex="-1"></a>因此 $\mathbf{Q} = \mathbf{K} = \mathbf{V} = \mathbf{X}$。</span>
<span id="cb10-361"><a href="#cb10-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-362"><a href="#cb10-362" aria-hidden="true" tabindex="-1"></a>**Step 3: 计算注意力分数 $\mathbf{Q}\mathbf{K}^\top$**</span>
<span id="cb10-363"><a href="#cb10-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-364"><a href="#cb10-364" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-365"><a href="#cb10-365" aria-hidden="true" tabindex="-1"></a>\mathbf{S} = \mathbf{X} \mathbf{X}^\top = \begin{bmatrix}</span>
<span id="cb10-366"><a href="#cb10-366" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_I^\top \mathbf{x}_I &amp; \mathbf{x}_I^\top \mathbf{x}_{love} &amp; \mathbf{x}_I^\top \mathbf{x}_{NLP} <span class="sc">\\</span></span>
<span id="cb10-367"><a href="#cb10-367" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_{love}^\top \mathbf{x}_I &amp; \mathbf{x}_{love}^\top \mathbf{x}_{love} &amp; \mathbf{x}_{love}^\top \mathbf{x}_{NLP} <span class="sc">\\</span></span>
<span id="cb10-368"><a href="#cb10-368" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_{NLP}^\top \mathbf{x}_I &amp; \mathbf{x}_{NLP}^\top \mathbf{x}_{love} &amp; \mathbf{x}_{NLP}^\top \mathbf{x}_{NLP}</span>
<span id="cb10-369"><a href="#cb10-369" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb10-370"><a href="#cb10-370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-371"><a href="#cb10-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-372"><a href="#cb10-372" aria-hidden="true" tabindex="-1"></a>逐个计算：</span>
<span id="cb10-373"><a href="#cb10-373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{x}_I^\top \mathbf{x}_I = 1^2 + 0^2 + 1^2 + 0^2 = 2.0$</span>
<span id="cb10-374"><a href="#cb10-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{x}_I^\top \mathbf{x}_{love} = 1 \times 0 + 0 \times 1 + 1 \times 0.5 + 0 \times 0.5 = 0.5$</span>
<span id="cb10-375"><a href="#cb10-375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{x}_I^\top \mathbf{x}_{NLP} = 1 \times 1 + 0 \times 1 + 1 \times 0 + 0 \times 0 = 1.0$</span>
<span id="cb10-376"><a href="#cb10-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{x}_{love}^\top \mathbf{x}_{love} = 0^2 + 1^2 + 0.5^2 + 0.5^2 = 1.5$</span>
<span id="cb10-377"><a href="#cb10-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{x}_{love}^\top \mathbf{x}_{NLP} = 0 \times 1 + 1 \times 1 + 0.5 \times 0 + 0.5 \times 0 = 1.0$</span>
<span id="cb10-378"><a href="#cb10-378" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{x}_{NLP}^\top \mathbf{x}_{NLP} = 1^2 + 1^2 + 0^2 + 0^2 = 2.0$</span>
<span id="cb10-379"><a href="#cb10-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-380"><a href="#cb10-380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-381"><a href="#cb10-381" aria-hidden="true" tabindex="-1"></a>\mathbf{S} = \begin{bmatrix}</span>
<span id="cb10-382"><a href="#cb10-382" aria-hidden="true" tabindex="-1"></a>2.0 &amp; 0.5 &amp; 1.0 <span class="sc">\\</span></span>
<span id="cb10-383"><a href="#cb10-383" aria-hidden="true" tabindex="-1"></a>0.5 &amp; 1.5 &amp; 1.0 <span class="sc">\\</span></span>
<span id="cb10-384"><a href="#cb10-384" aria-hidden="true" tabindex="-1"></a>1.0 &amp; 1.0 &amp; 2.0</span>
<span id="cb10-385"><a href="#cb10-385" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb10-386"><a href="#cb10-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-387"><a href="#cb10-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-388"><a href="#cb10-388" aria-hidden="true" tabindex="-1"></a>**Step 4: 缩放（除以 $\sqrt{d_k} = 2$）**</span>
<span id="cb10-389"><a href="#cb10-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-390"><a href="#cb10-390" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-391"><a href="#cb10-391" aria-hidden="true" tabindex="-1"></a>\frac{\mathbf{S}}{\sqrt{4}} = \begin{bmatrix}</span>
<span id="cb10-392"><a href="#cb10-392" aria-hidden="true" tabindex="-1"></a>1.0 &amp; 0.25 &amp; 0.5 <span class="sc">\\</span></span>
<span id="cb10-393"><a href="#cb10-393" aria-hidden="true" tabindex="-1"></a>0.25 &amp; 0.75 &amp; 0.5 <span class="sc">\\</span></span>
<span id="cb10-394"><a href="#cb10-394" aria-hidden="true" tabindex="-1"></a>0.5 &amp; 0.5 &amp; 1.0</span>
<span id="cb10-395"><a href="#cb10-395" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb10-396"><a href="#cb10-396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-397"><a href="#cb10-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-398"><a href="#cb10-398" aria-hidden="true" tabindex="-1"></a>**Step 5: Softmax（按行）**</span>
<span id="cb10-399"><a href="#cb10-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-400"><a href="#cb10-400" aria-hidden="true" tabindex="-1"></a>第一行：$<span class="co">[</span><span class="ot">e^{1.0}, e^{0.25}, e^{0.5}</span><span class="co">]</span> = <span class="co">[</span><span class="ot">2.72, 1.28, 1.65</span><span class="co">]</span>$，和为 $5.65$</span>
<span id="cb10-401"><a href="#cb10-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-402"><a href="#cb10-402" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-403"><a href="#cb10-403" aria-hidden="true" tabindex="-1"></a>\alpha_{I} = <span class="co">[</span><span class="ot">2.72/5.65, 1.28/5.65, 1.65/5.65</span><span class="co">]</span> = <span class="co">[</span><span class="ot">0.48, 0.23, 0.29</span><span class="co">]</span></span>
<span id="cb10-404"><a href="#cb10-404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-405"><a href="#cb10-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-406"><a href="#cb10-406" aria-hidden="true" tabindex="-1"></a>类似计算其他行：</span>
<span id="cb10-407"><a href="#cb10-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-408"><a href="#cb10-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-409"><a href="#cb10-409" aria-hidden="true" tabindex="-1"></a>\mathbf{A} = \begin{bmatrix}</span>
<span id="cb10-410"><a href="#cb10-410" aria-hidden="true" tabindex="-1"></a>0.48 &amp; 0.23 &amp; 0.29 <span class="sc">\\</span></span>
<span id="cb10-411"><a href="#cb10-411" aria-hidden="true" tabindex="-1"></a>0.26 &amp; 0.43 &amp; 0.31 <span class="sc">\\</span></span>
<span id="cb10-412"><a href="#cb10-412" aria-hidden="true" tabindex="-1"></a>0.31 &amp; 0.31 &amp; 0.38</span>
<span id="cb10-413"><a href="#cb10-413" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb10-414"><a href="#cb10-414" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-415"><a href="#cb10-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-416"><a href="#cb10-416" aria-hidden="true" tabindex="-1"></a>**Step 6: 加权聚合 $\mathbf{Z} = \mathbf{A}\mathbf{V}$**</span>
<span id="cb10-417"><a href="#cb10-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-418"><a href="#cb10-418" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-419"><a href="#cb10-419" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_I = 0.48 \times \mathbf{x}_I + 0.23 \times \mathbf{x}_{love} + 0.29 \times \mathbf{x}_{NLP}</span>
<span id="cb10-420"><a href="#cb10-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-421"><a href="#cb10-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-422"><a href="#cb10-422" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-423"><a href="#cb10-423" aria-hidden="true" tabindex="-1"></a>= 0.48 \times <span class="co">[</span><span class="ot">1, 0, 1, 0</span><span class="co">]</span> + 0.23 \times <span class="co">[</span><span class="ot">0, 1, 0.5, 0.5</span><span class="co">]</span> + 0.29 \times <span class="co">[</span><span class="ot">1, 1, 0, 0</span><span class="co">]</span></span>
<span id="cb10-424"><a href="#cb10-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-425"><a href="#cb10-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-426"><a href="#cb10-426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-427"><a href="#cb10-427" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.48 + 0 + 0.29, 0 + 0.23 + 0.29, 0.48 + 0.115 + 0, 0 + 0.115 + 0</span><span class="co">]</span></span>
<span id="cb10-428"><a href="#cb10-428" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-429"><a href="#cb10-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-430"><a href="#cb10-430" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-431"><a href="#cb10-431" aria-hidden="true" tabindex="-1"></a>= <span class="co">[</span><span class="ot">0.77, 0.52, 0.60, 0.12</span><span class="co">]</span></span>
<span id="cb10-432"><a href="#cb10-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-433"><a href="#cb10-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-434"><a href="#cb10-434" aria-hidden="true" tabindex="-1"></a>**解读**：</span>
<span id="cb10-435"><a href="#cb10-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-436"><a href="#cb10-436" aria-hidden="true" tabindex="-1"></a>"I" 的Self-Attention输出不再只是自己的表示，而是融合了整个句子的信息。注意力分布 $<span class="co">[</span><span class="ot">0.48, 0.23, 0.29</span><span class="co">]</span>$ 显示：</span>
<span id="cb10-437"><a href="#cb10-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I" 最关注自己（0.48）——这是合理的，自己的信息最相关</span>
<span id="cb10-438"><a href="#cb10-438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>对 "NLP"（0.29）的关注略高于 "love"（0.23）——在这个简化例子中，这来自于向量相似度</span>
<span id="cb10-439"><a href="#cb10-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-440"><a href="#cb10-440" aria-hidden="true" tabindex="-1"></a>在真实的训练模型中，这些权重会学习到语言的结构：主语会关注动词，动词会关注宾语，代词会关注它的指代对象。</span>
<span id="cb10-441"><a href="#cb10-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-442"><a href="#cb10-442" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Attention的复杂度分析</span></span>
<span id="cb10-443"><a href="#cb10-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-444"><a href="#cb10-444" aria-hidden="true" tabindex="-1"></a>**时间复杂度**：</span>
<span id="cb10-445"><a href="#cb10-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-446"><a href="#cb10-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>投影：$O(n \cdot d \cdot d_k)$，三次投影共 $O(n \cdot d^2)$</span>
<span id="cb10-447"><a href="#cb10-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算 $\mathbf{Q}\mathbf{K}^\top$：$O(n^2 \cdot d_k)$</span>
<span id="cb10-448"><a href="#cb10-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Softmax：$O(n^2)$</span>
<span id="cb10-449"><a href="#cb10-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算 $\mathbf{A}\mathbf{V}$：$O(n^2 \cdot d_v)$</span>
<span id="cb10-450"><a href="#cb10-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-451"><a href="#cb10-451" aria-hidden="true" tabindex="-1"></a>总体：$O(n^2 \cdot d + n \cdot d^2)$</span>
<span id="cb10-452"><a href="#cb10-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-453"><a href="#cb10-453" aria-hidden="true" tabindex="-1"></a>当 $n$ 很大时，$O(n^2)$ 是主导项——这是Self-Attention的主要瓶颈。</span>
<span id="cb10-454"><a href="#cb10-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-455"><a href="#cb10-455" aria-hidden="true" tabindex="-1"></a>**空间复杂度**：</span>
<span id="cb10-456"><a href="#cb10-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-457"><a href="#cb10-457" aria-hidden="true" tabindex="-1"></a>需要存储 $n \times n$ 的注意力矩阵，因此空间复杂度为 $O(n^2)$。</span>
<span id="cb10-458"><a href="#cb10-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-459"><a href="#cb10-459" aria-hidden="true" tabindex="-1"></a>**与RNN的对比**：</span>
<span id="cb10-460"><a href="#cb10-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-461"><a href="#cb10-461" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 指标 <span class="pp">|</span> RNN <span class="pp">|</span> Self-Attention <span class="pp">|</span></span>
<span id="cb10-462"><a href="#cb10-462" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----|----------------|</span></span>
<span id="cb10-463"><a href="#cb10-463" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 时间复杂度 <span class="pp">|</span> $O(n \cdot d^2)$ <span class="pp">|</span> $O(n^2 \cdot d)$ <span class="pp">|</span></span>
<span id="cb10-464"><a href="#cb10-464" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 并行度 <span class="pp">|</span> $O(1)$ <span class="pp">|</span> $O(n)$ <span class="pp">|</span></span>
<span id="cb10-465"><a href="#cb10-465" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 最长路径 <span class="pp">|</span> $O(n)$ <span class="pp">|</span> $O(1)$ <span class="pp">|</span></span>
<span id="cb10-466"><a href="#cb10-466" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 空间复杂度 <span class="pp">|</span> $O(d)$ <span class="pp">|</span> $O(n^2)$ <span class="pp">|</span></span>
<span id="cb10-467"><a href="#cb10-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-468"><a href="#cb10-468" aria-hidden="true" tabindex="-1"></a>Self-Attention用 $O(n^2)$ 的复杂度换来了：</span>
<span id="cb10-469"><a href="#cb10-469" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>完全并行化（从 $O(1)$ 到 $O(n)$）</span>
<span id="cb10-470"><a href="#cb10-470" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>任意位置之间的直接路径（从 $O(n)$ 到 $O(1)$）</span>
<span id="cb10-471"><a href="#cb10-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-472"><a href="#cb10-472" aria-hidden="true" tabindex="-1"></a>这个权衡在大多数NLP任务中是值得的，因为句子长度通常不超过几百个词。但对于超长序列（如文档、代码），$O(n^2)$ 会成为严重瓶颈——这催生了后来的高效注意力变体（Sparse Attention、Linear Attention等）。</span>
<span id="cb10-473"><a href="#cb10-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-474"><a href="#cb10-474" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-475"><a href="#cb10-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-476"><a href="#cb10-476" aria-hidden="true" tabindex="-1"></a><span class="fu">## Self-Attention的致命缺陷：位置信息丢失</span></span>
<span id="cb10-477"><a href="#cb10-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-478"><a href="#cb10-478" aria-hidden="true" tabindex="-1"></a><span class="fu">### 问题的本质</span></span>
<span id="cb10-479"><a href="#cb10-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-480"><a href="#cb10-480" aria-hidden="true" tabindex="-1"></a>仔细观察Self-Attention的计算公式：</span>
<span id="cb10-481"><a href="#cb10-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-482"><a href="#cb10-482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-483"><a href="#cb10-483" aria-hidden="true" tabindex="-1"></a>\mathbf{z}_i = \sum_{j=1}^{n} \text{softmax}\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right) \mathbf{v}_j</span>
<span id="cb10-484"><a href="#cb10-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-485"><a href="#cb10-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-486"><a href="#cb10-486" aria-hidden="true" tabindex="-1"></a>这里有一个致命的问题：**计算完全与位置无关**。</span>
<span id="cb10-487"><a href="#cb10-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-488"><a href="#cb10-488" aria-hidden="true" tabindex="-1"></a>如果我们打乱输入序列的顺序，比如把 "I love NLP" 变成 "NLP I love"，每个词的注意力权重只是相应地打乱，最终输出只是重新排列，与原来一一对应。</span>
<span id="cb10-489"><a href="#cb10-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-490"><a href="#cb10-490" aria-hidden="true" tabindex="-1"></a>更正式地说，Self-Attention是**置换等变的（permutation equivariant）**：</span>
<span id="cb10-491"><a href="#cb10-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-492"><a href="#cb10-492" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-493"><a href="#cb10-493" aria-hidden="true" tabindex="-1"></a>\text{SelfAttn}(\mathbf{P}\mathbf{X}) = \mathbf{P} \cdot \text{SelfAttn}(\mathbf{X})</span>
<span id="cb10-494"><a href="#cb10-494" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-495"><a href="#cb10-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-496"><a href="#cb10-496" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{P}$ 是任意置换矩阵。</span>
<span id="cb10-497"><a href="#cb10-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-498"><a href="#cb10-498" aria-hidden="true" tabindex="-1"></a>这意味着：Self-Attention完全不知道"谁在谁前面"。</span>
<span id="cb10-499"><a href="#cb10-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-500"><a href="#cb10-500" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么这是致命的？</span></span>
<span id="cb10-501"><a href="#cb10-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-502"><a href="#cb10-502" aria-hidden="true" tabindex="-1"></a>在自然语言中，顺序携带着关键信息：</span>
<span id="cb10-503"><a href="#cb10-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-504"><a href="#cb10-504" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**语法角色**："The dog bit the man" vs "The man bit the dog" 意思完全不同</span>
<span id="cb10-505"><a href="#cb10-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-506"><a href="#cb10-506" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**时态**："I will go" vs "I go" vs "I went" 依赖于词的相对位置</span>
<span id="cb10-507"><a href="#cb10-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-508"><a href="#cb10-508" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**指代消解**："John told Bill that he..." 中 "he" 通常指代较近的名词</span>
<span id="cb10-509"><a href="#cb10-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-510"><a href="#cb10-510" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**否定范围**："I didn't go to school" vs "I went to not school" 否定词的位置决定否定的范围</span>
<span id="cb10-511"><a href="#cb10-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-512"><a href="#cb10-512" aria-hidden="true" tabindex="-1"></a>如果模型不知道词的顺序，它怎么可能理解语言？</span>
<span id="cb10-513"><a href="#cb10-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-514"><a href="#cb10-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### 直觉演示：位置信息为什么重要</span></span>
<span id="cb10-515"><a href="#cb10-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-516"><a href="#cb10-516" aria-hidden="true" tabindex="-1"></a>让我们用一个简单的例子来直观感受位置信息的重要性。</span>
<span id="cb10-517"><a href="#cb10-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-518"><a href="#cb10-518" aria-hidden="true" tabindex="-1"></a>考虑两个句子：</span>
<span id="cb10-519"><a href="#cb10-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A: "The cat chased the mouse"</span>
<span id="cb10-520"><a href="#cb10-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>B: "The mouse chased the cat"</span>
<span id="cb10-521"><a href="#cb10-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-522"><a href="#cb10-522" aria-hidden="true" tabindex="-1"></a>在纯Self-Attention中（没有位置编码），两个句子的表示是**完全相同的**，因为包含的词集合相同，只是顺序不同。</span>
<span id="cb10-523"><a href="#cb10-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-524"><a href="#cb10-524" aria-hidden="true" tabindex="-1"></a>但这两个句子的意思完全相反！如果模型不能区分它们，就无法正确理解语言。</span>
<span id="cb10-525"><a href="#cb10-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-526"><a href="#cb10-526" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-527"><a href="#cb10-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-528"><a href="#cb10-528" aria-hidden="true" tabindex="-1"></a><span class="fu">## 位置编码：弥补缺失的顺序信息</span></span>
<span id="cb10-529"><a href="#cb10-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-530"><a href="#cb10-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### 设计目标</span></span>
<span id="cb10-531"><a href="#cb10-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-532"><a href="#cb10-532" aria-hidden="true" tabindex="-1"></a>我们需要一种方法，将位置信息注入到Self-Attention中。这种方法应该满足：</span>
<span id="cb10-533"><a href="#cb10-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-534"><a href="#cb10-534" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**唯一性**：不同位置的编码应该不同</span>
<span id="cb10-535"><a href="#cb10-535" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**可泛化**：模型应该能够处理训练时没见过的长度</span>
<span id="cb10-536"><a href="#cb10-536" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**相对关系**：编码应该能表达位置之间的相对关系，而非仅仅是绝对位置</span>
<span id="cb10-537"><a href="#cb10-537" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**有界性**：编码值不应随位置无限增长</span>
<span id="cb10-538"><a href="#cb10-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-539"><a href="#cb10-539" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方案一：可学习的位置嵌入</span></span>
<span id="cb10-540"><a href="#cb10-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-541"><a href="#cb10-541" aria-hidden="true" tabindex="-1"></a>最简单的想法：为每个位置学习一个向量。</span>
<span id="cb10-542"><a href="#cb10-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-543"><a href="#cb10-543" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-544"><a href="#cb10-544" aria-hidden="true" tabindex="-1"></a>\mathbf{x}_i' = \mathbf{x}_i + \mathbf{p}_i</span>
<span id="cb10-545"><a href="#cb10-545" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-546"><a href="#cb10-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-547"><a href="#cb10-547" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{p}_i \in \mathbb{R}^d$ 是位置 $i$ 的可学习嵌入。</span>
<span id="cb10-548"><a href="#cb10-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-549"><a href="#cb10-549" aria-hidden="true" tabindex="-1"></a>**优点**：</span>
<span id="cb10-550"><a href="#cb10-550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>简单直观</span>
<span id="cb10-551"><a href="#cb10-551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>模型可以自由学习位置表示</span>
<span id="cb10-552"><a href="#cb10-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-553"><a href="#cb10-553" aria-hidden="true" tabindex="-1"></a>**缺点**：</span>
<span id="cb10-554"><a href="#cb10-554" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>无法泛化到训练时没见过的长度</span>
<span id="cb10-555"><a href="#cb10-555" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>如果最长训练序列是512，就无法处理513长度的输入</span>
<span id="cb10-556"><a href="#cb10-556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>不能表达相对位置关系</span>
<span id="cb10-557"><a href="#cb10-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-558"><a href="#cb10-558" aria-hidden="true" tabindex="-1"></a>这种方式被BERT等模型采用，但需要配合固定的最大长度限制。</span>
<span id="cb10-559"><a href="#cb10-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-560"><a href="#cb10-560" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方案二：正弦位置编码</span></span>
<span id="cb10-561"><a href="#cb10-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-562"><a href="#cb10-562" aria-hidden="true" tabindex="-1"></a>Transformer论文提出了一种优雅的方案：使用不同频率的正弦和余弦函数。</span>
<span id="cb10-563"><a href="#cb10-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-564"><a href="#cb10-564" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-565"><a href="#cb10-565" aria-hidden="true" tabindex="-1"></a>PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)</span>
<span id="cb10-566"><a href="#cb10-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-567"><a href="#cb10-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-568"><a href="#cb10-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-569"><a href="#cb10-569" aria-hidden="true" tabindex="-1"></a>PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)</span>
<span id="cb10-570"><a href="#cb10-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-571"><a href="#cb10-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-572"><a href="#cb10-572" aria-hidden="true" tabindex="-1"></a>其中 $pos$ 是位置，$i$ 是维度索引。</span>
<span id="cb10-573"><a href="#cb10-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-574"><a href="#cb10-574" aria-hidden="true" tabindex="-1"></a>这个看起来复杂的公式实际上有着深刻的设计思想：</span>
<span id="cb10-575"><a href="#cb10-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-576"><a href="#cb10-576" aria-hidden="true" tabindex="-1"></a>**1. 唯一性**：每个位置有唯一的编码向量</span>
<span id="cb10-577"><a href="#cb10-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-578"><a href="#cb10-578" aria-hidden="true" tabindex="-1"></a>**2. 有界性**：正弦和余弦函数的值域是 $<span class="co">[</span><span class="ot">-1, 1</span><span class="co">]</span>$，不会随位置爆炸</span>
<span id="cb10-579"><a href="#cb10-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-580"><a href="#cb10-580" aria-hidden="true" tabindex="-1"></a>**3. 长度泛化**：函数是连续的，任何长度都可以计算</span>
<span id="cb10-581"><a href="#cb10-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-582"><a href="#cb10-582" aria-hidden="true" tabindex="-1"></a>**4. 相对位置可表达**：这是最精妙的部分。可以证明，位置 $pos + k$ 的编码可以表示为位置 $pos$ 的编码的线性变换：</span>
<span id="cb10-583"><a href="#cb10-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-584"><a href="#cb10-584" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-585"><a href="#cb10-585" aria-hidden="true" tabindex="-1"></a>PE_{pos+k} = T_k \cdot PE_{pos}</span>
<span id="cb10-586"><a href="#cb10-586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-587"><a href="#cb10-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-588"><a href="#cb10-588" aria-hidden="true" tabindex="-1"></a>其中 $T_k$ 是只依赖于 $k$（偏移量）的矩阵。这意味着模型有可能学习到相对位置关系。</span>
<span id="cb10-589"><a href="#cb10-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-590"><a href="#cb10-590" aria-hidden="true" tabindex="-1"></a><span class="fu">### 正弦编码的几何直觉</span></span>
<span id="cb10-591"><a href="#cb10-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-592"><a href="#cb10-592" aria-hidden="true" tabindex="-1"></a>想象一个时钟：</span>
<span id="cb10-593"><a href="#cb10-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-594"><a href="#cb10-594" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>秒针转一圈是60秒（高频）</span>
<span id="cb10-595"><a href="#cb10-595" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>分针转一圈是60分钟（中频）</span>
<span id="cb10-596"><a href="#cb10-596" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>时针转一圈是12小时（低频）</span>
<span id="cb10-597"><a href="#cb10-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-598"><a href="#cb10-598" aria-hidden="true" tabindex="-1"></a>不同的指针在不同的"频率"上运动，组合起来可以唯一地表示时间。</span>
<span id="cb10-599"><a href="#cb10-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-600"><a href="#cb10-600" aria-hidden="true" tabindex="-1"></a>正弦位置编码的原理类似：</span>
<span id="cb10-601"><a href="#cb10-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>低维度（小 $i$）的正弦波频率高，变化快，编码**局部位置信息**</span>
<span id="cb10-602"><a href="#cb10-602" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>高维度（大 $i$）的正弦波频率低，变化慢，编码**全局位置信息**</span>
<span id="cb10-603"><a href="#cb10-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-604"><a href="#cb10-604" aria-hidden="true" tabindex="-1"></a>组合所有维度，就能唯一标识每个位置。</span>
<span id="cb10-605"><a href="#cb10-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-608"><a href="#cb10-608" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-609"><a href="#cb10-609" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-610"><a href="#cb10-610" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-611"><a href="#cb10-611" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-612"><a href="#cb10-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-613"><a href="#cb10-613" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sinusoidal_position_encoding(max_len, d_model):</span>
<span id="cb10-614"><a href="#cb10-614" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""计算正弦位置编码"""</span></span>
<span id="cb10-615"><a href="#cb10-615" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((max_len, d_model))</span>
<span id="cb10-616"><a href="#cb10-616" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.arange(max_len)[:, np.newaxis]</span>
<span id="cb10-617"><a href="#cb10-617" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> np.exp(np.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb10-618"><a href="#cb10-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-619"><a href="#cb10-619" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)</span>
<span id="cb10-620"><a href="#cb10-620" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)</span>
<span id="cb10-621"><a href="#cb10-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-622"><a href="#cb10-622" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span>
<span id="cb10-623"><a href="#cb10-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-624"><a href="#cb10-624" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化</span></span>
<span id="cb10-625"><a href="#cb10-625" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> sinusoidal_position_encoding(<span class="dv">100</span>, <span class="dv">64</span>)</span>
<span id="cb10-626"><a href="#cb10-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-627"><a href="#cb10-627" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb10-628"><a href="#cb10-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-629"><a href="#cb10-629" aria-hidden="true" tabindex="-1"></a><span class="co"># 左图：热力图</span></span>
<span id="cb10-630"><a href="#cb10-630" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> axes[<span class="dv">0</span>].imshow(pe, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb10-631"><a href="#cb10-631" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Dimension'</span>)</span>
<span id="cb10-632"><a href="#cb10-632" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Position'</span>)</span>
<span id="cb10-633"><a href="#cb10-633" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Sinusoidal Position Encoding'</span>)</span>
<span id="cb10-634"><a href="#cb10-634" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb10-635"><a href="#cb10-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-636"><a href="#cb10-636" aria-hidden="true" tabindex="-1"></a><span class="co"># 右图：不同维度的波形</span></span>
<span id="cb10-637"><a href="#cb10-637" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dim <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>]:</span>
<span id="cb10-638"><a href="#cb10-638" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(pe[:, dim], label<span class="op">=</span><span class="ss">f'dim </span><span class="sc">{</span>dim<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-639"><a href="#cb10-639" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Position'</span>)</span>
<span id="cb10-640"><a href="#cb10-640" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Value'</span>)</span>
<span id="cb10-641"><a href="#cb10-641" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Different Dimensions = Different Frequencies'</span>)</span>
<span id="cb10-642"><a href="#cb10-642" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb10-643"><a href="#cb10-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-644"><a href="#cb10-644" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-645"><a href="#cb10-645" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-646"><a href="#cb10-646" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-647"><a href="#cb10-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-648"><a href="#cb10-648" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方案三：相对位置编码</span></span>
<span id="cb10-649"><a href="#cb10-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-650"><a href="#cb10-650" aria-hidden="true" tabindex="-1"></a>正弦编码虽然精巧，但它编码的是**绝对位置**。一些研究者认为，对于语言理解来说，**相对位置**（两个词之间的距离）可能比绝对位置更重要。</span>
<span id="cb10-651"><a href="#cb10-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-652"><a href="#cb10-652" aria-hidden="true" tabindex="-1"></a>相对位置编码的核心思想是修改注意力计算，让它直接考虑位置差：</span>
<span id="cb10-653"><a href="#cb10-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-654"><a href="#cb10-654" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-655"><a href="#cb10-655" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} \propto \exp\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{r}_{j-i}}{\sqrt{d_k}}\right)</span>
<span id="cb10-656"><a href="#cb10-656" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-657"><a href="#cb10-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-658"><a href="#cb10-658" aria-hidden="true" tabindex="-1"></a>其中 $\mathbf{r}_{j-i}$ 是表示相对位置 $j-i$ 的向量。</span>
<span id="cb10-659"><a href="#cb10-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-660"><a href="#cb10-660" aria-hidden="true" tabindex="-1"></a>这种方式的优点是：</span>
<span id="cb10-661"><a href="#cb10-661" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>直接建模相对关系</span>
<span id="cb10-662"><a href="#cb10-662" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>不同层可以有不同的相对位置偏好</span>
<span id="cb10-663"><a href="#cb10-663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>理论上可以泛化到任意长度</span>
<span id="cb10-664"><a href="#cb10-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-665"><a href="#cb10-665" aria-hidden="true" tabindex="-1"></a>后来的RoPE（Rotary Position Embedding）进一步发展了这个思想，成为现代大语言模型的标配——我们将在第26章详细讨论。</span>
<span id="cb10-666"><a href="#cb10-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-667"><a href="#cb10-667" aria-hidden="true" tabindex="-1"></a><span class="fu">### 位置编码的注入方式</span></span>
<span id="cb10-668"><a href="#cb10-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-669"><a href="#cb10-669" aria-hidden="true" tabindex="-1"></a>有了位置编码 $\mathbf{PE}$，如何将其与输入结合？主要有两种方式：</span>
<span id="cb10-670"><a href="#cb10-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-671"><a href="#cb10-671" aria-hidden="true" tabindex="-1"></a>**加法注入（Transformer采用）**：</span>
<span id="cb10-672"><a href="#cb10-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-673"><a href="#cb10-673" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-674"><a href="#cb10-674" aria-hidden="true" tabindex="-1"></a>\mathbf{X}' = \mathbf{X} + \mathbf{PE}</span>
<span id="cb10-675"><a href="#cb10-675" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-676"><a href="#cb10-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-677"><a href="#cb10-677" aria-hidden="true" tabindex="-1"></a>将位置编码直接加到词嵌入上。这假设位置信息和语义信息可以在同一空间中表达和混合。</span>
<span id="cb10-678"><a href="#cb10-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-679"><a href="#cb10-679" aria-hidden="true" tabindex="-1"></a>**拼接注入**：</span>
<span id="cb10-680"><a href="#cb10-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-681"><a href="#cb10-681" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-682"><a href="#cb10-682" aria-hidden="true" tabindex="-1"></a>\mathbf{X}' = <span class="co">[</span><span class="ot">\mathbf{X}; \mathbf{PE}</span><span class="co">]</span></span>
<span id="cb10-683"><a href="#cb10-683" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-684"><a href="#cb10-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-685"><a href="#cb10-685" aria-hidden="true" tabindex="-1"></a>将位置编码与词嵌入拼接，保持两者分离。这需要更多参数，但避免了信息混淆。</span>
<span id="cb10-686"><a href="#cb10-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-687"><a href="#cb10-687" aria-hidden="true" tabindex="-1"></a>实践中，加法注入更常用，因为它更简洁且效果良好。</span>
<span id="cb10-688"><a href="#cb10-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-689"><a href="#cb10-689" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-690"><a href="#cb10-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-691"><a href="#cb10-691" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb10-692"><a href="#cb10-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-693"><a href="#cb10-693" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实现Self-Attention</span></span>
<span id="cb10-694"><a href="#cb10-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-697"><a href="#cb10-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-698"><a href="#cb10-698" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-699"><a href="#cb10-699" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-700"><a href="#cb10-700" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-701"><a href="#cb10-701" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb10-702"><a href="#cb10-702" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb10-703"><a href="#cb10-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-704"><a href="#cb10-704" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb10-705"><a href="#cb10-705" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-706"><a href="#cb10-706" aria-hidden="true" tabindex="-1"></a><span class="co">    Self-Attention 模块</span></span>
<span id="cb10-707"><a href="#cb10-707" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-708"><a href="#cb10-708" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_k<span class="op">=</span><span class="va">None</span>, d_v<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-709"><a href="#cb10-709" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-710"><a href="#cb10-710" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb10-711"><a href="#cb10-711" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_k <span class="cf">if</span> d_k <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> d_model</span>
<span id="cb10-712"><a href="#cb10-712" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_v <span class="op">=</span> d_v <span class="cf">if</span> d_v <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> d_model</span>
<span id="cb10-713"><a href="#cb10-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-714"><a href="#cb10-714" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q, K, V 投影矩阵</span></span>
<span id="cb10-715"><a href="#cb10-715" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_Q <span class="op">=</span> nn.Linear(d_model, <span class="va">self</span>.d_k, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-716"><a href="#cb10-716" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_K <span class="op">=</span> nn.Linear(d_model, <span class="va">self</span>.d_k, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-717"><a href="#cb10-717" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_V <span class="op">=</span> nn.Linear(d_model, <span class="va">self</span>.d_v, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-718"><a href="#cb10-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-719"><a href="#cb10-719" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 缩放因子</span></span>
<span id="cb10-720"><a href="#cb10-720" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> math.sqrt(<span class="va">self</span>.d_k)</span>
<span id="cb10-721"><a href="#cb10-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-722"><a href="#cb10-722" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-723"><a href="#cb10-723" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-724"><a href="#cb10-724" aria-hidden="true" tabindex="-1"></a><span class="co">        X: [batch, seq_len, d_model]</span></span>
<span id="cb10-725"><a href="#cb10-725" aria-hidden="true" tabindex="-1"></a><span class="co">        mask: [batch, seq_len, seq_len], True 表示需要 mask 的位置</span></span>
<span id="cb10-726"><a href="#cb10-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-727"><a href="#cb10-727" aria-hidden="true" tabindex="-1"></a><span class="co">        返回:</span></span>
<span id="cb10-728"><a href="#cb10-728" aria-hidden="true" tabindex="-1"></a><span class="co">            output: [batch, seq_len, d_v]</span></span>
<span id="cb10-729"><a href="#cb10-729" aria-hidden="true" tabindex="-1"></a><span class="co">            attention_weights: [batch, seq_len, seq_len]</span></span>
<span id="cb10-730"><a href="#cb10-730" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-731"><a href="#cb10-731" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 线性投影</span></span>
<span id="cb10-732"><a href="#cb10-732" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.W_Q(X)  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb10-733"><a href="#cb10-733" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.W_K(X)  <span class="co"># [batch, seq_len, d_k]</span></span>
<span id="cb10-734"><a href="#cb10-734" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.W_V(X)  <span class="co"># [batch, seq_len, d_v]</span></span>
<span id="cb10-735"><a href="#cb10-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-736"><a href="#cb10-736" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 计算注意力分数</span></span>
<span id="cb10-737"><a href="#cb10-737" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.bmm(Q, K.transpose(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="op">/</span> <span class="va">self</span>.scale  <span class="co"># [batch, seq_len, seq_len]</span></span>
<span id="cb10-738"><a href="#cb10-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-739"><a href="#cb10-739" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 应用 mask</span></span>
<span id="cb10-740"><a href="#cb10-740" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-741"><a href="#cb10-741" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb10-742"><a href="#cb10-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-743"><a href="#cb10-743" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Softmax</span></span>
<span id="cb10-744"><a href="#cb10-744" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-745"><a href="#cb10-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-746"><a href="#cb10-746" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: 加权聚合</span></span>
<span id="cb10-747"><a href="#cb10-747" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.bmm(attention_weights, V)</span>
<span id="cb10-748"><a href="#cb10-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-749"><a href="#cb10-749" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb10-750"><a href="#cb10-750" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-751"><a href="#cb10-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-752"><a href="#cb10-752" aria-hidden="true" tabindex="-1"></a><span class="fu">### 实现位置编码</span></span>
<span id="cb10-753"><a href="#cb10-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-756"><a href="#cb10-756" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-757"><a href="#cb10-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-758"><a href="#cb10-758" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb10-759"><a href="#cb10-759" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-760"><a href="#cb10-760" aria-hidden="true" tabindex="-1"></a><span class="co">    正弦位置编码</span></span>
<span id="cb10-761"><a href="#cb10-761" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-762"><a href="#cb10-762" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb10-763"><a href="#cb10-763" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-764"><a href="#cb10-764" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb10-765"><a href="#cb10-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-766"><a href="#cb10-766" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 预计算位置编码</span></span>
<span id="cb10-767"><a href="#cb10-767" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb10-768"><a href="#cb10-768" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb10-769"><a href="#cb10-769" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb10-770"><a href="#cb10-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-771"><a href="#cb10-771" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb10-772"><a href="#cb10-772" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb10-773"><a href="#cb10-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-774"><a href="#cb10-774" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)  <span class="co"># [1, max_len, d_model]</span></span>
<span id="cb10-775"><a href="#cb10-775" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb10-776"><a href="#cb10-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-777"><a href="#cb10-777" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-778"><a href="#cb10-778" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-779"><a href="#cb10-779" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch, seq_len, d_model]</span></span>
<span id="cb10-780"><a href="#cb10-780" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-781"><a href="#cb10-781" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb10-782"><a href="#cb10-782" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb10-783"><a href="#cb10-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-784"><a href="#cb10-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-785"><a href="#cb10-785" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LearnablePositionalEmbedding(nn.Module):</span>
<span id="cb10-786"><a href="#cb10-786" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-787"><a href="#cb10-787" aria-hidden="true" tabindex="-1"></a><span class="co">    可学习的位置嵌入</span></span>
<span id="cb10-788"><a href="#cb10-788" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-789"><a href="#cb10-789" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb10-790"><a href="#cb10-790" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-791"><a href="#cb10-791" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb10-792"><a href="#cb10-792" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> nn.Embedding(max_len, d_model)</span>
<span id="cb10-793"><a href="#cb10-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-794"><a href="#cb10-794" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-795"><a href="#cb10-795" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-796"><a href="#cb10-796" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch, seq_len, d_model]</span></span>
<span id="cb10-797"><a href="#cb10-797" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-798"><a href="#cb10-798" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb10-799"><a href="#cb10-799" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>x.device).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb10-800"><a href="#cb10-800" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.position_embedding(positions)</span>
<span id="cb10-801"><a href="#cb10-801" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb10-802"><a href="#cb10-802" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-803"><a href="#cb10-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-804"><a href="#cb10-804" aria-hidden="true" tabindex="-1"></a><span class="fu">### 完整演示：Self-Attention的效果</span></span>
<span id="cb10-805"><a href="#cb10-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-808"><a href="#cb10-808" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-809"><a href="#cb10-809" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-810"><a href="#cb10-810" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建测试数据</span></span>
<span id="cb10-811"><a href="#cb10-811" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-812"><a href="#cb10-812" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb10-813"><a href="#cb10-813" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb10-814"><a href="#cb10-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-815"><a href="#cb10-815" aria-hidden="true" tabindex="-1"></a><span class="co"># 随机输入</span></span>
<span id="cb10-816"><a href="#cb10-816" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(batch_size, seq_len, d_model)</span>
<span id="cb10-817"><a href="#cb10-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-818"><a href="#cb10-818" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建模型</span></span>
<span id="cb10-819"><a href="#cb10-819" aria-hidden="true" tabindex="-1"></a>self_attn <span class="op">=</span> SelfAttention(d_model)</span>
<span id="cb10-820"><a href="#cb10-820" aria-hidden="true" tabindex="-1"></a>pos_enc <span class="op">=</span> PositionalEncoding(d_model)</span>
<span id="cb10-821"><a href="#cb10-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-822"><a href="#cb10-822" aria-hidden="true" tabindex="-1"></a><span class="co"># 添加位置编码</span></span>
<span id="cb10-823"><a href="#cb10-823" aria-hidden="true" tabindex="-1"></a>X_with_pos <span class="op">=</span> pos_enc(X)</span>
<span id="cb10-824"><a href="#cb10-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-825"><a href="#cb10-825" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算 Self-Attention</span></span>
<span id="cb10-826"><a href="#cb10-826" aria-hidden="true" tabindex="-1"></a>output, attention_weights <span class="op">=</span> self_attn(X_with_pos)</span>
<span id="cb10-827"><a href="#cb10-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-828"><a href="#cb10-828" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"输入形状: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-829"><a href="#cb10-829" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"输出形状: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-830"><a href="#cb10-830" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"注意力权重形状: </span><span class="sc">{</span>attention_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-831"><a href="#cb10-831" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">第一个样本的注意力矩阵:</span><span class="ch">\n</span><span class="sc">{</span>attention_weights[<span class="dv">0</span>]<span class="sc">.</span>detach()<span class="sc">.</span>numpy()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-832"><a href="#cb10-832" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-833"><a href="#cb10-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-834"><a href="#cb10-834" aria-hidden="true" tabindex="-1"></a><span class="fu">### 可视化注意力模式</span></span>
<span id="cb10-835"><a href="#cb10-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-838"><a href="#cb10-838" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-839"><a href="#cb10-839" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-840"><a href="#cb10-840" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-841"><a href="#cb10-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-842"><a href="#cb10-842" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建一个有意义的例子</span></span>
<span id="cb10-843"><a href="#cb10-843" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"The"</span>, <span class="st">"cat"</span>, <span class="st">"sat"</span>, <span class="st">"on"</span>, <span class="st">"mat"</span>]</span>
<span id="cb10-844"><a href="#cb10-844" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="bu">len</span>(words)</span>
<span id="cb10-845"><a href="#cb10-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-846"><a href="#cb10-846" aria-hidden="true" tabindex="-1"></a><span class="co"># 模拟一个训练好的注意力矩阵</span></span>
<span id="cb10-847"><a href="#cb10-847" aria-hidden="true" tabindex="-1"></a><span class="co"># 这个矩阵展示了一些语言学上合理的模式</span></span>
<span id="cb10-848"><a href="#cb10-848" aria-hidden="true" tabindex="-1"></a>attention_pattern <span class="op">=</span> torch.tensor([</span>
<span id="cb10-849"><a href="#cb10-849" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>],  <span class="co"># "The" 主要关注自己和 "cat"</span></span>
<span id="cb10-850"><a href="#cb10-850" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>],  <span class="co"># "cat" 关注 "sat"（动词）</span></span>
<span id="cb10-851"><a href="#cb10-851" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>],    <span class="co"># "sat" 关注 "cat"（主语）和 "on"</span></span>
<span id="cb10-852"><a href="#cb10-852" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.25</span>],  <span class="co"># "on" 关注 "sat" 和 "mat"</span></span>
<span id="cb10-853"><a href="#cb10-853" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>],  <span class="co"># "mat" 关注 "on" 和自己</span></span>
<span id="cb10-854"><a href="#cb10-854" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-855"><a href="#cb10-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-856"><a href="#cb10-856" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb10-857"><a href="#cb10-857" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax.imshow(attention_pattern, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb10-858"><a href="#cb10-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-859"><a href="#cb10-859" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(<span class="bu">range</span>(seq_len))</span>
<span id="cb10-860"><a href="#cb10-860" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(<span class="bu">range</span>(seq_len))</span>
<span id="cb10-861"><a href="#cb10-861" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(words)</span>
<span id="cb10-862"><a href="#cb10-862" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(words)</span>
<span id="cb10-863"><a href="#cb10-863" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Key (attended to)'</span>)</span>
<span id="cb10-864"><a href="#cb10-864" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Query (attending from)'</span>)</span>
<span id="cb10-865"><a href="#cb10-865" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Self-Attention Weights: "The cat sat on mat"'</span>)</span>
<span id="cb10-866"><a href="#cb10-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-867"><a href="#cb10-867" aria-hidden="true" tabindex="-1"></a><span class="co"># 添加数值标注</span></span>
<span id="cb10-868"><a href="#cb10-868" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb10-869"><a href="#cb10-869" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb10-870"><a href="#cb10-870" aria-hidden="true" tabindex="-1"></a>        ax.text(j, i, <span class="ss">f'</span><span class="sc">{</span>attention_pattern[i, j]<span class="sc">:.2f}</span><span class="ss">'</span>,</span>
<span id="cb10-871"><a href="#cb10-871" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'black'</span> <span class="cf">if</span> attention_pattern[i, j] <span class="op">&lt;</span> <span class="fl">0.3</span> <span class="cf">else</span> <span class="st">'white'</span>)</span>
<span id="cb10-872"><a href="#cb10-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-873"><a href="#cb10-873" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im)</span>
<span id="cb10-874"><a href="#cb10-874" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-875"><a href="#cb10-875" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-876"><a href="#cb10-876" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-877"><a href="#cb10-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-878"><a href="#cb10-878" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-879"><a href="#cb10-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-880"><a href="#cb10-880" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb10-881"><a href="#cb10-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-882"><a href="#cb10-882" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么Self-Attention有效？——理论视角</span></span>
<span id="cb10-883"><a href="#cb10-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-884"><a href="#cb10-884" aria-hidden="true" tabindex="-1"></a>**表达能力分析**</span>
<span id="cb10-885"><a href="#cb10-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-886"><a href="#cb10-886" aria-hidden="true" tabindex="-1"></a>Self-Attention可以看作一种特殊的图神经网络，其中图是完全连接的，边权重由注意力决定。Yun et al. (2020) 证明了：在适当条件下，Self-Attention是**通用函数逼近器**——它可以逼近任何连续的置换等变函数。</span>
<span id="cb10-887"><a href="#cb10-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-888"><a href="#cb10-888" aria-hidden="true" tabindex="-1"></a>**与卷积的对比**</span>
<span id="cb10-889"><a href="#cb10-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-890"><a href="#cb10-890" aria-hidden="true" tabindex="-1"></a>卷积神经网络（CNN）也可以处理序列，但它有固定的感受野。一个3-gram卷积只能看到相邻的3个词；要看到更远的词，需要堆叠多层。Self-Attention则一步就能看到全局。</span>
<span id="cb10-891"><a href="#cb10-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-892"><a href="#cb10-892" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 特性 <span class="pp">|</span> CNN <span class="pp">|</span> Self-Attention <span class="pp">|</span></span>
<span id="cb10-893"><a href="#cb10-893" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----|----------------|</span></span>
<span id="cb10-894"><a href="#cb10-894" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 感受野 <span class="pp">|</span> 局部（逐层扩大） <span class="pp">|</span> 全局（一层到位） <span class="pp">|</span></span>
<span id="cb10-895"><a href="#cb10-895" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 计算复杂度 <span class="pp">|</span> $O(n \cdot k^2 \cdot d)$ <span class="pp">|</span> $O(n^2 \cdot d)$ <span class="pp">|</span></span>
<span id="cb10-896"><a href="#cb10-896" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 位置偏置 <span class="pp">|</span> 强（相对位置关系固定） <span class="pp">|</span> 弱（需要位置编码） <span class="pp">|</span></span>
<span id="cb10-897"><a href="#cb10-897" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 参数共享 <span class="pp">|</span> 跨位置共享 <span class="pp">|</span> Q/K/V投影共享 <span class="pp">|</span></span>
<span id="cb10-898"><a href="#cb10-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-899"><a href="#cb10-899" aria-hidden="true" tabindex="-1"></a>**Memory Network视角**</span>
<span id="cb10-900"><a href="#cb10-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-901"><a href="#cb10-901" aria-hidden="true" tabindex="-1"></a>如前所述，Self-Attention可以理解为序列对自己进行Memory Network式的查询。每一层Self-Attention都是一次"多跳推理"——前一层的输出成为下一层的"记忆"。</span>
<span id="cb10-902"><a href="#cb10-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-903"><a href="#cb10-903" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb10-904"><a href="#cb10-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-905"><a href="#cb10-905" aria-hidden="true" tabindex="-1"></a>**Self-Attention的隐含假设**：</span>
<span id="cb10-906"><a href="#cb10-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-907"><a href="#cb10-907" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**全局相关性假设**：每个位置都可能与任何其他位置相关。但对于某些任务（如时间序列预测），局部相关性可能更重要。</span>
<span id="cb10-908"><a href="#cb10-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-909"><a href="#cb10-909" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**均匀计算假设**：所有位置之间的交互用相同的计算量。但直觉上，"重要"的交互可能需要更多计算。</span>
<span id="cb10-910"><a href="#cb10-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-911"><a href="#cb10-911" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**线性可分假设**：相关性可以通过向量点积来衡量。但有些复杂的关系可能无法用点积捕获。</span>
<span id="cb10-912"><a href="#cb10-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-913"><a href="#cb10-913" aria-hidden="true" tabindex="-1"></a>**失效条件**：</span>
<span id="cb10-914"><a href="#cb10-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-915"><a href="#cb10-915" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**超长序列**：当 $n &gt; 10000$ 时，$O(n^2)$ 复杂度变得不可接受。</span>
<span id="cb10-916"><a href="#cb10-916" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**强位置依赖任务**：对于需要精确位置推理的任务（如算术），Self-Attention常常失败。</span>
<span id="cb10-917"><a href="#cb10-917" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**数据量不足**：Self-Attention的弱归纳偏置需要大量数据来补偿。</span>
<span id="cb10-918"><a href="#cb10-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-919"><a href="#cb10-919" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb10-920"><a href="#cb10-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-921"><a href="#cb10-921" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**位置编码的最优设计**：正弦编码、可学习编码、相对位置编码——哪种最好？是否存在"最优"的位置编码？</span>
<span id="cb10-922"><a href="#cb10-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-923"><a href="#cb10-923" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Self-Attention的可解释性**：注意力权重真的代表了"重要性"吗？还是只是计算的副产品？</span>
<span id="cb10-924"><a href="#cb10-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-925"><a href="#cb10-925" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**效率与表达能力的权衡**：能否设计出 $O(n)$ 复杂度但保持 $O(n^2)$ 表达能力的注意力机制？</span>
<span id="cb10-926"><a href="#cb10-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-927"><a href="#cb10-927" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**归纳偏置的设计**：如何在Self-Attention中引入合适的归纳偏置，减少对数据量的依赖？</span>
<span id="cb10-928"><a href="#cb10-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-929"><a href="#cb10-929" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-930"><a href="#cb10-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-931"><a href="#cb10-931" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与展望</span></span>
<span id="cb10-932"><a href="#cb10-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-933"><a href="#cb10-933" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Attention的核心局限</span></span>
<span id="cb10-934"><a href="#cb10-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-935"><a href="#cb10-935" aria-hidden="true" tabindex="-1"></a>**1. 位置信息仍是"补丁"**</span>
<span id="cb10-936"><a href="#cb10-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-937"><a href="#cb10-937" aria-hidden="true" tabindex="-1"></a>虽然位置编码解决了问题，但它是一种事后补救。Self-Attention本身不具备位置感知能力，位置信息是外部注入的。这种设计是否最优？是否有更优雅的方式让注意力机制天生具有位置感知能力？</span>
<span id="cb10-938"><a href="#cb10-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-939"><a href="#cb10-939" aria-hidden="true" tabindex="-1"></a>**2. $O(n^2)$ 复杂度**</span>
<span id="cb10-940"><a href="#cb10-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-941"><a href="#cb10-941" aria-hidden="true" tabindex="-1"></a>全局注意力的代价是 $n \times n$ 的注意力矩阵。当序列长度达到数万甚至数十万时，这变得不可行。如何在保持全局视野的同时降低复杂度？</span>
<span id="cb10-942"><a href="#cb10-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-943"><a href="#cb10-943" aria-hidden="true" tabindex="-1"></a>**3. 单一的注意力模式**</span>
<span id="cb10-944"><a href="#cb10-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-945"><a href="#cb10-945" aria-hidden="true" tabindex="-1"></a>在一层Self-Attention中，每个位置只生成一组Query、Key、Value。但直觉上，一个位置可能需要同时关注多种不同类型的信息——比如主语需要关注动词（语法），也需要关注语境中的相关实体（语义）。能否让模型同时捕获多种关注模式？</span>
<span id="cb10-946"><a href="#cb10-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-947"><a href="#cb10-947" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限指向什么？</span></span>
<span id="cb10-948"><a href="#cb10-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-949"><a href="#cb10-949" aria-hidden="true" tabindex="-1"></a>Self-Attention为序列建模提供了一个强大的基础，但要构建完整的架构，还需要回答几个关键问题：</span>
<span id="cb10-950"><a href="#cb10-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-951"><a href="#cb10-951" aria-hidden="true" tabindex="-1"></a>**如何堆叠多层？** 简单地堆叠Self-Attention层会导致什么问题？需要什么额外的组件来保证训练稳定性？</span>
<span id="cb10-952"><a href="#cb10-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-953"><a href="#cb10-953" aria-hidden="true" tabindex="-1"></a>**如何处理编码器-解码器结构？** 机器翻译需要编码器理解源语言，解码器生成目标语言。Self-Attention如何在这个框架中使用？解码器如何既关注自己已生成的内容，又关注编码器的输出？</span>
<span id="cb10-954"><a href="#cb10-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-955"><a href="#cb10-955" aria-hidden="true" tabindex="-1"></a>**如何实现"多头"注意力？** 能否让多个注意力"头"同时工作，每个头关注不同的子空间，最后综合它们的输出？</span>
<span id="cb10-956"><a href="#cb10-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-957"><a href="#cb10-957" aria-hidden="true" tabindex="-1"></a>这些问题的答案，将在下一章揭晓。2017年，Google的研究团队在论文"Attention Is All You Need"中提出了**Transformer**架构——一个完全基于注意力的序列到序列模型。它不仅回答了上述所有问题，还以惊人的效果证明了一个大胆的论断：**注意力，就是你所需要的一切**。</span>
<span id="cb10-958"><a href="#cb10-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-959"><a href="#cb10-959" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Self-Attention让序列学会了"审视自己"，打破了RNN的顺序枷锁。但它只是革命的序曲。当研究者意识到可以完全抛弃循环结构，只用注意力来构建整个模型时，深度学习的历史翻开了新的一页。</span></span>
<span id="cb10-960"><a href="#cb10-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-961"><a href="#cb10-961" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-962"><a href="#cb10-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-963"><a href="#cb10-963" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb10-964"><a href="#cb10-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-965"><a href="#cb10-965" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb10-966"><a href="#cb10-966" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心要点</span></span>
<span id="cb10-967"><a href="#cb10-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-968"><a href="#cb10-968" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**问题**：如何让序列中的每个位置获得全局上下文信息，同时避免RNN的顺序计算瓶颈？</span>
<span id="cb10-969"><a href="#cb10-969" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**洞察**：让序列中的每个位置直接关注所有其他位置（Self-Attention），将 $O(n)$ 的间接路径变成 $O(1)$ 的直接路径</span>
<span id="cb10-970"><a href="#cb10-970" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**方法**：Query-Key-Value三元组结构，通过点积计算注意力权重，加权求和得到输出</span>
<span id="cb10-971"><a href="#cb10-971" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**代价**：Self-Attention丢失位置信息，需要位置编码来弥补；复杂度为 $O(n^2)$</span>
<span id="cb10-972"><a href="#cb10-972" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**意义**：Self-Attention为完全抛弃RNN、构建纯注意力架构奠定了基础</span>
<span id="cb10-973"><a href="#cb10-973" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-974"><a href="#cb10-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-975"><a href="#cb10-975" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb10-976"><a href="#cb10-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-977"><a href="#cb10-977" aria-hidden="true" tabindex="-1"></a>**Self-Attention**：</span>
<span id="cb10-978"><a href="#cb10-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-979"><a href="#cb10-979" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-980"><a href="#cb10-980" aria-hidden="true" tabindex="-1"></a>\text{SelfAttn}(\mathbf{X}) = \text{softmax}\left(\frac{\mathbf{X}\mathbf{W}_Q (\mathbf{X}\mathbf{W}_K)^\top}{\sqrt{d_k}}\right) \mathbf{X}\mathbf{W}_V</span>
<span id="cb10-981"><a href="#cb10-981" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-982"><a href="#cb10-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-983"><a href="#cb10-983" aria-hidden="true" tabindex="-1"></a>**正弦位置编码**：</span>
<span id="cb10-984"><a href="#cb10-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-985"><a href="#cb10-985" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-986"><a href="#cb10-986" aria-hidden="true" tabindex="-1"></a>PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)</span>
<span id="cb10-987"><a href="#cb10-987" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-988"><a href="#cb10-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-989"><a href="#cb10-989" aria-hidden="true" tabindex="-1"></a>**与Cross-Attention的对比**：</span>
<span id="cb10-990"><a href="#cb10-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-991"><a href="#cb10-991" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 类型 <span class="pp">|</span> Query来源 <span class="pp">|</span> Key/Value来源 <span class="pp">|</span> 用途 <span class="pp">|</span></span>
<span id="cb10-992"><a href="#cb10-992" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-----------|---------------|------|</span></span>
<span id="cb10-993"><a href="#cb10-993" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Cross-Attention <span class="pp">|</span> 解码器 <span class="pp">|</span> 编码器 <span class="pp">|</span> 跨序列关注 <span class="pp">|</span></span>
<span id="cb10-994"><a href="#cb10-994" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Self-Attention <span class="pp">|</span> 同一序列 <span class="pp">|</span> 同一序列 <span class="pp">|</span> 序列内部建模 <span class="pp">|</span></span>
<span id="cb10-995"><a href="#cb10-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-996"><a href="#cb10-996" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-997"><a href="#cb10-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-998"><a href="#cb10-998" aria-hidden="true" tabindex="-1"></a><span class="fu">## 思考题</span></span>
<span id="cb10-999"><a href="#cb10-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1000"><a href="#cb10-1000" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** Self-Attention为什么是"置换等变"的？设计一个简单的实验来验证这一点。如果我们想让模型区分 "dog bites man" 和 "man bites dog"，仅靠Self-Attention（不加位置编码）能做到吗？为什么？</span>
<span id="cb10-1001"><a href="#cb10-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1002"><a href="#cb10-1002" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 证明正弦位置编码的相对位置性质：$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性变换。具体写出变换矩阵的形式。（提示：利用三角函数的和差公式）</span>
<span id="cb10-1003"><a href="#cb10-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1004"><a href="#cb10-1004" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 实现一个简单的Self-Attention文本分类器（不使用RNN）：</span>
<span id="cb10-1005"><a href="#cb10-1005" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>输入：IMDB电影评论</span>
<span id="cb10-1006"><a href="#cb10-1006" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>架构：Embedding → Self-Attention → 平均池化 → 分类头</span>
<span id="cb10-1007"><a href="#cb10-1007" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>对比有无位置编码的效果差异</span>
<span id="cb10-1008"><a href="#cb10-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1009"><a href="#cb10-1009" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[批判思考]** Memory Networks和Self-Attention在形式上非常相似。它们的本质区别是什么？Memory Networks的"外部记忆"和Self-Attention的"序列作为记忆"有什么不同的设计考量？</span>
<span id="cb10-1010"><a href="#cb10-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1011"><a href="#cb10-1011" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[开放问题]** Self-Attention的 $O(n^2)$ 复杂度是一个根本限制吗？有没有可能设计出 $O(n)$ 复杂度的注意力机制，同时保持 $O(1)$ 的最长路径？现有的线性注意力（Linear Attention）为什么会有性能损失？</span>
<span id="cb10-1012"><a href="#cb10-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1013"><a href="#cb10-1013" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-1014"><a href="#cb10-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1015"><a href="#cb10-1015" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb10-1016"><a href="#cb10-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1017"><a href="#cb10-1017" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb10-1018"><a href="#cb10-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1019"><a href="#cb10-1019" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Vaswani et al., 2017] Attention Is All You Need**</span>
<span id="cb10-1020"><a href="#cb10-1020" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出Transformer和Scaled Dot-Product Attention</span>
<span id="cb10-1021"><a href="#cb10-1021" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 3.2（Attention）、Section 3.5（位置编码）</span>
<span id="cb10-1022"><a href="#cb10-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1706.03762</span><span class="co">](https://arxiv.org/abs/1706.03762)</span></span>
<span id="cb10-1023"><a href="#cb10-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1024"><a href="#cb10-1024" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Sukhbaatar et al., 2015] End-To-End Memory Networks**</span>
<span id="cb10-1025"><a href="#cb10-1025" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Self-Attention的思想先驱</span>
<span id="cb10-1026"><a href="#cb10-1026" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2（模型架构）</span>
<span id="cb10-1027"><a href="#cb10-1027" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1503.08895</span><span class="co">](https://arxiv.org/abs/1503.08895)</span></span>
<span id="cb10-1028"><a href="#cb10-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1029"><a href="#cb10-1029" aria-hidden="true" tabindex="-1"></a><span class="fu">### 理论基础</span></span>
<span id="cb10-1030"><a href="#cb10-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1031"><a href="#cb10-1031" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Yun et al., 2020] Are Transformers universal approximators of sequence-to-sequence functions?**</span>
<span id="cb10-1032"><a href="#cb10-1032" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>证明Transformer的通用逼近性质</span>
<span id="cb10-1033"><a href="#cb10-1033" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1912.10077</span><span class="co">](https://arxiv.org/abs/1912.10077)</span></span>
<span id="cb10-1034"><a href="#cb10-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1035"><a href="#cb10-1035" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Ramsauer et al., 2020] Hopfield Networks is All You Need**</span>
<span id="cb10-1036"><a href="#cb10-1036" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>将Attention解释为现代Hopfield网络</span>
<span id="cb10-1037"><a href="#cb10-1037" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">2008.02217</span><span class="co">](https://arxiv.org/abs/2008.02217)</span></span>
<span id="cb10-1038"><a href="#cb10-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1039"><a href="#cb10-1039" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb10-1040"><a href="#cb10-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1041"><a href="#cb10-1041" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Shaw et al., 2018] Self-Attention with Relative Position Representations**</span>
<span id="cb10-1042"><a href="#cb10-1042" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>相对位置编码的早期工作</span>
<span id="cb10-1043"><a href="#cb10-1043" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">1803.02155</span><span class="co">](https://arxiv.org/abs/1803.02155)</span></span>
<span id="cb10-1044"><a href="#cb10-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1045"><a href="#cb10-1045" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Su et al., 2021] RoFormer: Enhanced Transformer with Rotary Position Embedding**</span>
<span id="cb10-1046"><a href="#cb10-1046" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>提出RoPE，成为现代LLM的标配</span>
<span id="cb10-1047"><a href="#cb10-1047" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>arXiv: <span class="co">[</span><span class="ot">2104.09864</span><span class="co">](https://arxiv.org/abs/2104.09864)</span></span>
<span id="cb10-1048"><a href="#cb10-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1049"><a href="#cb10-1049" aria-hidden="true" tabindex="-1"></a><span class="fu">### 可视化资源</span></span>
<span id="cb10-1050"><a href="#cb10-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1051"><a href="#cb10-1051" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Jay Alammar] The Illustrated Transformer**</span>
<span id="cb10-1052"><a href="#cb10-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>最佳可视化教程</span>
<span id="cb10-1053"><a href="#cb10-1053" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>网址: <span class="co">[</span><span class="ot">jalammar.github.io/illustrated-transformer</span><span class="co">](https://jalammar.github.io/illustrated-transformer/)</span></span>
<span id="cb10-1054"><a href="#cb10-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1055"><a href="#cb10-1055" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-1056"><a href="#cb10-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1057"><a href="#cb10-1057" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb10-1058"><a href="#cb10-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1059"><a href="#cb10-1059" aria-hidden="true" tabindex="-1"></a>Self-Attention的想法并非凭空出现。在2017年Transformer论文之前，已经有多条研究脉络在探索类似的思想：</span>
<span id="cb10-1060"><a href="#cb10-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1061"><a href="#cb10-1061" aria-hidden="true" tabindex="-1"></a>**Memory Networks (2014-2015)**：Facebook AI Research的团队提出了用注意力机制从外部记忆中检索信息的想法。虽然他们的目标是问答系统而非序列建模，但Key-Value分离、软注意力检索等设计深刻影响了后来的发展。</span>
<span id="cb10-1062"><a href="#cb10-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1063"><a href="#cb10-1063" aria-hidden="true" tabindex="-1"></a>**Neural Turing Machines (2014)**：DeepMind提出的神经图灵机也使用了类似注意力的机制来读写外部记忆。虽然更加复杂，但它展示了神经网络可以学习类似"查找表"的操作。</span>
<span id="cb10-1064"><a href="#cb10-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1065"><a href="#cb10-1065" aria-hidden="true" tabindex="-1"></a>**Decomposable Attention (2016)**：Parikh等人提出了一个完全基于注意力的自然语言推理模型，不使用任何RNN。这是"纯注意力"模型的早期成功案例。</span>
<span id="cb10-1066"><a href="#cb10-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1067"><a href="#cb10-1067" aria-hidden="true" tabindex="-1"></a>**Transformer的贡献**：不是发明Self-Attention，而是将其系统化并证明它可以完全替代RNN。"Attention Is All You Need"这个大胆的标题，既是技术声明，也是研究宣言。</span>
<span id="cb10-1068"><a href="#cb10-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1069"><a href="#cb10-1069" aria-hidden="true" tabindex="-1"></a>有趣的是，Transformer论文的作者之一Ashish Vaswani后来回忆说，他们最初并不确定完全抛弃RNN是否可行。实验结果的惊人效果超出了所有人的预期——不仅效果更好，训练还快了一个数量级。这个结果改变了整个领域的方向。</span>
<span id="cb10-1070"><a href="#cb10-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-1071"><a href="#cb10-1071" aria-hidden="true" tabindex="-1"></a>从Memory Networks到Transformer，从辅助机制到核心架构，Self-Attention完成了从"配角"到"主角"的华丽转身。下一章，我们将见证这场革命的高潮：Transformer——"Attention Is All You Need"。</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>