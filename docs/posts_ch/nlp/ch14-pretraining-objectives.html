<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-26">
<meta name="description" content="BERT的MLM开创了双向预训练的先河，但它的15%信号效率、[MASK]标记的预训练-微调不一致、以及无法生成文本的局限性，催生了一系列创新的预训练目标：XLNet的排列语言建模、ELECTRA的替换词检测、T5的Span Corruption、以及对比学习在句子表示中的应用。">

<title>第14章：预训练目标的演进 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#预训练目标设计的核心矛盾" id="toc-预训练目标设计的核心矛盾" class="nav-link" data-scroll-target="#预训练目标设计的核心矛盾"><span class="header-section-number">2.1</span> 预训练目标设计的核心矛盾</a></li>
  <li><a href="#四个维度的权衡" id="toc-四个维度的权衡" class="nav-link" data-scroll-target="#四个维度的权衡"><span class="header-section-number">2.2</span> 四个维度的权衡</a></li>
  <li><a href="#一个不可能三角" id="toc-一个不可能三角" class="nav-link" data-scroll-target="#一个不可能三角"><span class="header-section-number">2.3</span> 一个不可能三角</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#xlnet打乱顺序来获得双向性" id="toc-xlnet打乱顺序来获得双向性" class="nav-link" data-scroll-target="#xlnet打乱顺序来获得双向性"><span class="header-section-number">3.1</span> XLNet：打乱顺序来获得双向性</a></li>
  <li><a href="#electra做判别题而非填空题" id="toc-electra做判别题而非填空题" class="nav-link" data-scroll-target="#electra做判别题而非填空题"><span class="header-section-number">3.2</span> ELECTRA：做判别题而非填空题</a></li>
  <li><a href="#t5一切皆文本" id="toc-t5一切皆文本" class="nav-link" data-scroll-target="#t5一切皆文本"><span class="header-section-number">3.3</span> T5：一切皆文本</a></li>
  <li><a href="#对比学习不预测词学习表示" id="toc-对比学习不预测词学习表示" class="nav-link" data-scroll-target="#对比学习不预测词学习表示"><span class="header-section-number">3.4</span> 对比学习：不预测词，学习表示</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#xlnet排列语言建模" id="toc-xlnet排列语言建模" class="nav-link" data-scroll-target="#xlnet排列语言建模"><span class="header-section-number">4.1</span> XLNet：排列语言建模</a></li>
  <li><a href="#electra替换词检测" id="toc-electra替换词检测" class="nav-link" data-scroll-target="#electra替换词检测"><span class="header-section-number">4.2</span> ELECTRA：替换词检测</a></li>
  <li><a href="#t5text-to-text统一框架" id="toc-t5text-to-text统一框架" class="nav-link" data-scroll-target="#t5text-to-text统一框架"><span class="header-section-number">4.3</span> T5：Text-to-Text统一框架</a></li>
  <li><a href="#对比学习在nlp中的应用" id="toc-对比学习在nlp中的应用" class="nav-link" data-scroll-target="#对比学习在nlp中的应用"><span class="header-section-number">4.4</span> 对比学习在NLP中的应用</a></li>
  <li><a href="#方法对比总结" id="toc-方法对比总结" class="nav-link" data-scroll-target="#方法对比总结"><span class="header-section-number">4.5</span> 方法对比总结</a></li>
  </ul></li>
  <li><a href="#工程实践" id="toc-工程实践" class="nav-link" data-scroll-target="#工程实践"><span class="header-section-number">5</span> 工程实践</a>
  <ul class="collapse">
  <li><a href="#electra的简化实现" id="toc-electra的简化实现" class="nav-link" data-scroll-target="#electra的简化实现"><span class="header-section-number">5.1</span> ELECTRA的简化实现</a></li>
  <li><a href="#t5的text-to-text任务格式化" id="toc-t5的text-to-text任务格式化" class="nav-link" data-scroll-target="#t5的text-to-text任务格式化"><span class="header-section-number">5.2</span> T5的Text-to-Text任务格式化</a></li>
  <li><a href="#simcse的训练核心" id="toc-simcse的训练核心" class="nav-link" data-scroll-target="#simcse的训练核心"><span class="header-section-number">5.3</span> SimCSE的训练核心</a></li>
  <li><a href="#使用hugging-face快速体验" id="toc-使用hugging-face快速体验" class="nav-link" data-scroll-target="#使用hugging-face快速体验"><span class="header-section-number">5.4</span> 使用Hugging Face快速体验</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">6</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#为什么有效实证视角" id="toc-为什么有效实证视角" class="nav-link" data-scroll-target="#为什么有效实证视角"><span class="header-section-number">6.2</span> 为什么有效？——实证视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">6.4</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#预训练目标改进的天花板" id="toc-预训练目标改进的天花板" class="nav-link" data-scroll-target="#预训练目标改进的天花板"><span class="header-section-number">7.1</span> 预训练目标改进的”天花板”</a></li>
  <li><a href="#训练策略-vs-训练目标" id="toc-训练策略-vs-训练目标" class="nav-link" data-scroll-target="#训练策略-vs-训练目标"><span class="header-section-number">7.2</span> 训练策略 vs 训练目标</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">7.3</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">8</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">8.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">9</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</a></li>
  <li><a href="#对比学习方向" id="toc-对比学习方向" class="nav-link" data-scroll-target="#对比学习方向"><span class="header-section-number">9.2</span> 对比学习方向</a></li>
  <li><a href="#前驱工作" id="toc-前驱工作" class="nav-link" data-scroll-target="#前驱工作"><span class="header-section-number">9.3</span> 前驱工作</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">9.4</span> 后续发展</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">9.5</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">10</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第14章：预训练目标的演进</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Beyond MLM: Permutation, Discrimination, Span Corruption, and Contrastive Learning</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">Pre-training</div>
    <div class="quarto-category">XLNet</div>
    <div class="quarto-category">ELECTRA</div>
    <div class="quarto-category">T5</div>
  </div>
  </div>

<div>
  <div class="description">
    BERT的MLM开创了双向预训练的先河，但它的15%信号效率、[MASK]标记的预训练-微调不一致、以及无法生成文本的局限性，催生了一系列创新的预训练目标：XLNet的排列语言建模、ELECTRA的替换词检测、T5的Span Corruption、以及对比学习在句子表示中的应用。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：BERT的掩码语言模型（MLM）存在信号效率低、预训练-微调不一致等缺陷——有没有更好的预训练目标？</p>
<p><strong>历史坐标</strong>：2019-2020年 | XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2020), T5 (Raffel et al., 2020), SimCSE (Gao et al., 2021) | 预训练目标的多元探索</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Yang et al.&nbsp;(2019)</strong> “XLNet: Generalized Autoregressive Pretraining for Language Understanding” (arXiv:1906.08237) — 参考了 Section 2（排列语言建模）、Section 2.3（双流注意力）、Table 1-6（实验结果）</li>
<li><strong>Clark et al.&nbsp;(2020)</strong> “ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators” (arXiv:2003.10555) — 参考了 Section 3（方法设计）、Figure 1（架构图）、Table 1-2（效率对比）</li>
<li><strong>Raffel et al.&nbsp;(2020)</strong> “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer” (arXiv:1910.10683) — 参考了 Section 3（系统性实验）、Table 3-14（预训练目标对比）</li>
<li><strong>Gao et al.&nbsp;(2021)</strong> “SimCSE: Simple Contrastive Learning of Sentence Embeddings” (arXiv:2104.08821) — 参考了 Section 3（对比学习框架）、Table 1-3（STS实验）</li>
</ul>
</section>
<section id="教材" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材"><span class="header-section-number">0.2</span> 教材</h3>
<ul>
<li><strong>D2L</strong> Section 15.8-15.10 — 参考了预训练模型对比的教学框架</li>
<li><strong>SLP3</strong> Chapter 11 — 参考了预训练目标的分类和讲解角度</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS224N</strong> Lecture 9-10 (2025) “Pretraining” — 参考了预训练目标演进的讲解思路</li>
<li><strong>CMU 11-711</strong> ANLP (Neubig) — 参考了预训练目标的对比分析框架</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们详细介绍了BERT——NLP历史上第一个真正实现深层双向预训练的模型。通过一个巧妙的改变——将”预测下一个词”替换为”预测被遮蔽的词”——BERT解除了双向性与语言建模之间的矛盾，让Transformer Encoder在预训练时每个位置都能同时看到左右两侧的上下文。在参数量几乎相同的条件下（BERT-Base 110M vs GPT-1 117M），BERT在GLUE平均分上超出GPT近7个百分点（79.6 vs 72.8），消融实验直接证明双向性是性能提升的主要来源。</p>
<p>然而，上一章结尾我们也系统梳理了BERT的三个核心局限。</p>
<p>第一个是<strong>信号效率低下</strong>。BERT的MLM每次随机遮蔽15%的token，只有这些被遮蔽的位置参与损失计算。其余85%的token虽然参与了前向传播（提供上下文），但不提供直接的训练信号。与GPT的因果语言建模相比——每个位置都预测下一个词，100%的token都贡献训练信号——BERT的信号效率大约只有GPT的<span class="math inline">\(1/7\)</span>。</p>
<p>第二个是<strong>预训练-微调不一致</strong>。尽管80-10-10策略做了缓解，MLM的本质问题无法回避：预训练时模型看到的输入中充斥着<code>[MASK]</code>标记，但微调时输入是干净的自然文本。模型需要在两种不同的输入分布上工作，这可能导致预训练学到的某些模式在微调时无法完全发挥作用。</p>
<p>第三个是<strong>不适合文本生成</strong>。BERT的Encoder-only架构天然不适合自回归生成——训练时每个位置看到了所有其他位置（包括”未来”），但生成时”未来”还不存在。这种训练与推理的不一致使得BERT无法胜任翻译、摘要、对话等生成任务。</p>
<p>2019到2020年间，研究者们从不同角度回应了这些局限，提出了多种创新的预训练目标。每一种方案都是对BERT某个具体痛点的直接回应。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：BERT之后的预训练目标创新沿着四条路线展开——XLNet用<strong>排列语言建模</strong>在保持自回归的同时获得双向上下文，消除了<code>[MASK]</code>标记；ELECTRA用<strong>替换词检测</strong>将信号效率从15%提升到100%；T5用<strong>Span Corruption</strong>和Encoder-Decoder架构统一了理解与生成；对比学习则跳出了”预测词”的范式，直接优化句子级表示。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="预训练目标设计的核心矛盾" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="预训练目标设计的核心矛盾"><span class="header-section-number">2.1</span> 预训练目标设计的核心矛盾</h3>
<p>在深入各个方案之前，我们先退一步思考：设计一个好的预训练目标，核心的矛盾是什么？</p>
<p>回忆BERT面临的根本困境：如果让模型看到双向上下文，它就可能”偷看”到答案，使得预测变得trivial；如果限制模型只看单向上下文，它就无法利用完整的语境信息。BERT用<code>[MASK]</code>标记来解决这个矛盾——把要预测的词遮住，这样即使模型看到了双向上下文，也无法作弊。但代价是引入了预训练-微调不一致和信号效率低下。</p>
<p>更一般地说，预训练目标的设计需要在多个维度上做权衡。</p>
</section>
<section id="四个维度的权衡" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="四个维度的权衡"><span class="header-section-number">2.2</span> 四个维度的权衡</h3>
<p><strong>第一个维度是上下文的方向性</strong>。预训练目标允许模型看到多少上下文？GPT只看左侧（单向），BERT看两侧但用<code>[MASK]</code>遮住目标（双向但有代价），XLNet通过排列看两侧且不用<code>[MASK]</code>（双向且无代价）。直觉上，看到的上下文越完整，模型对语言的理解就越深。但更多的上下文也意味着预测任务更容易，如何在”信息充分”和”任务有挑战性”之间取得平衡，是每个预训练目标都要面对的问题。</p>
<p><strong>第二个维度是信号效率</strong>。每个训练样本中，有多大比例的token贡献了训练信号？BERT是15%，GPT是100%，ELECTRA是100%。信号效率直接影响训练速度和数据利用率。在同样的计算预算下，信号效率更高的方法可以”学到更多”。</p>
<p><strong>第三个维度是预训练-微调一致性</strong>。预训练时模型看到的数据分布与微调时的数据分布有多接近？BERT的<code>[MASK]</code>标记、T5的<code>&lt;extra_id&gt;</code>哨兵标记都引入了人工token，造成了分布不匹配。理想情况下，预训练的输入格式应该尽量接近下游任务的自然文本。</p>
<p><strong>第四个维度是任务通用性</strong>。预训练目标让模型学到的表示能覆盖多广的下游任务？MLM训练的是token级别的预测能力，对词级别的任务（NER、完形填空）天然适配，但对句子级别的任务（文本分类、句子相似度）可能不够直接。对比学习则专注于学习句子级表示，但可能在token级任务上不够精细。</p>
</section>
<section id="一个不可能三角" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="一个不可能三角"><span class="header-section-number">2.3</span> 一个不可能三角</h3>
<p>如果我们把前三个维度画成一个三角——“完全双向 + 高信号效率 + 预训练-微调一致”——会发现没有一个方法能同时满足所有三个条件。BERT牺牲了信号效率和一致性来获得双向性。GPT牺牲了双向性来获得高效率和一致性。XLNet在理论上接近了三角的中心，但实现复杂度大幅增加。ELECTRA牺牲了部分一致性（引入替换而非自然token）来获得双向性和高效率。</p>
<p>理解了这个不可能三角，我们就能更清晰地理解每个方案的设计动机——它们不是在追求完美，而是在这个约束空间中寻找不同的最优权衡点。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<p>在进入数学之前，让我们先用直觉理解四种方案的核心洞察。每一种都从不同的角度切入同一个问题。</p>
<section id="xlnet打乱顺序来获得双向性" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="xlnet打乱顺序来获得双向性"><span class="header-section-number">3.1</span> XLNet：打乱顺序来获得双向性</h3>
<p>XLNet的核心洞察可以用一个类比来理解。想象你在做完形填空题”我去___存钱”。BERT的做法是把”银行”遮住变成”我去<code>[MASK]</code>存钱”，然后让你根据两侧上下文猜测。XLNet的做法则完全不同——它不遮住任何词，而是改变你”阅读”句子的顺序。</p>
<p>假设句子是”我 去 银行 存钱”（4个词），XLNet可能让你按照”存钱→我→银行→去”的顺序来处理。当模型轮到处理”银行”时，它已经看到了”存钱”和”我”，但还没有看到”去”。在另一种排列”去→存钱→银行→我”中，处理”银行”时已经看到了”去”和”存钱”。对所有可能的排列取期望，“银行”在不同排列中能看到不同的上下文子集——有时看到左侧，有时看到右侧，有时两侧都看到。这样，模型就在不使用<code>[MASK]</code>的情况下获得了双向上下文信息。</p>
<p>关键的优雅之处在于：虽然概念上我们在处理各种排列，但实际上输入序列的顺序并不改变。模型通过一种特殊的注意力掩码来模拟不同的排列——我们只是改变了”谁能看到谁”的规则，而不是真的把词打乱。</p>
</section>
<section id="electra做判别题而非填空题" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="electra做判别题而非填空题"><span class="header-section-number">3.2</span> ELECTRA：做判别题而非填空题</h3>
<p>ELECTRA的洞察更加直接。BERT让模型做”填空题”——遮住一个词，预测它是什么。但填空题有一个问题：每张试卷中只有15%的空需要填，其余85%的位置虽然读了但不打分。这就像一个老师只批改试卷的15%，其他部分直接跳过——明显浪费了评估学生理解程度的机会。</p>
<p>ELECTRA把”填空题”换成了”找错题”。首先，一个小型的”出题者”（生成器）会把句子中的某些词替换成看起来合理但可能不正确的词。然后，主模型（判别器）需要判断句子中的每一个词是原始的还是被替换过的。这就像考试改成了阅读理解中的”纠错题”——每个词都需要判断真伪，100%的位置都参与评分。</p>
<p>举个例子，原始句子”the chef cooked the meal”经过生成器可能变成”the chef ate the meal”（“cooked”被替换为”ate”）。判别器需要对每个位置做出判断——“the”是原始的，“chef”是原始的，“ate”是被替换的（应该是”cooked”），“the”是原始的，“meal”是原始的。五个位置中只有一个被替换了，但所有五个位置都提供了训练信号。信号效率是BERT的<span class="math inline">\(100\%/15\% \approx 6.7\)</span>倍。</p>
</section>
<section id="t5一切皆文本" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="t5一切皆文本"><span class="header-section-number">3.3</span> T5：一切皆文本</h3>
<p>T5的核心洞察是一个关于任务统一的思考。在BERT时代，不同的下游任务需要不同的输出头——分类任务用<code>[CLS]</code>向量加一个线性层，问答任务预测答案的起始和结束位置，NER任务在每个token上做标注。每种任务格式都需要专门的设计，这增加了工程复杂度，也阻碍了跨任务的知识共享。</p>
<p>T5的解决方案是彻底的简化：把所有NLP任务都统一为”文本到文本”（Text-to-Text）格式。翻译是文本到文本。摘要是文本到文本。分类也是文本到文本——输入”classify: I love this movie”，输出”positive”。问答、NER、文本蕴涵……一切皆可用文本到文本来表达。</p>
<p>与此相应，T5的预训练目标也采用了类似的思路——<strong>Span Corruption</strong>。模型随机遮蔽输入中的连续span，用哨兵标记（sentinel token）替代，然后生成被遮蔽的内容。与BERT的逐token遮蔽不同，Span Corruption遮蔽的是连续的片段，迫使模型进行更高层次的推理。而且，由于T5使用Encoder-Decoder架构，它天然支持生成任务——这是BERT做不到的。</p>
<div id="fig-t5-text-to-text" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-t5-text-to-text-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-14/original/fig-t5-text-to-text.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-t5-text-to-text-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: T5 的 Text-to-Text 统一框架：所有 NLP 任务——翻译、分类、语义相似度、摘要——都被转化为相同的”文本输入→文本输出”格式，使用同一个模型、同一个损失函数、同一套超参数进行训练。<em>Source: Raffel et al.&nbsp;(2020) “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, Figure 1</em>
</figcaption>
</figure>
</div>
</section>
<section id="对比学习不预测词学习表示" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="对比学习不预测词学习表示"><span class="header-section-number">3.4</span> 对比学习：不预测词，学习表示</h3>
<p>前面三种方案虽然各有创新，但本质上都是在”预测某种形式的词”——无论是预测被遮蔽的词（BERT/T5）、预测被排列到后面的词（XLNet）、还是判断词是否被替换（ELECTRA）。对比学习则跳出了这个范式，直接在句子级别优化表示的质量。</p>
<p>对比学习的直觉很简单：相似的句子应该有相似的表示，不相似的句子应该有不同的表示。SimCSE的巧妙之处在于它发现，同一个句子通过Transformer两次（由于dropout的随机性，两次前向传播会产生略微不同的表示），这两个表示就是天然的”正样本对”——它们来自同一个句子，语义完全相同，但在表示空间中有微小的差异。其他句子的表示则作为”负样本”。通过拉近正样本、推远负样本，模型学会了更均匀、更具判别力的句子表示。</p>
<p>这种方法的独特价值在于：它不需要任何标注数据，不需要特殊的预训练目标设计，就能大幅提升句子嵌入的质量——在语义文本相似度（STS）任务上，SimCSE将BERT的表现提升了高达16个百分点。</p>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="xlnet排列语言建模" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="xlnet排列语言建模"><span class="header-section-number">4.1</span> XLNet：排列语言建模</h3>
<section id="排列语言建模的形式化" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="排列语言建模的形式化"><span class="header-section-number">4.1.1</span> 排列语言建模的形式化</h4>
<p>标准的自回归语言建模将联合概率分解为从左到右的条件概率之积：</p>
<p><span class="math display">\[
P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, \ldots, x_{t-1})
\]</span></p>
<p>这种分解有一个隐含假设：因式分解的顺序是固定的（从左到右）。但概率论告诉我们，联合概率可以按照任意顺序分解——对于<span class="math inline">\(T\)</span>个随机变量，有<span class="math inline">\(T!\)</span>种合法的因式分解顺序，每一种都给出相同的联合概率。</p>
<p>XLNet正是利用了这一点。设<span class="math inline">\(\mathcal{Z}_T\)</span>为序列<span class="math inline">\([1, 2, \ldots, T]\)</span>的所有排列的集合，对于一个排列<span class="math inline">\(\mathbf{z} = [z_1, z_2, \ldots, z_T]\)</span>，XLNet的训练目标是最大化所有排列的期望对数似然：</p>
<p><span class="math display">\[
\max_\theta \quad \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_T} \left[ \sum_{t=1}^{T} \log P_\theta(x_{z_t} \mid \mathbf{x}_{\mathbf{z}_{&lt;t}}) \right]
\]</span></p>
<p>其中<span class="math inline">\(\mathbf{x}_{\mathbf{z}_{\lt t}}\)</span>表示在排列<span class="math inline">\(\mathbf{z}\)</span>中排在<span class="math inline">\(z_t\)</span>之前的所有token。由于对所有排列取期望，每个token<span class="math inline">\(x_{z_t}\)</span>在不同的排列中会以不同的子集作为上下文来预测——有时只有左侧token，有时只有右侧token，有时两侧都有。这样，模型就在保持自回归形式的同时，学到了双向上下文的信息。</p>
<p>一个重要的细节是：排列的是因式分解的顺序，而不是输入序列本身。输入序列始终保持原始的自然顺序”the cat sat on the mat”，改变的只是模型在预测每个位置时被允许看到哪些其他位置。这通过注意力掩码来实现——对于排列<span class="math inline">\(\mathbf{z}\)</span>和位置<span class="math inline">\(z_t\)</span>，注意力掩码允许<span class="math inline">\(z_t\)</span>关注<span class="math inline">\(\{z_1, z_2, \ldots, z_{t-1}\}\)</span>中的所有位置。</p>
</section>
<section id="双流自注意力" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="双流自注意力"><span class="header-section-number">4.1.2</span> 双流自注意力</h4>
<p>但排列语言建模有一个微妙的问题。在标准的Transformer中，位置<span class="math inline">\(z_t\)</span>的表示<span class="math inline">\(h_{z_t}\)</span>同时包含两种信息：（1）该位置的内容信息（这个位置的token是什么），和（2）该位置的位置信息（这是序列中的第几个位置）。当我们用<span class="math inline">\(h_{z_t}\)</span>去预测<span class="math inline">\(x_{z_t}\)</span>时，如果<span class="math inline">\(h_{z_t}\)</span>已经编码了<span class="math inline">\(x_{z_t}\)</span>的内容信息，预测就变成了trivial的自环——模型直接从自己的表示中读取答案。</p>
<p>但如果我们不在<span class="math inline">\(h_{z_t}\)</span>中编码<span class="math inline">\(x_{z_t}\)</span>的内容，那么当<span class="math inline">\(z_t\)</span>排在其他位置之前（即<span class="math inline">\(z_t\)</span>需要为后续位置提供上下文）时，后续位置就无法获得<span class="math inline">\(x_{z_t}\)</span>的内容信息。</p>
<p>XLNet的解决方案是引入<strong>双流自注意力（Two-Stream Self-Attention）</strong>：</p>
<p><strong>内容流（Content Stream）</strong><span class="math inline">\(h_{z_t}\)</span>：与标准的Transformer相同，编码了位置<span class="math inline">\(z_t\)</span>的内容信息和位置信息。当<span class="math inline">\(z_t\)</span>作为上下文为其他位置服务时，使用内容流。</p>
<p><strong>查询流（Query Stream）</strong><span class="math inline">\(g_{z_t}\)</span>：只编码位置信息和之前上下文的内容信息，不包含<span class="math inline">\(z_t\)</span>自身的内容。当需要预测<span class="math inline">\(z_t\)</span>位置的token时，使用查询流。</p>
<p>用数学表达：</p>
<p><span class="math display">\[
\begin{aligned}
g_{z_t}^{(m)} &amp;\leftarrow \text{Attention}(Q = g_{z_t}^{(m-1)}, \; KV = h_{\mathbf{z}_{&lt;t}}^{(m-1)}) \\
h_{z_t}^{(m)} &amp;\leftarrow \text{Attention}(Q = h_{z_t}^{(m-1)}, \; KV = h_{\mathbf{z}_{\leq t}}^{(m-1)})
\end{aligned}
\]</span></p>
<p>注意两个关键区别：查询流的KV中不包含<span class="math inline">\(z_t\)</span>自身（<span class="math inline">\(\mathbf{z}_{\lt t}\)</span>），而内容流的KV包含<span class="math inline">\(z_t\)</span>自身（<span class="math inline">\(\mathbf{z}_{\leq t}\)</span>）。初始化时，查询流使用一个可学习的向量<span class="math inline">\(g_{z_t}^{(0)} = w\)</span>（只含位置信息），内容流使用词嵌入<span class="math inline">\(h_{z_t}^{(0)} = e(x_{z_t})\)</span>（含内容信息）。</p>
<div id="fig-xlnet-two-stream" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-xlnet-two-stream-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-14/original/fig-xlnet-two-stream.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-xlnet-two-stream-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: XLNet 的双流自注意力机制：(a) 内容流（Content Stream）——与标准 Transformer 相同，编码位置和内容信息，可以看到自身；(b) 查询流（Query Stream）——只编码位置信息，不能看到自身内容，用于预测当前位置的 token；(c) 排列语言建模的整体示意，展示了不同排列下的注意力掩码。<em>Source: Yang et al.&nbsp;(2019) “XLNet: Generalized Autoregressive Pretraining for Language Understanding”, Figure 1</em>
</figcaption>
</figure>
</div>
</section>
<section id="数值示例排列语言建模" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="数值示例排列语言建模"><span class="header-section-number">4.1.3</span> 数值示例：排列语言建模</h4>
<p>为了建立直觉，让我们用一个小例子走通完整的过程。</p>
<p><strong>设定</strong>：句子”I love NLP”，3个token，位置为<span class="math inline">\([1, 2, 3]\)</span>。</p>
<p><strong>考虑排列 <span class="math inline">\(\mathbf{z} = [3, 1, 2]\)</span></strong>（处理顺序：先处理位置3的”NLP”，再处理位置1的”I”，最后处理位置2的”love”）。</p>
<p>在这个排列下，注意力掩码的规则是：</p>
<ul>
<li>处理位置3时（<span class="math inline">\(z_1 = 3\)</span>）：之前没有其他位置，只能看到自己</li>
<li>处理位置1时（<span class="math inline">\(z_2 = 1\)</span>）：之前有位置3，可以看到”NLP”和自己</li>
<li>处理位置2时（<span class="math inline">\(z_3 = 2\)</span>）：之前有位置3和1，可以看到”NLP”、“I”和自己</li>
</ul>
<p>注意力掩码矩阵（1表示可以看到，0表示不可以）：</p>
<p><span class="math display">\[
M_{\text{content}} = \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad
M_{\text{query}} = \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \end{bmatrix}
\]</span></p>
<p>内容流掩码中，位置1（行1）可以看到位置1和位置3（因为在排列中，位置3在位置1之前），但看不到位置2。查询流掩码中，位置1不能看到自己（去掉自环），只能看到位置3。</p>
<p><strong>考虑另一个排列 <span class="math inline">\(\mathbf{z}' = [2, 3, 1]\)</span></strong>：</p>
<ul>
<li>处理”love”时：只看到自己</li>
<li>处理”NLP”时：看到”love”和自己</li>
<li>处理”I”时：看到”love”、“NLP”和自己</li>
</ul>
<p>现在预测”I”时，模型同时看到了”love”和”NLP”——既有左侧上下文也有右侧上下文。而在排列<span class="math inline">\(\mathbf{z}\)</span>中，预测”love”时看到了”NLP”（右侧）和”I”（左侧）。通过在不同排列中采样，每个token最终都能获得来自各个方向的上下文信息。</p>
<p><strong>在实际训练中</strong>，我们不需要遍历所有<span class="math inline">\(T!\)</span>种排列。XLNet对每个训练样本随机采样一种排列，并且只预测排列中最后若干个位置的token（因为排在前面的位置能看到的上下文太少，预测信号质量较低）。具体地，对于一个排列<span class="math inline">\(\mathbf{z}\)</span>，只预测<span class="math inline">\(z_{c+1}, z_{c+2}, \ldots, z_T\)</span>（其中<span class="math inline">\(c\)</span>是一个截断点），使得每个被预测的token至少能看到<span class="math inline">\(c\)</span>个上下文token。</p>
</section>
<section id="xlnet的额外优势transformer-xl的继承" class="level4" data-number="4.1.4">
<h4 data-number="4.1.4" class="anchored" data-anchor-id="xlnet的额外优势transformer-xl的继承"><span class="header-section-number">4.1.4</span> XLNet的额外优势：Transformer-XL的继承</h4>
<p>XLNet的名字中包含”XL”，因为它还继承了Transformer-XL（Dai et al., 2019）的两个关键技术。</p>
<p><strong>片段循环机制（Segment Recurrence）</strong>：在处理长文本时，将文本分成固定长度的片段。处理当前片段时，保留上一个片段的隐藏状态作为额外的记忆，使模型的有效上下文长度超过单个片段的长度。</p>
<p><strong>相对位置编码</strong>：与BERT使用的绝对位置编码不同，XLNet使用相对位置编码，编码的是两个token之间的相对距离而非各自的绝对位置。这使得模型能更好地泛化到训练时未见过的序列长度。</p>
<p>这两项技术使XLNet不仅在预训练目标上超越了BERT，在处理长文档方面也具有优势。</p>
</section>
</section>
<section id="electra替换词检测" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="electra替换词检测"><span class="header-section-number">4.2</span> ELECTRA：替换词检测</h3>
<section id="从生成到判别的范式转变" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="从生成到判别的范式转变"><span class="header-section-number">4.2.1</span> 从生成到判别的范式转变</h4>
<p>BERT的MLM本质上是一个<strong>生成式</strong>预训练任务——给定上下文，生成（预测）被遮蔽的词。ELECTRA提出了一个根本性的范式转变：用<strong>判别式</strong>任务替代生成式任务。</p>
<p>ELECTRA的架构包含两个网络：一个小型的<strong>生成器</strong>（Generator）和一个大型的<strong>判别器</strong>（Discriminator）。</p>
<div id="fig-electra-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-electra-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-14/original/fig-electra-architecture.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-electra-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: ELECTRA 的生成器-判别器架构：生成器（Generator）是一个小型 MLM 模型，对被遮蔽的位置预测替代词（如将 “cooked” 替换为 “ate”）；判别器（Discriminator）接收生成器”修补”后的句子，对每个位置判断是”Original”还是”Replaced”。训练完成后丢弃生成器，只保留判别器用于下游任务。<em>Source: Clark et al.&nbsp;(2020) “ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators”, Figure 1</em>
</figcaption>
</figure>
</div>
<p><strong>生成器</strong>是一个小型的MLM模型。它接收被遮蔽的输入，对每个被遮蔽的位置预测一个替代词。生成器的目标与BERT的MLM完全相同：</p>
<p><span class="math display">\[
L_{\text{MLM}}(\theta_G) = -\sum_{i \in \mathcal{M}} \log P_G(x_i \mid \tilde{\mathbf{x}})
\]</span></p>
<p><strong>判别器</strong>是主模型。它接收生成器”修补”过的句子（被遮蔽的位置已被生成器的预测词替换），然后对每一个位置判断该token是”原始的”（original）还是”被替换的”（replaced）。这是一个二分类任务：</p>
<p><span class="math display">\[
L_{\text{Disc}}(\theta_D) = -\sum_{t=1}^{T} \left[ \mathbb{1}(x_t^{\text{corrupt}} = x_t) \log D(x_t^{\text{corrupt}}, t) + \mathbb{1}(x_t^{\text{corrupt}} \neq x_t) \log (1 - D(x_t^{\text{corrupt}}, t)) \right]
\]</span></p>
<p>其中<span class="math inline">\(D(x_t^{\text{corrupt}}, t)\)</span>是判别器预测位置<span class="math inline">\(t\)</span>的token是”原始”的概率。</p>
<p>总损失函数是生成器和判别器损失的加权和：</p>
<p><span class="math display">\[
L = L_{\text{MLM}}(\theta_G) + \lambda \cdot L_{\text{Disc}}(\theta_D)
\]</span></p>
<p>论文中<span class="math inline">\(\lambda = 50\)</span>，这意味着判别器的训练信号被大幅加权——这合理地反映了判别器才是我们最终关心的模型。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>算法：替换词检测训练流程（Clark et al., 2020）
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>输入</strong>：未标注语料 <span class="math inline">\(\mathcal{X}\)</span>，生成器 <span class="math inline">\(G\)</span>（小型 MLM），判别器 <span class="math inline">\(D\)</span>（大型 Encoder）</p>
<p><strong>For</strong> each batch <span class="math inline">\(\mathbf{x} \in \mathcal{X}\)</span>:</p>
<ol type="1">
<li>随机选择遮蔽位置集合 <span class="math inline">\(\mathcal{M} \subset \{1, \ldots, T\}\)</span>（比例 15%）</li>
<li>构建遮蔽输入：<span class="math inline">\(\tilde{\mathbf{x}} = \text{REPLACE}(\mathbf{x}, \mathcal{M}, \texttt{[MASK]})\)</span></li>
<li><strong>生成器前向</strong>：对每个 <span class="math inline">\(i \in \mathcal{M}\)</span>，从 <span class="math inline">\(P_G(\cdot \mid \tilde{\mathbf{x}})\)</span> 中采样替代词 <span class="math inline">\(\hat{x}_i\)</span></li>
<li>构建判别器输入：<span class="math inline">\(\mathbf{x}^{\text{corrupt}} = \text{REPLACE}(\mathbf{x}, \mathcal{M}, \hat{\mathbf{x}})\)</span></li>
<li><strong>判别器前向</strong>：对每个位置 <span class="math inline">\(t = 1, \ldots, T\)</span>，预测 <span class="math inline">\(D(\mathbf{x}^{\text{corrupt}}, t) = P(\text{original} \mid t)\)</span></li>
<li>计算联合损失并更新参数：<span class="math inline">\(L = L_{\text{MLM}}(\theta_G) + 50 \cdot L_{\text{Disc}}(\theta_D)\)</span></li>
</ol>
<p><strong>输出</strong>：训练好的判别器 <span class="math inline">\(D\)</span>（丢弃生成器 <span class="math inline">\(G\)</span>）</p>
<p><em>Source: Clark et al.&nbsp;(2020) “ELECTRA”, Section 3.1</em></p>
</div>
</div>
</section>
<section id="为什么不是gan" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="为什么不是gan"><span class="header-section-number">4.2.2</span> 为什么不是GAN？</h4>
<p>ELECTRA的生成器-判别器架构看起来很像GAN（生成对抗网络），但有一个根本区别：<strong>生成器和判别器不是对抗训练的</strong>。</p>
<p>在GAN中，生成器的目标是”欺骗”判别器——生成器试图产生让判别器无法分辨的样本。这种对抗训练会导致训练不稳定和模式崩溃等问题。</p>
<p>在ELECTRA中，生成器的训练目标是最大化MLM的对数似然，与判别器无关。生成器只是一个工具，用于产生”看起来合理但可能不正确”的替换词，让判别器有东西可以判断。两个网络独立地优化各自的目标函数，联合训练只是为了效率——让生成器的输出随着训练进展变得越来越逼真，从而给判别器提供越来越困难的训练样本。</p>
<p>这个设计选择的原因是实际的：替换词是离散的token（不是连续向量），梯度无法从判别器反向传播到生成器。要做对抗训练需要强化学习等技巧，会大幅增加复杂度。ELECTRA的作者发现，简单的联合MLM训练就已经足够好了。</p>
</section>
<section id="数值示例electra的训练流程" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="数值示例electra的训练流程"><span class="header-section-number">4.2.3</span> 数值示例：ELECTRA的训练流程</h4>
<p>让我们用一个具体例子走通ELECTRA的完整训练过程。</p>
<p><strong>输入句子</strong>：“the chef cooked the meal”</p>
<p><strong>Step 1: 随机遮蔽</strong>。按照BERT的方式，随机遮蔽15%的token。假设”cooked”和”meal”被选中遮蔽：</p>
<p><span class="math display">\[
\text{masked input: } \text{the chef [MASK] the [MASK]}
\]</span></p>
<p><strong>Step 2: 生成器预测</strong>。小型生成器（如BERT-Small）对每个<code>[MASK]</code>位置生成替代词：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>位置</th>
<th>原始词</th>
<th>生成器Top-5预测</th>
<th>采样结果</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>cooked</td>
<td>ate(0.25), prepared(0.20), cooked(0.18), made(0.15), served(0.12)</td>
<td><strong>ate</strong></td>
</tr>
<tr class="even">
<td>5</td>
<td>meal</td>
<td>meal(0.30), food(0.22), dinner(0.18), dish(0.15), lunch(0.10)</td>
<td><strong>meal</strong></td>
</tr>
</tbody>
</table>
<p>注意：生成器可能恰好预测出原始词（如位置5的”meal”），也可能预测出不同的词（如位置3的”ate”替代了”cooked”）。</p>
<p><strong>Step 3: 构建判别器输入</strong>。用生成器的预测替换<code>[MASK]</code>：</p>
<p><span class="math display">\[
\text{discriminator input: } \text{the chef ate the meal}
\]</span></p>
<p><strong>Step 4: 判别器逐位判断</strong>。判别器对每个位置输出”original”或”replaced”的概率：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>位置</th>
<th>Token</th>
<th>真实标签</th>
<th>判别器预测 <span class="math inline">\(D(x_t, t)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>the</td>
<td>original</td>
<td>0.95（正确）</td>
</tr>
<tr class="even">
<td>2</td>
<td>chef</td>
<td>original</td>
<td>0.92（正确）</td>
</tr>
<tr class="odd">
<td>3</td>
<td>ate</td>
<td><strong>replaced</strong></td>
<td>0.30（正确，认为它被替换了）</td>
</tr>
<tr class="even">
<td>4</td>
<td>the</td>
<td>original</td>
<td>0.88（正确）</td>
</tr>
<tr class="odd">
<td>5</td>
<td>meal</td>
<td>original</td>
<td>0.85（正确，因为生成器恰好预测了原词）</td>
</tr>
</tbody>
</table>
<p><strong>关键观察</strong>：所有5个位置都贡献了训练信号。即使位置1、2、4从未被遮蔽过，判别器也需要确认它们是原始的——这迫使模型对每个token建立深入的语义理解。位置5虽然经过了遮蔽-替换流程，但生成器恰好预测了原词，所以标签是”original”。</p>
</section>
<section id="效率分析小即是美" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="效率分析小即是美"><span class="header-section-number">4.2.4</span> 效率分析：小即是美</h4>
<p>ELECTRA的一个反直觉发现是：生成器应该是小的。</p>
<p>论文比较了不同大小的生成器对判别器性能的影响。当生成器与判别器大小相同时，性能反而不是最优的——因为太强的生成器生成的替换词太过逼真，判别器难以区分，训练信号变得微弱。最佳的生成器大小大约是判别器的<span class="math inline">\(1/4\)</span>到<span class="math inline">\(1/3\)</span>。这让生成器足够强大，能生成语法上合理的替换词（而非随机词），但又不至于太强，保持判别任务的适当难度。</p>
<p>在计算效率方面，ELECTRA的优势极为显著。ELECTRA-Small使用一块GPU训练4天，就能在GLUE基准上达到与GPT-Large（使用30倍计算资源训练）相当的性能。ELECTRA-Base在相同的预训练计算预算下超过了BERT-Large和XLNet-Large。</p>
</section>
</section>
<section id="t5text-to-text统一框架" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="t5text-to-text统一框架"><span class="header-section-number">4.3</span> T5：Text-to-Text统一框架</h3>
<section id="span-corruption预训练目标" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="span-corruption预训练目标"><span class="header-section-number">4.3.1</span> Span Corruption预训练目标</h4>
<p>T5的预训练目标叫做<strong>Span Corruption</strong>——随机选择输入中的若干连续span，将每个span替换为一个唯一的哨兵标记（sentinel token），然后让解码器生成被替换的内容。</p>
<p>与BERT的MLM相比，Span Corruption在四个方面有所不同。首先，BERT遮蔽的是随机选择的单个token，而T5遮蔽的是连续的span——这迫使模型学习更高层次的短语级补全能力。其次，BERT用统一的<code>[MASK]</code>标记替换所有被遮蔽的位置，而T5为每个被遮蔽的span分配一个唯一的哨兵标记（<code>&lt;extra_id_0&gt;</code>、<code>&lt;extra_id_1&gt;</code>等），使得解码器能明确知道自己在”修复”哪个span。第三，BERT在编码器端直接预测被遮蔽token的概率分布，而T5在解码器端以自回归的方式生成被替换span的完整文本。最后，T5的目标序列只包含被遮蔽的部分（加上哨兵标记），比原始序列短得多，提高了训练效率。</p>
</section>
<section id="数值示例span-corruption" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="数值示例span-corruption"><span class="header-section-number">4.3.2</span> 数值示例：Span Corruption</h4>
<p><strong>原始句子</strong>：“Thank you for inviting me to your party last week”</p>
<p><strong>Step 1: 随机选择span</strong>。遮蔽率15%（约1-2个token的span）。假设选中”for inviting”和”last week”：</p>
<p><strong>Step 2: 替换为哨兵标记</strong>：</p>
<p><span class="math display">\[
\text{Input: } \text{Thank you } \texttt{&lt;extra\_id\_0&gt;} \text{ me to your party } \texttt{&lt;extra\_id\_1&gt;}
\]</span></p>
<p><strong>Step 3: 构建目标序列</strong>：</p>
<p><span class="math display">\[
\text{Target: } \texttt{&lt;extra\_id\_0&gt;} \text{ for inviting } \texttt{&lt;extra\_id\_1&gt;} \text{ last week } \texttt{&lt;extra\_id\_2&gt;}
\]</span></p>
<p>目标序列的格式是：每个哨兵标记后面跟着它替代的原始span，最后一个哨兵标记<code>&lt;extra_id_2&gt;</code>标志着结束。</p>
<p><strong>这个设计的精妙之处</strong>在于目标序列比原始序列短得多。假设原始序列有512个token，遮蔽15%后目标序列大约只有77个token（被遮蔽的token加上哨兵标记）。这意味着解码器的计算量大大减少，训练效率更高。</p>
</section>
<section id="t5的系统性实验不只是一个模型" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="t5的系统性实验不只是一个模型"><span class="header-section-number">4.3.3</span> T5的系统性实验：不只是一个模型</h4>
<p>T5论文的最大贡献也许不是Span Corruption本身，而是它对预训练策略进行的前所未有的系统性消融实验。论文标题”Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”——“探索极限”是关键词。</p>
<p><strong>预训练目标的对比实验</strong>。T5在相同的计算预算下比较了多种预训练目标：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>预训练目标</th>
<th>GLUE平均分</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>前缀语言模型（Prefix LM）</td>
<td>82.5</td>
<td>编码器部分双向，解码器部分因果</td>
</tr>
<tr class="even">
<td>因果语言建模（Causal LM）</td>
<td>80.9</td>
<td>GPT风格，完全因果</td>
</tr>
<tr class="odd">
<td>BERT风格MLM（去噪）</td>
<td>84.0</td>
<td>随机遮蔽+预测</td>
</tr>
<tr class="even">
<td><strong>Span Corruption</strong></td>
<td><strong>84.6</strong></td>
<td>遮蔽连续span+生成</td>
</tr>
<tr class="odd">
<td>Span Corruption（不同遮蔽率）</td>
<td>变化不大</td>
<td>10%-25%差异不大</td>
</tr>
</tbody>
</table>
<p>结论是Span Corruption略优于BERT风格的MLM，因果语言建模（GPT风格）在同等计算量下效果最差。但差距并不巨大——预训练目标的选择没有模型规模和数据量的影响大。</p>
<p><strong>架构的对比实验</strong>。T5还比较了三种架构：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>架构</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Encoder-Decoder</td>
<td>编码器双向，解码器可生成</td>
<td>参数量翻倍（相对于纯编码器/解码器）</td>
</tr>
<tr class="even">
<td>Decoder-only（因果）</td>
<td>简单，适合生成</td>
<td>理解任务不够好</td>
</tr>
<tr class="odd">
<td>Decoder-only（前缀）</td>
<td>前缀部分双向</td>
<td>略差于Encoder-Decoder</td>
</tr>
</tbody>
</table>
<p>在控制总参数量相同的条件下，Encoder-Decoder架构略优于其他两种——因为在同等参数量下，Encoder-Decoder实际上有两倍的层数（编码器N层 + 解码器N层），虽然单次前向传播中只有部分层是”激活”的。</p>
<div id="fig-t5-attention-masks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-t5-attention-masks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-14/original/fig-t5-attention-masks.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-t5-attention-masks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: 三种架构对应的注意力掩码模式：<strong>Fully-visible</strong>（全可见）对应 Encoder，每个位置可以看到所有其他位置；<strong>Causal</strong>（因果）对应 Decoder-only，每个位置只能看到自己和之前的位置；<strong>Causal with prefix</strong>（带前缀的因果）对应前缀语言模型，前缀部分全可见，其余部分因果。<em>Source: Raffel et al.&nbsp;(2020) “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, Figure 3</em>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="对比学习在nlp中的应用" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="对比学习在nlp中的应用"><span class="header-section-number">4.4</span> 对比学习在NLP中的应用</h3>
<section id="simcse用dropout作为数据增强" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="simcse用dropout作为数据增强"><span class="header-section-number">4.4.1</span> SimCSE：用Dropout作为数据增强</h4>
<p>对比学习的核心框架可以概括为三步：（1）为每个样本生成正样本对，（2）将其他样本作为负样本，（3）用对比损失拉近正样本、推远负样本。在计算机视觉中，正样本对通常通过数据增强生成（如随机裁剪、颜色变换同一张图片）。但NLP中数据增强非常困难——改一个词可能改变整个句子的语义。</p>
<p>SimCSE（Gao et al., 2021）的突破在于发现了一种极其简单的数据增强方式：<strong>Dropout噪声</strong>。</p>
<p>将同一个句子通过预训练的Transformer两次，由于Dropout层的随机性，两次前向传播会产生略微不同的表示<span class="math inline">\(\mathbf{h}_i\)</span>和<span class="math inline">\(\mathbf{h}_i'\)</span>。这两个表示构成正样本对——语义完全相同，但在表示空间中存在微小差异（因为Dropout随机丢弃了不同的神经元）。</p>
<p>对于一个batch中的<span class="math inline">\(N\)</span>个句子，对比损失为：</p>
<p><span class="math display">\[
\ell_i = -\log \frac{e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_i') / \tau}}{\sum_{j=1}^{N} e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_j') / \tau}}
\]</span></p>
<p>其中<span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>是余弦相似度，<span class="math inline">\(\tau\)</span>是温度超参数。直觉上，分子希望<span class="math inline">\(\mathbf{h}_i\)</span>和<span class="math inline">\(\mathbf{h}_i'\)</span>（同一句子的两个表示）尽量接近，分母希望<span class="math inline">\(\mathbf{h}_i\)</span>和其他句子的表示尽量远离。</p>
</section>
<section id="数值示例对比学习的核心计算" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="数值示例对比学习的核心计算"><span class="header-section-number">4.4.2</span> 数值示例：对比学习的核心计算</h4>
<p><strong>设定</strong>：batch中有3个句子：<span class="math inline">\(s_1\)</span>=“A man is sleeping”，<span class="math inline">\(s_2\)</span>=“A cat sits on a mat”，<span class="math inline">\(s_3\)</span>=“A person is resting”。温度<span class="math inline">\(\tau = 0.05\)</span>。</p>
<p><strong>Step 1: 两次前向传播</strong>。每个句子通过BERT两次（不同Dropout mask），得到<code>[CLS]</code>向量：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 43%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>句子</th>
<th>第一次表示 <span class="math inline">\(\mathbf{h}\)</span></th>
<th>第二次表示 <span class="math inline">\(\mathbf{h}'\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(s_1\)</span> “A man is sleeping”</td>
<td><span class="math inline">\([0.2, 0.8, -0.1]\)</span></td>
<td><span class="math inline">\([0.3, 0.7, -0.2]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(s_2\)</span> “A cat sits on a mat”</td>
<td><span class="math inline">\([-0.5, 0.3, 0.6]\)</span></td>
<td><span class="math inline">\([-0.4, 0.2, 0.7]\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(s_3\)</span> “A person is resting”</td>
<td><span class="math inline">\([0.1, 0.9, 0.0]\)</span></td>
<td><span class="math inline">\([0.2, 0.8, -0.1]\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Step 2: 计算余弦相似度矩阵</strong>（<span class="math inline">\(\mathbf{h}_i\)</span>与<span class="math inline">\(\mathbf{h}_j'\)</span>之间）：</p>
<p><span class="math display">\[
\text{sim}(\mathbf{h}_1, \mathbf{h}_1') = \frac{0.2 \times 0.3 + 0.8 \times 0.7 + (-0.1) \times (-0.2)}{\sqrt{0.04+0.64+0.01} \times \sqrt{0.09+0.49+0.04}} \approx 0.987
\]</span></p>
<p>类似地计算其他相似度：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(\mathbf{h}_1'\)</span></th>
<th><span class="math inline">\(\mathbf{h}_2'\)</span></th>
<th><span class="math inline">\(\mathbf{h}_3'\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathbf{h}_1\)</span></td>
<td>0.987（正样本）</td>
<td>-0.280</td>
<td>0.994</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{h}_2\)</span></td>
<td>-0.185</td>
<td>0.991（正样本）</td>
<td>-0.249</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{h}_3\)</span></td>
<td>0.976</td>
<td>-0.325</td>
<td>0.998（正样本）</td>
</tr>
</tbody>
</table>
<p><strong>Step 3: 计算对比损失</strong>（以<span class="math inline">\(s_1\)</span>为例）：</p>
<p><span class="math display">\[
\ell_1 = -\log \frac{e^{0.987/0.05}}{e^{0.987/0.05} + e^{-0.280/0.05} + e^{0.994/0.05}}
\]</span></p>
<p>注意<span class="math inline">\(s_3\)</span>“A person is resting”与<span class="math inline">\(s_1\)</span>“A man is sleeping”语义非常接近（<span class="math inline">\(\text{sim} = 0.994 &gt; 0.987\)</span>），这个”假负样本”会让模型错误地推远语义相似的句子。这是无监督对比学习的一个已知问题。</p>
<p>SimCSE还提出了<strong>有监督版本</strong>（Supervised SimCSE）：利用NLI（自然语言推理）数据集，将蕴涵（entailment）句子对作为正样本，矛盾（contradiction）句子对作为硬负样本。例如：</p>
<ul>
<li>正样本对：(“A man is sleeping”, “A person is resting”) — 蕴涵关系</li>
<li>硬负样本：(“A man is sleeping”, “A man is awake”) — 矛盾关系</li>
</ul>
<p>有监督版本的性能显著优于无监督版本，在STS-B基准上达到86.2%（vs 无监督的76.3%）。</p>
</section>
<section id="对比学习解决了什么问题" class="level4" data-number="4.4.3">
<h4 data-number="4.4.3" class="anchored" data-anchor-id="对比学习解决了什么问题"><span class="header-section-number">4.4.3</span> 对比学习解决了什么问题？</h4>
<p>为什么BERT直接提取的句子嵌入质量不高？一个重要原因是<strong>表示塌缩（Representation Collapse）</strong>——BERT的<code>[CLS]</code>向量占据了表示空间中一个非常小的区域，不同句子的表示挤在一起，难以区分。</p>
<p>Li et al.&nbsp;(2020) 的研究发现，BERT的词嵌入存在<strong>各向异性（anisotropy）</strong>问题——嵌入向量集中在一个窄锥体中，而不是均匀分布在整个空间。这意味着任意两个句子的余弦相似度都很高（通常在0.6-0.8之间），难以区分语义上真正相似和不相似的句子。</p>
<p>对比学习通过显式地优化”相似句子接近、不同句子远离”，有效地”撑开”了表示空间，使其更加各向同性（isotropic），句子嵌入的质量大幅提升。</p>
</section>
</section>
<section id="方法对比总结" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="方法对比总结"><span class="header-section-number">4.5</span> 方法对比总结</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 14%">
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 30%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>BERT MLM</th>
<th>XLNet PLM</th>
<th>ELECTRA RTD</th>
<th>T5 Span Corruption</th>
<th>SimCSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>预训练类型</strong></td>
<td>去噪</td>
<td>自回归</td>
<td>判别式</td>
<td>去噪（seq2seq）</td>
<td>对比式</td>
</tr>
<tr class="even">
<td><strong>上下文方向</strong></td>
<td>双向（有[MASK]）</td>
<td>双向（无[MASK]）</td>
<td>双向（有替换词）</td>
<td>编码器双向</td>
<td>双向</td>
</tr>
<tr class="odd">
<td><strong>信号效率</strong></td>
<td>~15%</td>
<td>~15%（部分预测）</td>
<td><strong>100%</strong></td>
<td>~15%</td>
<td>句子级</td>
</tr>
<tr class="even">
<td><strong>架构</strong></td>
<td>Encoder-only</td>
<td>Encoder-only</td>
<td>Encoder-only</td>
<td>Encoder-Decoder</td>
<td>Encoder-only</td>
</tr>
<tr class="odd">
<td><strong>生成能力</strong></td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td><strong>✓</strong></td>
<td>✗</td>
</tr>
<tr class="even">
<td><strong>实现复杂度</strong></td>
<td>低</td>
<td><strong>高</strong></td>
<td>中</td>
<td>中</td>
<td><strong>低</strong></td>
</tr>
<tr class="odd">
<td><strong>计算效率</strong></td>
<td>基准</td>
<td>1.5-2×</td>
<td><strong>最优</strong></td>
<td>1-1.5×</td>
<td>低（仅微调）</td>
</tr>
<tr class="even">
<td><strong>典型应用</strong></td>
<td>理解任务</td>
<td>理解任务</td>
<td>理解任务</td>
<td>理解+生成</td>
<td>句子嵌入</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="工程实践" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="工程实践"><span class="header-section-number">5</span> 工程实践</h2>
<section id="electra的简化实现" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="electra的简化实现"><span class="header-section-number">5.1</span> ELECTRA的简化实现</h3>
<p>由于ELECTRA在计算效率上的巨大优势，它是最具实际工程价值的预训练目标改进。以下是判别器核心逻辑的简化实现：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertForMaskedLM</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ELECTRA(nn.Module):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""简化版ELECTRA：生成器-判别器联合训练"""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, gen_hidden<span class="op">=</span><span class="dv">256</span>, disc_hidden<span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 生成器：小型MLM模型</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> BertForMaskedLM.from_pretrained(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">'bert-base-uncased'</span>  <span class="co"># 实际中应用更小的模型</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 判别器：大型编码器 + 二分类头</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.discriminator <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.disc_head <span class="op">=</span> nn.Linear(disc_hidden, <span class="dv">1</span>)  <span class="co"># 二分类：original vs replaced</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, mask_positions):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">        input_ids: [batch, seq_len] 原始token序列</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">        mask_positions: [batch, seq_len] 被遮蔽位置的bool掩码</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 构建生成器输入（遮蔽指定位置）</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        masked_input <span class="op">=</span> input_ids.clone()</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        masked_input[mask_positions] <span class="op">=</span> <span class="dv">103</span>  <span class="co"># [MASK] token id</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 生成器预测被遮蔽位置的token</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        gen_output <span class="op">=</span> <span class="va">self</span>.generator(masked_input)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        gen_logits <span class="op">=</span> gen_output.logits  <span class="co"># [batch, seq_len, vocab_size]</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 从生成器的预测分布中采样替代词</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        gen_probs <span class="op">=</span> torch.softmax(gen_logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        sampled_tokens <span class="op">=</span> torch.multinomial(</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            gen_probs[mask_positions], num_samples<span class="op">=</span><span class="dv">1</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 构建判别器输入（用采样的token替换[MASK]）</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        disc_input <span class="op">=</span> input_ids.clone()</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        disc_input[mask_positions] <span class="op">=</span> sampled_tokens</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 判别器逐位判断：original or replaced?</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        disc_output <span class="op">=</span> <span class="va">self</span>.discriminator(disc_input)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        disc_logits <span class="op">=</span> <span class="va">self</span>.disc_head(disc_output.last_hidden_state).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># disc_logits: [batch, seq_len]，每个位置的"original"概率</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: 构建标签（生成器预测与原词不同的位置标记为"replaced"）</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> (disc_input <span class="op">!=</span> input_ids).<span class="bu">float</span>()</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> gen_logits, disc_logits, labels, mask_positions</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="t5的text-to-text任务格式化" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="t5的text-to-text任务格式化"><span class="header-section-number">5.2</span> T5的Text-to-Text任务格式化</h3>
<p>T5将所有任务统一为文本到文本格式。以下是几个典型任务的输入输出格式：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_t5_task(task_type, <span class="op">**</span>kwargs):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""将不同NLP任务格式化为T5的text-to-text格式"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> task_type <span class="op">==</span> <span class="st">"classification"</span>:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "classify: I love this movie"</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "positive"</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"classify: </span><span class="sc">{</span>kwargs[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'label'</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"translation"</span>:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "translate English to German: That is good."</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "Das ist gut."</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="ss">f"translate English to </span><span class="sc">{</span>kwargs[<span class="st">'target_lang'</span>]<span class="sc">}</span><span class="ss">: "</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>kwargs[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'translation'</span>])</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"summarization"</span>:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "summarize: [长文本]"</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "[摘要]"</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"summarize: </span><span class="sc">{</span>kwargs[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'summary'</span>]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"nli"</span>:</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "mnli premise: [前提] hypothesis: [假设]"</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "entailment" / "neutral" / "contradiction"</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="ss">f"mnli premise: </span><span class="sc">{</span>kwargs[<span class="st">'premise'</span>]<span class="sc">}</span><span class="ss"> "</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"hypothesis: </span><span class="sc">{</span>kwargs[<span class="st">'hypothesis'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'label'</span>])</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"qa"</span>:</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "question: [问题] context: [上下文]"</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "[答案]"</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="ss">f"question: </span><span class="sc">{</span>kwargs[<span class="st">'question'</span>]<span class="sc">}</span><span class="ss"> "</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"context: </span><span class="sc">{</span>kwargs[<span class="st">'context'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'answer'</span>])</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    format_t5_task(<span class="st">"classification"</span>,</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>                   text<span class="op">=</span><span class="st">"I love this movie"</span>, label<span class="op">=</span><span class="st">"positive"</span>),</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    format_t5_task(<span class="st">"translation"</span>,</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                   text<span class="op">=</span><span class="st">"That is good."</span>, target_lang<span class="op">=</span><span class="st">"German"</span>,</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>                   translation<span class="op">=</span><span class="st">"Das ist gut."</span>),</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    format_t5_task(<span class="st">"nli"</span>,</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>                   premise<span class="op">=</span><span class="st">"A man is sleeping."</span>,</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>                   hypothesis<span class="op">=</span><span class="st">"A person is resting."</span>,</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>                   label<span class="op">=</span><span class="st">"entailment"</span>),</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="simcse的训练核心" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="simcse的训练核心"><span class="header-section-number">5.3</span> SimCSE的训练核心</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simcse_loss(model, sentences, temperature<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    无监督SimCSE的核心训练逻辑</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    同一个句子过两次模型（不同dropout mask），构成正样本对</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="bu">len</span>(sentences)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 两次前向传播，不同的dropout mask</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    embeddings_1 <span class="op">=</span> model(sentences)  <span class="co"># [batch, hidden_dim]</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    embeddings_2 <span class="op">=</span> model(sentences)  <span class="co"># [batch, hidden_dim]，dropout不同</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 拼接：[2*batch, hidden_dim]</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    embeddings <span class="op">=</span> torch.cat([embeddings_1, embeddings_2], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算余弦相似度矩阵：[2*batch, 2*batch]</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    sim_matrix <span class="op">=</span> F.cosine_similarity(</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        embeddings.unsqueeze(<span class="dv">1</span>), embeddings.unsqueeze(<span class="dv">0</span>), dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">/</span> temperature</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构建标签：正样本是偏移batch_size的对角线</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 对于embeddings_1[i]，正样本是embeddings_2[i]</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(batch_size, device<span class="op">=</span>sim_matrix.device)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.cat([labels <span class="op">+</span> batch_size, labels], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 去掉自身相似度（对角线设为极小值）</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.eye(<span class="dv">2</span> <span class="op">*</span> batch_size, dtype<span class="op">=</span>torch.<span class="bu">bool</span>, device<span class="op">=</span>sim_matrix.device)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    sim_matrix.masked_fill_(mask, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 交叉熵损失</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(sim_matrix, labels)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="使用hugging-face快速体验" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="使用hugging-face快速体验"><span class="header-section-number">5.4</span> 使用Hugging Face快速体验</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    ElectraForPreTraining, ElectraTokenizer,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    T5ForConditionalGeneration, T5Tokenizer</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># === ELECTRA: 判断哪些词被替换了 ===</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> ElectraTokenizer.from_pretrained(<span class="st">"google/electra-base-discriminator"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ElectraForPreTraining.from_pretrained(<span class="st">"google/electra-base-discriminator"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">"The chef ate the meal"</span>  <span class="co"># "ate" 可能是替换词</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(sentence, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs.logits: 每个位置的"fake"概率分数</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> torch.sigmoid(outputs.logits)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions ≈ [0.02, 0.03, 0.85, 0.01, 0.02]</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 位置2（"ate"）被高概率判断为"replaced"</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># === T5: Text-to-Text 任务 ===</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained(<span class="st">"t5-base"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained(<span class="st">"t5-base"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 翻译</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"translate English to German: That is good."</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># → "Das ist gut."</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 情感分类</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"sst2 sentence: I love this movie"</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># → "positive"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="深入理解" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">6</span> 深入理解</h2>
<blockquote class="blockquote">
<p><strong>研究者必读</strong>：这一节探讨各预训练目标的理论基础、实证发现、边界条件和开放问题</p>
</blockquote>
<section id="为什么有效理论视角" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">6.1</span> 为什么有效？——理论视角</h3>
<p><strong>XLNet与排列的等价性</strong>。XLNet的排列语言建模在理论上有一个优雅的性质：对所有排列取期望后，模型等价于在学习真正的联合概率分布<span class="math inline">\(P(x_1, x_2, \ldots, x_T)\)</span>。与BERT的MLM不同，MLM学习的是条件概率<span class="math inline">\(P(x_i \mid \mathbf{x}_{\backslash i})\)</span>（给定其他所有词预测第<span class="math inline">\(i\)</span>个词），这些条件概率之间不一定是一致的——它们可能无法对应一个合法的联合概率分布。这在理论上被称为MLM的”伪似然”（pseudo-likelihood）问题。XLNet通过保持严格的自回归分解，回避了这个问题。</p>
<p><strong>ELECTRA的判别式优势</strong>。ELECTRA的核心理论优势可以从学习理论的角度理解。生成式任务（预测被遮蔽的词）要求模型学习整个词汇表上的条件概率分布——这是一个非常高维的输出空间（<span class="math inline">\(V \approx 30000\)</span>）。判别式任务（判断token是否被替换）只需要做二分类——输出空间维度为1。在相同的模型容量和训练数据下，低维输出空间的任务通常更容易学习，收敛更快。</p>
<p>从信息论的角度，ELECTRA判别器在每个位置接收的信号是1 bit（original vs replaced），但它需要对整个句子进行深层理解才能做出正确判断。这就像考试中的判断题——虽然答案只有对错两种，但正确判断需要全面的理解。</p>
<p><strong>T5与多任务学习</strong>。T5的Text-to-Text框架可以理解为一种隐式的多任务学习。当模型在预训练阶段学会了处理Span Corruption（一种去噪任务），它实际上学到了一种通用的”损坏→修复”能力。微调时，不同的任务前缀（“translate:”、“summarize:”、“classify:”）告诉模型应该以哪种”修复模式”来处理输入。这与人类的元认知能力类似——知道当前在做什么类型的任务，并相应调整处理策略。</p>
</section>
<section id="为什么有效实证视角" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="为什么有效实证视角"><span class="header-section-number">6.2</span> 为什么有效？——实证视角</h3>
<p><strong>关键的消融实验发现</strong>。T5论文进行了NLP领域最大规模的预训练消融研究，其中几个发现特别值得注意。</p>
<p>第一，预训练目标的选择对最终性能的影响<strong>没有想象中那么大</strong>。Span Corruption、BERT风格MLM、去噪自编码器之间的性能差距通常在1-2个百分点以内。相比之下，模型规模（从Base到Large到XL）带来的提升是5-10个百分点。这暗示了一个重要的规律：<strong>规模比目标设计更重要</strong>。</p>
<p>第二，预训练数据的质量和数量比预训练目标更关键。T5论文构建了C4（Colossal Clean Crawled Corpus）数据集，清洗了Common Crawl中的大量噪声数据。实验显示，使用清洗过的数据与使用原始噪声数据之间的性能差距，远大于不同预训练目标之间的差距。</p>
<p>第三，ELECTRA在<strong>计算效率</strong>维度上的优势是最显著的。在相同的计算预算下（FLOPs），ELECTRA一致性地超过了所有其他方法，包括XLNet和RoBERTa。这个发现对资源受限的研究者和企业特别有意义。</p>
</section>
<section id="方法的边界条件" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">6.3</span> 方法的边界条件</h3>
<p><strong>XLNet的实现复杂度问题</strong>。XLNet的排列语言建模和双流注意力在理论上优雅，但在工程实现上引入了显著的复杂度。双流注意力需要在每一层维护两套隐藏状态（内容流和查询流），增加了约50%的内存开销和计算量。在实际应用中，这种额外的复杂度是否值得，取决于任务和资源约束。事实上，在XLNet之后，很少有后续工作继续使用排列语言建模——研究社区的注意力更多地转向了更简单的方法（如RoBERTa、ELECTRA）。</p>
<p><strong>ELECTRA的生成器依赖</strong>。ELECTRA的性能对生成器的质量和大小敏感。如果生成器太弱（如随机替换），判别任务太简单，模型学不到深层的语言知识。如果生成器太强（如与判别器大小相同），替换词太过逼真，判别任务又变得太难。这个”Goldilocks问题”需要仔细调节，增加了超参数搜索的成本。</p>
<p>另一个边界条件是ELECTRA的判别目标可能不利于生成任务。由于判别器学习的是”这个词对不对”而非”这个位置应该是什么词”，ELECTRA的预训练模型在文本生成任务上的表现通常不如BERT或T5。</p>
<p><strong>T5的参数效率问题</strong>。T5使用Encoder-Decoder架构，在相同隐藏维度下参数量大约是Encoder-only或Decoder-only模型的两倍。例如，T5-Base有220M参数，而BERT-Base只有110M。虽然Encoder-Decoder在某些任务上略优，但参数效率的劣势使得T5在规模化时面临更大的成本压力。这也是为什么后来的LLM（如GPT-3、LLaMA）普遍选择了Decoder-only架构——在参数效率和扩展性上更有优势。</p>
<p><strong>对比学习的假负样本问题</strong>。如前面数值例子所示，无监督对比学习会将随机采样的同batch句子作为负样本，但这些”负样本”中可能包含语义上实际相似的句子。在大batch中这个问题有所缓解（因为随机碰到语义相近句子的概率降低），但无法完全消除。有监督版本通过NLI数据引入了人工标注的正负样本，显著缓解了这个问题，但又引入了对标注数据的依赖。</p>
</section>
<section id="开放研究问题" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">6.4</span> 开放研究问题</h3>
<p><strong>预训练目标与涌现能力的关系</strong>。随着模型规模的增长，大语言模型展现出了各种涌现能力（如CoT推理、代码生成）。这些涌现能力与预训练目标的选择之间有什么关系？为什么GPT-3的因果语言建模能涌现出Few-shot能力，而BERT的MLM似乎无法做到？是因为自回归生成本身就蕴含了某种推理能力，还是仅仅是因为规模化的Decoder-only模型恰好受益于因果语言建模的简单性？</p>
<p><strong>判别式预训练的规模化潜力</strong>。ELECTRA在中小规模上展现了显著的计算效率优势，但在超大规模（百亿参数以上）上还没有充分验证。判别式预训练是否能在规模化时保持其优势？或者在足够大的规模下，预训练目标的选择会变得不重要（被规模”稀释”）？</p>
<p><strong>超越”预测词”的预训练范式</strong>。当前的预训练目标——无论是MLM、PLM、RTD还是Span Corruption——本质上都在让模型学习”词与上下文的关系”。有没有可能设计完全不同的预训练目标，直接让模型学习更高层次的语言能力（如推理、规划、常识）？这是一个开放且深远的问题。</p>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">7</span> 局限性与未解决的问题</h2>
<section id="预训练目标改进的天花板" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="预训练目标改进的天花板"><span class="header-section-number">7.1</span> 预训练目标改进的”天花板”</h3>
<p>本章介绍的各种预训练目标改进，虽然在各自的维度上取得了进展，但一个不可回避的事实是：<strong>预训练目标的设计对最终性能的影响正在被模型规模所主导</strong>。</p>
<p>T5的系统性实验已经暗示了这一点：不同预训练目标之间的性能差距（1-2%），远小于模型从Base扩展到Large带来的提升（5-10%）。到2020年GPT-3（175B参数）的出现，这个趋势变得更加明显——GPT-3使用的是最简单的因果语言建模（与GPT-1完全相同的目标），但通过2600倍的参数量增长，在几乎所有任务上碾压了BERT、XLNet、ELECTRA等使用”更好”预训练目标的模型。</p>
<p>这引出了一个尖锐的问题：精巧的预训练目标设计是否只是在”小模型时代”有意义？当模型足够大时，最简单的因果语言建模是否就是最优解？这个问题至今没有确定的答案，但从实践来看，2020年之后的主流LLM几乎全部采用了因果语言建模——简单性和扩展性战胜了理论上的优雅性。</p>
</section>
<section id="训练策略-vs-训练目标" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="训练策略-vs-训练目标"><span class="header-section-number">7.2</span> 训练策略 vs 训练目标</h3>
<p>另一个被本章低估的维度是<strong>训练策略的重要性</strong>。RoBERTa（Liu et al., 2019）是一个教科书式的反例：它没有改变BERT的预训练目标（仍然是MLM），只是改进了训练策略——更多数据（160GB vs 16GB）、更大batch size（8K）、去掉NSP、动态遮蔽——就在大多数任务上超过了XLNet。</p>
<p>这暗示了一个被忽视的可能性：BERT的原始MLM也许已经足够好了，它的”不足”更多来自训练不充分，而非目标本身的缺陷。这种”训练更好”vs “设计更巧妙”的张力，是下一章的核心主题。</p>
</section>
<section id="这些局限导向了什么" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">7.3</span> 这些局限导向了什么？</h3>
<p>本章的方法论启示和未解决问题自然地导向了两个方向。</p>
<p>第一个方向是<strong>训练策略的系统优化</strong>——既然预训练目标的边际收益在递减，那么训练策略（数据量、batch size、学习率调度、训练时长）的优化是否能带来更大的收益？RoBERTa和ALBERT的实验给出了肯定的答案。这是第15章的主题。</p>
<p>第二个方向是<strong>架构选择的根本性反思</strong>——Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么根本的优劣？为什么T5选择了Encoder-Decoder，而GPT-3选择了Decoder-only？当规模足够大时，哪种架构的扩展性最好？这是第16章要深入讨论的问题。</p>
<blockquote class="blockquote">
<p>下一章预告：第15章将聚焦BERT系列模型的<strong>工程优化</strong>。RoBERTa如何通过”训练更好”就超越了精心设计的XLNet？ALBERT如何通过参数共享将BERT的参数量减少90%而性能不降？DistilBERT如何通过知识蒸馏将BERT压缩到原来的40%大小？这些工作共同揭示了一个重要的认识：<strong>模型的潜力往往被训练策略和工程优化所限制，而非架构或目标本身</strong>。</p>
</blockquote>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">8</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">8.1</span> 核心要点回顾</h3>
<p>这一章我们系统介绍了BERT之后预训练目标的四个演进方向，每一个都是对BERT某个具体局限的直接回应。</p>
<p>XLNet通过排列语言建模，在保持自回归框架的同时获得了双向上下文信息，彻底消除了<code>[MASK]</code>标记带来的预训练-微调不一致问题。其理论基础是概率论中联合分布的排列等价性，实现上通过双流自注意力来区分”提供上下文”和”做预测”两种角色。</p>
<p>ELECTRA将预训练任务从生成式（预测被遮蔽的词）转变为判别式（判断每个词是否被替换），将信号效率从15%提升到100%。在相同的计算预算下，ELECTRA一致性地超过了BERT、XLNet和RoBERTa，证明了判别式预训练的计算效率优势。</p>
<p>T5通过Text-to-Text框架和Span Corruption目标，将所有NLP任务统一为”文本到文本”格式，同时使用Encoder-Decoder架构兼顾了理解和生成能力。更重要的是，T5的系统性消融实验揭示了一个关键发现：模型规模和数据质量的影响远大于预训练目标的选择。</p>
<p>对比学习（以SimCSE为代表）跳出了”预测词”的范式，直接优化句子级表示的质量。通过巧妙地利用Dropout作为数据增强，SimCSE以极简的方式大幅提升了句子嵌入的性能。</p>
</section>
<section id="关键公式速查" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">8.2</span> 关键公式速查</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>公式</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_T} \left[ \sum_t \log P(x_{z_t} \mid \mathbf{x}_{\mathbf{z}_{\lt t}}) \right]\)</span></td>
<td>XLNet排列语言建模目标</td>
</tr>
<tr class="even">
<td><span class="math inline">\(L_{\text{Disc}} = -\sum_t [\mathbb{1}(x_t^c = x_t) \log D_t + \mathbb{1}(x_t^c \neq x_t) \log(1-D_t)]\)</span></td>
<td>ELECTRA判别器损失</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L = L_{\text{MLM}} + \lambda \cdot L_{\text{Disc}}\)</span></td>
<td>ELECTRA总损失（<span class="math inline">\(\lambda=50\)</span>）</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\ell_i = -\log \frac{e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_i') / \tau}}{\sum_j e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_j') / \tau}}\)</span></td>
<td>SimCSE对比损失</td>
</tr>
</tbody>
</table>
</section>
<section id="思考题" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">8.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> XLNet声称通过排列语言建模获得了双向上下文，但实际训练时只对每个排列预测最后若干个位置的token。如果预测所有位置的token，会出现什么问题？提示：考虑排在排列开头的位置能看到多少上下文。</p></li>
<li><p><strong>[数学推导]</strong> ELECTRA论文中<span class="math inline">\(\lambda = 50\)</span>，即判别器损失的权重是生成器的50倍。从梯度规模的角度分析这个选择：如果生成器和判别器的损失数值在同一量级，为什么需要大幅加权判别器？提示：考虑生成器损失（30000维softmax的交叉熵）和判别器损失（二分类的交叉熵）的梯度范数差异。</p></li>
<li><p><strong><a href="#工程实践">工程实践</a></strong> 使用Hugging Face的ELECTRA-Small模型，在SST-2数据集上微调并报告准确率。然后与同等大小的BERT-Small对比。验证ELECTRA在小模型规模上的效率优势是否成立。</p></li>
<li><p><strong>[对比分析]</strong> T5的系统性实验发现”预训练目标的选择没有模型规模重要”。如果这个结论成立，为什么我们还要研究预训练目标？请从以下角度讨论：(a) 计算效率（ELECTRA的例子）；(b) 特定任务的匹配度（对比学习在句子嵌入上的优势）；(c) 规模化之外的价值。</p></li>
<li><p><strong>[研究思考]</strong> 本章介绍的四种方法中，XLNet和T5都尝试解决BERT的生成能力缺失，但采用了完全不同的路线（XLNet保持Encoder-only但改变训练目标，T5采用Encoder-Decoder架构）。从2025年的视角回看，Decoder-only架构（GPT系列）最终成为主流。为什么一个看似”最简单”的方案反而赢了？XLNet和T5的教训是什么？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">9</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">9.1</span> 核心论文（必读）</h3>
<p><strong>Yang, Z. et al.&nbsp;(2019). “XLNet: Generalized Autoregressive Pretraining for Language Understanding”</strong>。排列语言建模的原始论文。重点阅读：Section 2.1（排列语言建模的动机和形式化）、Section 2.3（双流注意力——理解内容流和查询流的区别）。可快速浏览：Section 2.4（Transformer-XL的整合细节）。<a href="https://arxiv.org/abs/1906.08237">arXiv:1906.08237</a></p>
<p><strong>Clark, K. et al.&nbsp;(2020). “ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators”</strong>。替换词检测的原始论文。重点阅读：Section 3.1（方法描述——生成器-判别器的训练流程）、Table 1（效率对比——这是论文最有说服力的结果）。可快速浏览：Section 3.2-3.3（各种变体的消融）。<a href="https://arxiv.org/abs/2003.10555">arXiv:2003.10555</a></p>
<p><strong>Raffel, C. et al.&nbsp;(2020). “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”</strong>。T5的原始论文，也是预训练策略的百科全书。重点阅读：Section 3.3（预训练目标的系统对比）、Table 14（不同预训练目标的性能对比）。由于论文极长（67页），建议先读Table 1了解全貌，再选择感兴趣的实验深入。<a href="https://arxiv.org/abs/1910.10683">arXiv:1910.10683</a></p>
</section>
<section id="对比学习方向" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="对比学习方向"><span class="header-section-number">9.2</span> 对比学习方向</h3>
<p><strong>Gao, T., Yao, X., &amp; Chen, D. (2021). “SimCSE: Simple Contrastive Learning of Sentence Embeddings”</strong>。对比学习在句子嵌入上的突破。重点阅读：Section 3（无监督和有监督SimCSE的方法描述）、Section 5.2（Dropout作为数据增强的分析）。<a href="https://arxiv.org/abs/2104.08821">arXiv:2104.08821</a></p>
<p><strong>Li, B. et al.&nbsp;(2020). “On the Sentence Embeddings from Pre-trained Language Models”</strong>。揭示了BERT句子嵌入的各向异性问题，解释了为什么直接使用BERT的[CLS]向量作为句子嵌入效果不佳。<a href="https://arxiv.org/abs/2011.05864">arXiv:2011.05864</a></p>
</section>
<section id="前驱工作" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="前驱工作"><span class="header-section-number">9.3</span> 前驱工作</h3>
<p><strong>Dai, Z. et al.&nbsp;(2019). “Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context”</strong>。XLNet的前驱工作，提出了片段循环机制和相对位置编码，解决了Transformer在长文本建模上的限制。<a href="https://arxiv.org/abs/1901.02860">arXiv:1901.02860</a></p>
</section>
<section id="后续发展" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">9.4</span> 后续发展</h3>
<p><strong>He, P. et al.&nbsp;(2021). “DeBERTa: Decoding-enhanced BERT with Disentangled Attention”</strong>。在ELECTRA的判别式预训练基础上引入了解耦注意力（将内容和位置的注意力分开计算），在SuperGLUE上首次超过人类基准。<a href="https://arxiv.org/abs/2006.03654">arXiv:2006.03654</a></p>
<p><strong>Xue, L. et al.&nbsp;(2021). “mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer”</strong>。T5的多语言版本，在101种语言上进行预训练，展示了Text-to-Text框架在多语言场景中的通用性。<a href="https://arxiv.org/abs/2010.11934">arXiv:2010.11934</a></p>
</section>
<section id="代码资源" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">9.5</span> 代码资源</h3>
<ul>
<li><strong>Hugging Face ELECTRA</strong>: <a href="https://huggingface.co/google/electra-base-discriminator">huggingface.co/google/electra-base-discriminator</a> — 预训练ELECTRA模型</li>
<li><strong>Hugging Face T5</strong>: <a href="https://huggingface.co/t5-base">huggingface.co/t5-base</a> — 预训练T5模型</li>
<li><strong>SimCSE官方代码</strong>: <a href="https://github.com/princeton-nlp/SimCSE">github.com/princeton-nlp/SimCSE</a> — 无监督和有监督SimCSE的完整实现</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">10</span> 历史注脚</h2>
<p>XLNet、ELECTRA和T5分别代表了三个不同研究团队对BERT局限性的回应，它们的诞生几乎同时发生在2019年中到2020年初——这段时期可以称为预训练目标的”寒武纪爆发”。</p>
<p>XLNet由Carnegie Mellon大学的Zhilin Yang和William Cohen与Google Brain合作提出。有趣的是，XLNet在提交时引发了激烈的争论：它在20个基准任务上超越了BERT，但许多研究者指出XLNet使用了更多的数据（126GB vs 16GB）和更大的计算量，性能提升是否来自排列语言建模本身，还是仅仅来自更充分的训练？这个质疑后来被RoBERTa的实验所佐证——RoBERTa不改目标只改训练策略就追上了XLNet。</p>
<p>ELECTRA由Stanford的Kevin Clark和Google的Thang Luong（Attention变体那一章的主角）等人提出。ELECTRA最初并不被社区看好——“判别而非生成”的想法太过反直觉，很多人质疑一个二分类任务怎么可能学到丰富的语言表示。但ELECTRA-Small在单卡GPU上4天训练就超过GPT-Large的结果震惊了社区，证明了计算效率的重要性。对于很多资源有限的研究者和公司来说，ELECTRA成为了BERT的更实用的替代品。</p>
<p>T5由Google的Colin Raffel等人提出，其67页的论文几乎成为了预训练技术的”百科全书”——它系统地比较了几乎所有当时已知的预训练策略，包括架构、目标、数据、训练策略等。T5论文的学术贡献也许不在于提出了新的Span Corruption目标，而在于通过大规模实验为社区提供了可靠的实证指南。</p>
<p>从2020年的GPT-3开始，预训练目标的创新热潮逐渐平息。研究者们意识到，在足够大的规模下，最简单的因果语言建模可能就是最好的选择——不是因为它在理论上最优，而是因为它的简单性使其最容易规模化。这个认识标志着NLP从”精巧设计”时代向”暴力美学”时代的转变——一个至今仍在持续的范式变革。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第14章：预训练目标的演进"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Beyond MLM: Permutation, Discrimination, Span Corruption, and Contrastive Learning"</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-26"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, Pre-training, XLNet, ELECTRA, T5]</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [XLNet, ELECTRA, T5, 排列语言建模, 替换词检测, Span Corruption, 对比学习, SimCSE, 预训练目标]</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "BERT的MLM开创了双向预训练的先河，但它的15%信号效率、[MASK]标记的预训练-微调不一致、以及无法生成文本的局限性，催生了一系列创新的预训练目标：XLNet的排列语言建模、ELECTRA的替换词检测、T5的Span Corruption、以及对比学习在句子表示中的应用。"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-14/original/fig-t5-text-to-text.png"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：BERT的掩码语言模型（MLM）存在信号效率低、预训练-微调不一致等缺陷——有没有更好的预训练目标？</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2019-2020年 </span><span class="pp">|</span><span class="at"> XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2020), T5 (Raffel et al., 2020), SimCSE (Gao et al., 2021) </span><span class="pp">|</span><span class="at"> 预训练目标的多元探索</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Yang et al. (2019)** "XLNet: Generalized Autoregressive Pretraining for Language Understanding" (arXiv:1906.08237) — 参考了 Section 2（排列语言建模）、Section 2.3（双流注意力）、Table 1-6（实验结果）</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Clark et al. (2020)** "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" (arXiv:2003.10555) — 参考了 Section 3（方法设计）、Figure 1（架构图）、Table 1-2（效率对比）</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Raffel et al. (2020)** "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (arXiv:1910.10683) — 参考了 Section 3（系统性实验）、Table 3-14（预训练目标对比）</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gao et al. (2021)** "SimCSE: Simple Contrastive Learning of Sentence Embeddings" (arXiv:2104.08821) — 参考了 Section 3（对比学习框架）、Table 1-3（STS实验）</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**D2L** Section 15.8-15.10 — 参考了预训练模型对比的教学框架</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SLP3** Chapter 11 — 参考了预训练目标的分类和讲解角度</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS224N** Lecture 9-10 (2025) "Pretraining" — 参考了预训练目标演进的讲解思路</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CMU 11-711** ANLP (Neubig) — 参考了预训练目标的对比分析框架</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>上一章我们详细介绍了BERT——NLP历史上第一个真正实现深层双向预训练的模型。通过一个巧妙的改变——将"预测下一个词"替换为"预测被遮蔽的词"——BERT解除了双向性与语言建模之间的矛盾，让Transformer Encoder在预训练时每个位置都能同时看到左右两侧的上下文。在参数量几乎相同的条件下（BERT-Base 110M vs GPT-1 117M），BERT在GLUE平均分上超出GPT近7个百分点（79.6 vs 72.8），消融实验直接证明双向性是性能提升的主要来源。</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>然而，上一章结尾我们也系统梳理了BERT的三个核心局限。</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>第一个是**信号效率低下**。BERT的MLM每次随机遮蔽15%的token，只有这些被遮蔽的位置参与损失计算。其余85%的token虽然参与了前向传播（提供上下文），但不提供直接的训练信号。与GPT的因果语言建模相比——每个位置都预测下一个词，100%的token都贡献训练信号——BERT的信号效率大约只有GPT的$1/7$。</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>第二个是**预训练-微调不一致**。尽管80-10-10策略做了缓解，MLM的本质问题无法回避：预训练时模型看到的输入中充斥着<span class="in">`[MASK]`</span>标记，但微调时输入是干净的自然文本。模型需要在两种不同的输入分布上工作，这可能导致预训练学到的某些模式在微调时无法完全发挥作用。</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>第三个是**不适合文本生成**。BERT的Encoder-only架构天然不适合自回归生成——训练时每个位置看到了所有其他位置（包括"未来"），但生成时"未来"还不存在。这种训练与推理的不一致使得BERT无法胜任翻译、摘要、对话等生成任务。</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>2019到2020年间，研究者们从不同角度回应了这些局限，提出了多种创新的预训练目标。每一种方案都是对BERT某个具体痛点的直接回应。</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：BERT之后的预训练目标创新沿着四条路线展开——XLNet用**排列语言建模**在保持自回归的同时获得双向上下文，消除了`[MASK]`标记；ELECTRA用**替换词检测**将信号效率从15%提升到100%；T5用**Span Corruption**和Encoder-Decoder架构统一了理解与生成；对比学习则跳出了"预测词"的范式，直接优化句子级表示。</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练目标设计的核心矛盾</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>在深入各个方案之前，我们先退一步思考：设计一个好的预训练目标，核心的矛盾是什么？</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>回忆BERT面临的根本困境：如果让模型看到双向上下文，它就可能"偷看"到答案，使得预测变得trivial；如果限制模型只看单向上下文，它就无法利用完整的语境信息。BERT用<span class="in">`[MASK]`</span>标记来解决这个矛盾——把要预测的词遮住，这样即使模型看到了双向上下文，也无法作弊。但代价是引入了预训练-微调不一致和信号效率低下。</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>更一般地说，预训练目标的设计需要在多个维度上做权衡。</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a><span class="fu">### 四个维度的权衡</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>**第一个维度是上下文的方向性**。预训练目标允许模型看到多少上下文？GPT只看左侧（单向），BERT看两侧但用<span class="in">`[MASK]`</span>遮住目标（双向但有代价），XLNet通过排列看两侧且不用<span class="in">`[MASK]`</span>（双向且无代价）。直觉上，看到的上下文越完整，模型对语言的理解就越深。但更多的上下文也意味着预测任务更容易，如何在"信息充分"和"任务有挑战性"之间取得平衡，是每个预训练目标都要面对的问题。</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>**第二个维度是信号效率**。每个训练样本中，有多大比例的token贡献了训练信号？BERT是15%，GPT是100%，ELECTRA是100%。信号效率直接影响训练速度和数据利用率。在同样的计算预算下，信号效率更高的方法可以"学到更多"。</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>**第三个维度是预训练-微调一致性**。预训练时模型看到的数据分布与微调时的数据分布有多接近？BERT的<span class="in">`[MASK]`</span>标记、T5的<span class="in">`&lt;extra_id&gt;`</span>哨兵标记都引入了人工token，造成了分布不匹配。理想情况下，预训练的输入格式应该尽量接近下游任务的自然文本。</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>**第四个维度是任务通用性**。预训练目标让模型学到的表示能覆盖多广的下游任务？MLM训练的是token级别的预测能力，对词级别的任务（NER、完形填空）天然适配，但对句子级别的任务（文本分类、句子相似度）可能不够直接。对比学习则专注于学习句子级表示，但可能在token级任务上不够精细。</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="fu">### 一个不可能三角</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>如果我们把前三个维度画成一个三角——"完全双向 + 高信号效率 + 预训练-微调一致"——会发现没有一个方法能同时满足所有三个条件。BERT牺牲了信号效率和一致性来获得双向性。GPT牺牲了双向性来获得高效率和一致性。XLNet在理论上接近了三角的中心，但实现复杂度大幅增加。ELECTRA牺牲了部分一致性（引入替换而非自然token）来获得双向性和高效率。</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>理解了这个不可能三角，我们就能更清晰地理解每个方案的设计动机——它们不是在追求完美，而是在这个约束空间中寻找不同的最优权衡点。</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>在进入数学之前，让我们先用直觉理解四种方案的核心洞察。每一种都从不同的角度切入同一个问题。</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a><span class="fu">### XLNet：打乱顺序来获得双向性</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>XLNet的核心洞察可以用一个类比来理解。想象你在做完形填空题"我去___存钱"。BERT的做法是把"银行"遮住变成"我去<span class="in">`[MASK]`</span>存钱"，然后让你根据两侧上下文猜测。XLNet的做法则完全不同——它不遮住任何词，而是改变你"阅读"句子的顺序。</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>假设句子是"我 去 银行 存钱"（4个词），XLNet可能让你按照"存钱→我→银行→去"的顺序来处理。当模型轮到处理"银行"时，它已经看到了"存钱"和"我"，但还没有看到"去"。在另一种排列"去→存钱→银行→我"中，处理"银行"时已经看到了"去"和"存钱"。对所有可能的排列取期望，"银行"在不同排列中能看到不同的上下文子集——有时看到左侧，有时看到右侧，有时两侧都看到。这样，模型就在不使用<span class="in">`[MASK]`</span>的情况下获得了双向上下文信息。</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>关键的优雅之处在于：虽然概念上我们在处理各种排列，但实际上输入序列的顺序并不改变。模型通过一种特殊的注意力掩码来模拟不同的排列——我们只是改变了"谁能看到谁"的规则，而不是真的把词打乱。</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### ELECTRA：做判别题而非填空题</span></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>ELECTRA的洞察更加直接。BERT让模型做"填空题"——遮住一个词，预测它是什么。但填空题有一个问题：每张试卷中只有15%的空需要填，其余85%的位置虽然读了但不打分。这就像一个老师只批改试卷的15%，其他部分直接跳过——明显浪费了评估学生理解程度的机会。</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>ELECTRA把"填空题"换成了"找错题"。首先，一个小型的"出题者"（生成器）会把句子中的某些词替换成看起来合理但可能不正确的词。然后，主模型（判别器）需要判断句子中的每一个词是原始的还是被替换过的。这就像考试改成了阅读理解中的"纠错题"——每个词都需要判断真伪，100%的位置都参与评分。</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>举个例子，原始句子"the chef cooked the meal"经过生成器可能变成"the chef ate the meal"（"cooked"被替换为"ate"）。判别器需要对每个位置做出判断——"the"是原始的，"chef"是原始的，"ate"是被替换的（应该是"cooked"），"the"是原始的，"meal"是原始的。五个位置中只有一个被替换了，但所有五个位置都提供了训练信号。信号效率是BERT的$100\%/15\% \approx 6.7$倍。</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### T5：一切皆文本</span></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>T5的核心洞察是一个关于任务统一的思考。在BERT时代，不同的下游任务需要不同的输出头——分类任务用<span class="in">`[CLS]`</span>向量加一个线性层，问答任务预测答案的起始和结束位置，NER任务在每个token上做标注。每种任务格式都需要专门的设计，这增加了工程复杂度，也阻碍了跨任务的知识共享。</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>T5的解决方案是彻底的简化：把所有NLP任务都统一为"文本到文本"（Text-to-Text）格式。翻译是文本到文本。摘要是文本到文本。分类也是文本到文本——输入"classify: I love this movie"，输出"positive"。问答、NER、文本蕴涵……一切皆可用文本到文本来表达。</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>与此相应，T5的预训练目标也采用了类似的思路——**Span Corruption**。模型随机遮蔽输入中的连续span，用哨兵标记（sentinel token）替代，然后生成被遮蔽的内容。与BERT的逐token遮蔽不同，Span Corruption遮蔽的是连续的片段，迫使模型进行更高层次的推理。而且，由于T5使用Encoder-Decoder架构，它天然支持生成任务——这是BERT做不到的。</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>::: {#fig-t5-text-to-text}</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/chapter-14/original/fig-t5-text-to-text.png)</span>{width=90%}</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>T5 的 Text-to-Text 统一框架：所有 NLP 任务——翻译、分类、语义相似度、摘要——都被转化为相同的"文本输入→文本输出"格式，使用同一个模型、同一个损失函数、同一套超参数进行训练。*Source: Raffel et al. (2020) "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", Figure 1*</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对比学习：不预测词，学习表示</span></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>前面三种方案虽然各有创新，但本质上都是在"预测某种形式的词"——无论是预测被遮蔽的词（BERT/T5）、预测被排列到后面的词（XLNet）、还是判断词是否被替换（ELECTRA）。对比学习则跳出了这个范式，直接在句子级别优化表示的质量。</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>对比学习的直觉很简单：相似的句子应该有相似的表示，不相似的句子应该有不同的表示。SimCSE的巧妙之处在于它发现，同一个句子通过Transformer两次（由于dropout的随机性，两次前向传播会产生略微不同的表示），这两个表示就是天然的"正样本对"——它们来自同一个句子，语义完全相同，但在表示空间中有微小的差异。其他句子的表示则作为"负样本"。通过拉近正样本、推远负样本，模型学会了更均匀、更具判别力的句子表示。</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>这种方法的独特价值在于：它不需要任何标注数据，不需要特殊的预训练目标设计，就能大幅提升句子嵌入的质量——在语义文本相似度（STS）任务上，SimCSE将BERT的表现提升了高达16个百分点。</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### XLNet：排列语言建模</span></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 排列语言建模的形式化</span></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>标准的自回归语言建模将联合概率分解为从左到右的条件概率之积：</span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, \ldots, x_{t-1})</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>这种分解有一个隐含假设：因式分解的顺序是固定的（从左到右）。但概率论告诉我们，联合概率可以按照任意顺序分解——对于$T$个随机变量，有$T!$种合法的因式分解顺序，每一种都给出相同的联合概率。</span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a>XLNet正是利用了这一点。设$\mathcal{Z}_T$为序列$<span class="co">[</span><span class="ot">1, 2, \ldots, T</span><span class="co">]</span>$的所有排列的集合，对于一个排列$\mathbf{z} = <span class="co">[</span><span class="ot">z_1, z_2, \ldots, z_T</span><span class="co">]</span>$，XLNet的训练目标是最大化所有排列的期望对数似然：</span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a>\max_\theta \quad \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_T} \left[ \sum_{t=1}^{T} \log P_\theta(x_{z_t} \mid \mathbf{x}_{\mathbf{z}_{&lt;t}}) \right]</span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a>其中$\mathbf{x}_{\mathbf{z}_{\lt t}}$表示在排列$\mathbf{z}$中排在$z_t$之前的所有token。由于对所有排列取期望，每个token$x_{z_t}$在不同的排列中会以不同的子集作为上下文来预测——有时只有左侧token，有时只有右侧token，有时两侧都有。这样，模型就在保持自回归形式的同时，学到了双向上下文的信息。</span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>一个重要的细节是：排列的是因式分解的顺序，而不是输入序列本身。输入序列始终保持原始的自然顺序"the cat sat on the mat"，改变的只是模型在预测每个位置时被允许看到哪些其他位置。这通过注意力掩码来实现——对于排列$\mathbf{z}$和位置$z_t$，注意力掩码允许$z_t$关注$<span class="sc">\{</span>z_1, z_2, \ldots, z_{t-1}<span class="sc">\}</span>$中的所有位置。</span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 双流自注意力</span></span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a>但排列语言建模有一个微妙的问题。在标准的Transformer中，位置$z_t$的表示$h_{z_t}$同时包含两种信息：（1）该位置的内容信息（这个位置的token是什么），和（2）该位置的位置信息（这是序列中的第几个位置）。当我们用$h_{z_t}$去预测$x_{z_t}$时，如果$h_{z_t}$已经编码了$x_{z_t}$的内容信息，预测就变成了trivial的自环——模型直接从自己的表示中读取答案。</span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>但如果我们不在$h_{z_t}$中编码$x_{z_t}$的内容，那么当$z_t$排在其他位置之前（即$z_t$需要为后续位置提供上下文）时，后续位置就无法获得$x_{z_t}$的内容信息。</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a>XLNet的解决方案是引入**双流自注意力（Two-Stream Self-Attention）**：</span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a>**内容流（Content Stream）**$h_{z_t}$：与标准的Transformer相同，编码了位置$z_t$的内容信息和位置信息。当$z_t$作为上下文为其他位置服务时，使用内容流。</span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>**查询流（Query Stream）**$g_{z_t}$：只编码位置信息和之前上下文的内容信息，不包含$z_t$自身的内容。当需要预测$z_t$位置的token时，使用查询流。</span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>用数学表达：</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>g_{z_t}^{(m)} &amp;\leftarrow \text{Attention}(Q = g_{z_t}^{(m-1)}, \; KV = h_{\mathbf{z}_{&lt;t}}^{(m-1)}) <span class="sc">\\</span></span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a>h_{z_t}^{(m)} &amp;\leftarrow \text{Attention}(Q = h_{z_t}^{(m-1)}, \; KV = h_{\mathbf{z}_{\leq t}}^{(m-1)})</span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a>注意两个关键区别：查询流的KV中不包含$z_t$自身（$\mathbf{z}_{\lt t}$），而内容流的KV包含$z_t$自身（$\mathbf{z}_{\leq t}$）。初始化时，查询流使用一个可学习的向量$g_{z_t}^{(0)} = w$（只含位置信息），内容流使用词嵌入$h_{z_t}^{(0)} = e(x_{z_t})$（含内容信息）。</span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a>::: {#fig-xlnet-two-stream}</span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/chapter-14/original/fig-xlnet-two-stream.png)</span>{width=85%}</span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a>XLNet 的双流自注意力机制：(a) 内容流（Content Stream）——与标准 Transformer 相同，编码位置和内容信息，可以看到自身；(b) 查询流（Query Stream）——只编码位置信息，不能看到自身内容，用于预测当前位置的 token；(c) 排列语言建模的整体示意，展示了不同排列下的注意力掩码。*Source: Yang et al. (2019) "XLNet: Generalized Autoregressive Pretraining for Language Understanding", Figure 1*</span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：排列语言建模</span></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a>为了建立直觉，让我们用一个小例子走通完整的过程。</span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a>**设定**：句子"I love NLP"，3个token，位置为$<span class="co">[</span><span class="ot">1, 2, 3</span><span class="co">]</span>$。</span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>**考虑排列 $\mathbf{z} = [3, 1, 2]$**（处理顺序：先处理位置3的"NLP"，再处理位置1的"I"，最后处理位置2的"love"）。</span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a>在这个排列下，注意力掩码的规则是：</span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>处理位置3时（$z_1 = 3$）：之前没有其他位置，只能看到自己</span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>处理位置1时（$z_2 = 1$）：之前有位置3，可以看到"NLP"和自己</span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>处理位置2时（$z_3 = 2$）：之前有位置3和1，可以看到"NLP"、"I"和自己</span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a>注意力掩码矩阵（1表示可以看到，0表示不可以）：</span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a>M_{\text{content}} = \begin{bmatrix} 1 &amp; 0 &amp; 1 <span class="sc">\\</span> 0 &amp; 1 &amp; 0 <span class="sc">\\</span> 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad</span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a>M_{\text{query}} = \begin{bmatrix} 0 &amp; 0 &amp; 1 <span class="sc">\\</span> 0 &amp; 0 &amp; 0 <span class="sc">\\</span> 1 &amp; 0 &amp; 0 \end{bmatrix}</span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a>内容流掩码中，位置1（行1）可以看到位置1和位置3（因为在排列中，位置3在位置1之前），但看不到位置2。查询流掩码中，位置1不能看到自己（去掉自环），只能看到位置3。</span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a>**考虑另一个排列 $\mathbf{z}' = [2, 3, 1]$**：</span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>处理"love"时：只看到自己</span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>处理"NLP"时：看到"love"和自己</span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>处理"I"时：看到"love"、"NLP"和自己</span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a>现在预测"I"时，模型同时看到了"love"和"NLP"——既有左侧上下文也有右侧上下文。而在排列$\mathbf{z}$中，预测"love"时看到了"NLP"（右侧）和"I"（左侧）。通过在不同排列中采样，每个token最终都能获得来自各个方向的上下文信息。</span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a>**在实际训练中**，我们不需要遍历所有$T!$种排列。XLNet对每个训练样本随机采样一种排列，并且只预测排列中最后若干个位置的token（因为排在前面的位置能看到的上下文太少，预测信号质量较低）。具体地，对于一个排列$\mathbf{z}$，只预测$z_{c+1}, z_{c+2}, \ldots, z_T$（其中$c$是一个截断点），使得每个被预测的token至少能看到$c$个上下文token。</span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a><span class="fu">#### XLNet的额外优势：Transformer-XL的继承</span></span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a>XLNet的名字中包含"XL"，因为它还继承了Transformer-XL（Dai et al., 2019）的两个关键技术。</span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a>**片段循环机制（Segment Recurrence）**：在处理长文本时，将文本分成固定长度的片段。处理当前片段时，保留上一个片段的隐藏状态作为额外的记忆，使模型的有效上下文长度超过单个片段的长度。</span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a>**相对位置编码**：与BERT使用的绝对位置编码不同，XLNet使用相对位置编码，编码的是两个token之间的相对距离而非各自的绝对位置。这使得模型能更好地泛化到训练时未见过的序列长度。</span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a>这两项技术使XLNet不仅在预训练目标上超越了BERT，在处理长文档方面也具有优势。</span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### ELECTRA：替换词检测</span></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 从生成到判别的范式转变</span></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a>BERT的MLM本质上是一个**生成式**预训练任务——给定上下文，生成（预测）被遮蔽的词。ELECTRA提出了一个根本性的范式转变：用**判别式**任务替代生成式任务。</span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a>ELECTRA的架构包含两个网络：一个小型的**生成器**（Generator）和一个大型的**判别器**（Discriminator）。</span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a>::: {#fig-electra-architecture}</span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/chapter-14/original/fig-electra-architecture.png)</span>{width=85%}</span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a>ELECTRA 的生成器-判别器架构：生成器（Generator）是一个小型 MLM 模型，对被遮蔽的位置预测替代词（如将 "cooked" 替换为 "ate"）；判别器（Discriminator）接收生成器"修补"后的句子，对每个位置判断是"Original"还是"Replaced"。训练完成后丢弃生成器，只保留判别器用于下游任务。*Source: Clark et al. (2020) "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", Figure 1*</span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a>**生成器**是一个小型的MLM模型。它接收被遮蔽的输入，对每个被遮蔽的位置预测一个替代词。生成器的目标与BERT的MLM完全相同：</span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a>L_{\text{MLM}}(\theta_G) = -\sum_{i \in \mathcal{M}} \log P_G(x_i \mid \tilde{\mathbf{x}})</span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a>**判别器**是主模型。它接收生成器"修补"过的句子（被遮蔽的位置已被生成器的预测词替换），然后对每一个位置判断该token是"原始的"（original）还是"被替换的"（replaced）。这是一个二分类任务：</span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a>L_{\text{Disc}}(\theta_D) = -\sum_{t=1}^{T} \left<span class="co">[</span><span class="ot"> \mathbb{1}(x_t^{\text{corrupt}} = x_t) \log D(x_t^{\text{corrupt}}, t) + \mathbb{1}(x_t^{\text{corrupt}} \neq x_t) \log (1 - D(x_t^{\text{corrupt}}, t)) \right</span><span class="co">]</span></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a>其中$D(x_t^{\text{corrupt}}, t)$是判别器预测位置$t$的token是"原始"的概率。</span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a>总损失函数是生成器和判别器损失的加权和：</span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a>L = L_{\text{MLM}}(\theta_G) + \lambda \cdot L_{\text{Disc}}(\theta_D)</span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a>论文中$\lambda = 50$，这意味着判别器的训练信号被大幅加权——这合理地反映了判别器才是我们最终关心的模型。</span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a><span class="fu">## 算法：替换词检测训练流程（Clark et al., 2020）</span></span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a>**输入**：未标注语料 $\mathcal{X}$，生成器 $G$（小型 MLM），判别器 $D$（大型 Encoder）</span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a>**For** each batch $\mathbf{x} \in \mathcal{X}$:</span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>随机选择遮蔽位置集合 $\mathcal{M} \subset <span class="sc">\{</span>1, \ldots, T<span class="sc">\}</span>$（比例 15%）</span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>构建遮蔽输入：$\tilde{\mathbf{x}} = \text{REPLACE}(\mathbf{x}, \mathcal{M}, \texttt{<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>})$</span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**生成器前向**：对每个 $i \in \mathcal{M}$，从 $P_G(\cdot \mid \tilde{\mathbf{x}})$ 中采样替代词 $\hat{x}_i$</span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>构建判别器输入：$\mathbf{x}^{\text{corrupt}} = \text{REPLACE}(\mathbf{x}, \mathcal{M}, \hat{\mathbf{x}})$</span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**判别器前向**：对每个位置 $t = 1, \ldots, T$，预测 $D(\mathbf{x}^{\text{corrupt}}, t) = P(\text{original} \mid t)$</span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>计算联合损失并更新参数：$L = L_{\text{MLM}}(\theta_G) + 50 \cdot L_{\text{Disc}}(\theta_D)$</span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a>**输出**：训练好的判别器 $D$（丢弃生成器 $G$）</span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a>*Source: Clark et al. (2020) "ELECTRA", Section 3.1*</span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 为什么不是GAN？</span></span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a>ELECTRA的生成器-判别器架构看起来很像GAN（生成对抗网络），但有一个根本区别：**生成器和判别器不是对抗训练的**。</span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a>在GAN中，生成器的目标是"欺骗"判别器——生成器试图产生让判别器无法分辨的样本。这种对抗训练会导致训练不稳定和模式崩溃等问题。</span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a>在ELECTRA中，生成器的训练目标是最大化MLM的对数似然，与判别器无关。生成器只是一个工具，用于产生"看起来合理但可能不正确"的替换词，让判别器有东西可以判断。两个网络独立地优化各自的目标函数，联合训练只是为了效率——让生成器的输出随着训练进展变得越来越逼真，从而给判别器提供越来越困难的训练样本。</span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a>这个设计选择的原因是实际的：替换词是离散的token（不是连续向量），梯度无法从判别器反向传播到生成器。要做对抗训练需要强化学习等技巧，会大幅增加复杂度。ELECTRA的作者发现，简单的联合MLM训练就已经足够好了。</span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：ELECTRA的训练流程</span></span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a>让我们用一个具体例子走通ELECTRA的完整训练过程。</span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a>**输入句子**："the chef cooked the meal"</span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a>**Step 1: 随机遮蔽**。按照BERT的方式，随机遮蔽15%的token。假设"cooked"和"meal"被选中遮蔽：</span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a>\text{masked input: } \text{the chef <span class="co">[</span><span class="ot">MASK</span><span class="co">]</span> the <span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>}</span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a>**Step 2: 生成器预测**。小型生成器（如BERT-Small）对每个<span class="in">`[MASK]`</span>位置生成替代词：</span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 位置 <span class="pp">|</span> 原始词 <span class="pp">|</span> 生成器Top-5预测 <span class="pp">|</span> 采样结果 <span class="pp">|</span></span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|--------|----------------|----------|</span></span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 3 <span class="pp">|</span> cooked <span class="pp">|</span> ate(0.25), prepared(0.20), cooked(0.18), made(0.15), served(0.12) <span class="pp">|</span> **ate** <span class="pp">|</span></span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 5 <span class="pp">|</span> meal <span class="pp">|</span> meal(0.30), food(0.22), dinner(0.18), dish(0.15), lunch(0.10) <span class="pp">|</span> **meal** <span class="pp">|</span></span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a>注意：生成器可能恰好预测出原始词（如位置5的"meal"），也可能预测出不同的词（如位置3的"ate"替代了"cooked"）。</span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a>**Step 3: 构建判别器输入**。用生成器的预测替换<span class="in">`[MASK]`</span>：</span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a>\text{discriminator input: } \text{the chef ate the meal}</span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a>**Step 4: 判别器逐位判断**。判别器对每个位置输出"original"或"replaced"的概率：</span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 位置 <span class="pp">|</span> Token <span class="pp">|</span> 真实标签 <span class="pp">|</span> 判别器预测 $D(x_t, t)$ <span class="pp">|</span></span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|-------|----------|----------------------|</span></span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1 <span class="pp">|</span> the <span class="pp">|</span> original <span class="pp">|</span> 0.95（正确） <span class="pp">|</span></span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2 <span class="pp">|</span> chef <span class="pp">|</span> original <span class="pp">|</span> 0.92（正确） <span class="pp">|</span></span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 3 <span class="pp">|</span> ate <span class="pp">|</span> **replaced** <span class="pp">|</span> 0.30（正确，认为它被替换了）<span class="pp">|</span></span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 4 <span class="pp">|</span> the <span class="pp">|</span> original <span class="pp">|</span> 0.88（正确） <span class="pp">|</span></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 5 <span class="pp">|</span> meal <span class="pp">|</span> original <span class="pp">|</span> 0.85（正确，因为生成器恰好预测了原词）<span class="pp">|</span></span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a>**关键观察**：所有5个位置都贡献了训练信号。即使位置1、2、4从未被遮蔽过，判别器也需要确认它们是原始的——这迫使模型对每个token建立深入的语义理解。位置5虽然经过了遮蔽-替换流程，但生成器恰好预测了原词，所以标签是"original"。</span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 效率分析：小即是美</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a>ELECTRA的一个反直觉发现是：生成器应该是小的。</span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a>论文比较了不同大小的生成器对判别器性能的影响。当生成器与判别器大小相同时，性能反而不是最优的——因为太强的生成器生成的替换词太过逼真，判别器难以区分，训练信号变得微弱。最佳的生成器大小大约是判别器的$1/4$到$1/3$。这让生成器足够强大，能生成语法上合理的替换词（而非随机词），但又不至于太强，保持判别任务的适当难度。</span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a>在计算效率方面，ELECTRA的优势极为显著。ELECTRA-Small使用一块GPU训练4天，就能在GLUE基准上达到与GPT-Large（使用30倍计算资源训练）相当的性能。ELECTRA-Base在相同的预训练计算预算下超过了BERT-Large和XLNet-Large。</span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a><span class="fu">### T5：Text-to-Text统一框架</span></span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Span Corruption预训练目标</span></span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a>T5的预训练目标叫做**Span Corruption**——随机选择输入中的若干连续span，将每个span替换为一个唯一的哨兵标记（sentinel token），然后让解码器生成被替换的内容。</span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a>与BERT的MLM相比，Span Corruption在四个方面有所不同。首先，BERT遮蔽的是随机选择的单个token，而T5遮蔽的是连续的span——这迫使模型学习更高层次的短语级补全能力。其次，BERT用统一的<span class="in">`[MASK]`</span>标记替换所有被遮蔽的位置，而T5为每个被遮蔽的span分配一个唯一的哨兵标记（<span class="in">`&lt;extra_id_0&gt;`</span>、<span class="in">`&lt;extra_id_1&gt;`</span>等），使得解码器能明确知道自己在"修复"哪个span。第三，BERT在编码器端直接预测被遮蔽token的概率分布，而T5在解码器端以自回归的方式生成被替换span的完整文本。最后，T5的目标序列只包含被遮蔽的部分（加上哨兵标记），比原始序列短得多，提高了训练效率。</span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：Span Corruption</span></span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a>**原始句子**："Thank you for inviting me to your party last week"</span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a>**Step 1: 随机选择span**。遮蔽率15%（约1-2个token的span）。假设选中"for inviting"和"last week"：</span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a>**Step 2: 替换为哨兵标记**：</span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a>\text{Input: } \text{Thank you } \texttt{&lt;extra<span class="sc">\_</span>id<span class="sc">\_</span>0&gt;} \text{ me to your party } \texttt{&lt;extra<span class="sc">\_</span>id<span class="sc">\_</span>1&gt;}</span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a>**Step 3: 构建目标序列**：</span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a>\text{Target: } \texttt{&lt;extra<span class="sc">\_</span>id<span class="sc">\_</span>0&gt;} \text{ for inviting } \texttt{&lt;extra<span class="sc">\_</span>id<span class="sc">\_</span>1&gt;} \text{ last week } \texttt{&lt;extra<span class="sc">\_</span>id<span class="sc">\_</span>2&gt;}</span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-368"><a href="#cb5-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-369"><a href="#cb5-369" aria-hidden="true" tabindex="-1"></a>目标序列的格式是：每个哨兵标记后面跟着它替代的原始span，最后一个哨兵标记<span class="in">`&lt;extra_id_2&gt;`</span>标志着结束。</span>
<span id="cb5-370"><a href="#cb5-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-371"><a href="#cb5-371" aria-hidden="true" tabindex="-1"></a>**这个设计的精妙之处**在于目标序列比原始序列短得多。假设原始序列有512个token，遮蔽15%后目标序列大约只有77个token（被遮蔽的token加上哨兵标记）。这意味着解码器的计算量大大减少，训练效率更高。</span>
<span id="cb5-372"><a href="#cb5-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-373"><a href="#cb5-373" aria-hidden="true" tabindex="-1"></a><span class="fu">#### T5的系统性实验：不只是一个模型</span></span>
<span id="cb5-374"><a href="#cb5-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-375"><a href="#cb5-375" aria-hidden="true" tabindex="-1"></a>T5论文的最大贡献也许不是Span Corruption本身，而是它对预训练策略进行的前所未有的系统性消融实验。论文标题"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"——"探索极限"是关键词。</span>
<span id="cb5-376"><a href="#cb5-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-377"><a href="#cb5-377" aria-hidden="true" tabindex="-1"></a>**预训练目标的对比实验**。T5在相同的计算预算下比较了多种预训练目标：</span>
<span id="cb5-378"><a href="#cb5-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-379"><a href="#cb5-379" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 预训练目标 <span class="pp">|</span> GLUE平均分 <span class="pp">|</span> 特点 <span class="pp">|</span></span>
<span id="cb5-380"><a href="#cb5-380" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-----------|------|</span></span>
<span id="cb5-381"><a href="#cb5-381" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 前缀语言模型（Prefix LM） <span class="pp">|</span> 82.5 <span class="pp">|</span> 编码器部分双向，解码器部分因果 <span class="pp">|</span></span>
<span id="cb5-382"><a href="#cb5-382" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 因果语言建模（Causal LM） <span class="pp">|</span> 80.9 <span class="pp">|</span> GPT风格，完全因果 <span class="pp">|</span></span>
<span id="cb5-383"><a href="#cb5-383" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> BERT风格MLM（去噪） <span class="pp">|</span> 84.0 <span class="pp">|</span> 随机遮蔽+预测 <span class="pp">|</span></span>
<span id="cb5-384"><a href="#cb5-384" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Span Corruption** | **84.6** <span class="pp">|</span> 遮蔽连续span+生成 <span class="pp">|</span></span>
<span id="cb5-385"><a href="#cb5-385" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Span Corruption（不同遮蔽率） <span class="pp">|</span> 变化不大 <span class="pp">|</span> 10%-25%差异不大 <span class="pp">|</span></span>
<span id="cb5-386"><a href="#cb5-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-387"><a href="#cb5-387" aria-hidden="true" tabindex="-1"></a>结论是Span Corruption略优于BERT风格的MLM，因果语言建模（GPT风格）在同等计算量下效果最差。但差距并不巨大——预训练目标的选择没有模型规模和数据量的影响大。</span>
<span id="cb5-388"><a href="#cb5-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-389"><a href="#cb5-389" aria-hidden="true" tabindex="-1"></a>**架构的对比实验**。T5还比较了三种架构：</span>
<span id="cb5-390"><a href="#cb5-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-391"><a href="#cb5-391" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 架构 <span class="pp">|</span> 优势 <span class="pp">|</span> 劣势 <span class="pp">|</span></span>
<span id="cb5-392"><a href="#cb5-392" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|------|</span></span>
<span id="cb5-393"><a href="#cb5-393" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Encoder-Decoder <span class="pp">|</span> 编码器双向，解码器可生成 <span class="pp">|</span> 参数量翻倍（相对于纯编码器/解码器） <span class="pp">|</span></span>
<span id="cb5-394"><a href="#cb5-394" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Decoder-only（因果） <span class="pp">|</span> 简单，适合生成 <span class="pp">|</span> 理解任务不够好 <span class="pp">|</span></span>
<span id="cb5-395"><a href="#cb5-395" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Decoder-only（前缀） <span class="pp">|</span> 前缀部分双向 <span class="pp">|</span> 略差于Encoder-Decoder <span class="pp">|</span></span>
<span id="cb5-396"><a href="#cb5-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-397"><a href="#cb5-397" aria-hidden="true" tabindex="-1"></a>在控制总参数量相同的条件下，Encoder-Decoder架构略优于其他两种——因为在同等参数量下，Encoder-Decoder实际上有两倍的层数（编码器N层 + 解码器N层），虽然单次前向传播中只有部分层是"激活"的。</span>
<span id="cb5-398"><a href="#cb5-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-399"><a href="#cb5-399" aria-hidden="true" tabindex="-1"></a>::: {#fig-t5-attention-masks}</span>
<span id="cb5-400"><a href="#cb5-400" aria-hidden="true" tabindex="-1"></a><span class="al">![](figures/chapter-14/original/fig-t5-attention-masks.png)</span>{width=80%}</span>
<span id="cb5-401"><a href="#cb5-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-402"><a href="#cb5-402" aria-hidden="true" tabindex="-1"></a>三种架构对应的注意力掩码模式：**Fully-visible**（全可见）对应 Encoder，每个位置可以看到所有其他位置；**Causal**（因果）对应 Decoder-only，每个位置只能看到自己和之前的位置；**Causal with prefix**（带前缀的因果）对应前缀语言模型，前缀部分全可见，其余部分因果。*Source: Raffel et al. (2020) "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", Figure 3*</span>
<span id="cb5-403"><a href="#cb5-403" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-404"><a href="#cb5-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-405"><a href="#cb5-405" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对比学习在NLP中的应用</span></span>
<span id="cb5-406"><a href="#cb5-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-407"><a href="#cb5-407" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SimCSE：用Dropout作为数据增强</span></span>
<span id="cb5-408"><a href="#cb5-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-409"><a href="#cb5-409" aria-hidden="true" tabindex="-1"></a>对比学习的核心框架可以概括为三步：（1）为每个样本生成正样本对，（2）将其他样本作为负样本，（3）用对比损失拉近正样本、推远负样本。在计算机视觉中，正样本对通常通过数据增强生成（如随机裁剪、颜色变换同一张图片）。但NLP中数据增强非常困难——改一个词可能改变整个句子的语义。</span>
<span id="cb5-410"><a href="#cb5-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-411"><a href="#cb5-411" aria-hidden="true" tabindex="-1"></a>SimCSE（Gao et al., 2021）的突破在于发现了一种极其简单的数据增强方式：**Dropout噪声**。</span>
<span id="cb5-412"><a href="#cb5-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-413"><a href="#cb5-413" aria-hidden="true" tabindex="-1"></a>将同一个句子通过预训练的Transformer两次，由于Dropout层的随机性，两次前向传播会产生略微不同的表示$\mathbf{h}_i$和$\mathbf{h}_i'$。这两个表示构成正样本对——语义完全相同，但在表示空间中存在微小差异（因为Dropout随机丢弃了不同的神经元）。</span>
<span id="cb5-414"><a href="#cb5-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-415"><a href="#cb5-415" aria-hidden="true" tabindex="-1"></a>对于一个batch中的$N$个句子，对比损失为：</span>
<span id="cb5-416"><a href="#cb5-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-417"><a href="#cb5-417" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-418"><a href="#cb5-418" aria-hidden="true" tabindex="-1"></a>\ell_i = -\log \frac{e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_i') / \tau}}{\sum_{j=1}^{N} e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_j') / \tau}}</span>
<span id="cb5-419"><a href="#cb5-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-420"><a href="#cb5-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-421"><a href="#cb5-421" aria-hidden="true" tabindex="-1"></a>其中$\text{sim}(\cdot, \cdot)$是余弦相似度，$\tau$是温度超参数。直觉上，分子希望$\mathbf{h}_i$和$\mathbf{h}_i'$（同一句子的两个表示）尽量接近，分母希望$\mathbf{h}_i$和其他句子的表示尽量远离。</span>
<span id="cb5-422"><a href="#cb5-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-423"><a href="#cb5-423" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 数值示例：对比学习的核心计算</span></span>
<span id="cb5-424"><a href="#cb5-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-425"><a href="#cb5-425" aria-hidden="true" tabindex="-1"></a>**设定**：batch中有3个句子：$s_1$="A man is sleeping"，$s_2$="A cat sits on a mat"，$s_3$="A person is resting"。温度$\tau = 0.05$。</span>
<span id="cb5-426"><a href="#cb5-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-427"><a href="#cb5-427" aria-hidden="true" tabindex="-1"></a>**Step 1: 两次前向传播**。每个句子通过BERT两次（不同Dropout mask），得到<span class="in">`[CLS]`</span>向量：</span>
<span id="cb5-428"><a href="#cb5-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-429"><a href="#cb5-429" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 句子 <span class="pp">|</span> 第一次表示 $\mathbf{h}$ <span class="pp">|</span> 第二次表示 $\mathbf{h}'$ <span class="pp">|</span></span>
<span id="cb5-430"><a href="#cb5-430" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------------------------|-------------------------|</span></span>
<span id="cb5-431"><a href="#cb5-431" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $s_1$ "A man is sleeping" <span class="pp">|</span> $<span class="co">[</span><span class="ot">0.2, 0.8, -0.1</span><span class="co">]</span>$ <span class="pp">|</span> $<span class="co">[</span><span class="ot">0.3, 0.7, -0.2</span><span class="co">]</span>$ <span class="pp">|</span></span>
<span id="cb5-432"><a href="#cb5-432" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $s_2$ "A cat sits on a mat" <span class="pp">|</span> $<span class="co">[</span><span class="ot">-0.5, 0.3, 0.6</span><span class="co">]</span>$ <span class="pp">|</span> $<span class="co">[</span><span class="ot">-0.4, 0.2, 0.7</span><span class="co">]</span>$ <span class="pp">|</span></span>
<span id="cb5-433"><a href="#cb5-433" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $s_3$ "A person is resting" <span class="pp">|</span> $<span class="co">[</span><span class="ot">0.1, 0.9, 0.0</span><span class="co">]</span>$ <span class="pp">|</span> $<span class="co">[</span><span class="ot">0.2, 0.8, -0.1</span><span class="co">]</span>$ <span class="pp">|</span></span>
<span id="cb5-434"><a href="#cb5-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-435"><a href="#cb5-435" aria-hidden="true" tabindex="-1"></a>**Step 2: 计算余弦相似度矩阵**（$\mathbf{h}_i$与$\mathbf{h}_j'$之间）：</span>
<span id="cb5-436"><a href="#cb5-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-437"><a href="#cb5-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-438"><a href="#cb5-438" aria-hidden="true" tabindex="-1"></a>\text{sim}(\mathbf{h}_1, \mathbf{h}_1') = \frac{0.2 \times 0.3 + 0.8 \times 0.7 + (-0.1) \times (-0.2)}{\sqrt{0.04+0.64+0.01} \times \sqrt{0.09+0.49+0.04}} \approx 0.987</span>
<span id="cb5-439"><a href="#cb5-439" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-440"><a href="#cb5-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-441"><a href="#cb5-441" aria-hidden="true" tabindex="-1"></a>类似地计算其他相似度：</span>
<span id="cb5-442"><a href="#cb5-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-443"><a href="#cb5-443" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="pp">|</span> $\mathbf{h}_1'$ <span class="pp">|</span> $\mathbf{h}_2'$ <span class="pp">|</span> $\mathbf{h}_3'$ <span class="pp">|</span></span>
<span id="cb5-444"><a href="#cb5-444" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|---|</span></span>
<span id="cb5-445"><a href="#cb5-445" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathbf{h}_1$ <span class="pp">|</span> 0.987（正样本）<span class="pp">|</span> -0.280 <span class="pp">|</span> 0.994 <span class="pp">|</span></span>
<span id="cb5-446"><a href="#cb5-446" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathbf{h}_2$ <span class="pp">|</span> -0.185 <span class="pp">|</span> 0.991（正样本）<span class="pp">|</span> -0.249 <span class="pp">|</span></span>
<span id="cb5-447"><a href="#cb5-447" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathbf{h}_3$ <span class="pp">|</span> 0.976 <span class="pp">|</span> -0.325 <span class="pp">|</span> 0.998（正样本）<span class="pp">|</span></span>
<span id="cb5-448"><a href="#cb5-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-449"><a href="#cb5-449" aria-hidden="true" tabindex="-1"></a>**Step 3: 计算对比损失**（以$s_1$为例）：</span>
<span id="cb5-450"><a href="#cb5-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-451"><a href="#cb5-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-452"><a href="#cb5-452" aria-hidden="true" tabindex="-1"></a>\ell_1 = -\log \frac{e^{0.987/0.05}}{e^{0.987/0.05} + e^{-0.280/0.05} + e^{0.994/0.05}}</span>
<span id="cb5-453"><a href="#cb5-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-454"><a href="#cb5-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-455"><a href="#cb5-455" aria-hidden="true" tabindex="-1"></a>注意$s_3$"A person is resting"与$s_1$"A man is sleeping"语义非常接近（$\text{sim} = 0.994 &gt; 0.987$），这个"假负样本"会让模型错误地推远语义相似的句子。这是无监督对比学习的一个已知问题。</span>
<span id="cb5-456"><a href="#cb5-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-457"><a href="#cb5-457" aria-hidden="true" tabindex="-1"></a>SimCSE还提出了**有监督版本**（Supervised SimCSE）：利用NLI（自然语言推理）数据集，将蕴涵（entailment）句子对作为正样本，矛盾（contradiction）句子对作为硬负样本。例如：</span>
<span id="cb5-458"><a href="#cb5-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-459"><a href="#cb5-459" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>正样本对：("A man is sleeping", "A person is resting") — 蕴涵关系</span>
<span id="cb5-460"><a href="#cb5-460" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>硬负样本：("A man is sleeping", "A man is awake") — 矛盾关系</span>
<span id="cb5-461"><a href="#cb5-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-462"><a href="#cb5-462" aria-hidden="true" tabindex="-1"></a>有监督版本的性能显著优于无监督版本，在STS-B基准上达到86.2%（vs 无监督的76.3%）。</span>
<span id="cb5-463"><a href="#cb5-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-464"><a href="#cb5-464" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 对比学习解决了什么问题？</span></span>
<span id="cb5-465"><a href="#cb5-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-466"><a href="#cb5-466" aria-hidden="true" tabindex="-1"></a>为什么BERT直接提取的句子嵌入质量不高？一个重要原因是**表示塌缩（Representation Collapse）**——BERT的<span class="in">`[CLS]`</span>向量占据了表示空间中一个非常小的区域，不同句子的表示挤在一起，难以区分。</span>
<span id="cb5-467"><a href="#cb5-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-468"><a href="#cb5-468" aria-hidden="true" tabindex="-1"></a>Li et al. (2020) 的研究发现，BERT的词嵌入存在**各向异性（anisotropy）**问题——嵌入向量集中在一个窄锥体中，而不是均匀分布在整个空间。这意味着任意两个句子的余弦相似度都很高（通常在0.6-0.8之间），难以区分语义上真正相似和不相似的句子。</span>
<span id="cb5-469"><a href="#cb5-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-470"><a href="#cb5-470" aria-hidden="true" tabindex="-1"></a>对比学习通过显式地优化"相似句子接近、不同句子远离"，有效地"撑开"了表示空间，使其更加各向同性（isotropic），句子嵌入的质量大幅提升。</span>
<span id="cb5-471"><a href="#cb5-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-472"><a href="#cb5-472" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法对比总结</span></span>
<span id="cb5-473"><a href="#cb5-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-474"><a href="#cb5-474" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 维度 <span class="pp">|</span> BERT MLM <span class="pp">|</span> XLNet PLM <span class="pp">|</span> ELECTRA RTD <span class="pp">|</span> T5 Span Corruption <span class="pp">|</span> SimCSE <span class="pp">|</span></span>
<span id="cb5-475"><a href="#cb5-475" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|----------|-----------|-------------|---------------------|--------|</span></span>
<span id="cb5-476"><a href="#cb5-476" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **预训练类型** <span class="pp">|</span> 去噪 <span class="pp">|</span> 自回归 <span class="pp">|</span> 判别式 <span class="pp">|</span> 去噪（seq2seq） <span class="pp">|</span> 对比式 <span class="pp">|</span></span>
<span id="cb5-477"><a href="#cb5-477" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **上下文方向** <span class="pp">|</span> 双向（有<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>） <span class="pp">|</span> 双向（无<span class="co">[</span><span class="ot">MASK</span><span class="co">]</span>） <span class="pp">|</span> 双向（有替换词） <span class="pp">|</span> 编码器双向 <span class="pp">|</span> 双向 <span class="pp">|</span></span>
<span id="cb5-478"><a href="#cb5-478" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **信号效率** | ~15% | ~15%（部分预测）| **100%** <span class="pp">|</span> ~15% <span class="pp">|</span> 句子级 <span class="pp">|</span></span>
<span id="cb5-479"><a href="#cb5-479" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **架构** <span class="pp">|</span> Encoder-only <span class="pp">|</span> Encoder-only <span class="pp">|</span> Encoder-only <span class="pp">|</span> Encoder-Decoder <span class="pp">|</span> Encoder-only <span class="pp">|</span></span>
<span id="cb5-480"><a href="#cb5-480" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **生成能力** | ✗ | ✗ | ✗ | **✓** <span class="pp">|</span> ✗ <span class="pp">|</span></span>
<span id="cb5-481"><a href="#cb5-481" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **实现复杂度** | 低 | **高** | 中 | 中 | **低** <span class="pp">|</span></span>
<span id="cb5-482"><a href="#cb5-482" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **计算效率** | 基准 | 1.5-2× | **最优** <span class="pp">|</span> 1-1.5× <span class="pp">|</span> 低（仅微调）<span class="pp">|</span></span>
<span id="cb5-483"><a href="#cb5-483" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **典型应用** <span class="pp">|</span> 理解任务 <span class="pp">|</span> 理解任务 <span class="pp">|</span> 理解任务 <span class="pp">|</span> 理解+生成 <span class="pp">|</span> 句子嵌入 <span class="pp">|</span></span>
<span id="cb5-484"><a href="#cb5-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-485"><a href="#cb5-485" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-486"><a href="#cb5-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-487"><a href="#cb5-487" aria-hidden="true" tabindex="-1"></a><span class="fu">## 工程实践</span></span>
<span id="cb5-488"><a href="#cb5-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-489"><a href="#cb5-489" aria-hidden="true" tabindex="-1"></a><span class="fu">### ELECTRA的简化实现</span></span>
<span id="cb5-490"><a href="#cb5-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-491"><a href="#cb5-491" aria-hidden="true" tabindex="-1"></a>由于ELECTRA在计算效率上的巨大优势，它是最具实际工程价值的预训练目标改进。以下是判别器核心逻辑的简化实现：</span>
<span id="cb5-492"><a href="#cb5-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-493"><a href="#cb5-493" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-494"><a href="#cb5-494" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-495"><a href="#cb5-495" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-496"><a href="#cb5-496" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertForMaskedLM</span>
<span id="cb5-497"><a href="#cb5-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-498"><a href="#cb5-498" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ELECTRA(nn.Module):</span>
<span id="cb5-499"><a href="#cb5-499" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""简化版ELECTRA：生成器-判别器联合训练"""</span></span>
<span id="cb5-500"><a href="#cb5-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-501"><a href="#cb5-501" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, gen_hidden<span class="op">=</span><span class="dv">256</span>, disc_hidden<span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb5-502"><a href="#cb5-502" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-503"><a href="#cb5-503" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 生成器：小型MLM模型</span></span>
<span id="cb5-504"><a href="#cb5-504" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> BertForMaskedLM.from_pretrained(</span>
<span id="cb5-505"><a href="#cb5-505" aria-hidden="true" tabindex="-1"></a>            <span class="st">'bert-base-uncased'</span>  <span class="co"># 实际中应用更小的模型</span></span>
<span id="cb5-506"><a href="#cb5-506" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-507"><a href="#cb5-507" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 判别器：大型编码器 + 二分类头</span></span>
<span id="cb5-508"><a href="#cb5-508" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.discriminator <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb5-509"><a href="#cb5-509" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.disc_head <span class="op">=</span> nn.Linear(disc_hidden, <span class="dv">1</span>)  <span class="co"># 二分类：original vs replaced</span></span>
<span id="cb5-510"><a href="#cb5-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-511"><a href="#cb5-511" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, mask_positions):</span>
<span id="cb5-512"><a href="#cb5-512" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-513"><a href="#cb5-513" aria-hidden="true" tabindex="-1"></a><span class="co">        input_ids: [batch, seq_len] 原始token序列</span></span>
<span id="cb5-514"><a href="#cb5-514" aria-hidden="true" tabindex="-1"></a><span class="co">        mask_positions: [batch, seq_len] 被遮蔽位置的bool掩码</span></span>
<span id="cb5-515"><a href="#cb5-515" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-516"><a href="#cb5-516" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: 构建生成器输入（遮蔽指定位置）</span></span>
<span id="cb5-517"><a href="#cb5-517" aria-hidden="true" tabindex="-1"></a>        masked_input <span class="op">=</span> input_ids.clone()</span>
<span id="cb5-518"><a href="#cb5-518" aria-hidden="true" tabindex="-1"></a>        masked_input[mask_positions] <span class="op">=</span> <span class="dv">103</span>  <span class="co"># [MASK] token id</span></span>
<span id="cb5-519"><a href="#cb5-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-520"><a href="#cb5-520" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: 生成器预测被遮蔽位置的token</span></span>
<span id="cb5-521"><a href="#cb5-521" aria-hidden="true" tabindex="-1"></a>        gen_output <span class="op">=</span> <span class="va">self</span>.generator(masked_input)</span>
<span id="cb5-522"><a href="#cb5-522" aria-hidden="true" tabindex="-1"></a>        gen_logits <span class="op">=</span> gen_output.logits  <span class="co"># [batch, seq_len, vocab_size]</span></span>
<span id="cb5-523"><a href="#cb5-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-524"><a href="#cb5-524" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 从生成器的预测分布中采样替代词</span></span>
<span id="cb5-525"><a href="#cb5-525" aria-hidden="true" tabindex="-1"></a>        gen_probs <span class="op">=</span> torch.softmax(gen_logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-526"><a href="#cb5-526" aria-hidden="true" tabindex="-1"></a>        sampled_tokens <span class="op">=</span> torch.multinomial(</span>
<span id="cb5-527"><a href="#cb5-527" aria-hidden="true" tabindex="-1"></a>            gen_probs[mask_positions], num_samples<span class="op">=</span><span class="dv">1</span></span>
<span id="cb5-528"><a href="#cb5-528" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-529"><a href="#cb5-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-530"><a href="#cb5-530" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: 构建判别器输入（用采样的token替换[MASK]）</span></span>
<span id="cb5-531"><a href="#cb5-531" aria-hidden="true" tabindex="-1"></a>        disc_input <span class="op">=</span> input_ids.clone()</span>
<span id="cb5-532"><a href="#cb5-532" aria-hidden="true" tabindex="-1"></a>        disc_input[mask_positions] <span class="op">=</span> sampled_tokens</span>
<span id="cb5-533"><a href="#cb5-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-534"><a href="#cb5-534" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: 判别器逐位判断：original or replaced?</span></span>
<span id="cb5-535"><a href="#cb5-535" aria-hidden="true" tabindex="-1"></a>        disc_output <span class="op">=</span> <span class="va">self</span>.discriminator(disc_input)</span>
<span id="cb5-536"><a href="#cb5-536" aria-hidden="true" tabindex="-1"></a>        disc_logits <span class="op">=</span> <span class="va">self</span>.disc_head(disc_output.last_hidden_state).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-537"><a href="#cb5-537" aria-hidden="true" tabindex="-1"></a>        <span class="co"># disc_logits: [batch, seq_len]，每个位置的"original"概率</span></span>
<span id="cb5-538"><a href="#cb5-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-539"><a href="#cb5-539" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: 构建标签（生成器预测与原词不同的位置标记为"replaced"）</span></span>
<span id="cb5-540"><a href="#cb5-540" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> (disc_input <span class="op">!=</span> input_ids).<span class="bu">float</span>()</span>
<span id="cb5-541"><a href="#cb5-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-542"><a href="#cb5-542" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> gen_logits, disc_logits, labels, mask_positions</span>
<span id="cb5-543"><a href="#cb5-543" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-544"><a href="#cb5-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-545"><a href="#cb5-545" aria-hidden="true" tabindex="-1"></a><span class="fu">### T5的Text-to-Text任务格式化</span></span>
<span id="cb5-546"><a href="#cb5-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-547"><a href="#cb5-547" aria-hidden="true" tabindex="-1"></a>T5将所有任务统一为文本到文本格式。以下是几个典型任务的输入输出格式：</span>
<span id="cb5-548"><a href="#cb5-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-549"><a href="#cb5-549" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-550"><a href="#cb5-550" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_t5_task(task_type, <span class="op">**</span>kwargs):</span>
<span id="cb5-551"><a href="#cb5-551" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""将不同NLP任务格式化为T5的text-to-text格式"""</span></span>
<span id="cb5-552"><a href="#cb5-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-553"><a href="#cb5-553" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> task_type <span class="op">==</span> <span class="st">"classification"</span>:</span>
<span id="cb5-554"><a href="#cb5-554" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "classify: I love this movie"</span></span>
<span id="cb5-555"><a href="#cb5-555" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "positive"</span></span>
<span id="cb5-556"><a href="#cb5-556" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"classify: </span><span class="sc">{</span>kwargs[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'label'</span>]</span>
<span id="cb5-557"><a href="#cb5-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-558"><a href="#cb5-558" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"translation"</span>:</span>
<span id="cb5-559"><a href="#cb5-559" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "translate English to German: That is good."</span></span>
<span id="cb5-560"><a href="#cb5-560" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "Das ist gut."</span></span>
<span id="cb5-561"><a href="#cb5-561" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="ss">f"translate English to </span><span class="sc">{</span>kwargs[<span class="st">'target_lang'</span>]<span class="sc">}</span><span class="ss">: "</span></span>
<span id="cb5-562"><a href="#cb5-562" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>kwargs[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'translation'</span>])</span>
<span id="cb5-563"><a href="#cb5-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-564"><a href="#cb5-564" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"summarization"</span>:</span>
<span id="cb5-565"><a href="#cb5-565" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "summarize: [长文本]"</span></span>
<span id="cb5-566"><a href="#cb5-566" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "[摘要]"</span></span>
<span id="cb5-567"><a href="#cb5-567" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"summarize: </span><span class="sc">{</span>kwargs[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'summary'</span>]</span>
<span id="cb5-568"><a href="#cb5-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-569"><a href="#cb5-569" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"nli"</span>:</span>
<span id="cb5-570"><a href="#cb5-570" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "mnli premise: [前提] hypothesis: [假设]"</span></span>
<span id="cb5-571"><a href="#cb5-571" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "entailment" / "neutral" / "contradiction"</span></span>
<span id="cb5-572"><a href="#cb5-572" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="ss">f"mnli premise: </span><span class="sc">{</span>kwargs[<span class="st">'premise'</span>]<span class="sc">}</span><span class="ss"> "</span></span>
<span id="cb5-573"><a href="#cb5-573" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"hypothesis: </span><span class="sc">{</span>kwargs[<span class="st">'hypothesis'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'label'</span>])</span>
<span id="cb5-574"><a href="#cb5-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-575"><a href="#cb5-575" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> task_type <span class="op">==</span> <span class="st">"qa"</span>:</span>
<span id="cb5-576"><a href="#cb5-576" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输入: "question: [问题] context: [上下文]"</span></span>
<span id="cb5-577"><a href="#cb5-577" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出: "[答案]"</span></span>
<span id="cb5-578"><a href="#cb5-578" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="ss">f"question: </span><span class="sc">{</span>kwargs[<span class="st">'question'</span>]<span class="sc">}</span><span class="ss"> "</span></span>
<span id="cb5-579"><a href="#cb5-579" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"context: </span><span class="sc">{</span>kwargs[<span class="st">'context'</span>]<span class="sc">}</span><span class="ss">"</span>, kwargs[<span class="st">'answer'</span>])</span>
<span id="cb5-580"><a href="#cb5-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-581"><a href="#cb5-581" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用示例</span></span>
<span id="cb5-582"><a href="#cb5-582" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb5-583"><a href="#cb5-583" aria-hidden="true" tabindex="-1"></a>    format_t5_task(<span class="st">"classification"</span>,</span>
<span id="cb5-584"><a href="#cb5-584" aria-hidden="true" tabindex="-1"></a>                   text<span class="op">=</span><span class="st">"I love this movie"</span>, label<span class="op">=</span><span class="st">"positive"</span>),</span>
<span id="cb5-585"><a href="#cb5-585" aria-hidden="true" tabindex="-1"></a>    format_t5_task(<span class="st">"translation"</span>,</span>
<span id="cb5-586"><a href="#cb5-586" aria-hidden="true" tabindex="-1"></a>                   text<span class="op">=</span><span class="st">"That is good."</span>, target_lang<span class="op">=</span><span class="st">"German"</span>,</span>
<span id="cb5-587"><a href="#cb5-587" aria-hidden="true" tabindex="-1"></a>                   translation<span class="op">=</span><span class="st">"Das ist gut."</span>),</span>
<span id="cb5-588"><a href="#cb5-588" aria-hidden="true" tabindex="-1"></a>    format_t5_task(<span class="st">"nli"</span>,</span>
<span id="cb5-589"><a href="#cb5-589" aria-hidden="true" tabindex="-1"></a>                   premise<span class="op">=</span><span class="st">"A man is sleeping."</span>,</span>
<span id="cb5-590"><a href="#cb5-590" aria-hidden="true" tabindex="-1"></a>                   hypothesis<span class="op">=</span><span class="st">"A person is resting."</span>,</span>
<span id="cb5-591"><a href="#cb5-591" aria-hidden="true" tabindex="-1"></a>                   label<span class="op">=</span><span class="st">"entailment"</span>),</span>
<span id="cb5-592"><a href="#cb5-592" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-593"><a href="#cb5-593" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-594"><a href="#cb5-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-595"><a href="#cb5-595" aria-hidden="true" tabindex="-1"></a><span class="fu">### SimCSE的训练核心</span></span>
<span id="cb5-596"><a href="#cb5-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-597"><a href="#cb5-597" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-598"><a href="#cb5-598" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-599"><a href="#cb5-599" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-600"><a href="#cb5-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-601"><a href="#cb5-601" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simcse_loss(model, sentences, temperature<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb5-602"><a href="#cb5-602" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-603"><a href="#cb5-603" aria-hidden="true" tabindex="-1"></a><span class="co">    无监督SimCSE的核心训练逻辑</span></span>
<span id="cb5-604"><a href="#cb5-604" aria-hidden="true" tabindex="-1"></a><span class="co">    同一个句子过两次模型（不同dropout mask），构成正样本对</span></span>
<span id="cb5-605"><a href="#cb5-605" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-606"><a href="#cb5-606" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="bu">len</span>(sentences)</span>
<span id="cb5-607"><a href="#cb5-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-608"><a href="#cb5-608" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 两次前向传播，不同的dropout mask</span></span>
<span id="cb5-609"><a href="#cb5-609" aria-hidden="true" tabindex="-1"></a>    embeddings_1 <span class="op">=</span> model(sentences)  <span class="co"># [batch, hidden_dim]</span></span>
<span id="cb5-610"><a href="#cb5-610" aria-hidden="true" tabindex="-1"></a>    embeddings_2 <span class="op">=</span> model(sentences)  <span class="co"># [batch, hidden_dim]，dropout不同</span></span>
<span id="cb5-611"><a href="#cb5-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-612"><a href="#cb5-612" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 拼接：[2*batch, hidden_dim]</span></span>
<span id="cb5-613"><a href="#cb5-613" aria-hidden="true" tabindex="-1"></a>    embeddings <span class="op">=</span> torch.cat([embeddings_1, embeddings_2], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-614"><a href="#cb5-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-615"><a href="#cb5-615" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算余弦相似度矩阵：[2*batch, 2*batch]</span></span>
<span id="cb5-616"><a href="#cb5-616" aria-hidden="true" tabindex="-1"></a>    sim_matrix <span class="op">=</span> F.cosine_similarity(</span>
<span id="cb5-617"><a href="#cb5-617" aria-hidden="true" tabindex="-1"></a>        embeddings.unsqueeze(<span class="dv">1</span>), embeddings.unsqueeze(<span class="dv">0</span>), dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb5-618"><a href="#cb5-618" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">/</span> temperature</span>
<span id="cb5-619"><a href="#cb5-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-620"><a href="#cb5-620" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 构建标签：正样本是偏移batch_size的对角线</span></span>
<span id="cb5-621"><a href="#cb5-621" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 对于embeddings_1[i]，正样本是embeddings_2[i]</span></span>
<span id="cb5-622"><a href="#cb5-622" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(batch_size, device<span class="op">=</span>sim_matrix.device)</span>
<span id="cb5-623"><a href="#cb5-623" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.cat([labels <span class="op">+</span> batch_size, labels], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-624"><a href="#cb5-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-625"><a href="#cb5-625" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 去掉自身相似度（对角线设为极小值）</span></span>
<span id="cb5-626"><a href="#cb5-626" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.eye(<span class="dv">2</span> <span class="op">*</span> batch_size, dtype<span class="op">=</span>torch.<span class="bu">bool</span>, device<span class="op">=</span>sim_matrix.device)</span>
<span id="cb5-627"><a href="#cb5-627" aria-hidden="true" tabindex="-1"></a>    sim_matrix.masked_fill_(mask, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb5-628"><a href="#cb5-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-629"><a href="#cb5-629" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 交叉熵损失</span></span>
<span id="cb5-630"><a href="#cb5-630" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(sim_matrix, labels)</span>
<span id="cb5-631"><a href="#cb5-631" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb5-632"><a href="#cb5-632" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-633"><a href="#cb5-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-634"><a href="#cb5-634" aria-hidden="true" tabindex="-1"></a><span class="fu">### 使用Hugging Face快速体验</span></span>
<span id="cb5-635"><a href="#cb5-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-636"><a href="#cb5-636" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-637"><a href="#cb5-637" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb5-638"><a href="#cb5-638" aria-hidden="true" tabindex="-1"></a>    ElectraForPreTraining, ElectraTokenizer,</span>
<span id="cb5-639"><a href="#cb5-639" aria-hidden="true" tabindex="-1"></a>    T5ForConditionalGeneration, T5Tokenizer</span>
<span id="cb5-640"><a href="#cb5-640" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-641"><a href="#cb5-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-642"><a href="#cb5-642" aria-hidden="true" tabindex="-1"></a><span class="co"># === ELECTRA: 判断哪些词被替换了 ===</span></span>
<span id="cb5-643"><a href="#cb5-643" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> ElectraTokenizer.from_pretrained(<span class="st">"google/electra-base-discriminator"</span>)</span>
<span id="cb5-644"><a href="#cb5-644" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ElectraForPreTraining.from_pretrained(<span class="st">"google/electra-base-discriminator"</span>)</span>
<span id="cb5-645"><a href="#cb5-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-646"><a href="#cb5-646" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">"The chef ate the meal"</span>  <span class="co"># "ate" 可能是替换词</span></span>
<span id="cb5-647"><a href="#cb5-647" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(sentence, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb5-648"><a href="#cb5-648" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb5-649"><a href="#cb5-649" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs.logits: 每个位置的"fake"概率分数</span></span>
<span id="cb5-650"><a href="#cb5-650" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> torch.sigmoid(outputs.logits)</span>
<span id="cb5-651"><a href="#cb5-651" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions ≈ [0.02, 0.03, 0.85, 0.01, 0.02]</span></span>
<span id="cb5-652"><a href="#cb5-652" aria-hidden="true" tabindex="-1"></a><span class="co"># 位置2（"ate"）被高概率判断为"replaced"</span></span>
<span id="cb5-653"><a href="#cb5-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-654"><a href="#cb5-654" aria-hidden="true" tabindex="-1"></a><span class="co"># === T5: Text-to-Text 任务 ===</span></span>
<span id="cb5-655"><a href="#cb5-655" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained(<span class="st">"t5-base"</span>)</span>
<span id="cb5-656"><a href="#cb5-656" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained(<span class="st">"t5-base"</span>)</span>
<span id="cb5-657"><a href="#cb5-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-658"><a href="#cb5-658" aria-hidden="true" tabindex="-1"></a><span class="co"># 翻译</span></span>
<span id="cb5-659"><a href="#cb5-659" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"translate English to German: That is good."</span></span>
<span id="cb5-660"><a href="#cb5-660" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb5-661"><a href="#cb5-661" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids)</span>
<span id="cb5-662"><a href="#cb5-662" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb5-663"><a href="#cb5-663" aria-hidden="true" tabindex="-1"></a><span class="co"># → "Das ist gut."</span></span>
<span id="cb5-664"><a href="#cb5-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-665"><a href="#cb5-665" aria-hidden="true" tabindex="-1"></a><span class="co"># 情感分类</span></span>
<span id="cb5-666"><a href="#cb5-666" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"sst2 sentence: I love this movie"</span></span>
<span id="cb5-667"><a href="#cb5-667" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb5-668"><a href="#cb5-668" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids)</span>
<span id="cb5-669"><a href="#cb5-669" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb5-670"><a href="#cb5-670" aria-hidden="true" tabindex="-1"></a><span class="co"># → "positive"</span></span>
<span id="cb5-671"><a href="#cb5-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-672"><a href="#cb5-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-673"><a href="#cb5-673" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-674"><a href="#cb5-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-675"><a href="#cb5-675" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb5-676"><a href="#cb5-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-677"><a href="#cb5-677" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **研究者必读**：这一节探讨各预训练目标的理论基础、实证发现、边界条件和开放问题</span></span>
<span id="cb5-678"><a href="#cb5-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-679"><a href="#cb5-679" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb5-680"><a href="#cb5-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-681"><a href="#cb5-681" aria-hidden="true" tabindex="-1"></a>**XLNet与排列的等价性**。XLNet的排列语言建模在理论上有一个优雅的性质：对所有排列取期望后，模型等价于在学习真正的联合概率分布$P(x_1, x_2, \ldots, x_T)$。与BERT的MLM不同，MLM学习的是条件概率$P(x_i \mid \mathbf{x}_{\backslash i})$（给定其他所有词预测第$i$个词），这些条件概率之间不一定是一致的——它们可能无法对应一个合法的联合概率分布。这在理论上被称为MLM的"伪似然"（pseudo-likelihood）问题。XLNet通过保持严格的自回归分解，回避了这个问题。</span>
<span id="cb5-682"><a href="#cb5-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-683"><a href="#cb5-683" aria-hidden="true" tabindex="-1"></a>**ELECTRA的判别式优势**。ELECTRA的核心理论优势可以从学习理论的角度理解。生成式任务（预测被遮蔽的词）要求模型学习整个词汇表上的条件概率分布——这是一个非常高维的输出空间（$V \approx 30000$）。判别式任务（判断token是否被替换）只需要做二分类——输出空间维度为1。在相同的模型容量和训练数据下，低维输出空间的任务通常更容易学习，收敛更快。</span>
<span id="cb5-684"><a href="#cb5-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-685"><a href="#cb5-685" aria-hidden="true" tabindex="-1"></a>从信息论的角度，ELECTRA判别器在每个位置接收的信号是1 bit（original vs replaced），但它需要对整个句子进行深层理解才能做出正确判断。这就像考试中的判断题——虽然答案只有对错两种，但正确判断需要全面的理解。</span>
<span id="cb5-686"><a href="#cb5-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-687"><a href="#cb5-687" aria-hidden="true" tabindex="-1"></a>**T5与多任务学习**。T5的Text-to-Text框架可以理解为一种隐式的多任务学习。当模型在预训练阶段学会了处理Span Corruption（一种去噪任务），它实际上学到了一种通用的"损坏→修复"能力。微调时，不同的任务前缀（"translate:"、"summarize:"、"classify:"）告诉模型应该以哪种"修复模式"来处理输入。这与人类的元认知能力类似——知道当前在做什么类型的任务，并相应调整处理策略。</span>
<span id="cb5-688"><a href="#cb5-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-689"><a href="#cb5-689" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——实证视角</span></span>
<span id="cb5-690"><a href="#cb5-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-691"><a href="#cb5-691" aria-hidden="true" tabindex="-1"></a>**关键的消融实验发现**。T5论文进行了NLP领域最大规模的预训练消融研究，其中几个发现特别值得注意。</span>
<span id="cb5-692"><a href="#cb5-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-693"><a href="#cb5-693" aria-hidden="true" tabindex="-1"></a>第一，预训练目标的选择对最终性能的影响**没有想象中那么大**。Span Corruption、BERT风格MLM、去噪自编码器之间的性能差距通常在1-2个百分点以内。相比之下，模型规模（从Base到Large到XL）带来的提升是5-10个百分点。这暗示了一个重要的规律：**规模比目标设计更重要**。</span>
<span id="cb5-694"><a href="#cb5-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-695"><a href="#cb5-695" aria-hidden="true" tabindex="-1"></a>第二，预训练数据的质量和数量比预训练目标更关键。T5论文构建了C4（Colossal Clean Crawled Corpus）数据集，清洗了Common Crawl中的大量噪声数据。实验显示，使用清洗过的数据与使用原始噪声数据之间的性能差距，远大于不同预训练目标之间的差距。</span>
<span id="cb5-696"><a href="#cb5-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-697"><a href="#cb5-697" aria-hidden="true" tabindex="-1"></a>第三，ELECTRA在**计算效率**维度上的优势是最显著的。在相同的计算预算下（FLOPs），ELECTRA一致性地超过了所有其他方法，包括XLNet和RoBERTa。这个发现对资源受限的研究者和企业特别有意义。</span>
<span id="cb5-698"><a href="#cb5-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-699"><a href="#cb5-699" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb5-700"><a href="#cb5-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-701"><a href="#cb5-701" aria-hidden="true" tabindex="-1"></a>**XLNet的实现复杂度问题**。XLNet的排列语言建模和双流注意力在理论上优雅，但在工程实现上引入了显著的复杂度。双流注意力需要在每一层维护两套隐藏状态（内容流和查询流），增加了约50%的内存开销和计算量。在实际应用中，这种额外的复杂度是否值得，取决于任务和资源约束。事实上，在XLNet之后，很少有后续工作继续使用排列语言建模——研究社区的注意力更多地转向了更简单的方法（如RoBERTa、ELECTRA）。</span>
<span id="cb5-702"><a href="#cb5-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-703"><a href="#cb5-703" aria-hidden="true" tabindex="-1"></a>**ELECTRA的生成器依赖**。ELECTRA的性能对生成器的质量和大小敏感。如果生成器太弱（如随机替换），判别任务太简单，模型学不到深层的语言知识。如果生成器太强（如与判别器大小相同），替换词太过逼真，判别任务又变得太难。这个"Goldilocks问题"需要仔细调节，增加了超参数搜索的成本。</span>
<span id="cb5-704"><a href="#cb5-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-705"><a href="#cb5-705" aria-hidden="true" tabindex="-1"></a>另一个边界条件是ELECTRA的判别目标可能不利于生成任务。由于判别器学习的是"这个词对不对"而非"这个位置应该是什么词"，ELECTRA的预训练模型在文本生成任务上的表现通常不如BERT或T5。</span>
<span id="cb5-706"><a href="#cb5-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-707"><a href="#cb5-707" aria-hidden="true" tabindex="-1"></a>**T5的参数效率问题**。T5使用Encoder-Decoder架构，在相同隐藏维度下参数量大约是Encoder-only或Decoder-only模型的两倍。例如，T5-Base有220M参数，而BERT-Base只有110M。虽然Encoder-Decoder在某些任务上略优，但参数效率的劣势使得T5在规模化时面临更大的成本压力。这也是为什么后来的LLM（如GPT-3、LLaMA）普遍选择了Decoder-only架构——在参数效率和扩展性上更有优势。</span>
<span id="cb5-708"><a href="#cb5-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-709"><a href="#cb5-709" aria-hidden="true" tabindex="-1"></a>**对比学习的假负样本问题**。如前面数值例子所示，无监督对比学习会将随机采样的同batch句子作为负样本，但这些"负样本"中可能包含语义上实际相似的句子。在大batch中这个问题有所缓解（因为随机碰到语义相近句子的概率降低），但无法完全消除。有监督版本通过NLI数据引入了人工标注的正负样本，显著缓解了这个问题，但又引入了对标注数据的依赖。</span>
<span id="cb5-710"><a href="#cb5-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-711"><a href="#cb5-711" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb5-712"><a href="#cb5-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-713"><a href="#cb5-713" aria-hidden="true" tabindex="-1"></a>**预训练目标与涌现能力的关系**。随着模型规模的增长，大语言模型展现出了各种涌现能力（如CoT推理、代码生成）。这些涌现能力与预训练目标的选择之间有什么关系？为什么GPT-3的因果语言建模能涌现出Few-shot能力，而BERT的MLM似乎无法做到？是因为自回归生成本身就蕴含了某种推理能力，还是仅仅是因为规模化的Decoder-only模型恰好受益于因果语言建模的简单性？</span>
<span id="cb5-714"><a href="#cb5-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-715"><a href="#cb5-715" aria-hidden="true" tabindex="-1"></a>**判别式预训练的规模化潜力**。ELECTRA在中小规模上展现了显著的计算效率优势，但在超大规模（百亿参数以上）上还没有充分验证。判别式预训练是否能在规模化时保持其优势？或者在足够大的规模下，预训练目标的选择会变得不重要（被规模"稀释"）？</span>
<span id="cb5-716"><a href="#cb5-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-717"><a href="#cb5-717" aria-hidden="true" tabindex="-1"></a>**超越"预测词"的预训练范式**。当前的预训练目标——无论是MLM、PLM、RTD还是Span Corruption——本质上都在让模型学习"词与上下文的关系"。有没有可能设计完全不同的预训练目标，直接让模型学习更高层次的语言能力（如推理、规划、常识）？这是一个开放且深远的问题。</span>
<span id="cb5-718"><a href="#cb5-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-719"><a href="#cb5-719" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-720"><a href="#cb5-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-721"><a href="#cb5-721" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb5-722"><a href="#cb5-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-723"><a href="#cb5-723" aria-hidden="true" tabindex="-1"></a><span class="fu">### 预训练目标改进的"天花板"</span></span>
<span id="cb5-724"><a href="#cb5-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-725"><a href="#cb5-725" aria-hidden="true" tabindex="-1"></a>本章介绍的各种预训练目标改进，虽然在各自的维度上取得了进展，但一个不可回避的事实是：**预训练目标的设计对最终性能的影响正在被模型规模所主导**。</span>
<span id="cb5-726"><a href="#cb5-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-727"><a href="#cb5-727" aria-hidden="true" tabindex="-1"></a>T5的系统性实验已经暗示了这一点：不同预训练目标之间的性能差距（1-2%），远小于模型从Base扩展到Large带来的提升（5-10%）。到2020年GPT-3（175B参数）的出现，这个趋势变得更加明显——GPT-3使用的是最简单的因果语言建模（与GPT-1完全相同的目标），但通过2600倍的参数量增长，在几乎所有任务上碾压了BERT、XLNet、ELECTRA等使用"更好"预训练目标的模型。</span>
<span id="cb5-728"><a href="#cb5-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-729"><a href="#cb5-729" aria-hidden="true" tabindex="-1"></a>这引出了一个尖锐的问题：精巧的预训练目标设计是否只是在"小模型时代"有意义？当模型足够大时，最简单的因果语言建模是否就是最优解？这个问题至今没有确定的答案，但从实践来看，2020年之后的主流LLM几乎全部采用了因果语言建模——简单性和扩展性战胜了理论上的优雅性。</span>
<span id="cb5-730"><a href="#cb5-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-731"><a href="#cb5-731" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练策略 vs 训练目标</span></span>
<span id="cb5-732"><a href="#cb5-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-733"><a href="#cb5-733" aria-hidden="true" tabindex="-1"></a>另一个被本章低估的维度是**训练策略的重要性**。RoBERTa（Liu et al., 2019）是一个教科书式的反例：它没有改变BERT的预训练目标（仍然是MLM），只是改进了训练策略——更多数据（160GB vs 16GB）、更大batch size（8K）、去掉NSP、动态遮蔽——就在大多数任务上超过了XLNet。</span>
<span id="cb5-734"><a href="#cb5-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-735"><a href="#cb5-735" aria-hidden="true" tabindex="-1"></a>这暗示了一个被忽视的可能性：BERT的原始MLM也许已经足够好了，它的"不足"更多来自训练不充分，而非目标本身的缺陷。这种"训练更好"vs "设计更巧妙"的张力，是下一章的核心主题。</span>
<span id="cb5-736"><a href="#cb5-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-737"><a href="#cb5-737" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb5-738"><a href="#cb5-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-739"><a href="#cb5-739" aria-hidden="true" tabindex="-1"></a>本章的方法论启示和未解决问题自然地导向了两个方向。</span>
<span id="cb5-740"><a href="#cb5-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-741"><a href="#cb5-741" aria-hidden="true" tabindex="-1"></a>第一个方向是**训练策略的系统优化**——既然预训练目标的边际收益在递减，那么训练策略（数据量、batch size、学习率调度、训练时长）的优化是否能带来更大的收益？RoBERTa和ALBERT的实验给出了肯定的答案。这是第15章的主题。</span>
<span id="cb5-742"><a href="#cb5-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-743"><a href="#cb5-743" aria-hidden="true" tabindex="-1"></a>第二个方向是**架构选择的根本性反思**——Encoder-only、Decoder-only、Encoder-Decoder三种架构各有什么根本的优劣？为什么T5选择了Encoder-Decoder，而GPT-3选择了Decoder-only？当规模足够大时，哪种架构的扩展性最好？这是第16章要深入讨论的问题。</span>
<span id="cb5-744"><a href="#cb5-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-745"><a href="#cb5-745" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 下一章预告：第15章将聚焦BERT系列模型的**工程优化**。RoBERTa如何通过"训练更好"就超越了精心设计的XLNet？ALBERT如何通过参数共享将BERT的参数量减少90%而性能不降？DistilBERT如何通过知识蒸馏将BERT压缩到原来的40%大小？这些工作共同揭示了一个重要的认识：**模型的潜力往往被训练策略和工程优化所限制，而非架构或目标本身**。</span></span>
<span id="cb5-746"><a href="#cb5-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-747"><a href="#cb5-747" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-748"><a href="#cb5-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-749"><a href="#cb5-749" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb5-750"><a href="#cb5-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-751"><a href="#cb5-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb5-752"><a href="#cb5-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-753"><a href="#cb5-753" aria-hidden="true" tabindex="-1"></a>这一章我们系统介绍了BERT之后预训练目标的四个演进方向，每一个都是对BERT某个具体局限的直接回应。</span>
<span id="cb5-754"><a href="#cb5-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-755"><a href="#cb5-755" aria-hidden="true" tabindex="-1"></a>XLNet通过排列语言建模，在保持自回归框架的同时获得了双向上下文信息，彻底消除了<span class="in">`[MASK]`</span>标记带来的预训练-微调不一致问题。其理论基础是概率论中联合分布的排列等价性，实现上通过双流自注意力来区分"提供上下文"和"做预测"两种角色。</span>
<span id="cb5-756"><a href="#cb5-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-757"><a href="#cb5-757" aria-hidden="true" tabindex="-1"></a>ELECTRA将预训练任务从生成式（预测被遮蔽的词）转变为判别式（判断每个词是否被替换），将信号效率从15%提升到100%。在相同的计算预算下，ELECTRA一致性地超过了BERT、XLNet和RoBERTa，证明了判别式预训练的计算效率优势。</span>
<span id="cb5-758"><a href="#cb5-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-759"><a href="#cb5-759" aria-hidden="true" tabindex="-1"></a>T5通过Text-to-Text框架和Span Corruption目标，将所有NLP任务统一为"文本到文本"格式，同时使用Encoder-Decoder架构兼顾了理解和生成能力。更重要的是，T5的系统性消融实验揭示了一个关键发现：模型规模和数据质量的影响远大于预训练目标的选择。</span>
<span id="cb5-760"><a href="#cb5-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-761"><a href="#cb5-761" aria-hidden="true" tabindex="-1"></a>对比学习（以SimCSE为代表）跳出了"预测词"的范式，直接优化句子级表示的质量。通过巧妙地利用Dropout作为数据增强，SimCSE以极简的方式大幅提升了句子嵌入的性能。</span>
<span id="cb5-762"><a href="#cb5-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-763"><a href="#cb5-763" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb5-764"><a href="#cb5-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-765"><a href="#cb5-765" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 公式 <span class="pp">|</span> 含义 <span class="pp">|</span></span>
<span id="cb5-766"><a href="#cb5-766" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|------|</span></span>
<span id="cb5-767"><a href="#cb5-767" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_T} \left[ \sum_t \log P(x_{z_t} \mid \mathbf{x}_{\mathbf{z}_{\lt t}}) \right]$ <span class="pp">|</span> XLNet排列语言建模目标 <span class="pp">|</span></span>
<span id="cb5-768"><a href="#cb5-768" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $L_{\text{Disc}} = -\sum_t <span class="co">[</span><span class="ot">\mathbb{1}(x_t^c = x_t) \log D_t + \mathbb{1}(x_t^c \neq x_t) \log(1-D_t)</span><span class="co">]</span>$ <span class="pp">|</span> ELECTRA判别器损失 <span class="pp">|</span></span>
<span id="cb5-769"><a href="#cb5-769" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $L = L_{\text{MLM}} + \lambda \cdot L_{\text{Disc}}$ <span class="pp">|</span> ELECTRA总损失（$\lambda=50$） <span class="pp">|</span></span>
<span id="cb5-770"><a href="#cb5-770" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\ell_i = -\log \frac{e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_i') / \tau}}{\sum_j e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_j') / \tau}}$ <span class="pp">|</span> SimCSE对比损失 <span class="pp">|</span></span>
<span id="cb5-771"><a href="#cb5-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-772"><a href="#cb5-772" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb5-773"><a href="#cb5-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-774"><a href="#cb5-774" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** XLNet声称通过排列语言建模获得了双向上下文，但实际训练时只对每个排列预测最后若干个位置的token。如果预测所有位置的token，会出现什么问题？提示：考虑排在排列开头的位置能看到多少上下文。</span>
<span id="cb5-775"><a href="#cb5-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-776"><a href="#cb5-776" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** ELECTRA论文中$\lambda = 50$，即判别器损失的权重是生成器的50倍。从梯度规模的角度分析这个选择：如果生成器和判别器的损失数值在同一量级，为什么需要大幅加权判别器？提示：考虑生成器损失（30000维softmax的交叉熵）和判别器损失（二分类的交叉熵）的梯度范数差异。</span>
<span id="cb5-777"><a href="#cb5-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-778"><a href="#cb5-778" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用Hugging Face的ELECTRA-Small模型，在SST-2数据集上微调并报告准确率。然后与同等大小的BERT-Small对比。验证ELECTRA在小模型规模上的效率优势是否成立。</span>
<span id="cb5-779"><a href="#cb5-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-780"><a href="#cb5-780" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[对比分析]** T5的系统性实验发现"预训练目标的选择没有模型规模重要"。如果这个结论成立，为什么我们还要研究预训练目标？请从以下角度讨论：(a) 计算效率（ELECTRA的例子）；(b) 特定任务的匹配度（对比学习在句子嵌入上的优势）；(c) 规模化之外的价值。</span>
<span id="cb5-781"><a href="#cb5-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-782"><a href="#cb5-782" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**[研究思考]** 本章介绍的四种方法中，XLNet和T5都尝试解决BERT的生成能力缺失，但采用了完全不同的路线（XLNet保持Encoder-only但改变训练目标，T5采用Encoder-Decoder架构）。从2025年的视角回看，Decoder-only架构（GPT系列）最终成为主流。为什么一个看似"最简单"的方案反而赢了？XLNet和T5的教训是什么？</span>
<span id="cb5-783"><a href="#cb5-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-784"><a href="#cb5-784" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-785"><a href="#cb5-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-786"><a href="#cb5-786" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb5-787"><a href="#cb5-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-788"><a href="#cb5-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb5-789"><a href="#cb5-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-790"><a href="#cb5-790" aria-hidden="true" tabindex="-1"></a>**Yang, Z. et al. (2019). "XLNet: Generalized Autoregressive Pretraining for Language Understanding"**。排列语言建模的原始论文。重点阅读：Section 2.1（排列语言建模的动机和形式化）、Section 2.3（双流注意力——理解内容流和查询流的区别）。可快速浏览：Section 2.4（Transformer-XL的整合细节）。<span class="co">[</span><span class="ot">arXiv:1906.08237</span><span class="co">](https://arxiv.org/abs/1906.08237)</span></span>
<span id="cb5-791"><a href="#cb5-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-792"><a href="#cb5-792" aria-hidden="true" tabindex="-1"></a>**Clark, K. et al. (2020). "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"**。替换词检测的原始论文。重点阅读：Section 3.1（方法描述——生成器-判别器的训练流程）、Table 1（效率对比——这是论文最有说服力的结果）。可快速浏览：Section 3.2-3.3（各种变体的消融）。<span class="co">[</span><span class="ot">arXiv:2003.10555</span><span class="co">](https://arxiv.org/abs/2003.10555)</span></span>
<span id="cb5-793"><a href="#cb5-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-794"><a href="#cb5-794" aria-hidden="true" tabindex="-1"></a>**Raffel, C. et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**。T5的原始论文，也是预训练策略的百科全书。重点阅读：Section 3.3（预训练目标的系统对比）、Table 14（不同预训练目标的性能对比）。由于论文极长（67页），建议先读Table 1了解全貌，再选择感兴趣的实验深入。<span class="co">[</span><span class="ot">arXiv:1910.10683</span><span class="co">](https://arxiv.org/abs/1910.10683)</span></span>
<span id="cb5-795"><a href="#cb5-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-796"><a href="#cb5-796" aria-hidden="true" tabindex="-1"></a><span class="fu">### 对比学习方向</span></span>
<span id="cb5-797"><a href="#cb5-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-798"><a href="#cb5-798" aria-hidden="true" tabindex="-1"></a>**Gao, T., Yao, X., &amp; Chen, D. (2021). "SimCSE: Simple Contrastive Learning of Sentence Embeddings"**。对比学习在句子嵌入上的突破。重点阅读：Section 3（无监督和有监督SimCSE的方法描述）、Section 5.2（Dropout作为数据增强的分析）。<span class="co">[</span><span class="ot">arXiv:2104.08821</span><span class="co">](https://arxiv.org/abs/2104.08821)</span></span>
<span id="cb5-799"><a href="#cb5-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-800"><a href="#cb5-800" aria-hidden="true" tabindex="-1"></a>**Li, B. et al. (2020). "On the Sentence Embeddings from Pre-trained Language Models"**。揭示了BERT句子嵌入的各向异性问题，解释了为什么直接使用BERT的<span class="co">[</span><span class="ot">CLS</span><span class="co">]</span>向量作为句子嵌入效果不佳。<span class="co">[</span><span class="ot">arXiv:2011.05864</span><span class="co">](https://arxiv.org/abs/2011.05864)</span></span>
<span id="cb5-801"><a href="#cb5-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-802"><a href="#cb5-802" aria-hidden="true" tabindex="-1"></a><span class="fu">### 前驱工作</span></span>
<span id="cb5-803"><a href="#cb5-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-804"><a href="#cb5-804" aria-hidden="true" tabindex="-1"></a>**Dai, Z. et al. (2019). "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"**。XLNet的前驱工作，提出了片段循环机制和相对位置编码，解决了Transformer在长文本建模上的限制。<span class="co">[</span><span class="ot">arXiv:1901.02860</span><span class="co">](https://arxiv.org/abs/1901.02860)</span></span>
<span id="cb5-805"><a href="#cb5-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-806"><a href="#cb5-806" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb5-807"><a href="#cb5-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-808"><a href="#cb5-808" aria-hidden="true" tabindex="-1"></a>**He, P. et al. (2021). "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"**。在ELECTRA的判别式预训练基础上引入了解耦注意力（将内容和位置的注意力分开计算），在SuperGLUE上首次超过人类基准。<span class="co">[</span><span class="ot">arXiv:2006.03654</span><span class="co">](https://arxiv.org/abs/2006.03654)</span></span>
<span id="cb5-809"><a href="#cb5-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-810"><a href="#cb5-810" aria-hidden="true" tabindex="-1"></a>**Xue, L. et al. (2021). "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"**。T5的多语言版本，在101种语言上进行预训练，展示了Text-to-Text框架在多语言场景中的通用性。<span class="co">[</span><span class="ot">arXiv:2010.11934</span><span class="co">](https://arxiv.org/abs/2010.11934)</span></span>
<span id="cb5-811"><a href="#cb5-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-812"><a href="#cb5-812" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb5-813"><a href="#cb5-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-814"><a href="#cb5-814" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face ELECTRA**: <span class="co">[</span><span class="ot">huggingface.co/google/electra-base-discriminator</span><span class="co">](https://huggingface.co/google/electra-base-discriminator)</span> — 预训练ELECTRA模型</span>
<span id="cb5-815"><a href="#cb5-815" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face T5**: <span class="co">[</span><span class="ot">huggingface.co/t5-base</span><span class="co">](https://huggingface.co/t5-base)</span> — 预训练T5模型</span>
<span id="cb5-816"><a href="#cb5-816" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SimCSE官方代码**: <span class="co">[</span><span class="ot">github.com/princeton-nlp/SimCSE</span><span class="co">](https://github.com/princeton-nlp/SimCSE)</span> — 无监督和有监督SimCSE的完整实现</span>
<span id="cb5-817"><a href="#cb5-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-818"><a href="#cb5-818" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-819"><a href="#cb5-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-820"><a href="#cb5-820" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb5-821"><a href="#cb5-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-822"><a href="#cb5-822" aria-hidden="true" tabindex="-1"></a>XLNet、ELECTRA和T5分别代表了三个不同研究团队对BERT局限性的回应，它们的诞生几乎同时发生在2019年中到2020年初——这段时期可以称为预训练目标的"寒武纪爆发"。</span>
<span id="cb5-823"><a href="#cb5-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-824"><a href="#cb5-824" aria-hidden="true" tabindex="-1"></a>XLNet由Carnegie Mellon大学的Zhilin Yang和William Cohen与Google Brain合作提出。有趣的是，XLNet在提交时引发了激烈的争论：它在20个基准任务上超越了BERT，但许多研究者指出XLNet使用了更多的数据（126GB vs 16GB）和更大的计算量，性能提升是否来自排列语言建模本身，还是仅仅来自更充分的训练？这个质疑后来被RoBERTa的实验所佐证——RoBERTa不改目标只改训练策略就追上了XLNet。</span>
<span id="cb5-825"><a href="#cb5-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-826"><a href="#cb5-826" aria-hidden="true" tabindex="-1"></a>ELECTRA由Stanford的Kevin Clark和Google的Thang Luong（Attention变体那一章的主角）等人提出。ELECTRA最初并不被社区看好——"判别而非生成"的想法太过反直觉，很多人质疑一个二分类任务怎么可能学到丰富的语言表示。但ELECTRA-Small在单卡GPU上4天训练就超过GPT-Large的结果震惊了社区，证明了计算效率的重要性。对于很多资源有限的研究者和公司来说，ELECTRA成为了BERT的更实用的替代品。</span>
<span id="cb5-827"><a href="#cb5-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-828"><a href="#cb5-828" aria-hidden="true" tabindex="-1"></a>T5由Google的Colin Raffel等人提出，其67页的论文几乎成为了预训练技术的"百科全书"——它系统地比较了几乎所有当时已知的预训练策略，包括架构、目标、数据、训练策略等。T5论文的学术贡献也许不在于提出了新的Span Corruption目标，而在于通过大规模实验为社区提供了可靠的实证指南。</span>
<span id="cb5-829"><a href="#cb5-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-830"><a href="#cb5-830" aria-hidden="true" tabindex="-1"></a>从2020年的GPT-3开始，预训练目标的创新热潮逐渐平息。研究者们意识到，在足够大的规模下，最简单的因果语言建模可能就是最好的选择——不是因为它在理论上最优，而是因为它的简单性使其最容易规模化。这个认识标志着NLP从"精巧设计"时代向"暴力美学"时代的转变——一个至今仍在持续的范式变革。</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>