<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ying Zha">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="Dense Transformer有一个根本性的浪费：对于每一个token，所有参数都会被激活。一个关于烹饪的token真的需要激活’数学知识’的参数吗？Mixture of Experts (MoE) 提供了一种优雅的解决方案：将FFN层拆分为多个Expert，让Router动态选择每个token应该走哪几个Expert。这种稀疏激活让模型可以拥有数千亿的总参数，但每次推理只激活其中一小部分——实现了参数量与计算量的解耦。从Shazeer 2017的开创性工作，到Switch Transformer的简化设计，再到Mixtral 8x7B和DeepSeek-V3的工业级成功，MoE正在成为大语言模型的标准架构选择。">

<title>第27章：Mixture of Experts——稀疏激活的智慧 – Tech Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-1b3db88def35042d172274863c1cdcf0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-6ee47bd5d569ce80d002539aadcc850f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-ea2c01f27a86cd888be845a75ab84aaf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Force refresh if cache is stale -->

<script>

(function() {

  var SITE_VERSION = '2025-11-14-v2'; // Update this to force all users to refresh

  var stored = localStorage.getItem('site_version');

  if (stored !== SITE_VERSION) {

    localStorage.setItem('site_version', SITE_VERSION);

    if (stored !== null) {

      // Not first visit, force reload from server

      window.location.reload(true);

    }

  }

})();

</script>

<script>

// Default to dark scheme on first visit (no prior preference stored)

try {

  var key = 'quarto-color-scheme';

  if (window && window.localStorage && window.localStorage.getItem(key) === null) {

    window.localStorage.setItem(key, 'alternate');

  }

} catch (e) {

  // ignore storage errors (privacy mode, etc.)

}

</script>

<!-- Aggressive cache prevention for HTML pages -->

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate, max-age=0">

<meta http-equiv="Pragma" content="no-cache">

<meta http-equiv="Expires" content="0">

<meta name="revisit-after" content="1 days">

<meta name="robots" content="noarchive">




  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tech Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../home.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts_en.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tags.html"> 
<span class="menu-text">Tags</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#从上一章说起" id="toc-从上一章说起" class="nav-link active" data-scroll-target="#从上一章说起"><span class="header-section-number">1</span> 从上一章说起</a></li>
  <li><a href="#问题的本质是什么" id="toc-问题的本质是什么" class="nav-link" data-scroll-target="#问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</a>
  <ul class="collapse">
  <li><a href="#dense-model-的参数效率困境" id="toc-dense-model-的参数效率困境" class="nav-link" data-scroll-target="#dense-model-的参数效率困境"><span class="header-section-number">2.1</span> Dense Model 的参数效率困境</a></li>
  <li><a href="#条件计算的直觉" id="toc-条件计算的直觉" class="nav-link" data-scroll-target="#条件计算的直觉"><span class="header-section-number">2.2</span> 条件计算的直觉</a></li>
  <li><a href="#moe-的核心思想" id="toc-moe-的核心思想" class="nav-link" data-scroll-target="#moe-的核心思想"><span class="header-section-number">2.3</span> MoE 的核心思想</a></li>
  <li><a href="#我们需要解决什么问题" id="toc-我们需要解决什么问题" class="nav-link" data-scroll-target="#我们需要解决什么问题"><span class="header-section-number">2.4</span> 我们需要解决什么问题？</a></li>
  </ul></li>
  <li><a href="#核心思想与直觉" id="toc-核心思想与直觉" class="nav-link" data-scroll-target="#核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</a>
  <ul class="collapse">
  <li><a href="#从通才到专家分工" id="toc-从通才到专家分工" class="nav-link" data-scroll-target="#从通才到专家分工"><span class="header-section-number">3.1</span> 从”通才”到”专家分工”</a></li>
  <li><a href="#token-级别的专家选择" id="toc-token-级别的专家选择" class="nav-link" data-scroll-target="#token-级别的专家选择"><span class="header-section-number">3.2</span> Token 级别的专家选择</a></li>
  <li><a href="#门控机制的数学形式" id="toc-门控机制的数学形式" class="nav-link" data-scroll-target="#门控机制的数学形式"><span class="header-section-number">3.3</span> 门控机制的数学形式</a></li>
  </ul></li>
  <li><a href="#技术细节" id="toc-技术细节" class="nav-link" data-scroll-target="#技术细节"><span class="header-section-number">4</span> 技术细节</a>
  <ul class="collapse">
  <li><a href="#门控网络的设计" id="toc-门控网络的设计" class="nav-link" data-scroll-target="#门控网络的设计"><span class="header-section-number">4.1</span> 门控网络的设计</a></li>
  <li><a href="#负载均衡机制" id="toc-负载均衡机制" class="nav-link" data-scroll-target="#负载均衡机制"><span class="header-section-number">4.2</span> 负载均衡机制</a></li>
  <li><a href="#训练稳定性技巧" id="toc-训练稳定性技巧" class="nav-link" data-scroll-target="#训练稳定性技巧"><span class="header-section-number">4.3</span> 训练稳定性技巧</a></li>
  <li><a href="#数值示例top-2-路由计算" id="toc-数值示例top-2-路由计算" class="nav-link" data-scroll-target="#数值示例top-2-路由计算"><span class="header-section-number">4.4</span> 数值示例：Top-2 路由计算</a></li>
  </ul></li>
  <li><a href="#moe-的演进历程" id="toc-moe-的演进历程" class="nav-link" data-scroll-target="#moe-的演进历程"><span class="header-section-number">5</span> MoE 的演进历程</a>
  <ul class="collapse">
  <li><a href="#早期探索jacobs-1991-与-shazeer-2017" id="toc-早期探索jacobs-1991-与-shazeer-2017" class="nav-link" data-scroll-target="#早期探索jacobs-1991-与-shazeer-2017"><span class="header-section-number">5.1</span> 早期探索：Jacobs 1991 与 Shazeer 2017</a></li>
  <li><a href="#gshard-2020工业级规模化" id="toc-gshard-2020工业级规模化" class="nav-link" data-scroll-target="#gshard-2020工业级规模化"><span class="header-section-number">5.2</span> GShard (2020)：工业级规模化</a></li>
  <li><a href="#switch-transformer-2021简化与极致稀疏" id="toc-switch-transformer-2021简化与极致稀疏" class="nav-link" data-scroll-target="#switch-transformer-2021简化与极致稀疏"><span class="header-section-number">5.3</span> Switch Transformer (2021)：简化与极致稀疏</a></li>
  <li><a href="#st-moe-2022稳定性与可迁移性" id="toc-st-moe-2022稳定性与可迁移性" class="nav-link" data-scroll-target="#st-moe-2022稳定性与可迁移性"><span class="header-section-number">5.4</span> ST-MoE (2022)：稳定性与可迁移性</a></li>
  <li><a href="#mixtral-8x7b-2023开源-moe-的里程碑" id="toc-mixtral-8x7b-2023开源-moe-的里程碑" class="nav-link" data-scroll-target="#mixtral-8x7b-2023开源-moe-的里程碑"><span class="header-section-number">5.5</span> Mixtral 8x7B (2023)：开源 MoE 的里程碑</a></li>
  <li><a href="#deepseek-v2v3-2024极致效率" id="toc-deepseek-v2v3-2024极致效率" class="nav-link" data-scroll-target="#deepseek-v2v3-2024极致效率"><span class="header-section-number">5.6</span> DeepSeek-V2/V3 (2024)：极致效率</a></li>
  </ul></li>
  <li><a href="#moe-的核心挑战" id="toc-moe-的核心挑战" class="nav-link" data-scroll-target="#moe-的核心挑战"><span class="header-section-number">6</span> MoE 的核心挑战</a>
  <ul class="collapse">
  <li><a href="#负载均衡的根本困难" id="toc-负载均衡的根本困难" class="nav-link" data-scroll-target="#负载均衡的根本困难"><span class="header-section-number">6.1</span> 负载均衡的根本困难</a></li>
  <li><a href="#训练不稳定性" id="toc-训练不稳定性" class="nav-link" data-scroll-target="#训练不稳定性"><span class="header-section-number">6.2</span> 训练不稳定性</a></li>
  <li><a href="#工程复杂性" id="toc-工程复杂性" class="nav-link" data-scroll-target="#工程复杂性"><span class="header-section-number">6.3</span> 工程复杂性</a></li>
  </ul></li>
  <li><a href="#深入理解" id="toc-深入理解" class="nav-link" data-scroll-target="#深入理解"><span class="header-section-number">7</span> 深入理解</a>
  <ul class="collapse">
  <li><a href="#为什么有效理论视角" id="toc-为什么有效理论视角" class="nav-link" data-scroll-target="#为什么有效理论视角"><span class="header-section-number">7.1</span> 为什么有效？——理论视角</a></li>
  <li><a href="#为什么有效实证视角" id="toc-为什么有效实证视角" class="nav-link" data-scroll-target="#为什么有效实证视角"><span class="header-section-number">7.2</span> 为什么有效？——实证视角</a></li>
  <li><a href="#方法的边界条件" id="toc-方法的边界条件" class="nav-link" data-scroll-target="#方法的边界条件"><span class="header-section-number">7.3</span> 方法的边界条件</a></li>
  <li><a href="#开放研究问题" id="toc-开放研究问题" class="nav-link" data-scroll-target="#开放研究问题"><span class="header-section-number">7.4</span> 开放研究问题</a></li>
  </ul></li>
  <li><a href="#局限性与未解决的问题" id="toc-局限性与未解决的问题" class="nav-link" data-scroll-target="#局限性与未解决的问题"><span class="header-section-number">8</span> 局限性与未解决的问题</a>
  <ul class="collapse">
  <li><a href="#本方法的局限" id="toc-本方法的局限" class="nav-link" data-scroll-target="#本方法的局限"><span class="header-section-number">8.1</span> 本方法的局限</a></li>
  <li><a href="#这些局限导向了什么" id="toc-这些局限导向了什么" class="nav-link" data-scroll-target="#这些局限导向了什么"><span class="header-section-number">8.2</span> 这些局限导向了什么？</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结"><span class="header-section-number">9</span> 本章小结</a>
  <ul class="collapse">
  <li><a href="#核心要点回顾" id="toc-核心要点回顾" class="nav-link" data-scroll-target="#核心要点回顾"><span class="header-section-number">9.1</span> 核心要点回顾</a></li>
  <li><a href="#关键公式速查" id="toc-关键公式速查" class="nav-link" data-scroll-target="#关键公式速查"><span class="header-section-number">9.2</span> 关键公式速查</a></li>
  <li><a href="#思考题" id="toc-思考题" class="nav-link" data-scroll-target="#思考题"><span class="header-section-number">9.3</span> 思考题</a></li>
  </ul></li>
  <li><a href="#延伸阅读" id="toc-延伸阅读" class="nav-link" data-scroll-target="#延伸阅读"><span class="header-section-number">10</span> 延伸阅读</a>
  <ul class="collapse">
  <li><a href="#核心论文必读" id="toc-核心论文必读" class="nav-link" data-scroll-target="#核心论文必读"><span class="header-section-number">10.1</span> 核心论文（必读）</a></li>
  <li><a href="#后续发展" id="toc-后续发展" class="nav-link" data-scroll-target="#后续发展"><span class="header-section-number">10.2</span> 后续发展</a></li>
  <li><a href="#综述与教程" id="toc-综述与教程" class="nav-link" data-scroll-target="#综述与教程"><span class="header-section-number">10.3</span> 综述与教程</a></li>
  <li><a href="#代码资源" id="toc-代码资源" class="nav-link" data-scroll-target="#代码资源"><span class="header-section-number">10.4</span> 代码资源</a></li>
  </ul></li>
  <li><a href="#历史注脚" id="toc-历史注脚" class="nav-link" data-scroll-target="#历史注脚"><span class="header-section-number">11</span> 历史注脚</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">第27章：Mixture of Experts——稀疏激活的智慧</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Conditional Computation: Scaling Parameters Without Scaling Compute</p>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Deep Learning</div>
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">MoE</div>
    <div class="quarto-category">稀疏模型</div>
    <div class="quarto-category">条件计算</div>
  </div>
  </div>

<div>
  <div class="description">
    Dense Transformer有一个根本性的浪费：对于每一个token，所有参数都会被激活。一个关于烹饪的token真的需要激活’数学知识’的参数吗？Mixture of Experts (MoE) 提供了一种优雅的解决方案：将FFN层拆分为多个Expert，让Router动态选择每个token应该走哪几个Expert。这种稀疏激活让模型可以拥有数千亿的总参数，但每次推理只激活其中一小部分——实现了参数量与计算量的解耦。从Shazeer 2017的开创性工作，到Switch Transformer的简化设计，再到Mixtral 8x7B和DeepSeek-V3的工业级成功，MoE正在成为大语言模型的标准架构选择。
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ying Zha </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 28, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>核心问题</strong>：Dense Transformer对每个token激活所有参数，这种”大锅饭”式的计算是否存在浪费？能否让不同的token使用不同的参数子集，从而在保持计算量的同时大幅提升模型容量？</p>
<p><strong>历史坐标</strong>：2017–2024 | Shazeer (2017) → GShard (2020) → Switch Transformer (2021) → ST-MoE (2022) → Mixtral (2023) → DeepSeek-V3 (2024) | 从研究原型到工业级部署</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>本章参考来源
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="论文" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="论文"><span class="header-section-number">0.1</span> 论文</h3>
<ul>
<li><strong>Shazeer et al.&nbsp;(2017)</strong> “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer” (<a href="https://arxiv.org/abs/1701.06538">arXiv:1701.06538</a>) — 参考了 Section 2-3 (MoE架构设计、门控机制)；ICLR 2017，现代MoE的奠基之作</li>
<li><strong>Lepikhin et al.&nbsp;(2020)</strong> “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding” (<a href="https://arxiv.org/abs/2006.16668">arXiv:2006.16668</a>) — 参考了 Section 2-3 (大规模MoE训练、专家容量限制)；第一个600B参数的MoE模型</li>
<li><strong>Fedus et al.&nbsp;(2021)</strong> “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity” (<a href="https://arxiv.org/abs/2101.03961">arXiv:2101.03961</a>) — 参考了 Section 2-4 (Top-1路由简化、训练稳定性技巧)、Figure 1-4；JMLR 2022</li>
<li><strong>Zoph et al.&nbsp;(2022)</strong> “ST-MoE: Designing Stable and Transferable Sparse Expert Models” (<a href="https://arxiv.org/abs/2202.08906">arXiv:2202.08906</a>) — 参考了 Section 2-4 (训练稳定性最佳实践、微调策略)</li>
<li><strong>Jiang et al.&nbsp;(2024)</strong> “Mixtral of Experts” (<a href="https://arxiv.org/abs/2401.04088">arXiv:2401.04088</a>) — 参考了 Section 2-3 (Mixtral架构细节、性能评估)；开源MoE的里程碑</li>
<li><strong>Dai et al.&nbsp;(2024)</strong> “DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models” (<a href="https://arxiv.org/abs/2401.06066">arXiv:2401.06066</a>) — 参考了 Section 2-3 (细粒度专家分割、共享专家隔离)</li>
<li><strong>DeepSeek-AI (2024)</strong> “DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model” (<a href="https://arxiv.org/abs/2405.04434">arXiv:2405.04434</a>) — 参考了架构设计</li>
<li><strong>DeepSeek-AI (2024)</strong> “DeepSeek-V3 Technical Report” (<a href="https://arxiv.org/abs/2412.19437">arXiv:2412.19437</a>) — 参考了 Section 2-3 (无辅助损失负载均衡、多token预测)</li>
</ul>
</section>
<section id="教材与教程" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="教材与教程"><span class="header-section-number">0.2</span> 教材与教程</h3>
<ul>
<li><strong>Hugging Face Blog</strong> “Mixture of Experts Explained” — 参考了MoE基础概念的可视化解释</li>
<li><strong>Maarten Grootendorst</strong> “A Visual Guide to Mixture of Experts (MoE)” — 参考了MoE可视化图解</li>
</ul>
</section>
<section id="课程" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="课程"><span class="header-section-number">0.3</span> 课程</h3>
<ul>
<li><strong>Stanford CS25</strong> “Mixture of Experts Paradigm and the Switch Transformer” — Barret Zoph 的 MoE 讲座</li>
</ul>
</section>
</div>
</div>
</div>
<hr>
<section id="从上一章说起" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="从上一章说起"><span class="header-section-number">1</span> 从上一章说起</h2>
<p>上一章我们深入探讨了长上下文与高效推理的完整技术栈——从位置编码的演进（RoPE、ALiBi）到长度外推技术（Position Interpolation、YaRN），从 FlashAttention 对注意力计算的 IO 感知革命到 PagedAttention 的 KV Cache 内存管理。这些技术突破了 <span class="math inline">\(O(n^2)\)</span> 的瓶颈，让大语言模型能够处理从 512 token 到百万 token 的超长序列。</p>
<p>然而，长上下文解决的是”如何让模型看得更远”的问题，它假设模型本身已经足够强大。但模型的能力来自哪里？答案是<strong>参数</strong>——更多的参数意味着更大的模型容量，能够存储更多的知识和学习更复杂的模式。</p>
<p>这里存在一个根本性的矛盾：<strong>参数越多，计算越慢</strong>。</p>
<p>一个 70B 参数的模型在推理时，每个 token 都需要经过所有 70B 参数的计算。如果我们想要 700B 参数的模型呢？计算量直接增加 10 倍。如果我们想要 7T（万亿）参数呢？这在传统架构下几乎是不可能的。</p>
<p>但这种”所有参数服务每个 token”的设计真的合理吗？让我们仔细思考一下。</p>
<p>当模型处理一个关于”如何做番茄炒蛋”的 token 时，它真的需要激活那些编码了微积分公式的参数吗？当模型在翻译一句法语句子时，它真的需要激活那些专门处理代码语法的参数吗？直觉告诉我们，不同类型的输入可能需要不同的”专业知识”——而模型中大量的参数可能在处理特定输入时是”冗余”的。</p>
<p>这就是 <strong>Dense Transformer 的第一个根本挑战：所有参数对每个 token 都激活，存在巨大的计算浪费</strong>。</p>
<p>2017 年，Google 的 Noam Shazeer 等人提出了一个大胆的想法：<strong>让不同的 token 使用不同的参数子集</strong>。他们将模型中的一部分参数（具体来说是 FFN 层）拆分成多个”专家”（Expert），然后用一个”门控网络”（Gating Network）来决定每个 token 应该被发送到哪个专家处理。</p>
<p>这就是 <strong>Mixture of Experts（MoE）</strong> 的核心思想：用条件计算（conditional computation）实现稀疏激活（sparse activation），从而在不增加计算量的情况下大幅提升模型容量。</p>
<blockquote class="blockquote">
<p>💡 <strong>本章核心洞察</strong>：MoE 实现了参数量与计算量的解耦——模型可以拥有数千亿甚至万亿参数，但每次推理只激活其中一小部分。这种”专家分工”的设计让我们能够以相对较低的计算成本训练出容量巨大的模型，为大语言模型的规模化提供了一条全新的路径。</p>
</blockquote>
<hr>
</section>
<section id="问题的本质是什么" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="问题的本质是什么"><span class="header-section-number">2</span> 问题的本质是什么？</h2>
<section id="dense-model-的参数效率困境" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="dense-model-的参数效率困境"><span class="header-section-number">2.1</span> Dense Model 的参数效率困境</h3>
<p>让我们用具体数字来理解 Dense Transformer 的问题。</p>
<p>一个标准的 Transformer 层包含两个主要组件：自注意力（Self-Attention）和前馈网络（FFN）。对于一个隐藏维度为 <span class="math inline">\(d\)</span> 的模型，典型的 FFN 设计是：</p>
<p><span class="math display">\[
\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 \cdot x + b_1) + b_2
\]</span></p>
<p>其中 <span class="math inline">\(W_1 \in \mathbb{R}^{4d \times d}\)</span>，<span class="math inline">\(W_2 \in \mathbb{R}^{d \times 4d}\)</span>。这意味着 FFN 层的参数量约为 <span class="math inline">\(8d^2\)</span>，在一个标准 Transformer 层中占据约 2/3 的参数。</p>
<p>以 LLaMA-2 70B 为例：</p>
<ul>
<li>隐藏维度 <span class="math inline">\(d = 8192\)</span></li>
<li>FFN 中间维度 <span class="math inline">\(d_{ff} = 28672\)</span>（约 3.5 倍）</li>
<li>80 层 Transformer</li>
<li><strong>每层 FFN 参数量</strong>：<span class="math inline">\(2 \times 8192 \times 28672 \approx 470M\)</span></li>
<li><strong>全部 FFN 参数量</strong>：<span class="math inline">\(470M \times 80 \approx 37.6B\)</span></li>
</ul>
<p>这 37.6B 参数中的每一个，在处理每一个 token 时都会参与计算。一个关于”法国首都是巴黎”的 token，和一个关于”Python 列表推导式语法”的 token，使用的是<strong>完全相同</strong>的 37.6B 参数。</p>
<p>这合理吗？</p>
</section>
<section id="条件计算的直觉" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="条件计算的直觉"><span class="header-section-number">2.2</span> 条件计算的直觉</h3>
<p>条件计算（Conditional Computation）的核心思想是：<strong>让输入决定使用哪些参数</strong>。</p>
<p>最极端的例子是决策树：每个输入沿着一条从根到叶的路径，只使用路径上的参数，完全不接触其他分支的参数。决策树是 100% 稀疏的——每个样本只用到整个模型的一小部分。</p>
<p>但决策树的问题是它的表达能力有限，而且是”硬决策”——一旦选择了某个分支就不能回头。</p>
<p>神经网络的优势在于它的”软决策”——每一层都可以做连续的、可微分的变换。那么，能否设计一种既保留神经网络的表达能力，又实现部分稀疏激活的架构呢？</p>
<p>这就是 Mixture of Experts 的出发点。</p>
</section>
<section id="moe-的核心思想" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="moe-的核心思想"><span class="header-section-number">2.3</span> MoE 的核心思想</h3>
<p>Mixture of Experts 的基本思想可以追溯到 1991 年 Robert Jacobs 等人的工作，但真正将 MoE 带入深度学习时代的是 Shazeer 2017 年的论文。</p>
<p>核心设计如下：</p>
<ol type="1">
<li><strong>将 FFN 层替换为多个”专家”</strong>：不再使用单个 FFN，而是使用 <span class="math inline">\(n\)</span> 个并行的 FFN（称为 Expert），每个 Expert 的结构与原 FFN 相同</li>
<li><strong>添加一个”路由器”</strong>：一个轻量级的门控网络（Router / Gating Network），根据输入决定将每个 token 发送到哪个（些）Expert</li>
<li><strong>稀疏激活</strong>：每个 token 只被发送到 <span class="math inline">\(k\)</span> 个 Expert（<span class="math inline">\(k \ll n\)</span>），其他 Expert 完全不参与这个 token 的计算</li>
</ol>
<p>假设我们有 8 个 Expert，每个 token 只选择 2 个。那么：</p>
<ul>
<li><strong>总参数量</strong>：8 倍于原始 FFN</li>
<li><strong>每个 token 的计算量</strong>：只有 2/8 = 25% 的增加（2 个 Expert 替代 1 个 FFN）</li>
<li><strong>有效模型容量</strong>：8 倍</li>
</ul>
<p>这就是 MoE 的魔法：<strong>参数量可以任意扩展，而计算量只与激活的专家数有关</strong>。</p>
</section>
<section id="我们需要解决什么问题" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="我们需要解决什么问题"><span class="header-section-number">2.4</span> 我们需要解决什么问题？</h3>
<p>MoE 的概念听起来简单优雅，但实际实现中有几个关键问题需要解决。</p>
<p><strong>路由机制设计</strong>：如何决定每个 token 应该被发送到哪个 Expert？最直接的方法是用一个小型神经网络（门控网络）来预测每个 Expert 的”适合度”得分，然后选择得分最高的 <span class="math inline">\(k\)</span> 个。但这个选择是离散的，如何让它可微分？如何保证梯度能够回传？</p>
<p><strong>负载均衡</strong>：如果让模型自由学习路由，它会倾向于把大多数 token 都发送到少数几个”热门” Expert，导致这些 Expert 过载，而其他 Expert 几乎从未被使用——这被称为”路由崩塌”（Routing Collapse）。如何确保所有 Expert 都能得到均匀的训练？</p>
<p><strong>训练稳定性</strong>：稀疏模型的训练比 Dense 模型更不稳定。当某些 Expert 处理的 token 数量波动剧烈时，梯度的方差会很大。如何保证训练过程的稳定性？</p>
<p><strong>通信开销</strong>：在分布式训练中，不同的 Expert 可能位于不同的 GPU 上。当一个 token 需要被路由到某个 Expert 时，可能需要跨 GPU 通信。如何设计高效的通信模式？</p>
<p>这些问题在过去 7 年的 MoE 研究中被逐步解决，形成了今天我们看到的成熟技术方案。</p>
<hr>
</section>
</section>
<section id="核心思想与直觉" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="核心思想与直觉"><span class="header-section-number">3</span> 核心思想与直觉</h2>
<section id="从通才到专家分工" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="从通才到专家分工"><span class="header-section-number">3.1</span> 从”通才”到”专家分工”</h3>
<p>理解 MoE 的最好方式是用一个现实世界的类比。</p>
<p>想象一家医院。传统的做法是培养”全科医生”——每个医生都掌握所有医学知识，能够处理所有类型的疾病。这意味着每个医生需要学习的东西非常多（参数量大），每次问诊都需要调动全部知识（计算量大）。</p>
<p>MoE 的做法是引入”专科分诊”制度：</p>
<ul>
<li><strong>专家（Expert）</strong>：医院有内科、外科、儿科、眼科等多个专科，每个专科只专注于自己的领域</li>
<li><strong>路由器（Router）</strong>：相当于”分诊台”，根据病人的症状决定将病人发送到哪个专科</li>
<li><strong>稀疏激活</strong>：每个病人只需要看 1-2 个专科医生，不需要每个专科都走一遍</li>
</ul>
<p>这样设计的好处是：</p>
<ul>
<li><strong>总容量大</strong>：医院可以拥有各个领域的顶级专家，覆盖所有可能的疾病</li>
<li><strong>单次效率高</strong>：每个病人只需要看相关的专家，不需要浪费时间在无关的科室</li>
<li><strong>专业化深入</strong>：每个专家只专注于自己的领域，可以积累更深的专业知识</li>
</ul>
</section>
<section id="token-级别的专家选择" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="token-级别的专家选择"><span class="header-section-number">3.2</span> Token 级别的专家选择</h3>
<p>MoE 的一个关键设计决策是：<strong>路由发生在 token 级别，而不是 sequence 级别</strong>。</p>
<p>这意味着同一个句子中的不同 token 可能被路由到完全不同的 Expert。比如在处理”我用 Python 写了一个冒泡排序算法”这句话时：</p>
<ul>
<li>“我”、“用”、“写”、“了”、“一个” 可能被路由到通用语言理解的 Expert</li>
<li>“Python”、“冒泡排序”、“算法” 可能被路由到编程相关的 Expert</li>
</ul>
<p>这种 token 级别的细粒度路由让模型能够灵活地组合不同 Expert 的能力来处理复杂的输入。</p>
</section>
<section id="门控机制的数学形式" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="门控机制的数学形式"><span class="header-section-number">3.3</span> 门控机制的数学形式</h3>
<p>让我们用数学语言精确描述 MoE 层的工作方式。</p>
<p>假设我们有 <span class="math inline">\(n\)</span> 个 Expert，分别记为 <span class="math inline">\(E_1, E_2, \ldots, E_n\)</span>。对于输入 token 的表示 <span class="math inline">\(x \in \mathbb{R}^d\)</span>：</p>
<p><strong>Step 1：计算路由得分</strong></p>
<p>门控网络 <span class="math inline">\(G\)</span> 计算每个 Expert 的”适合度”得分：</p>
<p><span class="math display">\[
h = W_g \cdot x, \quad h \in \mathbb{R}^n
\]</span></p>
<p>其中 <span class="math inline">\(W_g \in \mathbb{R}^{n \times d}\)</span> 是门控网络的参数（一个简单的线性层）。</p>
<p><strong>Step 2：选择 Top-k Expert</strong></p>
<p>从 <span class="math inline">\(n\)</span> 个得分中选择最高的 <span class="math inline">\(k\)</span> 个：</p>
<p><span class="math display">\[
\text{TopK}(h) = \{i_1, i_2, \ldots, i_k\} \quad \text{where } h_{i_1} \geq h_{i_2} \geq \cdots \geq h_{i_k}
\]</span></p>
<p><strong>Step 3：计算门控权重</strong></p>
<p>对选中的 <span class="math inline">\(k\)</span> 个 Expert 的得分做 Softmax，得到归一化的权重：</p>
<p><span class="math display">\[
g_j = \frac{\exp(h_{i_j})}{\sum_{l=1}^{k} \exp(h_{i_l})}, \quad j = 1, \ldots, k
\]</span></p>
<p><strong>Step 4：加权组合 Expert 输出</strong></p>
<p>将 token 发送到选中的 <span class="math inline">\(k\)</span> 个 Expert，用门控权重加权组合它们的输出：</p>
<p><span class="math display">\[
y = \sum_{j=1}^{k} g_j \cdot E_{i_j}(x)
\]</span></p>
<p>整个过程可以写成一个统一的公式：</p>
<p><span class="math display">\[
\text{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)
\]</span></p>
<p>其中 <span class="math inline">\(G(x)_i\)</span> 在 <span class="math inline">\(i \notin \text{TopK}(h)\)</span> 时为 0，实现了稀疏激活。</p>
<div id="fig-moe-layer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-moe-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-27/original/fig-mixtral-moe-layer.png" class="img-fluid figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moe-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Mixture of Experts Layer 架构：输入经过 Router 后被路由到多个 Expert 中的 Top-k 个，各 Expert 的输出由 gating weights 加权求和得到最终输出。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Jiang et al.&nbsp;(2024) “Mixtral of Experts”, Figure 1</em></p>
</div>
<hr>
</section>
</section>
<section id="技术细节" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="技术细节"><span class="header-section-number">4</span> 技术细节</h2>
<section id="门控网络的设计" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="门控网络的设计"><span class="header-section-number">4.1</span> 门控网络的设计</h3>
<section id="简单线性门控" class="level4" data-number="4.1.1">
<h4 data-number="4.1.1" class="anchored" data-anchor-id="简单线性门控"><span class="header-section-number">4.1.1</span> 简单线性门控</h4>
<p>最简单的门控网络就是一个线性层加 Softmax：</p>
<p><span class="math display">\[
G(x) = \text{Softmax}(W_g \cdot x)
\]</span></p>
<p>然后取 Top-k。这是 Switch Transformer 采用的设计，简单有效。</p>
</section>
<section id="带噪声的门控" class="level4" data-number="4.1.2">
<h4 data-number="4.1.2" class="anchored" data-anchor-id="带噪声的门控"><span class="header-section-number">4.1.2</span> 带噪声的门控</h4>
<p>Shazeer 2017 的原始设计在门控得分中加入可学习的噪声：</p>
<p><span class="math display">\[
H(x) = W_g \cdot x + \epsilon \cdot \text{Softplus}(W_{noise} \cdot x)
\]</span></p>
<p>其中 <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span> 是标准高斯噪声。</p>
<p>噪声的作用是增加探索性：即使某个 Expert 的得分略低，也有一定概率被选中。这有助于防止路由崩塌。</p>
</section>
<section id="top-k-选择的变体" class="level4" data-number="4.1.3">
<h4 data-number="4.1.3" class="anchored" data-anchor-id="top-k-选择的变体"><span class="header-section-number">4.1.3</span> Top-k 选择的变体</h4>
<p><strong>Top-1 路由（Switch Transformer）</strong>：</p>
<p>每个 token 只选择 1 个 Expert。这是最极端的稀疏形式，计算效率最高，但表达能力可能受限。</p>
<p><span class="math display">\[
\text{Switch}(x) = G(x)_{i^*} \cdot E_{i^*}(x), \quad i^* = \arg\max_i (W_g \cdot x)_i
\]</span></p>
<p><strong>Top-2 路由（GShard, Mixtral）</strong>：</p>
<p>每个 token 选择 2 个 Expert。这是目前最流行的选择，在效率和表达能力之间取得了良好平衡。</p>
<p><strong>Expert Choice 路由</strong>：</p>
<p>一种反转的思路：不是让 token 选择 Expert，而是让 Expert 选择 token。每个 Expert 选择得分最高的若干个 token 来处理。这自动保证了负载均衡，但打破了因果顺序，只能用于编码器。</p>
</section>
</section>
<section id="负载均衡机制" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="负载均衡机制"><span class="header-section-number">4.2</span> 负载均衡机制</h3>
<section id="为什么需要负载均衡" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="为什么需要负载均衡"><span class="header-section-number">4.2.1</span> 为什么需要负载均衡？</h4>
<p>如果让路由器自由学习，它会倾向于将 token 都发送到少数”明星” Expert：</p>
<ol type="1">
<li>某些 Expert 在早期训练中偶然获得了优势</li>
<li>更多 token 被路由到这些 Expert，它们得到更多训练</li>
<li>它们变得更强，吸引更多 token</li>
<li>形成正反馈循环，最终导致路由崩塌</li>
</ol>
<p>路由崩塌的后果是灾难性的：大多数 Expert 几乎从未被使用，模型的有效容量退化到只有几个 Expert 的水平。</p>
</section>
<section id="辅助损失auxiliary-loss" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="辅助损失auxiliary-loss"><span class="header-section-number">4.2.2</span> 辅助损失（Auxiliary Loss）</h4>
<p>最经典的解决方案是在训练损失中加入一个辅助项，惩罚不均匀的 Expert 负载：</p>
<p><span class="math display">\[
\mathcal{L}_{aux} = \alpha \cdot n \cdot \sum_{i=1}^{n} f_i \cdot p_i
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(f_i\)</span> = Expert <span class="math inline">\(i\)</span> 被选中的比例（实际负载）</li>
<li><span class="math inline">\(p_i\)</span> = Expert <span class="math inline">\(i\)</span> 的平均路由概率</li>
<li><span class="math inline">\(\alpha\)</span> 是超参数（通常设为 0.01-0.1）</li>
<li><span class="math inline">\(n\)</span> 是 Expert 数量</li>
</ul>
<p>这个损失在所有 Expert 被均匀使用时最小。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>辅助损失的直觉
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(f_i \cdot p_i\)</span> 的巧妙之处在于：</p>
<ul>
<li><span class="math inline">\(f_i\)</span> 衡量”实际”负载，受 Top-k 选择的离散决策影响</li>
<li><span class="math inline">\(p_i\)</span> 衡量”意愿”负载，是 Softmax 输出的连续值，可微分</li>
</ul>
<p>最小化 <span class="math inline">\(\sum f_i \cdot p_i\)</span> 就是要让高负载的 Expert（<span class="math inline">\(f_i\)</span> 大）降低其吸引力（<span class="math inline">\(p_i\)</span> 小），实现负载再平衡。</p>
</div>
</div>
</section>
<section id="expert-容量限制" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="expert-容量限制"><span class="header-section-number">4.2.3</span> Expert 容量限制</h4>
<p>除了辅助损失，GShard 还引入了”Expert 容量”（Expert Capacity）的概念：</p>
<p><span class="math display">\[
\text{capacity} = \left\lceil \frac{T \cdot k}{n} \cdot \text{capacity\_factor} \right\rceil
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(T\)</span> = batch 中的 token 总数</li>
<li><span class="math inline">\(k\)</span> = 每个 token 选择的 Expert 数</li>
<li><span class="math inline">\(n\)</span> = Expert 总数</li>
<li>capacity_factor = 1.0 到 2.0 的超参数</li>
</ul>
<p>每个 Expert 最多只能处理 <code>capacity</code> 个 token。如果超过这个限制，多余的 token 会”溢出”：</p>
<ul>
<li>直接丢弃（通过残差连接保留原始表示）</li>
<li>或发送到备选 Expert</li>
</ul>
<p>容量限制有两个作用：</p>
<ol type="1">
<li>防止单个 Expert 过载，保证内存使用可预测</li>
<li>强制路由器学会分散 token，减少溢出</li>
</ol>
<div id="fig-token-routing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-token-routing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-27/original/fig-switch-token-routing.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-token-routing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Expert Capacity 和 Token Routing 示意图。左侧展示了术语定义（Experts、Expert Capacity、Capacity Factor）；中间和右侧对比了 Capacity Factor = 1.0 和 1.5 时的 token 分配情况——当容量不足时（左图），溢出的 token（红色虚线）会被丢弃；增大 capacity factor（右图）可以缓解溢出问题。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Fedus et al.&nbsp;(2021) “Switch Transformers”, Figure 3</em></p>
</div>
</section>
<section id="无辅助损失的负载均衡deepseek-v3" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="无辅助损失的负载均衡deepseek-v3"><span class="header-section-number">4.2.4</span> 无辅助损失的负载均衡（DeepSeek-V3）</h4>
<p>辅助损失虽然有效，但会引入额外的梯度干扰，可能损害模型性能。DeepSeek-V3 提出了一种”无辅助损失”的负载均衡策略：</p>
<p><span class="math display">\[
g_i' = g_i + b_i
\]</span></p>
<p>在路由得分上添加一个 Expert 级别的偏置 <span class="math inline">\(b_i\)</span>，根据每个 Expert 的历史负载动态调整：</p>
<ul>
<li>负载过高的 Expert：降低偏置，减少未来被选中的概率</li>
<li>负载过低的 Expert：提高偏置，增加未来被选中的概率</li>
</ul>
<p>这种方法不需要修改训练损失，完全通过推理时的偏置调整来实现平衡。</p>
</section>
</section>
<section id="训练稳定性技巧" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="训练稳定性技巧"><span class="header-section-number">4.3</span> 训练稳定性技巧</h3>
<p>稀疏模型比 Dense 模型更容易出现训练不稳定问题。以下是一些关键技巧：</p>
<section id="使用较低精度训练" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="使用较低精度训练"><span class="header-section-number">4.3.1</span> 使用较低精度训练</h4>
<p>Switch Transformer 首次证明了可以用 BF16 训练大规模 MoE 模型。关键是将路由器的计算保持在高精度（FP32），而 Expert 的计算可以用低精度。</p>
</section>
<section id="更小的参数初始化" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="更小的参数初始化"><span class="header-section-number">4.3.2</span> 更小的参数初始化</h4>
<p>MoE 层的输出是多个 Expert 输出的加权和。如果每个 Expert 的输出方差与 Dense 层相同，加权和的方差会更大。因此需要更小的初始化：</p>
<p><span class="math display">\[
\text{std} = \text{std}_{dense} / \sqrt{k}
\]</span></p>
<p>其中 <span class="math inline">\(k\)</span> 是每个 token 选择的 Expert 数。</p>
</section>
<section id="router-z-loss" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="router-z-loss"><span class="header-section-number">4.3.3</span> Router Z-Loss</h4>
<p>ST-MoE 提出了一种额外的正则化损失，惩罚路由器输出的 logits 过大：</p>
<p><span class="math display">\[
\mathcal{L}_z = \frac{1}{B} \sum_{i=1}^{B} \left( \log \sum_{j=1}^{n} e^{h_{ij}} \right)^2
\]</span></p>
<p>这防止了 Softmax 输入过大导致的数值不稳定。</p>
</section>
</section>
<section id="数值示例top-2-路由计算" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="数值示例top-2-路由计算"><span class="header-section-number">4.4</span> 数值示例：Top-2 路由计算</h3>
<p>让我们用一个具体的数值示例来展示 MoE 的完整计算流程。</p>
<p><strong>设定</strong>：</p>
<ul>
<li>4 个 Expert：<span class="math inline">\(E_1, E_2, E_3, E_4\)</span></li>
<li>Top-k = 2</li>
<li>输入 token 表示：<span class="math inline">\(x \in \mathbb{R}^4\)</span></li>
<li>门控网络权重：<span class="math inline">\(W_g \in \mathbb{R}^{4 \times 4}\)</span></li>
</ul>
<p><strong>Step 1：输入和门控</strong></p>
<p><span class="math display">\[
x = [0.5, -0.2, 0.8, 0.1]
\]</span></p>
<p><span class="math display">\[
W_g = \begin{bmatrix}
0.3 &amp; -0.1 &amp; 0.5 &amp; 0.2 \\
-0.2 &amp; 0.4 &amp; 0.1 &amp; -0.3 \\
0.1 &amp; 0.2 &amp; -0.4 &amp; 0.6 \\
0.4 &amp; -0.3 &amp; 0.2 &amp; 0.1
\end{bmatrix}
\]</span></p>
<p><strong>Step 2：计算路由得分</strong></p>
<p><span class="math display">\[
h = W_g \cdot x = \begin{bmatrix}
0.3 \times 0.5 + (-0.1) \times (-0.2) + 0.5 \times 0.8 + 0.2 \times 0.1 \\
-0.2 \times 0.5 + 0.4 \times (-0.2) + 0.1 \times 0.8 + (-0.3) \times 0.1 \\
0.1 \times 0.5 + 0.2 \times (-0.2) + (-0.4) \times 0.8 + 0.6 \times 0.1 \\
0.4 \times 0.5 + (-0.3) \times (-0.2) + 0.2 \times 0.8 + 0.1 \times 0.1
\end{bmatrix}
= \begin{bmatrix}
0.59 \\
-0.11 \\
-0.21 \\
0.43
\end{bmatrix}
\]</span></p>
<p><strong>Step 3：Top-2 选择</strong></p>
<p>得分排序：<span class="math inline">\(h_1 = 0.59 &gt; h_4 = 0.43 &gt; h_2 = -0.11 &gt; h_3 = -0.21\)</span></p>
<p>选中：<strong>Expert 1</strong> 和 <strong>Expert 4</strong></p>
<p><strong>Step 4：计算门控权重</strong></p>
<p>对选中 Expert 的得分做 Softmax：</p>
<p><span class="math display">\[
g_1 = \frac{e^{0.59}}{e^{0.59} + e^{0.43}} = \frac{1.80}{1.80 + 1.54} = 0.54
\]</span></p>
<p><span class="math display">\[
g_4 = \frac{e^{0.43}}{e^{0.59} + e^{0.43}} = \frac{1.54}{1.80 + 1.54} = 0.46
\]</span></p>
<p><strong>Step 5：加权组合输出</strong></p>
<p>假设两个 Expert 的输出分别为：</p>
<p><span class="math display">\[
E_1(x) = [1.2, -0.3, 0.7, 0.4]
\]</span></p>
<p><span class="math display">\[
E_4(x) = [0.8, 0.5, -0.2, 0.9]
\]</span></p>
<p>最终 MoE 层输出：</p>
<p><span class="math display">\[
y = 0.54 \times E_1(x) + 0.46 \times E_4(x) = [1.02, 0.07, 0.29, 0.63]
\]</span></p>
<p><strong>关键观察</strong>：</p>
<ul>
<li>Expert 2 和 Expert 3 完全没有参与计算</li>
<li>计算量只有完整 4 个 Expert 的 50%</li>
<li>但模型仍然拥有 4 个 Expert 的参数容量</li>
</ul>
<hr>
</section>
</section>
<section id="moe-的演进历程" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="moe-的演进历程"><span class="header-section-number">5</span> MoE 的演进历程</h2>
<section id="早期探索jacobs-1991-与-shazeer-2017" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="早期探索jacobs-1991-与-shazeer-2017"><span class="header-section-number">5.1</span> 早期探索：Jacobs 1991 与 Shazeer 2017</h3>
<p>MoE 的思想最早可追溯到 1991 年 Jacobs 等人的工作，他们将其用于监督学习任务中的函数拟合。但真正让 MoE 在深度学习时代焕发生机的是 Shazeer 2017 的论文。</p>
<p>Shazeer 团队将 MoE 应用于语言建模和机器翻译任务，在 LSTM 层之间插入 MoE 层。他们的关键贡献包括：</p>
<ol type="1">
<li><strong>稀疏门控机制</strong>：只激活 Top-k 个 Expert</li>
<li><strong>1000 倍容量扩展</strong>：在保持计算量可控的情况下，将模型容量提升到 1370 亿参数</li>
<li><strong>辅助损失</strong>：解决负载均衡问题</li>
</ol>
<p>这篇论文证明了 MoE 在大规模语言模型中的可行性，为后续工作奠定了基础。</p>
</section>
<section id="gshard-2020工业级规模化" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="gshard-2020工业级规模化"><span class="header-section-number">5.2</span> GShard (2020)：工业级规模化</h3>
<p>Google 的 GShard 是第一个将 MoE 扩展到 600B 参数规模的工作。</p>
<p><strong>关键贡献</strong>：</p>
<ol type="1">
<li><strong>自动分片</strong>：GShard 是一个编程框架，能够自动将 MoE 模型分布到数千个 TPU 上</li>
<li><strong>Top-2 路由</strong>：选择 2 个 Expert 而非更多，在效率和性能之间取得平衡</li>
<li><strong>Expert 容量限制</strong>：引入 capacity factor 概念，防止 Expert 过载</li>
<li><strong>多语言翻译</strong>：在 100 种语言到英语的翻译任务上取得了显著提升</li>
</ol>
<p>GShard 证明了 MoE 可以在工业规模下稳定训练，是从研究原型到实际应用的关键一步。</p>
</section>
<section id="switch-transformer-2021简化与极致稀疏" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="switch-transformer-2021简化与极致稀疏"><span class="header-section-number">5.3</span> Switch Transformer (2021)：简化与极致稀疏</h3>
<p>Switch Transformer 是 MoE 发展史上的另一个里程碑。它的核心理念是<strong>简化</strong>：</p>
<div id="fig-switch-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-switch-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/chapter-27/original/fig-switch-transformer-architecture.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-switch-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Switch Transformer 架构对比。左侧是标准 Transformer 层结构（将 FFN 替换为 Switching FFN Layer）；右侧展示了 Switch FFN Layer 的内部细节——Router 根据输入为每个 token 选择一个 Expert（如 FFN 2），并用路由概率（如 p=0.65）对 Expert 输出进行加权。
</figcaption>
</figure>
</div>
<div class="figure-caption">
<p><em>Source: Fedus et al.&nbsp;(2021) “Switch Transformers”, Figure 2</em></p>
</div>
<p><strong>Top-1 路由</strong>：每个 token 只选择 1 个 Expert（而非 Top-2 或 Top-4）。</p>
<p>这看起来是一个激进的简化，但论文证明了它不仅不损害质量，还带来了多重好处：</p>
<ul>
<li>路由计算量减少</li>
<li>通信模式更简单</li>
<li>训练更稳定</li>
</ul>
<p><strong>预训练加速</strong>：相比同等计算量的 T5 模型，Switch Transformer 实现了：</p>
<ul>
<li>T5-Base/Large 配置下 7 倍预训练加速</li>
<li>T5-XXL 配置下 4 倍加速</li>
</ul>
<p><strong>训练稳定性技巧</strong>：</p>
<ul>
<li>BF16 训练（路由器保持 FP32）</li>
<li>更小的初始化</li>
<li>更高的 Expert dropout</li>
</ul>
<p><strong>万亿参数模型</strong>：成功训练了 1.6 万亿参数的模型，证明了 MoE 的极限扩展能力。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Algorithm 1: Simplified Routing in Switch Transformer
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> switch_routing(x, W_gate, experts, capacity):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Switch Transformer 的 Top-1 路由</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, seq_len, d_model] 输入表示</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        W_gate: [d_model, n_experts] 门控权重</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        experts: n 个 Expert 网络</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        capacity: 每个 Expert 的最大 token 容量</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        y: [batch_size, seq_len, d_model] 输出</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 计算路由得分</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    router_logits <span class="op">=</span> x <span class="op">@</span> W_gate  <span class="co"># [batch_size, seq_len, n_experts]</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    router_probs <span class="op">=</span> softmax(router_logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Top-1 选择</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    expert_index <span class="op">=</span> argmax(router_probs, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch_size, seq_len]</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    expert_gate <span class="op">=</span> <span class="bu">max</span>(router_probs, dim<span class="op">=-</span><span class="dv">1</span>)       <span class="co"># 对应的门控值</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 按 Expert 分组 token</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_experts):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 找到被路由到 Expert i 的所有 token</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> (expert_index <span class="op">==</span> i)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        tokens_for_expert_i <span class="op">=</span> x[mask]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 容量检查</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(tokens_for_expert_i) <span class="op">&gt;</span> capacity:</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 超出容量的 token 被丢弃（通过残差连接处理）</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            tokens_for_expert_i <span class="op">=</span> tokens_for_expert_i[:capacity]</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            overflow_mask <span class="op">=</span> create_overflow_mask(mask, capacity)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Expert 计算</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        expert_output <span class="op">=</span> experts[i](tokens_for_expert_i)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: 加权并放回原位置</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        y[mask] <span class="op">=</span> expert_gate[mask] <span class="op">*</span> expert_output</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><em>Source: Adapted from Fedus et al.&nbsp;(2021) “Switch Transformers”, Algorithm 1</em></p>
</div>
</div>
</section>
<section id="st-moe-2022稳定性与可迁移性" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="st-moe-2022稳定性与可迁移性"><span class="header-section-number">5.4</span> ST-MoE (2022)：稳定性与可迁移性</h3>
<p>ST-MoE（Stable and Transferable MoE）专注于解决 MoE 模型的两个实际问题：</p>
<ol type="1">
<li><strong>训练稳定性</strong>：如何让 MoE 训练更稳定，减少 loss spike</li>
<li><strong>迁移学习</strong>：如何让预训练的 MoE 模型更好地微调到下游任务</li>
</ol>
<p><strong>关键发现</strong>：</p>
<ul>
<li><strong>Router Z-Loss</strong>：正则化路由器的 logits 大小，防止数值溢出</li>
<li><strong>Expert dropout</strong>：在训练时随机丢弃整个 Expert，增强鲁棒性</li>
<li><strong>微调策略</strong>：MoE 模型在微调时容易过拟合，需要更强的正则化</li>
</ul>
<p>ST-MoE-32B（269B 参数，32B 激活参数）在多个 NLP 基准上首次让稀疏模型超越了同等计算量的 Dense 模型。</p>
</section>
<section id="mixtral-8x7b-2023开源-moe-的里程碑" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="mixtral-8x7b-2023开源-moe-的里程碑"><span class="header-section-number">5.5</span> Mixtral 8x7B (2023)：开源 MoE 的里程碑</h3>
<p>Mixtral 由 Mistral AI 发布，是开源 MoE 模型的标杆。</p>
<p><strong>架构设计</strong>：</p>
<ul>
<li>基于 Mistral 7B 架构</li>
<li>每层 8 个 Expert（替代原 FFN）</li>
<li>Top-2 路由</li>
<li>32K 上下文窗口</li>
<li><strong>总参数</strong>：46.7B</li>
<li><strong>激活参数</strong>：12.9B（每个 token）</li>
</ul>
<p><strong>性能表现</strong>：</p>
<ul>
<li>在大多数基准上匹配或超越 LLaMA-2 70B</li>
<li>在数学、代码生成、多语言任务上显著优于 LLaMA-2 70B</li>
<li>推理成本只有 LLaMA-2 70B 的 ~18%（12.9B vs 70B 激活参数）</li>
</ul>
<p><strong>开源影响</strong>：</p>
<p>Mixtral 以 Apache 2.0 许可开源，让开发者可以自由使用和修改。这大大推动了 MoE 技术的普及和社区研究。</p>
</section>
<section id="deepseek-v2v3-2024极致效率" class="level3" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="deepseek-v2v3-2024极致效率"><span class="header-section-number">5.6</span> DeepSeek-V2/V3 (2024)：极致效率</h3>
<p>DeepSeek 系列代表了 MoE 技术的最新进展，特别是在<strong>计算效率</strong>和<strong>训练成本</strong>上的突破。</p>
<section id="deepseekmoe细粒度专家分割" class="level4" data-number="5.6.1">
<h4 data-number="5.6.1" class="anchored" data-anchor-id="deepseekmoe细粒度专家分割"><span class="header-section-number">5.6.1</span> DeepSeekMoE：细粒度专家分割</h4>
<p>传统 MoE 使用与原 FFN 相同规模的 Expert。DeepSeekMoE 提出将 Expert 分割得更细：</p>
<ul>
<li><strong>原方案</strong>：8 个 Expert，每个与原 FFN 等大，Top-2 激活</li>
<li><strong>细粒度方案</strong>：64 个更小的 Expert，Top-6 激活</li>
</ul>
<p>细粒度 Expert 的好处：</p>
<ul>
<li>更灵活的专家组合</li>
<li>每个 Expert 可以更专业化</li>
<li>路由决策的粒度更细</li>
</ul>
</section>
<section id="共享专家隔离" class="level4" data-number="5.6.2">
<h4 data-number="5.6.2" class="anchored" data-anchor-id="共享专家隔离"><span class="header-section-number">5.6.2</span> 共享专家隔离</h4>
<p>DeepSeek 将 Expert 分为两类：</p>
<ol type="1">
<li><strong>共享专家（Shared Expert）</strong>：每个 token 都会经过，处理通用知识</li>
<li><strong>路由专家（Routed Expert）</strong>：根据 token 内容选择性激活，处理专业知识</li>
</ol>
<p>这种设计让模型既有”基础常识”，又有”专业技能”。</p>
</section>
<section id="deepseek-v3-的规模与效率" class="level4" data-number="5.6.3">
<h4 data-number="5.6.3" class="anchored" data-anchor-id="deepseek-v3-的规模与效率"><span class="header-section-number">5.6.3</span> DeepSeek-V3 的规模与效率</h4>
<p>DeepSeek-V3 是目前最强的开源 MoE 模型之一：</p>
<ul>
<li><strong>总参数</strong>：671B</li>
<li><strong>激活参数</strong>：37B</li>
<li><strong>训练成本</strong>：2.788M H800 GPU hours（约 5.58M 美元）</li>
</ul>
<p>相比之下，类似能力的 Dense 模型（如 LLaMA-3 405B）需要更高的训练成本和推理成本。</p>
<p>DeepSeek-V3 还引入了两项创新：</p>
<ol type="1">
<li><strong>无辅助损失负载均衡</strong>：通过动态偏置而非损失项来平衡负载</li>
<li><strong>多 Token 预测</strong>：训练目标从预测下一个 token 扩展到预测多个 token</li>
</ol>
<hr>
</section>
</section>
</section>
<section id="moe-的核心挑战" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="moe-的核心挑战"><span class="header-section-number">6</span> MoE 的核心挑战</h2>
<section id="负载均衡的根本困难" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="负载均衡的根本困难"><span class="header-section-number">6.1</span> 负载均衡的根本困难</h3>
<p>尽管有辅助损失和容量限制，负载均衡仍然是 MoE 最棘手的问题。</p>
<p><strong>问题本质</strong>：</p>
<ul>
<li>辅助损失太强 → 路由过于均匀，牺牲了专业化（Expert 变成了”通才”）</li>
<li>辅助损失太弱 → 路由崩塌，只有少数 Expert 被使用</li>
</ul>
<p>这是一个根本性的权衡：<strong>负载均衡与专家专业化之间的矛盾</strong>。</p>
<p><strong>Token 溢出问题</strong>：</p>
<p>在实际训练中，即使有负载均衡策略，某些 Expert 仍可能超过容量限制。溢出的 token 会被丢弃或降级处理，导致：</p>
<ul>
<li>信息损失</li>
<li>训练信号不稳定</li>
<li>下游任务性能波动</li>
</ul>
<p><strong>研究方向</strong>：</p>
<ul>
<li>Expert Choice 路由（让 Expert 选择 token 而非反过来）</li>
<li>动态容量调整</li>
<li>无辅助损失方法（如 DeepSeek-V3 的偏置调整）</li>
</ul>
</section>
<section id="训练不稳定性" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="训练不稳定性"><span class="header-section-number">6.2</span> 训练不稳定性</h3>
<p>MoE 模型的训练比 Dense 模型更容易出现以下问题：</p>
<ol type="1">
<li><strong>Loss Spike</strong>：训练过程中突然出现的损失飙升，可能需要从 checkpoint 回退</li>
<li><strong>梯度方差大</strong>：由于稀疏激活，每个 Expert 的梯度来自不同子集的 token，方差更大</li>
<li><strong>Expert 退化</strong>：某些 Expert 可能在训练过程中逐渐失去作用</li>
</ol>
<p><strong>缓解策略</strong>：</p>
<ul>
<li>更保守的学习率</li>
<li>更强的梯度裁剪</li>
<li>Router Z-Loss</li>
<li>更长的 warmup</li>
<li>FP32 路由器 + BF16 Expert</li>
</ul>
</section>
<section id="工程复杂性" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="工程复杂性"><span class="header-section-number">6.3</span> 工程复杂性</h3>
<section id="分布式通信" class="level4" data-number="6.3.1">
<h4 data-number="6.3.1" class="anchored" data-anchor-id="分布式通信"><span class="header-section-number">6.3.1</span> 分布式通信</h4>
<p>在多 GPU 训练中，Expert 通常分布在不同的设备上（Expert Parallelism）。当一个 token 需要被路由到某个 Expert 时：</p>
<ol type="1">
<li>Token 的表示需要发送到 Expert 所在的 GPU</li>
<li>Expert 计算完成后，输出需要发送回原 GPU</li>
</ol>
<p>这涉及 All-to-All 通信模式，通信开销与 Expert 数量和 GPU 数量成正比。</p>
<p><strong>优化策略</strong>：</p>
<ul>
<li><strong>Expert Parallelism + Data Parallelism 混合</strong>：每组 GPU 内共享 Expert，组间做数据并行</li>
<li><strong>批量通信</strong>：收集多个 token 后一次性发送，减少通信次数</li>
<li><strong>本地 Expert 优先</strong>：设计路由偏向于选择同 GPU 上的 Expert</li>
</ul>
</section>
<section id="推理时的-expert-缓存" class="level4" data-number="6.3.2">
<h4 data-number="6.3.2" class="anchored" data-anchor-id="推理时的-expert-缓存"><span class="header-section-number">6.3.2</span> 推理时的 Expert 缓存</h4>
<p>在推理时，不同的请求可能需要不同的 Expert。如果 Expert 分布在多个 GPU 上，需要仔细管理 Expert 的加载和缓存。</p>
<hr>
</section>
</section>
</section>
<section id="深入理解" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="深入理解"><span class="header-section-number">7</span> 深入理解</h2>
<section id="为什么有效理论视角" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="为什么有效理论视角"><span class="header-section-number">7.1</span> 为什么有效？——理论视角</h3>
<section id="moe-的表达能力" class="level4" data-number="7.1.1">
<h4 data-number="7.1.1" class="anchored" data-anchor-id="moe-的表达能力"><span class="header-section-number">7.1.1</span> MoE 的表达能力</h4>
<p>从理论角度，MoE 可以被理解为一种<strong>条件计算</strong>的形式。对于输入 <span class="math inline">\(x\)</span>，MoE 的输出是：</p>
<p><span class="math display">\[
f_{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)
\]</span></p>
<p>这是一个<strong>混合模型</strong>（Mixture Model）的形式。不同的是：</p>
<ul>
<li>传统混合模型的混合权重是固定的或简单的</li>
<li>MoE 的混合权重 <span class="math inline">\(G(x)\)</span> 是输入的复杂函数</li>
</ul>
<p>这让 MoE 能够根据输入自适应地选择”计算路径”，理论上可以表达比 Dense 模型更复杂的函数族。</p>
</section>
<section id="scaling-law-的差异" class="level4" data-number="7.1.2">
<h4 data-number="7.1.2" class="anchored" data-anchor-id="scaling-law-的差异"><span class="header-section-number">7.1.2</span> Scaling Law 的差异</h4>
<p>Dense 模型遵循经典的 Scaling Law：</p>
<p><span class="math display">\[
L \propto N^{-\alpha}
\]</span></p>
<p>其中 <span class="math inline">\(L\)</span> 是损失，<span class="math inline">\(N\)</span> 是参数量，<span class="math inline">\(\alpha\)</span> 约为 0.07。</p>
<p>MoE 模型的 Scaling Law 有所不同。由于参数量和计算量解耦，需要分别考虑：</p>
<ul>
<li><strong>总参数量</strong>：决定模型的”容量”</li>
<li><strong>激活参数量</strong>：决定每次推理的”计算量”</li>
<li><strong>训练 Token 数</strong>：决定模型看过多少数据</li>
</ul>
<p>研究表明，MoE 模型在相同计算预算下可以达到更低的损失，但这种优势随着规模增大而逐渐减弱。</p>
</section>
</section>
<section id="为什么有效实证视角" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="为什么有效实证视角"><span class="header-section-number">7.2</span> 为什么有效？——实证视角</h3>
<section id="expert-专业化的证据" class="level4" data-number="7.2.1">
<h4 data-number="7.2.1" class="anchored" data-anchor-id="expert-专业化的证据"><span class="header-section-number">7.2.1</span> Expert 专业化的证据</h4>
<p>通过分析 MoE 模型中不同 Expert 的激活模式，研究者发现：</p>
<ol type="1">
<li><strong>领域专业化</strong>：某些 Expert 更多处理特定领域的 token（如代码、数学、法语）</li>
<li><strong>语法专业化</strong>：某些 Expert 更多处理特定语法角色的 token（如动词、名词）</li>
<li><strong>位置专业化</strong>：某些 Expert 更多处理特定位置的 token（如句首、句尾）</li>
</ol>
<p>但这种专业化程度因模型和训练策略而异，并非总是清晰可辨。</p>
</section>
<section id="消融实验的发现" class="level4" data-number="7.2.2">
<h4 data-number="7.2.2" class="anchored" data-anchor-id="消融实验的发现"><span class="header-section-number">7.2.2</span> 消融实验的发现</h4>
<p>Switch Transformer 论文中的消融实验揭示：</p>
<ul>
<li><strong>Expert 数量</strong>：增加 Expert 数量带来的收益逐渐递减</li>
<li><strong>Top-k 选择</strong>：Top-1 与 Top-2 在质量上差异不大，但 Top-1 效率更高</li>
<li><strong>容量因子</strong>：1.25 到 2.0 之间较优，太小导致溢出，太大浪费计算</li>
</ul>
</section>
</section>
<section id="方法的边界条件" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="方法的边界条件"><span class="header-section-number">7.3</span> 方法的边界条件</h3>
<section id="什么时候-moe-不如-dense" class="level4" data-number="7.3.1">
<h4 data-number="7.3.1" class="anchored" data-anchor-id="什么时候-moe-不如-dense"><span class="header-section-number">7.3.1</span> 什么时候 MoE 不如 Dense？</h4>
<ol type="1">
<li><strong>小规模场景</strong>：当模型规模较小时，MoE 的路由开销可能超过收益</li>
<li><strong>极端稀疏</strong>：Top-1 路由在某些任务上可能损失表达能力</li>
<li><strong>微调场景</strong>：MoE 模型在微调时更容易过拟合，需要更强的正则化</li>
<li><strong>延迟敏感场景</strong>：Expert Parallelism 引入的通信延迟可能不可接受</li>
</ol>
</section>
<section id="负载不均的根本困境" class="level4" data-number="7.3.2">
<h4 data-number="7.3.2" class="anchored" data-anchor-id="负载不均的根本困境"><span class="header-section-number">7.3.2</span> 负载不均的根本困境</h4>
<p>目前的负载均衡方法都无法完美解决这个问题。辅助损失是一种妥协：</p>
<ul>
<li>它确保了 Expert 被均匀使用</li>
<li>但可能阻止了最优的专业化分工</li>
</ul>
<p>理想的路由应该是：让每个 Expert 专注于自己擅长的 token 类型，同时保证整体负载均衡。这两个目标之间存在内在张力。</p>
</section>
</section>
<section id="开放研究问题" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="开放研究问题"><span class="header-section-number">7.4</span> 开放研究问题</h3>
<ol type="1">
<li><p><strong>最优路由策略</strong>：目前的 Top-k 路由是最优的吗？是否存在更好的路由方式？</p></li>
<li><p><strong>Expert 的可解释性</strong>：每个 Expert 到底学到了什么？如何可视化和理解 Expert 的专业化？</p></li>
<li><p><strong>MoE 与其他稀疏技术的结合</strong>：MoE + 稀疏注意力？MoE + 剪枝？</p></li>
<li><p><strong>动态 Expert 数量</strong>：能否根据输入复杂度动态决定激活多少 Expert？</p></li>
<li><p><strong>MoE 的涌现能力</strong>：MoE 是否有与 Dense 模型不同的涌现行为？</p></li>
</ol>
<hr>
</section>
</section>
<section id="局限性与未解决的问题" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="局限性与未解决的问题"><span class="header-section-number">8</span> 局限性与未解决的问题</h2>
<section id="本方法的局限" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="本方法的局限"><span class="header-section-number">8.1</span> 本方法的局限</h3>
<p><strong>负载均衡仍是难题</strong></p>
<p>尽管有多种技术，在实际训练中仍难以实现完美的负载均衡。这影响了：</p>
<ul>
<li>训练效率（某些 Expert 闲置）</li>
<li>模型质量（某些 Expert 欠训练）</li>
<li>推理效率（需要为最差情况预留资源）</li>
</ul>
<p><strong>工程复杂度高</strong></p>
<p>相比 Dense 模型，MoE 的训练和部署都更复杂：</p>
<ul>
<li>需要 Expert Parallelism 支持</li>
<li>通信模式更复杂</li>
<li>Debug 更困难</li>
<li>推理时需要加载所有 Expert（即使只激活一部分）</li>
</ul>
<p><strong>微调不稳定</strong></p>
<p>MoE 模型在微调时表现不如 Dense 模型稳定。原因可能是：</p>
<ul>
<li>稀疏激活导致部分 Expert 在微调数据上得不到充分更新</li>
<li>路由器需要适应新的数据分布</li>
</ul>
</section>
<section id="这些局限导向了什么" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="这些局限导向了什么"><span class="header-section-number">8.2</span> 这些局限导向了什么？</h3>
<p>MoE 解决了”参数效率”的问题，但引入了新的复杂性。这促使我们思考：</p>
<p><strong>是否有比 MoE 更优雅的条件计算方式？</strong></p>
<p>MoE 的路由是在 FFN 层级别的”粗粒度”选择。能否设计更细粒度的条件计算，比如在神经元级别动态激活？</p>
<p><strong>是否有完全不同的大模型扩展路径？</strong></p>
<p>除了让模型”更大”（更多参数），还有其他方向：</p>
<ul>
<li>让模型”更深入思考”（Test-time compute scaling）</li>
<li>让模型”更好地利用外部知识”（RAG）</li>
<li>让模型”更高效地利用参数”（下一章将讨论的状态空间模型）</li>
</ul>
<hr>
</section>
</section>
<section id="本章小结" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="本章小结"><span class="header-section-number">9</span> 本章小结</h2>
<section id="核心要点回顾" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="核心要点回顾"><span class="header-section-number">9.1</span> 核心要点回顾</h3>
<ol type="1">
<li><p><strong>问题</strong>：Dense Transformer 对每个 token 激活所有参数，存在巨大的计算浪费。模型容量和计算量紧密耦合，难以单独扩展。</p></li>
<li><p><strong>洞察</strong>：不同的 token 可能需要不同的”专业知识”。通过让路由器动态选择 Expert，可以实现<strong>稀疏激活</strong>——用小比例的计算访问大规模的参数。</p></li>
<li><p><strong>方法</strong>：</p>
<ul>
<li>将 FFN 替换为多个 Expert</li>
<li>用门控网络进行 Top-k 路由</li>
<li>用辅助损失和容量限制保证负载均衡</li>
<li>用多种技巧保证训练稳定性</li>
</ul></li>
<li><p><strong>意义</strong>：MoE 实现了<strong>参数量与计算量的解耦</strong>，让我们能够以相对较低的成本训练出参数规模达到万亿级别的模型。这为大语言模型的规模化提供了一条切实可行的路径。</p></li>
</ol>
</section>
<section id="关键公式速查" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="关键公式速查"><span class="header-section-number">9.2</span> 关键公式速查</h3>
<p><strong>MoE 层输出</strong>：</p>
<p><span class="math display">\[
\text{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)
\]</span></p>
<p><strong>门控网络</strong>：</p>
<p><span class="math display">\[
G(x) = \text{TopK-Softmax}(W_g \cdot x)
\]</span></p>
<p><strong>辅助损失（负载均衡）</strong>：</p>
<p><span class="math display">\[
\mathcal{L}_{aux} = \alpha \cdot n \cdot \sum_{i=1}^{n} f_i \cdot p_i
\]</span></p>
<p><strong>Expert 容量</strong>：</p>
<p><span class="math display">\[
\text{capacity} = \left\lceil \frac{T \cdot k}{n} \cdot \text{capacity\_factor} \right\rceil
\]</span></p>
</section>
<section id="思考题" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="思考题"><span class="header-section-number">9.3</span> 思考题</h3>
<ol type="1">
<li><p><strong>[概念理解]</strong> 为什么 MoE 的辅助损失使用 <span class="math inline">\(f_i \cdot p_i\)</span> 而不是简单地最小化 <span class="math inline">\(\text{Var}(f_1, \ldots, f_n)\)</span>？这两种设计有什么区别？</p></li>
<li><p><strong>[数学推导]</strong> 假设一个 MoE 层有 8 个 Expert，每个 Expert 的参数量与原 FFN 相同。如果使用 Top-2 路由，计算：(a) 总参数量相对于原 FFN 的倍数；(b) 每个 token 的计算量相对于原 FFN 的倍数。</p></li>
<li><p><strong>[工程实践]</strong> 使用 Hugging Face 的 <code>transformers</code> 库加载 Mixtral 8x7B 模型，分析不同输入（代码、数学、自然语言）激活的 Expert 分布是否有差异。</p></li>
<li><p><strong>[开放思考]</strong> MoE 通过路由实现稀疏激活，但路由本身也需要计算。如果我们想进一步扩展到数万个 Expert，路由计算本身会成为瓶颈吗？如何解决？</p></li>
</ol>
<hr>
</section>
</section>
<section id="延伸阅读" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="延伸阅读"><span class="header-section-number">10</span> 延伸阅读</h2>
<section id="核心论文必读" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="核心论文必读"><span class="header-section-number">10.1</span> 核心论文（必读）</h3>
<ul>
<li><strong>Shazeer et al.&nbsp;(2017)</strong> “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer”
<ul>
<li>重点读：Section 2 (MoE 架构)、Section 3 (门控机制)</li>
<li>现代 MoE 的奠基之作</li>
</ul></li>
<li><strong>Fedus et al.&nbsp;(2021)</strong> “Switch Transformers: Scaling to Trillion Parameter Models”
<ul>
<li>重点读：Section 2 (简化设计)、Section 4 (训练稳定性)</li>
<li>Top-1 路由的简化与万亿参数模型</li>
</ul></li>
<li><strong>Jiang et al.&nbsp;(2024)</strong> “Mixtral of Experts”
<ul>
<li>重点读：Section 2 (架构细节)、Section 3 (性能评估)</li>
<li>开源 MoE 的里程碑</li>
</ul></li>
</ul>
</section>
<section id="后续发展" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="后续发展"><span class="header-section-number">10.2</span> 后续发展</h3>
<ul>
<li><strong>Zhou et al.&nbsp;(2022)</strong> “Mixture-of-Experts with Expert Choice Routing”
<ul>
<li>反转路由方向：让 Expert 选择 token</li>
</ul></li>
<li><strong>DeepSeek-AI (2024)</strong> “DeepSeek-V3 Technical Report”
<ul>
<li>无辅助损失负载均衡、多 token 预测</li>
</ul></li>
</ul>
</section>
<section id="综述与教程" class="level3" data-number="10.3">
<h3 data-number="10.3" class="anchored" data-anchor-id="综述与教程"><span class="header-section-number">10.3</span> 综述与教程</h3>
<ul>
<li><strong>Hugging Face Blog</strong> “Mixture of Experts Explained” — 优秀的入门教程</li>
<li><strong>Maarten Grootendorst</strong> “A Visual Guide to Mixture of Experts” — 可视化图解</li>
</ul>
</section>
<section id="代码资源" class="level3" data-number="10.4">
<h3 data-number="10.4" class="anchored" data-anchor-id="代码资源"><span class="header-section-number">10.4</span> 代码资源</h3>
<ul>
<li><strong>Hugging Face Transformers</strong>：Mixtral 官方实现</li>
<li><strong>DeepSpeed-MoE</strong>：微软的 MoE 训练框架</li>
<li><strong>Megablocks</strong>：高效 MoE 实现</li>
</ul>
<hr>
</section>
</section>
<section id="历史注脚" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="历史注脚"><span class="header-section-number">11</span> 历史注脚</h2>
<p>MoE 的思想最早可追溯到 1991 年 Robert Jacobs、Michael Jordan 等人的论文 “Adaptive Mixtures of Local Experts”。他们在监督学习的背景下提出了这个概念，用于函数逼近任务。</p>
<p>有趣的是，这篇 1991 年的论文直到 2017 年才被 Noam Shazeer 重新发现并应用于深度学习。Shazeer 在 Google Brain 工作期间，正在思考如何扩展神经网络的规模而不线性增加计算量。他在阅读经典文献时发现了 MoE，意识到这正是他需要的思路。</p>
<p>Shazeer 2017 的论文标题 “Outrageously Large Neural Networks”（大得离谱的神经网络）本身就是一个宣言：他们要证明，通过稀疏激活，神经网络可以拥有比当时任何人想象的都要多得多的参数。这篇论文训练了 1370 亿参数的模型——在 2017 年，这个规模几乎是不可想象的。</p>
<p>Shazeer 后来又参与了 Transformer 的发明（他是 “Attention Is All You Need” 的作者之一），并继续推动 MoE 技术的发展。2021 年的 Switch Transformer 正是他与 William Fedus 等人的合作成果。</p>
<p>如今，MoE 已经从一个研究概念变成了大语言模型的标准架构选择。Mixtral、DeepSeek、Grok（xAI）等模型都采用了 MoE 架构，证明了这条技术路线的成熟和实用性。</p>
<p>从 1991 到 2024，MoE 的演进跨越了 33 年——这再次提醒我们，很多”新”技术其实有着深厚的历史根源，关键是在合适的时机用合适的方式重新发现和应用它们。</p>


<!-- -->

</section>

</main> <!-- /main -->
﻿<script>

// Simple EN / 中文 language toggle for posts; robust via meta[quarto:offset]

(function() {

  const KEY = 'siteLang'; // 'en' | 'zh'

  const defaultLang = 'en';

  const POSTS_EN = 'posts_en.html';

  const POSTS_ZH = 'posts_zh.html';

  const TAGS = 'tags.html';



  function currentLang() { try { return localStorage.getItem(KEY) || defaultLang; } catch(e) { return defaultLang; } }

  function setLang(v) { try { localStorage.setItem(KEY, v); } catch(e) {} }

  function offset() {

    const meta = document.querySelector('meta[name="quarto:offset"]');

    const off = meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

    return off;

  }

  function targetFor(lang) { return lang === 'zh' ? POSTS_ZH : POSTS_EN; }

  function goToLang(lang) {

    const off = offset();

    const path = window.location.pathname;

    setLang(lang);

    if (path.endsWith('/' + TAGS) || path.endsWith(TAGS)) {

      window.location.href = off + TAGS;

    } else {

      window.location.href = off + targetFor(lang);

    }

  }

  function updateNavbarPostsLink() {

    const off = offset();

    const href = off + targetFor(currentLang());

    const links = document.querySelectorAll('header .navbar a.nav-link');

    links.forEach((a) => {

      const h = a.getAttribute('href') || '';

      if (h.endsWith(POSTS_EN) || h.endsWith(POSTS_ZH)) a.setAttribute('href', href);

    });

  }

  function mountToggle() {

    const tools = document.querySelector('.quarto-navbar-tools');

    if (!tools) return;

    const wrapper = document.createElement('div');

    wrapper.style.display = 'inline-flex';

    wrapper.style.alignItems = 'center';

    wrapper.style.gap = '0.35rem';

    wrapper.style.marginLeft = '0.35rem';



    const en = document.createElement('a');

    en.href = '';

    en.textContent = 'EN';

    en.className = 'quarto-navigation-tool px-1';

    en.onclick = function(){ goToLang('en'); return false; };



    const sep = document.createElement('span');

    sep.textContent = '|';

    sep.style.opacity = '0.6';



    const zh = document.createElement('a');

    zh.href = '';

    zh.textContent = '中文';

    zh.className = 'quarto-navigation-tool px-1';

    zh.onclick = function(){ goToLang('zh'); return false; };



    const lang = currentLang();

    (lang === 'en' ? en : zh).style.fontWeight = '700';



    wrapper.appendChild(en);

    wrapper.appendChild(sep);

    wrapper.appendChild(zh);

    tools.appendChild(wrapper);

    updateNavbarPostsLink();

  }

  document.addEventListener('DOMContentLoaded', mountToggle);

})();

</script>

<script>

(function(){

  function offset(){

    var meta = document.querySelector('meta[name="quarto:offset"]');

    return meta && meta.getAttribute('content') ? meta.getAttribute('content') : './';

  }

  document.addEventListener('DOMContentLoaded', function(){

    var brand = document.querySelector('header .navbar a.navbar-brand');

    if (brand) {

      brand.setAttribute('href', offset() + 'home.html');

    }

  });

})();

</script>



<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "第27章：Mixture of Experts——稀疏激活的智慧"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Conditional Computation: Scaling Parameters Without Scaling Compute"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Ying Zha"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2026-01-28"</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [NLP, Deep Learning, LLM, MoE, 稀疏模型, 条件计算]</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span><span class="co"> [MoE, Mixture of Experts, Switch Transformer, Mixtral, DeepSeek, GShard, 稀疏激活, 路由, Expert, 负载均衡]</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Dense Transformer有一个根本性的浪费：对于每一个token，所有参数都会被激活。一个关于烹饪的token真的需要激活'数学知识'的参数吗？Mixture of Experts (MoE) 提供了一种优雅的解决方案：将FFN层拆分为多个Expert，让Router动态选择每个token应该走哪几个Expert。这种稀疏激活让模型可以拥有数千亿的总参数，但每次推理只激活其中一小部分——实现了参数量与计算量的解耦。从Shazeer 2017的开创性工作，到Switch Transformer的简化设计，再到Mixtral 8x7B和DeepSeek-V3的工业级成功，MoE正在成为大语言模型的标准架构选择。"</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figures/chapter-27/original/fig-switch-transformer-architecture.png"</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-cap-location: bottom</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **核心问题**：Dense Transformer对每个token激活所有参数，这种"大锅饭"式的计算是否存在浪费？能否让不同的token使用不同的参数子集，从而在保持计算量的同时大幅提升模型容量？</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **历史坐标**：2017–2024 </span><span class="pp">|</span><span class="at"> Shazeer (2017) → GShard (2020) → Switch Transformer (2021) → ST-MoE (2022) → Mixtral (2023) → DeepSeek-V3 (2024) </span><span class="pp">|</span><span class="at"> 从研究原型到工业级部署</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章参考来源</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### 论文</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Shazeer et al. (2017)** "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer" (<span class="co">[</span><span class="ot">arXiv:1701.06538</span><span class="co">](https://arxiv.org/abs/1701.06538)</span>) — 参考了 Section 2-3 (MoE架构设计、门控机制)；ICLR 2017，现代MoE的奠基之作</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lepikhin et al. (2020)** "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding" (<span class="co">[</span><span class="ot">arXiv:2006.16668</span><span class="co">](https://arxiv.org/abs/2006.16668)</span>) — 参考了 Section 2-3 (大规模MoE训练、专家容量限制)；第一个600B参数的MoE模型</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fedus et al. (2021)** "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity" (<span class="co">[</span><span class="ot">arXiv:2101.03961</span><span class="co">](https://arxiv.org/abs/2101.03961)</span>) — 参考了 Section 2-4 (Top-1路由简化、训练稳定性技巧)、Figure 1-4；JMLR 2022</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zoph et al. (2022)** "ST-MoE: Designing Stable and Transferable Sparse Expert Models" (<span class="co">[</span><span class="ot">arXiv:2202.08906</span><span class="co">](https://arxiv.org/abs/2202.08906)</span>) — 参考了 Section 2-4 (训练稳定性最佳实践、微调策略)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Jiang et al. (2024)** "Mixtral of Experts" (<span class="co">[</span><span class="ot">arXiv:2401.04088</span><span class="co">](https://arxiv.org/abs/2401.04088)</span>) — 参考了 Section 2-3 (Mixtral架构细节、性能评估)；开源MoE的里程碑</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dai et al. (2024)** "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models" (<span class="co">[</span><span class="ot">arXiv:2401.06066</span><span class="co">](https://arxiv.org/abs/2401.06066)</span>) — 参考了 Section 2-3 (细粒度专家分割、共享专家隔离)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepSeek-AI (2024)** "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model" (<span class="co">[</span><span class="ot">arXiv:2405.04434</span><span class="co">](https://arxiv.org/abs/2405.04434)</span>) — 参考了架构设计</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepSeek-AI (2024)** "DeepSeek-V3 Technical Report" (<span class="co">[</span><span class="ot">arXiv:2412.19437</span><span class="co">](https://arxiv.org/abs/2412.19437)</span>) — 参考了 Section 2-3 (无辅助损失负载均衡、多token预测)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="fu">### 教材与教程</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Blog** "Mixture of Experts Explained" — 参考了MoE基础概念的可视化解释</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Maarten Grootendorst** "A Visual Guide to Mixture of Experts (MoE)" — 参考了MoE可视化图解</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="fu">### 课程</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stanford CS25** "Mixture of Experts Paradigm and the Switch Transformer" — Barret Zoph 的 MoE 讲座</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## 从上一章说起</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>上一章我们深入探讨了长上下文与高效推理的完整技术栈——从位置编码的演进（RoPE、ALiBi）到长度外推技术（Position Interpolation、YaRN），从 FlashAttention 对注意力计算的 IO 感知革命到 PagedAttention 的 KV Cache 内存管理。这些技术突破了 $O(n^2)$ 的瓶颈，让大语言模型能够处理从 512 token 到百万 token 的超长序列。</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>然而，长上下文解决的是"如何让模型看得更远"的问题，它假设模型本身已经足够强大。但模型的能力来自哪里？答案是**参数**——更多的参数意味着更大的模型容量，能够存储更多的知识和学习更复杂的模式。</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>这里存在一个根本性的矛盾：**参数越多，计算越慢**。</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>一个 70B 参数的模型在推理时，每个 token 都需要经过所有 70B 参数的计算。如果我们想要 700B 参数的模型呢？计算量直接增加 10 倍。如果我们想要 7T（万亿）参数呢？这在传统架构下几乎是不可能的。</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>但这种"所有参数服务每个 token"的设计真的合理吗？让我们仔细思考一下。</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>当模型处理一个关于"如何做番茄炒蛋"的 token 时，它真的需要激活那些编码了微积分公式的参数吗？当模型在翻译一句法语句子时，它真的需要激活那些专门处理代码语法的参数吗？直觉告诉我们，不同类型的输入可能需要不同的"专业知识"——而模型中大量的参数可能在处理特定输入时是"冗余"的。</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>这就是 **Dense Transformer 的第一个根本挑战：所有参数对每个 token 都激活，存在巨大的计算浪费**。</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>2017 年，Google 的 Noam Shazeer 等人提出了一个大胆的想法：**让不同的 token 使用不同的参数子集**。他们将模型中的一部分参数（具体来说是 FFN 层）拆分成多个"专家"（Expert），然后用一个"门控网络"（Gating Network）来决定每个 token 应该被发送到哪个专家处理。</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>这就是 **Mixture of Experts（MoE）** 的核心思想：用条件计算（conditional computation）实现稀疏激活（sparse activation），从而在不增加计算量的情况下大幅提升模型容量。</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 💡 **本章核心洞察**：MoE 实现了参数量与计算量的解耦——模型可以拥有数千亿甚至万亿参数，但每次推理只激活其中一小部分。这种"专家分工"的设计让我们能够以相对较低的计算成本训练出容量巨大的模型，为大语言模型的规模化提供了一条全新的路径。</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="fu">## 问题的本质是什么？</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dense Model 的参数效率困境</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>让我们用具体数字来理解 Dense Transformer 的问题。</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>一个标准的 Transformer 层包含两个主要组件：自注意力（Self-Attention）和前馈网络（FFN）。对于一个隐藏维度为 $d$ 的模型，典型的 FFN 设计是：</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 \cdot x + b_1) + b_2</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>其中 $W_1 \in \mathbb{R}^{4d \times d}$，$W_2 \in \mathbb{R}^{d \times 4d}$。这意味着 FFN 层的参数量约为 $8d^2$，在一个标准 Transformer 层中占据约 2/3 的参数。</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>以 LLaMA-2 70B 为例：</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>隐藏维度 $d = 8192$</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>FFN 中间维度 $d_{ff} = 28672$（约 3.5 倍）</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>80 层 Transformer</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**每层 FFN 参数量**：$2 \times 8192 \times 28672 \approx 470M$</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**全部 FFN 参数量**：$470M \times 80 \approx 37.6B$</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>这 37.6B 参数中的每一个，在处理每一个 token 时都会参与计算。一个关于"法国首都是巴黎"的 token，和一个关于"Python 列表推导式语法"的 token，使用的是**完全相同**的 37.6B 参数。</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>这合理吗？</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="fu">### 条件计算的直觉</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>条件计算（Conditional Computation）的核心思想是：**让输入决定使用哪些参数**。</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>最极端的例子是决策树：每个输入沿着一条从根到叶的路径，只使用路径上的参数，完全不接触其他分支的参数。决策树是 100% 稀疏的——每个样本只用到整个模型的一小部分。</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>但决策树的问题是它的表达能力有限，而且是"硬决策"——一旦选择了某个分支就不能回头。</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>神经网络的优势在于它的"软决策"——每一层都可以做连续的、可微分的变换。那么，能否设计一种既保留神经网络的表达能力，又实现部分稀疏激活的架构呢？</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>这就是 Mixture of Experts 的出发点。</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### MoE 的核心思想</span></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>Mixture of Experts 的基本思想可以追溯到 1991 年 Robert Jacobs 等人的工作，但真正将 MoE 带入深度学习时代的是 Shazeer 2017 年的论文。</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>核心设计如下：</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**将 FFN 层替换为多个"专家"**：不再使用单个 FFN，而是使用 $n$ 个并行的 FFN（称为 Expert），每个 Expert 的结构与原 FFN 相同</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**添加一个"路由器"**：一个轻量级的门控网络（Router / Gating Network），根据输入决定将每个 token 发送到哪个（些）Expert</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**稀疏激活**：每个 token 只被发送到 $k$ 个 Expert（$k \ll n$），其他 Expert 完全不参与这个 token 的计算</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>假设我们有 8 个 Expert，每个 token 只选择 2 个。那么：</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数量**：8 倍于原始 FFN</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**每个 token 的计算量**：只有 2/8 = 25% 的增加（2 个 Expert 替代 1 个 FFN）</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**有效模型容量**：8 倍</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>这就是 MoE 的魔法：**参数量可以任意扩展，而计算量只与激活的专家数有关**。</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a><span class="fu">### 我们需要解决什么问题？</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>MoE 的概念听起来简单优雅，但实际实现中有几个关键问题需要解决。</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>**路由机制设计**：如何决定每个 token 应该被发送到哪个 Expert？最直接的方法是用一个小型神经网络（门控网络）来预测每个 Expert 的"适合度"得分，然后选择得分最高的 $k$ 个。但这个选择是离散的，如何让它可微分？如何保证梯度能够回传？</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>**负载均衡**：如果让模型自由学习路由，它会倾向于把大多数 token 都发送到少数几个"热门" Expert，导致这些 Expert 过载，而其他 Expert 几乎从未被使用——这被称为"路由崩塌"（Routing Collapse）。如何确保所有 Expert 都能得到均匀的训练？</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>**训练稳定性**：稀疏模型的训练比 Dense 模型更不稳定。当某些 Expert 处理的 token 数量波动剧烈时，梯度的方差会很大。如何保证训练过程的稳定性？</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>**通信开销**：在分布式训练中，不同的 Expert 可能位于不同的 GPU 上。当一个 token 需要被路由到某个 Expert 时，可能需要跨 GPU 通信。如何设计高效的通信模式？</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>这些问题在过去 7 年的 MoE 研究中被逐步解决，形成了今天我们看到的成熟技术方案。</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a><span class="fu">## 核心思想与直觉</span></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### 从"通才"到"专家分工"</span></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>理解 MoE 的最好方式是用一个现实世界的类比。</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>想象一家医院。传统的做法是培养"全科医生"——每个医生都掌握所有医学知识，能够处理所有类型的疾病。这意味着每个医生需要学习的东西非常多（参数量大），每次问诊都需要调动全部知识（计算量大）。</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>MoE 的做法是引入"专科分诊"制度：</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**专家（Expert）**：医院有内科、外科、儿科、眼科等多个专科，每个专科只专注于自己的领域</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**路由器（Router）**：相当于"分诊台"，根据病人的症状决定将病人发送到哪个专科</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**稀疏激活**：每个病人只需要看 1-2 个专科医生，不需要每个专科都走一遍</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>这样设计的好处是：</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总容量大**：医院可以拥有各个领域的顶级专家，覆盖所有可能的疾病</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**单次效率高**：每个病人只需要看相关的专家，不需要浪费时间在无关的科室</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**专业化深入**：每个专家只专注于自己的领域，可以积累更深的专业知识</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### Token 级别的专家选择</span></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>MoE 的一个关键设计决策是：**路由发生在 token 级别，而不是 sequence 级别**。</span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>这意味着同一个句子中的不同 token 可能被路由到完全不同的 Expert。比如在处理"我用 Python 写了一个冒泡排序算法"这句话时：</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"我"、"用"、"写"、"了"、"一个" 可能被路由到通用语言理解的 Expert</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Python"、"冒泡排序"、"算法" 可能被路由到编程相关的 Expert</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>这种 token 级别的细粒度路由让模型能够灵活地组合不同 Expert 的能力来处理复杂的输入。</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### 门控机制的数学形式</span></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>让我们用数学语言精确描述 MoE 层的工作方式。</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>假设我们有 $n$ 个 Expert，分别记为 $E_1, E_2, \ldots, E_n$。对于输入 token 的表示 $x \in \mathbb{R}^d$：</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>**Step 1：计算路由得分**</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>门控网络 $G$ 计算每个 Expert 的"适合度"得分：</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>h = W_g \cdot x, \quad h \in \mathbb{R}^n</span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>其中 $W_g \in \mathbb{R}^{n \times d}$ 是门控网络的参数（一个简单的线性层）。</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>**Step 2：选择 Top-k Expert**</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>从 $n$ 个得分中选择最高的 $k$ 个：</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>\text{TopK}(h) = <span class="sc">\{</span>i_1, i_2, \ldots, i_k<span class="sc">\}</span> \quad \text{where } h_{i_1} \geq h_{i_2} \geq \cdots \geq h_{i_k}</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>**Step 3：计算门控权重**</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>对选中的 $k$ 个 Expert 的得分做 Softmax，得到归一化的权重：</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>g_j = \frac{\exp(h_{i_j})}{\sum_{l=1}^{k} \exp(h_{i_l})}, \quad j = 1, \ldots, k</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>**Step 4：加权组合 Expert 输出**</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>将 token 发送到选中的 $k$ 个 Expert，用门控权重加权组合它们的输出：</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>y = \sum_{j=1}^{k} g_j \cdot E_{i_j}(x)</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>整个过程可以写成一个统一的公式：</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>\text{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>其中 $G(x)_i$ 在 $i \notin \text{TopK}(h)$ 时为 0，实现了稀疏激活。</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a><span class="al">![Mixture of Experts Layer 架构：输入经过 Router 后被路由到多个 Expert 中的 Top-k 个，各 Expert 的输出由 gating weights 加权求和得到最终输出。](figures/chapter-27/original/fig-mixtral-moe-layer.png)</span>{#fig-moe-layer width=85%}</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>*Source: Jiang et al. (2024) "Mixtral of Experts", Figure 1*</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a><span class="fu">## 技术细节</span></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a><span class="fu">### 门控网络的设计</span></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 简单线性门控</span></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>最简单的门控网络就是一个线性层加 Softmax：</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a>G(x) = \text{Softmax}(W_g \cdot x)</span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>然后取 Top-k。这是 Switch Transformer 采用的设计，简单有效。</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 带噪声的门控</span></span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a>Shazeer 2017 的原始设计在门控得分中加入可学习的噪声：</span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>H(x) = W_g \cdot x + \epsilon \cdot \text{Softplus}(W_{noise} \cdot x)</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>其中 $\epsilon \sim \mathcal{N}(0, 1)$ 是标准高斯噪声。</span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>噪声的作用是增加探索性：即使某个 Expert 的得分略低，也有一定概率被选中。这有助于防止路由崩塌。</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Top-k 选择的变体</span></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a>**Top-1 路由（Switch Transformer）**：</span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>每个 token 只选择 1 个 Expert。这是最极端的稀疏形式，计算效率最高，但表达能力可能受限。</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>\text{Switch}(x) = G(x)_{i^*} \cdot E_{i^*}(x), \quad i^* = \arg\max_i (W_g \cdot x)_i</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a>**Top-2 路由（GShard, Mixtral）**：</span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>每个 token 选择 2 个 Expert。这是目前最流行的选择，在效率和表达能力之间取得了良好平衡。</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>**Expert Choice 路由**：</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a>一种反转的思路：不是让 token 选择 Expert，而是让 Expert 选择 token。每个 Expert 选择得分最高的若干个 token 来处理。这自动保证了负载均衡，但打破了因果顺序，只能用于编码器。</span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a><span class="fu">### 负载均衡机制</span></span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 为什么需要负载均衡？</span></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>如果让路由器自由学习，它会倾向于将 token 都发送到少数"明星" Expert：</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>某些 Expert 在早期训练中偶然获得了优势</span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>更多 token 被路由到这些 Expert，它们得到更多训练</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>它们变得更强，吸引更多 token</span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>形成正反馈循环，最终导致路由崩塌</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a>路由崩塌的后果是灾难性的：大多数 Expert 几乎从未被使用，模型的有效容量退化到只有几个 Expert 的水平。</span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 辅助损失（Auxiliary Loss）</span></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a>最经典的解决方案是在训练损失中加入一个辅助项，惩罚不均匀的 Expert 负载：</span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{aux} = \alpha \cdot n \cdot \sum_{i=1}^{n} f_i \cdot p_i</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>其中：</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f_i$ = Expert $i$ 被选中的比例（实际负载）</span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p_i$ = Expert $i$ 的平均路由概率</span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha$ 是超参数（通常设为 0.01-0.1）</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$n$ 是 Expert 数量</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a>这个损失在所有 Expert 被均匀使用时最小。</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a><span class="fu">## 辅助损失的直觉</span></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a>$f_i \cdot p_i$ 的巧妙之处在于：</span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f_i$ 衡量"实际"负载，受 Top-k 选择的离散决策影响</span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p_i$ 衡量"意愿"负载，是 Softmax 输出的连续值，可微分</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>最小化 $\sum f_i \cdot p_i$ 就是要让高负载的 Expert（$f_i$ 大）降低其吸引力（$p_i$ 小），实现负载再平衡。</span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Expert 容量限制</span></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>除了辅助损失，GShard 还引入了"Expert 容量"（Expert Capacity）的概念：</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a>\text{capacity} = \left\lceil \frac{T \cdot k}{n} \cdot \text{capacity<span class="sc">\_</span>factor} \right\rceil</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>其中：</span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$T$ = batch 中的 token 总数</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k$ = 每个 token 选择的 Expert 数</span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$n$ = Expert 总数</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>capacity_factor = 1.0 到 2.0 的超参数</span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a>每个 Expert 最多只能处理 <span class="in">`capacity`</span> 个 token。如果超过这个限制，多余的 token 会"溢出"：</span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>直接丢弃（通过残差连接保留原始表示）</span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>或发送到备选 Expert</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a>容量限制有两个作用：</span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>防止单个 Expert 过载，保证内存使用可预测</span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>强制路由器学会分散 token，减少溢出</span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a><span class="al">![Expert Capacity 和 Token Routing 示意图。左侧展示了术语定义（Experts、Expert Capacity、Capacity Factor）；中间和右侧对比了 Capacity Factor = 1.0 和 1.5 时的 token 分配情况——当容量不足时（左图），溢出的 token（红色虚线）会被丢弃；增大 capacity factor（右图）可以缓解溢出问题。](figures/chapter-27/original/fig-switch-token-routing.png)</span>{#fig-token-routing width=95%}</span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>*Source: Fedus et al. (2021) "Switch Transformers", Figure 3*</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 无辅助损失的负载均衡（DeepSeek-V3）</span></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a>辅助损失虽然有效，但会引入额外的梯度干扰，可能损害模型性能。DeepSeek-V3 提出了一种"无辅助损失"的负载均衡策略：</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>g_i' = g_i + b_i</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>在路由得分上添加一个 Expert 级别的偏置 $b_i$，根据每个 Expert 的历史负载动态调整：</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>负载过高的 Expert：降低偏置，减少未来被选中的概率</span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>负载过低的 Expert：提高偏置，增加未来被选中的概率</span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a>这种方法不需要修改训练损失，完全通过推理时的偏置调整来实现平衡。</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练稳定性技巧</span></span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>稀疏模型比 Dense 模型更容易出现训练不稳定问题。以下是一些关键技巧：</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 使用较低精度训练</span></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a>Switch Transformer 首次证明了可以用 BF16 训练大规模 MoE 模型。关键是将路由器的计算保持在高精度（FP32），而 Expert 的计算可以用低精度。</span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 更小的参数初始化</span></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a>MoE 层的输出是多个 Expert 输出的加权和。如果每个 Expert 的输出方差与 Dense 层相同，加权和的方差会更大。因此需要更小的初始化：</span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a>\text{std} = \text{std}_{dense} / \sqrt{k}</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>其中 $k$ 是每个 token 选择的 Expert 数。</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Router Z-Loss</span></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>ST-MoE 提出了一种额外的正则化损失，惩罚路由器输出的 logits 过大：</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_z = \frac{1}{B} \sum_{i=1}^{B} \left( \log \sum_{j=1}^{n} e^{h_{ij}} \right)^2</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a>这防止了 Softmax 输入过大导致的数值不稳定。</span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a><span class="fu">### 数值示例：Top-2 路由计算</span></span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a>让我们用一个具体的数值示例来展示 MoE 的完整计算流程。</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a>**设定**：</span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>4 个 Expert：$E_1, E_2, E_3, E_4$</span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Top-k = 2</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>输入 token 表示：$x \in \mathbb{R}^4$</span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>门控网络权重：$W_g \in \mathbb{R}^{4 \times 4}$</span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a>**Step 1：输入和门控**</span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a>x = <span class="co">[</span><span class="ot">0.5, -0.2, 0.8, 0.1</span><span class="co">]</span></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a>W_g = \begin{bmatrix}</span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>0.3 &amp; -0.1 &amp; 0.5 &amp; 0.2 <span class="sc">\\</span></span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a>-0.2 &amp; 0.4 &amp; 0.1 &amp; -0.3 <span class="sc">\\</span></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>0.1 &amp; 0.2 &amp; -0.4 &amp; 0.6 <span class="sc">\\</span></span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a>0.4 &amp; -0.3 &amp; 0.2 &amp; 0.1</span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a>**Step 2：计算路由得分**</span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>h = W_g \cdot x = \begin{bmatrix}</span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a>0.3 \times 0.5 + (-0.1) \times (-0.2) + 0.5 \times 0.8 + 0.2 \times 0.1 <span class="sc">\\</span></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>-0.2 \times 0.5 + 0.4 \times (-0.2) + 0.1 \times 0.8 + (-0.3) \times 0.1 <span class="sc">\\</span></span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a>0.1 \times 0.5 + 0.2 \times (-0.2) + (-0.4) \times 0.8 + 0.6 \times 0.1 <span class="sc">\\</span></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a>0.4 \times 0.5 + (-0.3) \times (-0.2) + 0.2 \times 0.8 + 0.1 \times 0.1</span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a>= \begin{bmatrix}</span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a>0.59 <span class="sc">\\</span></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a>-0.11 <span class="sc">\\</span></span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>-0.21 <span class="sc">\\</span></span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a>0.43</span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>**Step 3：Top-2 选择**</span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a>得分排序：$h_1 = 0.59 &gt; h_4 = 0.43 &gt; h_2 = -0.11 &gt; h_3 = -0.21$</span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>选中：**Expert 1** 和 **Expert 4**</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>**Step 4：计算门控权重**</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>对选中 Expert 的得分做 Softmax：</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a>g_1 = \frac{e^{0.59}}{e^{0.59} + e^{0.43}} = \frac{1.80}{1.80 + 1.54} = 0.54</span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a>g_4 = \frac{e^{0.43}}{e^{0.59} + e^{0.43}} = \frac{1.54}{1.80 + 1.54} = 0.46</span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a>**Step 5：加权组合输出**</span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>假设两个 Expert 的输出分别为：</span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a>E_1(x) = <span class="co">[</span><span class="ot">1.2, -0.3, 0.7, 0.4</span><span class="co">]</span></span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a>E_4(x) = <span class="co">[</span><span class="ot">0.8, 0.5, -0.2, 0.9</span><span class="co">]</span></span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a>最终 MoE 层输出：</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a>y = 0.54 \times E_1(x) + 0.46 \times E_4(x) = <span class="co">[</span><span class="ot">1.02, 0.07, 0.29, 0.63</span><span class="co">]</span></span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>**关键观察**：</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Expert 2 和 Expert 3 完全没有参与计算</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算量只有完整 4 个 Expert 的 50%</span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>但模型仍然拥有 4 个 Expert 的参数容量</span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## MoE 的演进历程</span></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a><span class="fu">### 早期探索：Jacobs 1991 与 Shazeer 2017</span></span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a>MoE 的思想最早可追溯到 1991 年 Jacobs 等人的工作，他们将其用于监督学习任务中的函数拟合。但真正让 MoE 在深度学习时代焕发生机的是 Shazeer 2017 的论文。</span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a>Shazeer 团队将 MoE 应用于语言建模和机器翻译任务，在 LSTM 层之间插入 MoE 层。他们的关键贡献包括：</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**稀疏门控机制**：只激活 Top-k 个 Expert</span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**1000 倍容量扩展**：在保持计算量可控的情况下，将模型容量提升到 1370 亿参数</span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**辅助损失**：解决负载均衡问题</span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a>这篇论文证明了 MoE 在大规模语言模型中的可行性，为后续工作奠定了基础。</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a><span class="fu">### GShard (2020)：工业级规模化</span></span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a>Google 的 GShard 是第一个将 MoE 扩展到 600B 参数规模的工作。</span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a>**关键贡献**：</span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**自动分片**：GShard 是一个编程框架，能够自动将 MoE 模型分布到数千个 TPU 上</span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Top-2 路由**：选择 2 个 Expert 而非更多，在效率和性能之间取得平衡</span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Expert 容量限制**：引入 capacity factor 概念，防止 Expert 过载</span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**多语言翻译**：在 100 种语言到英语的翻译任务上取得了显著提升</span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a>GShard 证明了 MoE 可以在工业规模下稳定训练，是从研究原型到实际应用的关键一步。</span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a><span class="fu">### Switch Transformer (2021)：简化与极致稀疏</span></span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a>Switch Transformer 是 MoE 发展史上的另一个里程碑。它的核心理念是**简化**：</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a><span class="al">![Switch Transformer 架构对比。左侧是标准 Transformer 层结构（将 FFN 替换为 Switching FFN Layer）；右侧展示了 Switch FFN Layer 的内部细节——Router 根据输入为每个 token 选择一个 Expert（如 FFN 2），并用路由概率（如 p=0.65）对 Expert 输出进行加权。](figures/chapter-27/original/fig-switch-transformer-architecture.png)</span>{#fig-switch-arch width=95%}</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a>::: {.figure-caption}</span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a>*Source: Fedus et al. (2021) "Switch Transformers", Figure 2*</span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a>**Top-1 路由**：每个 token 只选择 1 个 Expert（而非 Top-2 或 Top-4）。</span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a>这看起来是一个激进的简化，但论文证明了它不仅不损害质量，还带来了多重好处：</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>路由计算量减少</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>通信模式更简单</span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>训练更稳定</span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a>**预训练加速**：相比同等计算量的 T5 模型，Switch Transformer 实现了：</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>T5-Base/Large 配置下 7 倍预训练加速</span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>T5-XXL 配置下 4 倍加速</span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a>**训练稳定性技巧**：</span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>BF16 训练（路由器保持 FP32）</span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>更小的初始化</span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>更高的 Expert dropout</span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a>**万亿参数模型**：成功训练了 1.6 万亿参数的模型，证明了 MoE 的极限扩展能力。</span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algorithm 1: Simplified Routing in Switch Transformer</span></span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> switch_routing(x, W_gate, experts, capacity):</span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a><span class="co">    Switch Transformer 的 Top-1 路由</span></span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a><span class="co">        x: [batch_size, seq_len, d_model] 输入表示</span></span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a><span class="co">        W_gate: [d_model, n_experts] 门控权重</span></span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a><span class="co">        experts: n 个 Expert 网络</span></span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a><span class="co">        capacity: 每个 Expert 的最大 token 容量</span></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a><span class="co">        y: [batch_size, seq_len, d_model] 输出</span></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: 计算路由得分</span></span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a>    router_logits <span class="op">=</span> x <span class="op">@</span> W_gate  <span class="co"># [batch_size, seq_len, n_experts]</span></span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a>    router_probs <span class="op">=</span> softmax(router_logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Top-1 选择</span></span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a>    expert_index <span class="op">=</span> argmax(router_probs, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch_size, seq_len]</span></span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a>    expert_gate <span class="op">=</span> <span class="bu">max</span>(router_probs, dim<span class="op">=-</span><span class="dv">1</span>)       <span class="co"># 对应的门控值</span></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: 按 Expert 分组 token</span></span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_experts):</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 找到被路由到 Expert i 的所有 token</span></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> (expert_index <span class="op">==</span> i)</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a>        tokens_for_expert_i <span class="op">=</span> x[mask]</span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 容量检查</span></span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(tokens_for_expert_i) <span class="op">&gt;</span> capacity:</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 超出容量的 token 被丢弃（通过残差连接处理）</span></span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a>            tokens_for_expert_i <span class="op">=</span> tokens_for_expert_i[:capacity]</span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a>            overflow_mask <span class="op">=</span> create_overflow_mask(mask, capacity)</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Expert 计算</span></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a>        expert_output <span class="op">=</span> experts[i](tokens_for_expert_i)</span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: 加权并放回原位置</span></span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a>        y[mask] <span class="op">=</span> expert_gate[mask] <span class="op">*</span> expert_output</span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a>*Source: Adapted from Fedus et al. (2021) "Switch Transformers", Algorithm 1*</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a><span class="fu">### ST-MoE (2022)：稳定性与可迁移性</span></span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a>ST-MoE（Stable and Transferable MoE）专注于解决 MoE 模型的两个实际问题：</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**训练稳定性**：如何让 MoE 训练更稳定，减少 loss spike</span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**迁移学习**：如何让预训练的 MoE 模型更好地微调到下游任务</span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a>**关键发现**：</span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Router Z-Loss**：正则化路由器的 logits 大小，防止数值溢出</span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Expert dropout**：在训练时随机丢弃整个 Expert，增强鲁棒性</span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**微调策略**：MoE 模型在微调时容易过拟合，需要更强的正则化</span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a>ST-MoE-32B（269B 参数，32B 激活参数）在多个 NLP 基准上首次让稀疏模型超越了同等计算量的 Dense 模型。</span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mixtral 8x7B (2023)：开源 MoE 的里程碑</span></span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a>Mixtral 由 Mistral AI 发布，是开源 MoE 模型的标杆。</span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a>**架构设计**：</span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>基于 Mistral 7B 架构</span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>每层 8 个 Expert（替代原 FFN）</span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Top-2 路由</span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>32K 上下文窗口</span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数**：46.7B</span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**激活参数**：12.9B（每个 token）</span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a>**性能表现**：</span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>在大多数基准上匹配或超越 LLaMA-2 70B</span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>在数学、代码生成、多语言任务上显著优于 LLaMA-2 70B</span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>推理成本只有 LLaMA-2 70B 的 ~18%（12.9B vs 70B 激活参数）</span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a>**开源影响**：</span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a>Mixtral 以 Apache 2.0 许可开源，让开发者可以自由使用和修改。这大大推动了 MoE 技术的普及和社区研究。</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a><span class="fu">### DeepSeek-V2/V3 (2024)：极致效率</span></span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a>DeepSeek 系列代表了 MoE 技术的最新进展，特别是在**计算效率**和**训练成本**上的突破。</span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a><span class="fu">#### DeepSeekMoE：细粒度专家分割</span></span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a>传统 MoE 使用与原 FFN 相同规模的 Expert。DeepSeekMoE 提出将 Expert 分割得更细：</span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**原方案**：8 个 Expert，每个与原 FFN 等大，Top-2 激活</span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**细粒度方案**：64 个更小的 Expert，Top-6 激活</span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a>细粒度 Expert 的好处：</span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>更灵活的专家组合</span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>每个 Expert 可以更专业化</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>路由决策的粒度更细</span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 共享专家隔离</span></span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a>DeepSeek 将 Expert 分为两类：</span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**共享专家（Shared Expert）**：每个 token 都会经过，处理通用知识</span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**路由专家（Routed Expert）**：根据 token 内容选择性激活，处理专业知识</span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a>这种设计让模型既有"基础常识"，又有"专业技能"。</span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a><span class="fu">#### DeepSeek-V3 的规模与效率</span></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a>DeepSeek-V3 是目前最强的开源 MoE 模型之一：</span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数**：671B</span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**激活参数**：37B</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**训练成本**：2.788M H800 GPU hours（约 5.58M 美元）</span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a>相比之下，类似能力的 Dense 模型（如 LLaMA-3 405B）需要更高的训练成本和推理成本。</span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>DeepSeek-V3 还引入了两项创新：</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**无辅助损失负载均衡**：通过动态偏置而非损失项来平衡负载</span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**多 Token 预测**：训练目标从预测下一个 token 扩展到预测多个 token</span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a><span class="fu">## MoE 的核心挑战</span></span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a><span class="fu">### 负载均衡的根本困难</span></span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a>尽管有辅助损失和容量限制，负载均衡仍然是 MoE 最棘手的问题。</span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a>**问题本质**：</span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>辅助损失太强 → 路由过于均匀，牺牲了专业化（Expert 变成了"通才"）</span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>辅助损失太弱 → 路由崩塌，只有少数 Expert 被使用</span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a>这是一个根本性的权衡：**负载均衡与专家专业化之间的矛盾**。</span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a>**Token 溢出问题**：</span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a>在实际训练中，即使有负载均衡策略，某些 Expert 仍可能超过容量限制。溢出的 token 会被丢弃或降级处理，导致：</span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>信息损失</span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>训练信号不稳定</span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>下游任务性能波动</span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a>**研究方向**：</span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Expert Choice 路由（让 Expert 选择 token 而非反过来）</span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>动态容量调整</span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>无辅助损失方法（如 DeepSeek-V3 的偏置调整）</span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a><span class="fu">### 训练不稳定性</span></span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a>MoE 模型的训练比 Dense 模型更容易出现以下问题：</span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Loss Spike**：训练过程中突然出现的损失飙升，可能需要从 checkpoint 回退</span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**梯度方差大**：由于稀疏激活，每个 Expert 的梯度来自不同子集的 token，方差更大</span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Expert 退化**：某些 Expert 可能在训练过程中逐渐失去作用</span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a>**缓解策略**：</span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>更保守的学习率</span>
<span id="cb2-703"><a href="#cb2-703" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>更强的梯度裁剪</span>
<span id="cb2-704"><a href="#cb2-704" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Router Z-Loss</span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>更长的 warmup</span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>FP32 路由器 + BF16 Expert</span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a><span class="fu">### 工程复杂性</span></span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 分布式通信</span></span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a>在多 GPU 训练中，Expert 通常分布在不同的设备上（Expert Parallelism）。当一个 token 需要被路由到某个 Expert 时：</span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Token 的表示需要发送到 Expert 所在的 GPU</span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Expert 计算完成后，输出需要发送回原 GPU</span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a>这涉及 All-to-All 通信模式，通信开销与 Expert 数量和 GPU 数量成正比。</span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a>**优化策略**：</span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Expert Parallelism + Data Parallelism 混合**：每组 GPU 内共享 Expert，组间做数据并行</span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**批量通信**：收集多个 token 后一次性发送，减少通信次数</span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**本地 Expert 优先**：设计路由偏向于选择同 GPU 上的 Expert</span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 推理时的 Expert 缓存</span></span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a>在推理时，不同的请求可能需要不同的 Expert。如果 Expert 分布在多个 GPU 上，需要仔细管理 Expert 的加载和缓存。</span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a><span class="fu">## 深入理解</span></span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——理论视角</span></span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a><span class="fu">#### MoE 的表达能力</span></span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a>从理论角度，MoE 可以被理解为一种**条件计算**的形式。对于输入 $x$，MoE 的输出是：</span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a>f_{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)</span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a>这是一个**混合模型**（Mixture Model）的形式。不同的是：</span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>传统混合模型的混合权重是固定的或简单的</span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MoE 的混合权重 $G(x)$ 是输入的复杂函数</span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a>这让 MoE 能够根据输入自适应地选择"计算路径"，理论上可以表达比 Dense 模型更复杂的函数族。</span>
<span id="cb2-749"><a href="#cb2-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-750"><a href="#cb2-750" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Scaling Law 的差异</span></span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a>Dense 模型遵循经典的 Scaling Law：</span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-755"><a href="#cb2-755" aria-hidden="true" tabindex="-1"></a>L \propto N^{-\alpha}</span>
<span id="cb2-756"><a href="#cb2-756" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-757"><a href="#cb2-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-758"><a href="#cb2-758" aria-hidden="true" tabindex="-1"></a>其中 $L$ 是损失，$N$ 是参数量，$\alpha$ 约为 0.07。</span>
<span id="cb2-759"><a href="#cb2-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-760"><a href="#cb2-760" aria-hidden="true" tabindex="-1"></a>MoE 模型的 Scaling Law 有所不同。由于参数量和计算量解耦，需要分别考虑：</span>
<span id="cb2-761"><a href="#cb2-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-762"><a href="#cb2-762" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**总参数量**：决定模型的"容量"</span>
<span id="cb2-763"><a href="#cb2-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**激活参数量**：决定每次推理的"计算量"</span>
<span id="cb2-764"><a href="#cb2-764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**训练 Token 数**：决定模型看过多少数据</span>
<span id="cb2-765"><a href="#cb2-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-766"><a href="#cb2-766" aria-hidden="true" tabindex="-1"></a>研究表明，MoE 模型在相同计算预算下可以达到更低的损失，但这种优势随着规模增大而逐渐减弱。</span>
<span id="cb2-767"><a href="#cb2-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-768"><a href="#cb2-768" aria-hidden="true" tabindex="-1"></a><span class="fu">### 为什么有效？——实证视角</span></span>
<span id="cb2-769"><a href="#cb2-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-770"><a href="#cb2-770" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Expert 专业化的证据</span></span>
<span id="cb2-771"><a href="#cb2-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-772"><a href="#cb2-772" aria-hidden="true" tabindex="-1"></a>通过分析 MoE 模型中不同 Expert 的激活模式，研究者发现：</span>
<span id="cb2-773"><a href="#cb2-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-774"><a href="#cb2-774" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**领域专业化**：某些 Expert 更多处理特定领域的 token（如代码、数学、法语）</span>
<span id="cb2-775"><a href="#cb2-775" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**语法专业化**：某些 Expert 更多处理特定语法角色的 token（如动词、名词）</span>
<span id="cb2-776"><a href="#cb2-776" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**位置专业化**：某些 Expert 更多处理特定位置的 token（如句首、句尾）</span>
<span id="cb2-777"><a href="#cb2-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-778"><a href="#cb2-778" aria-hidden="true" tabindex="-1"></a>但这种专业化程度因模型和训练策略而异，并非总是清晰可辨。</span>
<span id="cb2-779"><a href="#cb2-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-780"><a href="#cb2-780" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 消融实验的发现</span></span>
<span id="cb2-781"><a href="#cb2-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-782"><a href="#cb2-782" aria-hidden="true" tabindex="-1"></a>Switch Transformer 论文中的消融实验揭示：</span>
<span id="cb2-783"><a href="#cb2-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-784"><a href="#cb2-784" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Expert 数量**：增加 Expert 数量带来的收益逐渐递减</span>
<span id="cb2-785"><a href="#cb2-785" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Top-k 选择**：Top-1 与 Top-2 在质量上差异不大，但 Top-1 效率更高</span>
<span id="cb2-786"><a href="#cb2-786" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**容量因子**：1.25 到 2.0 之间较优，太小导致溢出，太大浪费计算</span>
<span id="cb2-787"><a href="#cb2-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-788"><a href="#cb2-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### 方法的边界条件</span></span>
<span id="cb2-789"><a href="#cb2-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-790"><a href="#cb2-790" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 什么时候 MoE 不如 Dense？</span></span>
<span id="cb2-791"><a href="#cb2-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-792"><a href="#cb2-792" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**小规模场景**：当模型规模较小时，MoE 的路由开销可能超过收益</span>
<span id="cb2-793"><a href="#cb2-793" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**极端稀疏**：Top-1 路由在某些任务上可能损失表达能力</span>
<span id="cb2-794"><a href="#cb2-794" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**微调场景**：MoE 模型在微调时更容易过拟合，需要更强的正则化</span>
<span id="cb2-795"><a href="#cb2-795" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**延迟敏感场景**：Expert Parallelism 引入的通信延迟可能不可接受</span>
<span id="cb2-796"><a href="#cb2-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-797"><a href="#cb2-797" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 负载不均的根本困境</span></span>
<span id="cb2-798"><a href="#cb2-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-799"><a href="#cb2-799" aria-hidden="true" tabindex="-1"></a>目前的负载均衡方法都无法完美解决这个问题。辅助损失是一种妥协：</span>
<span id="cb2-800"><a href="#cb2-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-801"><a href="#cb2-801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>它确保了 Expert 被均匀使用</span>
<span id="cb2-802"><a href="#cb2-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>但可能阻止了最优的专业化分工</span>
<span id="cb2-803"><a href="#cb2-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-804"><a href="#cb2-804" aria-hidden="true" tabindex="-1"></a>理想的路由应该是：让每个 Expert 专注于自己擅长的 token 类型，同时保证整体负载均衡。这两个目标之间存在内在张力。</span>
<span id="cb2-805"><a href="#cb2-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-806"><a href="#cb2-806" aria-hidden="true" tabindex="-1"></a><span class="fu">### 开放研究问题</span></span>
<span id="cb2-807"><a href="#cb2-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-808"><a href="#cb2-808" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**最优路由策略**：目前的 Top-k 路由是最优的吗？是否存在更好的路由方式？</span>
<span id="cb2-809"><a href="#cb2-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-810"><a href="#cb2-810" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Expert 的可解释性**：每个 Expert 到底学到了什么？如何可视化和理解 Expert 的专业化？</span>
<span id="cb2-811"><a href="#cb2-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-812"><a href="#cb2-812" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**MoE 与其他稀疏技术的结合**：MoE + 稀疏注意力？MoE + 剪枝？</span>
<span id="cb2-813"><a href="#cb2-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-814"><a href="#cb2-814" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**动态 Expert 数量**：能否根据输入复杂度动态决定激活多少 Expert？</span>
<span id="cb2-815"><a href="#cb2-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-816"><a href="#cb2-816" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**MoE 的涌现能力**：MoE 是否有与 Dense 模型不同的涌现行为？</span>
<span id="cb2-817"><a href="#cb2-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-818"><a href="#cb2-818" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-819"><a href="#cb2-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-820"><a href="#cb2-820" aria-hidden="true" tabindex="-1"></a><span class="fu">## 局限性与未解决的问题</span></span>
<span id="cb2-821"><a href="#cb2-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-822"><a href="#cb2-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### 本方法的局限</span></span>
<span id="cb2-823"><a href="#cb2-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-824"><a href="#cb2-824" aria-hidden="true" tabindex="-1"></a>**负载均衡仍是难题**</span>
<span id="cb2-825"><a href="#cb2-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-826"><a href="#cb2-826" aria-hidden="true" tabindex="-1"></a>尽管有多种技术，在实际训练中仍难以实现完美的负载均衡。这影响了：</span>
<span id="cb2-827"><a href="#cb2-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-828"><a href="#cb2-828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>训练效率（某些 Expert 闲置）</span>
<span id="cb2-829"><a href="#cb2-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>模型质量（某些 Expert 欠训练）</span>
<span id="cb2-830"><a href="#cb2-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>推理效率（需要为最差情况预留资源）</span>
<span id="cb2-831"><a href="#cb2-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-832"><a href="#cb2-832" aria-hidden="true" tabindex="-1"></a>**工程复杂度高**</span>
<span id="cb2-833"><a href="#cb2-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-834"><a href="#cb2-834" aria-hidden="true" tabindex="-1"></a>相比 Dense 模型，MoE 的训练和部署都更复杂：</span>
<span id="cb2-835"><a href="#cb2-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-836"><a href="#cb2-836" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>需要 Expert Parallelism 支持</span>
<span id="cb2-837"><a href="#cb2-837" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>通信模式更复杂</span>
<span id="cb2-838"><a href="#cb2-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Debug 更困难</span>
<span id="cb2-839"><a href="#cb2-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>推理时需要加载所有 Expert（即使只激活一部分）</span>
<span id="cb2-840"><a href="#cb2-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-841"><a href="#cb2-841" aria-hidden="true" tabindex="-1"></a>**微调不稳定**</span>
<span id="cb2-842"><a href="#cb2-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-843"><a href="#cb2-843" aria-hidden="true" tabindex="-1"></a>MoE 模型在微调时表现不如 Dense 模型稳定。原因可能是：</span>
<span id="cb2-844"><a href="#cb2-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-845"><a href="#cb2-845" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>稀疏激活导致部分 Expert 在微调数据上得不到充分更新</span>
<span id="cb2-846"><a href="#cb2-846" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>路由器需要适应新的数据分布</span>
<span id="cb2-847"><a href="#cb2-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-848"><a href="#cb2-848" aria-hidden="true" tabindex="-1"></a><span class="fu">### 这些局限导向了什么？</span></span>
<span id="cb2-849"><a href="#cb2-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-850"><a href="#cb2-850" aria-hidden="true" tabindex="-1"></a>MoE 解决了"参数效率"的问题，但引入了新的复杂性。这促使我们思考：</span>
<span id="cb2-851"><a href="#cb2-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-852"><a href="#cb2-852" aria-hidden="true" tabindex="-1"></a>**是否有比 MoE 更优雅的条件计算方式？**</span>
<span id="cb2-853"><a href="#cb2-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-854"><a href="#cb2-854" aria-hidden="true" tabindex="-1"></a>MoE 的路由是在 FFN 层级别的"粗粒度"选择。能否设计更细粒度的条件计算，比如在神经元级别动态激活？</span>
<span id="cb2-855"><a href="#cb2-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-856"><a href="#cb2-856" aria-hidden="true" tabindex="-1"></a>**是否有完全不同的大模型扩展路径？**</span>
<span id="cb2-857"><a href="#cb2-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-858"><a href="#cb2-858" aria-hidden="true" tabindex="-1"></a>除了让模型"更大"（更多参数），还有其他方向：</span>
<span id="cb2-859"><a href="#cb2-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-860"><a href="#cb2-860" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>让模型"更深入思考"（Test-time compute scaling）</span>
<span id="cb2-861"><a href="#cb2-861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>让模型"更好地利用外部知识"（RAG）</span>
<span id="cb2-862"><a href="#cb2-862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>让模型"更高效地利用参数"（下一章将讨论的状态空间模型）</span>
<span id="cb2-863"><a href="#cb2-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-864"><a href="#cb2-864" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-865"><a href="#cb2-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-866"><a href="#cb2-866" aria-hidden="true" tabindex="-1"></a><span class="fu">## 本章小结</span></span>
<span id="cb2-867"><a href="#cb2-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-868"><a href="#cb2-868" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心要点回顾</span></span>
<span id="cb2-869"><a href="#cb2-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-870"><a href="#cb2-870" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**问题**：Dense Transformer 对每个 token 激活所有参数，存在巨大的计算浪费。模型容量和计算量紧密耦合，难以单独扩展。</span>
<span id="cb2-871"><a href="#cb2-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-872"><a href="#cb2-872" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**洞察**：不同的 token 可能需要不同的"专业知识"。通过让路由器动态选择 Expert，可以实现**稀疏激活**——用小比例的计算访问大规模的参数。</span>
<span id="cb2-873"><a href="#cb2-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-874"><a href="#cb2-874" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**方法**：</span>
<span id="cb2-875"><a href="#cb2-875" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>将 FFN 替换为多个 Expert</span>
<span id="cb2-876"><a href="#cb2-876" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>用门控网络进行 Top-k 路由</span>
<span id="cb2-877"><a href="#cb2-877" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>用辅助损失和容量限制保证负载均衡</span>
<span id="cb2-878"><a href="#cb2-878" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>用多种技巧保证训练稳定性</span>
<span id="cb2-879"><a href="#cb2-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-880"><a href="#cb2-880" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**意义**：MoE 实现了**参数量与计算量的解耦**，让我们能够以相对较低的成本训练出参数规模达到万亿级别的模型。这为大语言模型的规模化提供了一条切实可行的路径。</span>
<span id="cb2-881"><a href="#cb2-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-882"><a href="#cb2-882" aria-hidden="true" tabindex="-1"></a><span class="fu">### 关键公式速查</span></span>
<span id="cb2-883"><a href="#cb2-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-884"><a href="#cb2-884" aria-hidden="true" tabindex="-1"></a>**MoE 层输出**：</span>
<span id="cb2-885"><a href="#cb2-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-886"><a href="#cb2-886" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-887"><a href="#cb2-887" aria-hidden="true" tabindex="-1"></a>\text{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)</span>
<span id="cb2-888"><a href="#cb2-888" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-889"><a href="#cb2-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-890"><a href="#cb2-890" aria-hidden="true" tabindex="-1"></a>**门控网络**：</span>
<span id="cb2-891"><a href="#cb2-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-892"><a href="#cb2-892" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-893"><a href="#cb2-893" aria-hidden="true" tabindex="-1"></a>G(x) = \text{TopK-Softmax}(W_g \cdot x)</span>
<span id="cb2-894"><a href="#cb2-894" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-895"><a href="#cb2-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-896"><a href="#cb2-896" aria-hidden="true" tabindex="-1"></a>**辅助损失（负载均衡）**：</span>
<span id="cb2-897"><a href="#cb2-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-898"><a href="#cb2-898" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-899"><a href="#cb2-899" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{aux} = \alpha \cdot n \cdot \sum_{i=1}^{n} f_i \cdot p_i</span>
<span id="cb2-900"><a href="#cb2-900" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-901"><a href="#cb2-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-902"><a href="#cb2-902" aria-hidden="true" tabindex="-1"></a>**Expert 容量**：</span>
<span id="cb2-903"><a href="#cb2-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-904"><a href="#cb2-904" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-905"><a href="#cb2-905" aria-hidden="true" tabindex="-1"></a>\text{capacity} = \left\lceil \frac{T \cdot k}{n} \cdot \text{capacity<span class="sc">\_</span>factor} \right\rceil</span>
<span id="cb2-906"><a href="#cb2-906" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-907"><a href="#cb2-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-908"><a href="#cb2-908" aria-hidden="true" tabindex="-1"></a><span class="fu">### 思考题</span></span>
<span id="cb2-909"><a href="#cb2-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-910"><a href="#cb2-910" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**[概念理解]** 为什么 MoE 的辅助损失使用 $f_i \cdot p_i$ 而不是简单地最小化 $\text{Var}(f_1, \ldots, f_n)$？这两种设计有什么区别？</span>
<span id="cb2-911"><a href="#cb2-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-912"><a href="#cb2-912" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**[数学推导]** 假设一个 MoE 层有 8 个 Expert，每个 Expert 的参数量与原 FFN 相同。如果使用 Top-2 路由，计算：(a) 总参数量相对于原 FFN 的倍数；(b) 每个 token 的计算量相对于原 FFN 的倍数。</span>
<span id="cb2-913"><a href="#cb2-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-914"><a href="#cb2-914" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**[工程实践]** 使用 Hugging Face 的 <span class="in">`transformers`</span> 库加载 Mixtral 8x7B 模型，分析不同输入（代码、数学、自然语言）激活的 Expert 分布是否有差异。</span>
<span id="cb2-915"><a href="#cb2-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-916"><a href="#cb2-916" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**[开放思考]** MoE 通过路由实现稀疏激活，但路由本身也需要计算。如果我们想进一步扩展到数万个 Expert，路由计算本身会成为瓶颈吗？如何解决？</span>
<span id="cb2-917"><a href="#cb2-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-918"><a href="#cb2-918" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-919"><a href="#cb2-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-920"><a href="#cb2-920" aria-hidden="true" tabindex="-1"></a><span class="fu">## 延伸阅读</span></span>
<span id="cb2-921"><a href="#cb2-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-922"><a href="#cb2-922" aria-hidden="true" tabindex="-1"></a><span class="fu">### 核心论文（必读）</span></span>
<span id="cb2-923"><a href="#cb2-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-924"><a href="#cb2-924" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Shazeer et al. (2017)** "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"</span>
<span id="cb2-925"><a href="#cb2-925" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2 (MoE 架构)、Section 3 (门控机制)</span>
<span id="cb2-926"><a href="#cb2-926" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>现代 MoE 的奠基之作</span>
<span id="cb2-927"><a href="#cb2-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-928"><a href="#cb2-928" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fedus et al. (2021)** "Switch Transformers: Scaling to Trillion Parameter Models"</span>
<span id="cb2-929"><a href="#cb2-929" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2 (简化设计)、Section 4 (训练稳定性)</span>
<span id="cb2-930"><a href="#cb2-930" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Top-1 路由的简化与万亿参数模型</span>
<span id="cb2-931"><a href="#cb2-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-932"><a href="#cb2-932" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Jiang et al. (2024)** "Mixtral of Experts"</span>
<span id="cb2-933"><a href="#cb2-933" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>重点读：Section 2 (架构细节)、Section 3 (性能评估)</span>
<span id="cb2-934"><a href="#cb2-934" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>开源 MoE 的里程碑</span>
<span id="cb2-935"><a href="#cb2-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-936"><a href="#cb2-936" aria-hidden="true" tabindex="-1"></a><span class="fu">### 后续发展</span></span>
<span id="cb2-937"><a href="#cb2-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-938"><a href="#cb2-938" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zhou et al. (2022)** "Mixture-of-Experts with Expert Choice Routing"</span>
<span id="cb2-939"><a href="#cb2-939" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>反转路由方向：让 Expert 选择 token</span>
<span id="cb2-940"><a href="#cb2-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-941"><a href="#cb2-941" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepSeek-AI (2024)** "DeepSeek-V3 Technical Report"</span>
<span id="cb2-942"><a href="#cb2-942" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>无辅助损失负载均衡、多 token 预测</span>
<span id="cb2-943"><a href="#cb2-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-944"><a href="#cb2-944" aria-hidden="true" tabindex="-1"></a><span class="fu">### 综述与教程</span></span>
<span id="cb2-945"><a href="#cb2-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-946"><a href="#cb2-946" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Blog** "Mixture of Experts Explained" — 优秀的入门教程</span>
<span id="cb2-947"><a href="#cb2-947" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Maarten Grootendorst** "A Visual Guide to Mixture of Experts" — 可视化图解</span>
<span id="cb2-948"><a href="#cb2-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-949"><a href="#cb2-949" aria-hidden="true" tabindex="-1"></a><span class="fu">### 代码资源</span></span>
<span id="cb2-950"><a href="#cb2-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-951"><a href="#cb2-951" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hugging Face Transformers**：Mixtral 官方实现</span>
<span id="cb2-952"><a href="#cb2-952" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepSpeed-MoE**：微软的 MoE 训练框架</span>
<span id="cb2-953"><a href="#cb2-953" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Megablocks**：高效 MoE 实现</span>
<span id="cb2-954"><a href="#cb2-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-955"><a href="#cb2-955" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb2-956"><a href="#cb2-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-957"><a href="#cb2-957" aria-hidden="true" tabindex="-1"></a><span class="fu">## 历史注脚</span></span>
<span id="cb2-958"><a href="#cb2-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-959"><a href="#cb2-959" aria-hidden="true" tabindex="-1"></a>MoE 的思想最早可追溯到 1991 年 Robert Jacobs、Michael Jordan 等人的论文 "Adaptive Mixtures of Local Experts"。他们在监督学习的背景下提出了这个概念，用于函数逼近任务。</span>
<span id="cb2-960"><a href="#cb2-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-961"><a href="#cb2-961" aria-hidden="true" tabindex="-1"></a>有趣的是，这篇 1991 年的论文直到 2017 年才被 Noam Shazeer 重新发现并应用于深度学习。Shazeer 在 Google Brain 工作期间，正在思考如何扩展神经网络的规模而不线性增加计算量。他在阅读经典文献时发现了 MoE，意识到这正是他需要的思路。</span>
<span id="cb2-962"><a href="#cb2-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-963"><a href="#cb2-963" aria-hidden="true" tabindex="-1"></a>Shazeer 2017 的论文标题 "Outrageously Large Neural Networks"（大得离谱的神经网络）本身就是一个宣言：他们要证明，通过稀疏激活，神经网络可以拥有比当时任何人想象的都要多得多的参数。这篇论文训练了 1370 亿参数的模型——在 2017 年，这个规模几乎是不可想象的。</span>
<span id="cb2-964"><a href="#cb2-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-965"><a href="#cb2-965" aria-hidden="true" tabindex="-1"></a>Shazeer 后来又参与了 Transformer 的发明（他是 "Attention Is All You Need" 的作者之一），并继续推动 MoE 技术的发展。2021 年的 Switch Transformer 正是他与 William Fedus 等人的合作成果。</span>
<span id="cb2-966"><a href="#cb2-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-967"><a href="#cb2-967" aria-hidden="true" tabindex="-1"></a>如今，MoE 已经从一个研究概念变成了大语言模型的标准架构选择。Mixtral、DeepSeek、Grok（xAI）等模型都采用了 MoE 架构，证明了这条技术路线的成熟和实用性。</span>
<span id="cb2-968"><a href="#cb2-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-969"><a href="#cb2-969" aria-hidden="true" tabindex="-1"></a>从 1991 到 2024，MoE 的演进跨越了 33 年——这再次提醒我们，很多"新"技术其实有着深厚的历史根源，关键是在合适的时机用合适的方式重新发现和应用它们。</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>