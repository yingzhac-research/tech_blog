---
title: "DeepSeek-R1：推理增强的大语言模型"
description: "用散文式方式解释 DeepSeek-R1：从普通大模型到会认真思考的模型，讲清推理轨迹、奖励模型和强化学习训练，并为每个数学符号和张量变量补上含义与 shape。"
date: 2025-01-20
categories: [大模型, 强化学习, 推理, DeepSeek]
---

2025 年 1 月，DeepSeek 发布了 DeepSeek-R1，一种专门强化「推理能力」的大语言模型。

如果只看论文里的结构图，很容易有一种「每一块都一笔带过」的感觉：有公式、有 loss、有 reward，但难以在脑子里形成一条连续的故事线。

这篇文章按「散文式讲解」来写，目标是：

- 把大模型里的「推理」说清楚，而不是一句话略过。
- 从普通监督微调（SFT）讲起，一步步解释 DeepSeek-R1 多做了什么。
- 每个数学符号都交代清楚含义。
- 每个张量变量尽量标出典型的 shape，方便在脑中对应到 PyTorch 里的 `tensor`。

---

## 大模型里的「推理」到底是什么

先不要急着看公式，我们从一个具体的小例子开始。

假设有这么一道题：

> 「一个正方形的对角线长度是 10，问这个正方形的面积是多少？」

早期的大语言模型（比如原始的 GPT-3 或普通的 LLaMA）在面对这种题目时，典型的行为是：

1. 把题目转成一串 token。
2. 经过一堆 Transformer 层，直接给出一个答案，比如「50」。
3. 中间几乎不展示思考过程，或者只写一两句很粗糙的话。

也就是说，它更像是在「一口气猜答案」，而不是一步步推理。

### 语言模型在做什么

从数学上看，一个自回归大语言模型在拟合的，其实是一个条件概率分布：

$$
p_\theta(\mathbf{y} \mid \mathbf{x})
$$

- $\mathbf{x}$：输入 token 序列，比如题目文本。
- $\mathbf{y}$：输出 token 序列，比如推理过程加最终答案。
- $\theta$：模型参数，包含所有层的权重。

如果从张量的角度看，在一个 batch 里，我们通常可以这样写：

$$
\mathbf{x} \in \mathbb{R}^{B \times T_{\text{in}}}
$$

- $B$：batch size，一次输入多少条样本。
- $T_{\text{in}}$：每条输入序列的长度（token 个数）。
- 第 $b$ 行 $\mathbf{x}_{b,:}$：第 $b$ 条样本的 token id 序列。

Embedding 和多层 Transformer 之后，我们得到每个位置的隐藏向量：

$$
\mathbf{H} \in \mathbb{R}^{B \times T_{\text{out}} \times d_{\text{model}}}
$$

- $T_{\text{out}}$：输出序列的长度（生成多少个 token）。
- $d_{\text{model}}$：隐藏层维度，比如 4096 或 8192。
- 对第 $b$ 条样本第 $t$ 个位置，隐藏向量是 $\mathbf{H}_{b,t,:} \in \mathbb{R}^{d_{\text{model}}}$。

再接一个线性层（通常共享词向量矩阵），把隐藏向量映射到词表维度：

$$
\mathbf{Z} \in \mathbb{R}^{B \times T_{\text{out}} \times V}
$$

- $V$：词表大小。
- $\mathbf{Z}_{b,t,:} \in \mathbb{R}^{V}$：第 $b$ 条样本第 $t$ 个位置上，对每个词的打分（logits）。

经过 softmax 就得到每一步的概率分布：

$$
p_\theta(y_{b,t} = v \mid \mathbf{x}_b, y_{b,<t})
= \text{softmax}(\mathbf{Z}_{b,t,:})_v
$$

在推理时，模型就是不断根据这些分布采样下一个 token，直到遇到终止符。

### 「直接给答案」和「写出推理」的差别

再回到这道几何题。

如果我们期望的是「会推理的模型」，我们希望它的输出更像下面这样：

```text
已知正方形的对角线长度为 d = 10。
设正方形的边长为 a。
正方形的两条边与对角线构成一个直角三角形，
根据勾股定理，有：
  d² = a² + a² = 2a²
所以：
  a² = d² / 2 = 100 / 2 = 50
正方形的面积 A = a² = 50。
因此答案是 50。
```

这里有几个关键特征：

- 引入了**中间变量** $a$（边长），而不是直接从 $d$ 跳到面积。
- 显式写出了勾股定理 $d^2 = a^2 + a^2$，而不是在模型内部「默默算」。
- 把「求边长」和「求面积」拆成了两个清晰的步骤。

我们可以把这样的输出称为一条**推理轨迹**（reasoning trace）：模型不仅给出结果，还给出一条逻辑上连贯的思考过程。

DeepSeek-R1 的核心目标，就是让模型在训练阶段就学会「写出并优化这样的推理轨迹」，而不是只拟合最终答案。

---

## 监督微调基线：从数据和损失函数讲起

在解释 DeepSeek-R1 的特殊设计之前，先把普通大模型的「基线训练方式」讲清楚。所有后续改造，其实都是在这个基线上往前加东西。

### 数据集与张量 shape

在监督微调（SFT, Supervised Fine-Tuning）阶段，我们有一个数据集：

$$
\mathcal{D} = \{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N
$$

- $N$：样本总数。
- $\mathbf{x}_i$：第 $i$ 条样本的输入 token 序列（问题、上下文等）。
- $\mathbf{y}_i$：第 $i$ 条样本的目标输出 token 序列（标准答案，可以包含一些解释）。

在一个 batch 内，我们通常会把它们整理成两个张量：

- 输入张量：$\mathbf{X} \in \mathbb{R}^{B \times T_{\text{in}}}$。
  - 第 $b$ 行 $\mathbf{X}_{b,:}$ 是某个 $\mathbf{x}_i$ padding 之后的版本。
- 目标张量：$\mathbf{Y} \in \mathbb{R}^{B \times T_{\text{out}}}$。
  - 第 $b$ 行 $\mathbf{Y}_{b,:}$ 是对应的 $\mathbf{y}_i$。

这里的 $T_{\text{in}}$ 与 $T_{\text{out}}$ 通常相近，也可以合并成一个统一的序列长度，通过掩码来区分输入与输出。

### SFT 目标函数与含义

标准的 SFT 目标就是最大化模型在目标序列上的条件概率，等价于最小化负对数似然：

$$
\mathcal{L}_{\text{SFT}}(\theta)
= -\sum_{i=1}^N \log p_\theta(\mathbf{y}_i \mid \mathbf{x}_i)
$$

进一步展开一条样本的 log 概率：

$$
\log p_\theta(\mathbf{y}_i \mid \mathbf{x}_i)
= \sum_{t=1}^{T_i}
  \log p_\theta(y_{i,t} \mid \mathbf{x}_i, y_{i,<t})
$$

- $T_i$：第 $i$ 条样本输出序列的长度。
- $y_{i,t}$：第 $i$ 条样本在时间步 $t$ 的目标 token。
- $y_{i,<t}$：第 $t$ 步之前已经生成的 token 序列。

因此，$\mathcal{L}_{\text{SFT}}$ 本质上就是所有 token 的交叉熵损失之和。

关键的一点是：在大多数数据集中，$\mathbf{y}_i$ 里**只有最终答案或者很短的解释**。模型能不能写出细致的推理轨迹，完全取决于数据中有没有这样的示例，训练目标本身并**没有显式鼓励「过程好不好」**。

这就是 DeepSeek-R1 要突破的第一道限制。

---

## DeepSeek-R1 的整体训练流程

从宏观上看，DeepSeek-R1 相对于普通 SFT，大致多做了三件事：

1. 让模型显式生成推理轨迹，而不是直接给答案。
2. 训练一个**奖励模型**，对推理轨迹的质量打分。
3. 用强化学习（policy gradient 一类方法）对原模型进行二次优化，让它更倾向于生成高分轨迹。

下面从「数据流」的视角看一遍。

### 预训练与基础 SFT

这一阶段和普通大模型相似：

- 先在海量无标注文本上做语言模型预训练，得到基础模型 $M_{\text{base}}$。
- 再在任务相关的数据集上做一次 SFT，得到一个初始模型 $M_{\text{sft}}$。

此时的 $M_{\text{sft}}$ 已经能解决不少任务，也能写出一些简单的「因为...所以...」式解释，但推理质量和稳定性都有限。

### 生成推理轨迹

接下来，用 $M_{\text{sft}}$ 在大量题目上自动生成推理轨迹。

对于一道题目 $\mathbf{x}$，我们希望模型输出的序列可以拆成：

$$
\mathbf{y} = [\mathbf{y}^{(r)}, \mathbf{y}^{(a)}]
$$

- $\mathbf{y}^{(r)}$：推理部分（reasoning），由一串 token 组成。
- $\mathbf{y}^{(a)}$：最终答案部分（answer），也是一串 token。

在张量层面，如果一次生成 $B$ 条样本，可以记为：

- 推理序列：$\mathbf{Y}^{(r)} \in \mathbb{R}^{B \times T_r}$。
- 答案序列：$\mathbf{Y}^{(a)} \in \mathbb{R}^{B \times T_a}$。
- 整体输出：$\mathbf{Y} \in \mathbb{R}^{B \times T}$，其中 $T = T_r + T_a$。

也就是说，每条样本变成了「题目 + 推理 + 答案」的完整文本。

### 训练奖励模型

有了大量自动生成的推理轨迹，就可以训练一个奖励模型 $R_\phi$：

$$
R_\phi(\mathbf{x}, \mathbf{y}) \in \mathbb{R}
$$

- 参数 $\phi$：奖励模型本身的参数（通常也是一个 Transformer）。
- 输入：
  - 题目 token 序列 $\mathbf{x}$，形状类似 $\mathbb{R}^{T_{\text{in}}}$。
  - 模型输出 $\mathbf{y}$，形状类似 $\mathbb{R}^{T}$。
  - 在 batch 中，可以把它们拼成一个张量
    $\mathbf{Z} \in \mathbb{R}^{B \times T_{\text{rm}}}$。
- 输出：对每条样本的一个标量分数，shape 为 $\mathbb{R}^{B \times 1}$。

训练 $R_\phi$ 需要监督信号，主要有两种来源：

1. 明确标注「这条推理是好还是坏」的样本。
2. 对同一题目下两条不同推理的偏好比较（哪条更好）。

常见做法是用偏好学习（preference learning）：给定同一题目的两条推理 $\mathbf{y}^{(1)}$、$\mathbf{y}^{(2)}$，如果标注者认为 $\mathbf{y}^{(1)}$ 更好，就训练 $R_\phi$ 让
$R_\phi(\mathbf{x}, \mathbf{y}^{(1)})$ 大于
$R_\phi(\mathbf{x}, \mathbf{y}^{(2)})$。

### 强化学习优化策略

有了奖励模型之后，就可以把原来的语言模型视作一个策略：

- 策略：$\pi_\theta(\mathbf{y} \mid \mathbf{x})$，表示给定题目 $\mathbf{x}$ 时，生成一条完整输出 $\mathbf{y}$ 的概率。
- 奖励：$R(\mathbf{x}, \mathbf{y})$，由 $R_\phi$ 和一些规则（如长度惩罚、答案对错）共同定义。

我们想最大化期望奖励：

$$
J(\theta) =
\mathbb{E}_{\mathbf{x} \sim \mathcal{D},\,
  \mathbf{y} \sim \pi_\theta(\cdot \mid \mathbf{x})}
  \left[ R(\mathbf{x}, \mathbf{y}) \right]
$$

训练时，通常采用类似 PPO / GRPO 的策略梯度方法：用旧策略采样轨迹，计算奖励，再用加权的 log-prob 梯度更新参数。

直观理解就是：用奖励模型来告诉语言模型「哪些推理写法更好」，然后不断鼓励模型多走这些高分路径。

---

## 推理轨迹：如何形式化「思考过程」

为了训练和评估方便，我们需要把「推理过程」形式化，而不是仅仅当成一段模糊的文本。

### 输出拆分为推理与答案

对于一道题目 $\mathbf{x}$，模型输出的整个序列写作：

$$
\mathbf{y} = [\mathbf{y}^{(r)}, \mathbf{y}^{(a)}]
$$

- $\mathbf{y}^{(r)} = [y^{(r)}_1, \dots, y^{(r)}_{T_r}]$：
  - 推理部分的 token 序列。
  - 在 batch 中 shape 为 $\mathbb{R}^{B \times T_r}$。
- $\mathbf{y}^{(a)} = [y^{(a)}_1, \dots, y^{(a)}_{T_a}]$：
  - 最终答案部分的 token 序列。
  - 在 batch 中 shape 为 $\mathbb{R}^{B \times T_a}$。

生成时，策略先生成 $\mathbf{y}^{(r)}$，再在它的基础上生成 $\mathbf{y}^{(a)}$。

### 用几何题示范一条完整轨迹

还是刚才那道正方形面积的题。

- 题目：$\mathbf{x}$。
- 推理部分 $\mathbf{y}^{(r)}$ 如下：

  ```text
  题目给出正方形的对角线长度 d = 10。
  设正方形的边长为 a。
  正方形的两条边与对角线构成一个直角三角形，
  根据勾股定理，有：
    d² = a² + a² = 2a²
  因此：
    a² = d² / 2 = 100 / 2 = 50
  ```

- 答案部分 $\mathbf{y}^{(a)}$：

  ```text
  正方形的面积等于边长的平方，
  所以面积 A = a² = 50。
  ```

在训练中，DeepSeek-R1 会把「题目 + 推理 + 答案」整体送入奖励模型，请它从**整体逻辑**的角度给出评分，而不是只看最后一行数字。

---

## 奖励模型：给推理过程打分

现在重点看奖励模型 $R_\phi$，它不只是判断「答案对不对」，而是对整条推理轨迹进行更细致的评价。

### 输入张量和 shape

在一个 batch 内，奖励模型的输入可以表示为：

$$
\mathbf{Z} \in \mathbb{R}^{B \times T_{\text{rm}}}
$$

- $B$：batch size。
- $T_{\text{rm}}$：奖励模型看到的总长度（包含题目、推理、答案）。

奖励模型通常也是一个 Transformer，把整段序列编码成隐藏表示：

$$
\mathbf{H}_{\text{rm}} \in \mathbb{R}^{B \times T_{\text{rm}} \times d_{\text{rm}}}
$$

- $d_{\text{rm}}$：奖励模型的隐藏维度。

接着通常取最后一个位置或做 pooling，得到每条样本的向量：

$$
\mathbf{h}_{\text{rm}} \in \mathbb{R}^{B \times d_{\text{rm}}}
$$

再接一个线性层输出标量分数：

$$
R_\phi(\mathbf{x}, \mathbf{y}) \in \mathbb{R}^{B \times 1}
$$

对第 $b$ 条样本来说，$R_\phi(\mathbf{x}_b, \mathbf{y}_b)$ 就是这条推理轨迹的综合得分。

### 答案正确性奖励

最直观的一部分奖励来自最终答案是否正确，可以抽象成：

$$
r_{\text{correct}}(\mathbf{x}, \mathbf{y}) =
\begin{cases}
1, & \text{如果答案正确} \\
0, & \text{否则}
\end{cases}
$$

这里的「答案正确」通常由一个外部评测器给出：

- 数学题：解析出最后答案，和标准答案对比。
- 编程题：运行代码，统计通过的测试用例数量。

### 过程奖励（process reward）

更关键的一部分，是对推理过程本身的评价。这个奖励不直接看最终答案，而是看过程是否：

- 条理清晰。
- 步骤完整。
- 没有明显逻辑错误。

可以直接用奖励模型输出来作为过程奖励：

$$
r_{\text{process}}(\mathbf{x}, \mathbf{y})
= R_\phi(\mathbf{x}, \mathbf{y})
$$

训练 $R_\phi$ 时常用偏好数据：对同一道题目下两条推理 $\mathbf{y}^{(1)}, \mathbf{y}^{(2)}$，如果标注者更偏好 $\mathbf{y}^{(1)}$，就希望
$R_\phi(\mathbf{x}, \mathbf{y}^{(1)})$ 大于
$R_\phi(\mathbf{x}, \mathbf{y}^{(2)})$。

可以用类似 Bradley–Terry 的形式构造损失：

$$
\mathcal{L}_{\text{rm}}(\phi) =
-\log \sigma\Big(
R_\phi(\mathbf{x}, \mathbf{y}^{(1)})
- R_\phi(\mathbf{x}, \mathbf{y}^{(2)})
\Big)
$$

- $\sigma(\cdot)$：sigmoid 函数。
- 当 $R_\phi(\mathbf{x}, \mathbf{y}^{(1)})$ 明显大于
  $R_\phi(\mathbf{x}, \mathbf{y}^{(2)})$ 时，这个损失就很小。

### 长度惩罚

为了避免模型用「水字数」的方式获取高分，一般还会对长度做惩罚：

$$
r_{\text{length}}(\mathbf{y})
= -\lambda \cdot \frac{T(\mathbf{y})}{T_{\max}}
$$

- $T(\mathbf{y})$：这条轨迹的总 token 数（推理 + 答案）。
- $T_{\max}$：设定的最大合理长度。
- $\lambda$：一个正的标量超参数，控制惩罚强弱。

这个项并不是让推理越短越好，而是防止模型为了多拿 process reward 而无限啰嗦。

### 奖励加权组合

最终，总奖励通常是几项加权求和：

$$
R_{\text{total}}(\mathbf{x}, \mathbf{y})
= w_1 r_{\text{correct}}
+ w_2 r_{\text{process}}
+ w_3 r_{\text{length}}
$$

- $w_1, w_2, w_3$：三个非负权重（标量），用来平衡答案正确性、过程质量和长度控制。

在实现中，$r_{\text{correct}}$ 和 $r_{\text{length}}$ 可以通过规则快速算出，
$r_{\text{process}}$ 则完全依赖训练好的奖励模型。

---

## 强化学习阶段：让模型走高分路径

有了奖励函数，就可以进入强化学习阶段，把语言模型当成在离散空间上做多步决策的策略。

### 把语言模型看成 MDP

可以把整个生成过程看成一个马尔可夫决策过程（MDP）：

- 状态 $s_t$：
  - 当前已经生成的 token 序列（包含题目和此前所有输出）。
- 动作 $a_t$：
  - 在时间步 $t$ 选择的下一个 token。
- 策略 $\pi_\theta(a_t \mid s_t)$：
  - 给定当前状态 $s_t$，选择动作 $a_t$ 的概率。
- 轨迹：

  $$
  \tau = (s_0, a_0, s_1, a_1, \dots, s_T, a_T)
  $$

- 总奖励 $R(\tau)$：
  - 对应我们前面定义的 $R_{\text{total}}(\mathbf{x}, \mathbf{y})$。

在一个 batch 里，如果一次生成 $B$ 条轨迹，它们的 token 序列可以堆成一个张量：

$$
\mathbf{Y} \in \mathbb{R}^{B \times T}
$$

- 第 $b$ 行 $\mathbf{Y}_{b,:}$ 就是第 $b$ 条样本的完整输出。

### 策略梯度目标与直觉

强化学习的目标是最大化期望奖励：

$$
J(\theta) =
\mathbb{E}_{\mathbf{x} \sim \mathcal{D},\,
  \mathbf{y} \sim \pi_\theta(\cdot \mid \mathbf{x})}
  \left[ R_{\text{total}}(\mathbf{x}, \mathbf{y}) \right]
$$

使用策略梯度（Policy Gradient），可以写出梯度估计：

$$
\nabla_\theta J(\theta)
\approx
\mathbb{E}_{\mathbf{x}, \mathbf{y}}
\left[
  A(\mathbf{x}, \mathbf{y})\,
  \nabla_\theta \log \pi_\theta(\mathbf{y} \mid \mathbf{x})
\right]
$$

- $A(\mathbf{x}, \mathbf{y})$：advantage（优势函数），通常是总奖励减去某个基线，表示「比期望好多少」。
- $\log \pi_\theta(\mathbf{y} \mid \mathbf{x})$：
  - 可以进一步拆成所有 token log-prob 的和。

DeepSeek-R1 采用的是类似 PPO / GRPO 的改进方法：

- 用旧策略 $\pi_{\theta_{\text{old}}}$ 采样一批轨迹。
- 在更新时加入「概率比值裁剪」等项，限制每次更新的步子不要迈得太大。

直观来看，训练循环就是反复做三件事：

1. 让当前模型自己解题、写推理。
2. 用奖励模型和规则给每条解答打分。
3. 调整模型参数，让高分解答出现得更频繁。

---

## 工程细节：让训练真正跑起来

上面说的是「理论层面」的结构，真正要把 DeepSeek-R1 训出来，还要在工程上处理好几个关键问题。

### 采样策略与多样性

如果在采样推理轨迹时温度太低、`top_p` 太小，模型会反复输出几乎一样的推理；如果温度太高，又会产生大量质量参差不齐的文本。

合理的做法是：

- 在采样阶段保持一定多样性，让奖励模型有足够多的「好/坏样本」可以区分。
- 在强化学习阶段，靠奖励信号慢慢把分布收紧到「一簇高质量推理模式」上。

这和 AlphaGo 里「自对弈产生多样棋谱 + 价值网络筛选」的组合思路类似。

### 课程式训练

如果一上来就把所有难题丢给模型做 RL，很容易什么也学不会。

因此 DeepSeek-R1 会采用类似 Curriculum Learning 的策略：

- 先在相对简单、推理链较短的题目上做强化学习。
- 随着效果提升，再逐步加入更难、推理更长的题目。

这就像先教会模型「一步题、两步题」，再教它「多步题」。

### 计算成本与效率

生成推理轨迹、训练奖励模型和做 RL 更新，都是非常耗算力的步骤。

为了让整个流程可行，需要做一些权衡：

- 限制推理轨迹的最大长度 $T_{\max}$，避免序列过长导致成本爆炸。
- 控制奖励模型的规模 $d_{\text{rm}}$ 以及层数，让其评估开销在可接受范围内。
- 对 RL 更新步数、batch 大小做精心设计，避免在收益递减区间持续烧算力。

这些工程细节在论文里可能只占几行，但对实际复现效果非常关键。

---

## 总结：从 DeepSeek-R1 学到什么

把整条线索串起来，我们可以看到 DeepSeek-R1 带来的，不只是「分数更高」，更重要的是一种方法论上的升级：

- 在**问题层面**，正视了「只给答案不写过程」这一类大模型的核心短板。
- 在**建模层面**，把推理轨迹当成一等公民，用奖励模型显式评价过程质量。
- 在**优化层面**，用强化学习让模型偏向那些「过程合理、答案正确、长度适中」的解题路径。
- 在**工程层面**，通过采样策略、课程设计和算力权衡，把这一整套理论真正跑了起来。

对我们这些使用和研究大模型的人来说，DeepSeek-R1 给出的最重要启发之一是：

> 不再满足于「给我一个结果」，而是系统地训练模型去「把想法说出来，并且说得越来越好」。

这种「先让模型把思考过程显性化，再对过程本身施加优化目标」的范式，很可能会成为今后推理增强类模型的通用模板。

