---
title: "Vision-Language-Action (VLA) 模型综述：从动机到知识指导的触觉 VLA"
author: "在此填写作者"
date: today
lang: zh
categories: [机器人, 大模型, 多模态, 强化学习]
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# 摘要 / TL;DR

- **VLA（Vision-Language-Action）模型**的目标，是把视觉、语言和机器人动作统一到一个大模型里：给定视觉观测和语言指令，直接预测机器人动作序列。
- 主流路线是：视觉编码器 + 语言编码器 + 状态编码器，把图像/视频、文本指令、机器人状态整理成一串多模态 token，统一喂进一个 **decoder-only Transformer**，输出“动作 token”。
- 动作 token 可以有不同粒度：代码 token（VLAC）、关键姿态 token（VLAKP）、稠密控制 token（VLADP），再通过动作解码器映射成真实控制信号（关节位置/速度/力矩）。
- 数学上，VLA 把“在环境中执行任务”形式化成一个**条件序列生成问题**：在观测和指令条件下，自回归地预测动作 token，训练目标主要是行为克隆（BC）和离线 RL。
- 未来一个非常关键的方向是**知识指导的触觉 VLA**：在视觉+语言的基础上引入触觉（力/扭矩/触觉贴片）和显式知识，使机器人不仅“看得懂、听得懂”，还“摸得懂、想得对”。

---

# 1. 引言：从“看图说话”到“看图做事”

大模型在自然语言和视觉领域已经非常成熟：

- LLM 能“看懂”文本，进行复杂的推理和规划；
- VLM（Vision-Language Model）能“看图说话”：对图像/视频进行描述、问答和理解。

但对于机器人来说，仅仅“理解”还远远不够，真正困难的是：

> 如何把视觉和语言上的“理解”，转化为在物理世界里的“行动”？

传统管线往往是：

1. 感知模块（检测、分割、姿态估计）；
2. 状态估计与建图；
3. 任务规划与运动规划；
4. 控制器与接触控制。

这样的分层设计可解释、可分析，但工程成本极高，难以迁移和扩展。与此同时，基于 RL/模仿学习的策略又往往缺乏对现实世界的知识理解，很难直接利用 web 规模的数据。

在这样的背景下，VLA 应运而生：它试图用一个统一的多模态大模型，直接从视觉和语言出发，预测机器人动作。

---

# 2. 什么是 VLA？直观理解与形式化定义

## 2.1 非正式直觉

用一个非正式但直观的版本来讲 VLA：

1. 把图像/视频喂给一个视觉编码器（比如 ViT / CLIP / DINOv2）；
2. 把语言指令喂给文本侧（可以是 LLM 的文本 embedding）；
3. 把机器人内部状态（关节角、速度、力、抓手开合等）编码成状态 token；
4. 把这些视觉 token、文本 token、状态 token 拼成一串**多模态 token 序列**；
5. 统一喂进一个大的 decoder-only Transformer；
6. Transformer 输出的是一串“动作 token”：
   - 代码 token（VLAC）：类似于调用高层技能的代码；
   - 关键姿态 token（VLAKP）：表示一些关键帧位姿；
   - 稠密控制 token（VLADP）：表示低层的关节/力矩命令；
7. 最后通过一个动作解码器/控制器，把动作 token 变成真实的控制信号，驱动机器人连续运动。

这就是“一眼看上去”的 VLA。

## 2.2 形式化视角：条件策略建模

更形式化一点，设机器人处于一个离散时间的 MDP 中：

- $o_t$：第 $t$ 步的观测，包含图像、触觉、关节传感等；
- $s_t$：内部状态（有时我们把 $s_t = (o_t, s_t^{\text{int}})$）；
- $a_t$：第 $t$ 步动作，可以是连续控制向量，也可以是动作 token；
- $x$：任务描述/语言指令；
- $\pi(a_t \mid h_t, x)$：策略，其中 $h_t$ 是截至 $t$ 的历史。

传统 RL 想直接学 $\pi(a_t \mid s_t)$ 或 $\pi(a_t \mid h_t)$。VLA 的思路则是：

1. 把视觉/语言/状态映射到一个**共享的 token 序列**；
2. 用自回归 Transformer 建模联合分布：
   $$
   p_\theta(\mathbf{u}_{1:L}) = \prod_{i=1}^L p_\theta(\mathbf{u}_i \mid \mathbf{u}_{<i}) ,
   $$
   其中 $\mathbf{u}_{1:L}$ 里既包含观测 token，也包含动作 token；
3. 在训练时，固定观测部分，只对动作 token 做预测和优化。

换句话说，VLA 把“在物理世界中行动”转写成了“在多模态 token 序列上做条件语言建模”。

---

# 3. VLA 的动机：为什么需要统一的 Vision-Language-Action 模型？

## 3.1 传统方法的局限性

### 3.1.1 经典机器人管线

经典方法的特点：

- 手工设计模块：感知 → 建图 → 规划 → 控制；
- 模块间接口清晰、可解释；
- 针对具体场景高度优化，可在可靠性很重要的工业场景发挥巨大价值。

局限性：

- 工程复杂：每个新场景、新任务都需要大量工程改造；
- 可扩展性差：复用与泛化困难；
- 很难直接利用互联网规模的视觉和文本数据。

### 3.1.2 强化学习与模仿学习

RL/IL 的优势：

- 可以直接在状态-动作空间里学习策略；
- 在仿真中可以自动生成大量数据。

局限性：

- 数据昂贵：现实世界的交互成本很高；
- 泛化性有限：很难凭少量示例完成跨任务、跨环境迁移；
- 缺乏“世界知识”：不能直接读懂说明书、教程、网页上的知识。

## 3.2 VLM 的成功与缺口

Vision-Language Model（VLM）已经展示了强大的“看图说话”和视觉推理能力：

- 能理解场景、物体关系和文本说明；
- 能回答“把哪一个杯子放到碗里？”这类问题。

但它输出的是**文本**，不是机器人可以直接执行的**动作**。VLM 可以告诉你“红色杯子在右上角”，但不会给你一串关节轨迹或力矩曲线。

## 3.3 VLA 带来的统一视角

VLA 试图解决的关键问题是：

> 能否用一个统一的大模型，将视觉理解、语言理解和动作生成合在一起，让机器人“看图做事”？

这样的统一视角带来几方面潜在好处：

- 能直接利用 web 规模的多模态预训练；
- 能在多机器人、多任务、多场景上共享一个策略网络；
- 能在一个模型里同时处理“看、想、做”，减少人工接口设计。

---

# 4. VLA 的整体架构

## 4.1 四大核心模块

典型 VLA 可以拆成四大模块：

1. **视觉编码器 $f_{\text{vision}}$**
   - 输入：单帧图像、多帧视频、深度图、点云等；
   - 典型实现：ViT、CLIP、DINOv2、Video Swin、TimeSformer 等；
   - 输出：视觉 token 序列 $\mathbf{v}_1,\dots,\mathbf{v}_M$。

2. **语言编码器 $f_{\text{text}}$**
   - 输入：自然语言任务指令、对话历史；
   - 典型实现：LLM 的 embedding 模块，或一个冻结的文本 encoder；
   - 输出：文本 token 序列 $\mathbf{w}_1,\dots,\mathbf{w}_N$。

3. **状态编码器 $f_{\text{state}}$**
   - 输入：机器人内部状态 $s_t$（关节角、速度、末端位姿、抓手状态等），必要时加入历史窗口；
   - 输出：状态 token 序列 $\mathbf{s}_1,\dots,\mathbf{s}_K$。

4. **动作解码器 / 大模型主体（Transformer）**
   - 一般采用 **decoder-only Transformer**；
   - 输入：视觉 token、文本 token、状态 token，以及之前的动作 token；
   - 输出：下一个动作 token 的分布。

整个链路可以描述为：

```text
图像/视频  ─┐
            ├─> 视觉编码器  ─┐
文本指令  ──┤                │
机器状态  ──┘   文本编码器  ─┤
                              ├─> 多模态 token 序列 ─> Transformer ─> 动作 token
触觉/力信号 (可选) ─> 触觉编码器 ─┘
```

行动层面：

- Transformer 输出的动作 token 通过动作解码器（action decoder），转换为关节位置/速度/力矩，驱动机器人执行。

## 4.2 动作 token 的三种形态：VLAC / VLAKP / VLADP

你的初始理解已经很好，这里系统化展开：

### 4.2.1 VLAC：Code token（高层代码/技能调用）

- 把动作看成一段“程序”或“API 调用”；
- 例如：
  - `PICK(object=red_mug)`
  - `PLACE(target=bowl)`
  - `OPEN_DOOR(handle=door1)`
- 特点：
  - 高层语义强、可解释；
  - 可以和 LLM 的工具调用接口对齐；
  - 需要一个外部 **技能库 / 行为库**，将 token 映射为具体轨迹/控制器。

### 4.2.2 VLAKP：Key pose token（关键姿态）

- 表示机器人在任务执行过程中的关键帧姿态：
  - 如“手臂到杯子上方”、“抓取姿态”、“移动到碗上方”、“放置姿态”等；
- 每个 token 对应一个末端 6D 位姿 + 抓手开合等参数；
- 典型动作链路：
  - VLA 预测一串关键姿态 token；
  - 下游使用插值 + 轨迹优化生成连续轨迹。

特点：

- 比 code token 更贴近几何和物理；
- 序列长度比每周期控制短很多；
- 有一定可解释性，便于可视化与调试。

### 4.2.3 VLADP：Dense pose / 低层控制 token

- 直接表示每一个控制周期的低层命令：
  - 如关节位置/速度/力矩；
  - 或任务空间速度/加速度；
- 优点：
  - 可以直接驱动机器人；
  - 不需要额外 planner 或轨迹生成器。
- 缺点：
  - 序列很长，对长序列建模和稳定性要求极高；
  - 需要大量高频控制数据才能训练稳定。

### 4.2.4 混合动作表示

实际系统往往采用混合策略：

- 高层：用 VLAC 表示任务拆解和技能调用；
- 中层：用 VLAKP 表示关键姿态；
- 底层：在必要时使用 VLADP 做精细控制。

VLA 在统一的 token 序列中，混合了这些不同粒度的动作 token。

---

# 5. 数学推导：把 VLA 看成条件序列生成

本节给出较完整但不过分形式化的数学推导。

## 5.1 数据与目标：行为克隆视角

设有数据集：

$$
\mathcal{D} = \{ (x^{(k)}, o^{(k)}_{1:T_k}, s^{(k)}_{1:T_k}, a^{(k)}_{1:T_k})\}_{k=1}^N ,
$$

其中：

- $x^{(k)}$：第 $k$ 条轨迹的任务描述/语言指令；
- $o^{(k)}_{t}$：第 $t$ 步的外部观测（图像、触觉等）；
- $s^{(k)}_{t}$：第 $t$ 步内部状态（关节角等）；
- $a^{(k)}_{t}$：第 $t$ 步的动作（可以是 token 或连续向量）。

VLA 的目标是学习一个参数为 $\theta$ 的策略 $\pi_\theta$，去最大化行为克隆（BC）目标：

$$
\max_\theta 
\sum_{k=1}^N \sum_{t=1}^{T_k}
\log \pi_\theta(a^{(k)}_t \mid x^{(k)}, o^{(k)}_{\le t}, s^{(k)}_{\le t}, a^{(k)}_{<t}) .
$$

这和语言模型的自回归目标形态非常类似，只不过：

- 语言模型的 token 是字/词；
- VLA 的 token 是视觉/状态/动作等多模态符号。

## 5.2 多模态 token 化与向量量化

视觉、状态、动作很多是连续变量，需要先**离散化**成 token：

### 5.2.1 向量量化（VQ）

对动作或状态向量 $z \in \mathbb{R}^d$，构建一个 codebook：

$$
\mathcal{C} = \{ c_1, \dots, c_K \}, \quad c_i \in \mathbb{R}^d .
$$

定义量化过程：

$$
\text{VQ}(z) = \arg\min_{i} \|z - c_i\|_2 .
$$

得到一个索引 $i^\* \in \{1,\dots,K\}$，我们把这个索引视为动作 token 的 id。

训练时可以：

- 对 VQ 模块单独训练（类似 VQ-VAE）；
- 或联合训练，使 codebook 更适配 VLA 的动作分布。

如果动作采用 VQ token 表示，则行为克隆的目标变为交叉熵：

$$
\mathcal{L}_{\text{BC}} =
- \sum_{t} \log p_\theta(z_t^{\text{true}} \mid \text{context}) ,
$$

其中 $z_t^{\text{true}}$ 是真实动作向量量化后对应的 codebook 索引。

### 5.2.2 混合离散-连续输出

在某些设计中：

- 高层动作（VLAC、VLAKP）用离散 token；
- 低层控制（VLADP）直接回归连续向量：

$$
\mathcal{L} =
\mathcal{L}_{\text{CE}}(\text{离散 token}) + 
\lambda \, \|\hat{a}_t^{\text{cont}} - a_t^{\text{cont}}\|_2^2 .
$$

## 5.3 自回归 Transformer 的联合建模

定义统一序列：

$$
\mathbf{u}_{1:L} = (
\text{[TASK]}, x_1,\dots,x_{N_x}, 
\text{[OBS]}, o_1,\dots,o_{N_o}, 
\text{[STATE]}, s_1,\dots,s_{N_s},
\text{[ACT]}, a_1,\dots,a_{N_a}
).
$$

用一个 decoder-only Transformer 建模：

$$
\hat{p}_\theta(\mathbf{u}_{1:L})
= \prod_{i=1}^L p_\theta(\mathbf{u}_i \mid \mathbf{u}_{<i}) .
$$

训练时通常：

- 把观测和指令部分作为条件给定；
- 仅对动作 token 计算损失并反向传播。

推理时：

1. 给定任务指令 $x$；
2. 读入当前的视觉、状态、触觉观测；
3. 用自回归方式一步一步生成动作 token，直到到达时间窗口或终止 token。

---

# 6. 训练与推理中的关键工程细节

## 6.1 多任务、多机器人、多场景

VLA 的一个关键目标，是“一个模型统一多机器人、多任务、多环境”。常见做法：

- **任务 token**：
  - 引入表征任务类别/任务描述的特殊 token；
  - 例如 `[TASK_pick_and_place]`、`[TASK_open_door]` 等；
  - 数学上就是把任务 id $g$ 作为额外条件：
    $$
    \pi_\theta(a_t \mid o_{\le t}, x, g) .
    $$

- **机器人 ID / 形态 token**：
  - 不同机器人（七自由度机械臂、移动底盘、人形机器人）用不同 ID；
  - 强制模型在共享参数的前提下，区分控制模式。

- **环境 ID token**：
  - 对不同工位、不同房间、仿真环境，引入环境 token；
  - 辅助模型泛化到未见环境组合。

## 6.2 RL 与安全约束：超越纯行为克隆

在纯 BC 之外，VLA 还可以结合 RL 和安全约束：

- **离线 RL / 离线策略优化**：
  - 在离线数据上估计回报值；
  - 重新加权轨迹，使模型偏向高回报动作。

- **偏好学习（RLHF/RLAIF）**：
  - 由人类或 LLM 对执行片段给出偏好；
  - 用类似语言模型 RLHF 的方式优化策略。

- **安全损失**：
  - 对超出力矩限制、碰撞、越界等行为施加惩罚；
  - 在训练时引入额外项：
    $$
    \mathcal{L}_{\text{safety}} =
    \mathbb{E}[\max(0, \|\tau_t\| - \tau_{\max})] + \dots
    $$

整体损失可写成：

$$
\mathcal{L} 
= \mathcal{L}_{\text{BC}} 
+ \lambda_{\text{RL}} \mathcal{L}_{\text{RL}} 
+ \lambda_{\text{safety}} \mathcal{L}_{\text{safety}} .
$$

## 6.3 推理中的控制回路

部署 VLA 一般采用分层控制：

1. **决策层（低频，10–50 Hz）**：
   - VLA 接收最新视觉、语言、状态输入；
   - 输出一小段动作 token 或关键姿态；
2. **执行层（高频，100–1000 Hz）**：
   - 轨迹插值、力控、阻抗控制等；
   - 实时闭环稳定执行。

关键问题：

- 时延：感知-决策-执行延迟不能太大，否则轨迹抖动；
- 纠错：VLA 需要能够应对执行偏差，在线重规划；
- 安全：在异常触觉/视觉信号下，需要快速中断或切换控制模式。

---

# 7. 与现有工作：从 Web 知识到通用 VLA

不列具体论文名，可以用抽象类别来看 VLA 生态。

## 7.1 Web 规模预训练 + 机器人微调

一大类工作类似于：

- 在 web-scale 图像-文本数据上预训练视觉-语言 backbone；
- 再在机器人数据集上进行行为克隆或 RL 微调。

好处是：

- 大幅减少机器人专用数据的需求；
- 让机器人继承了大模型的“世界知识”和语言理解能力。

## 7.2 通用 VLA 模型与开放框架

另一类工作强调：

- 多机器人、多任务、多环境的统一；
- 开源模型和训练框架；
- 强调 token 化设计、架构细节等。

这类工作通常对 VLA 架构做了较系统的工程实践，提供了统一的接口和训练/推理代码，使社区更容易复用和扩展。

## 7.3 代码式 VLA：Code as Action

还有一类更极端的设计：

- 直接把动作视为一段“程序”或 DSL（领域特定语言）；
- 模型输出例如：

  ```text
  move_to(obj="red_mug", above=0.1);
  grasp();
  move_to(target="bowl", above=0.1);
  release();
  ```

- 将这些代码通过解释器映射为运动规划与控制命令。

优点：

- 高度可解释、易于调试与验证；
- 可以借用软件工程的形式化验证方法。

缺点：

- 对解释器/技能库要求很高；
- 需要解决“代码里描述的动作”和“物理世界实际可执行动作”之间的鸿沟。

## 7.4 接触丰富任务上的 VLA

针对插拔、拧螺丝、柔性物体操作等任务，单靠视觉不够，出现了“触觉增强”的 VLA 变体：

- 把触觉/力信号编码成额外 token ；
- 在多模态序列上联合建模；
- 尝试让模型学会利用接触反馈调整动作。

这自然引出下一节的重点：**知识指导的触觉 VLA**。

---

# 8. 知识指导的触觉 VLA：让机器人“摸得懂、想得对”

本节是偏前沿和展望性质的内容，适合作为 blog 的亮点部分。

## 8.1 为什么仅有视觉和语言还不够？

对于许多“远距离操作”任务（拿起物体、移动到目标位置等），视觉+语言已经能做得不错。但以下任务对触觉尤为敏感：

- 插拔：插头插入插座、数据线插接口；
- 拧紧/松开：旋钮、螺丝、瓶盖；
- 柔性物体操作：叠衣服、挤牙膏、捏碎包装等；
- 打磨/抛光/按压：需要力控和接触状态感知。

这些任务的核心难点：

- 需要感知“微小的力变化”和“接触模式变化”；
- 需要知道“什么时候该加力、什么时候该卸力”。

简单的基于视觉的 VLA 不足以胜任，因为许多关键信息（比如是否卡住、是否滑脱）完全体现在触觉/力信号中。

## 8.2 触觉信号的多模态扩展

在架构上，最自然的做法是把触觉当作新的模态：

- 输入：
  - 末端力/力矩传感器读数；
  - 关节力矩估计；
  - 触觉贴片阵列（类似一个低分辨率“触觉图像”）；
  - 高频时间序列（1kHz 甚至更高）。
- 触觉编码器 $f_{\text{tactile}}$：
  - 对标量时间序列，可以用 1D CNN 或小型 Transformer；
  - 对触觉图像，可以用 CNN/ViT；
  - 输出触觉 token $\mathbf{h}_1,\dots,\mathbf{h}_{N_h}$。

整体策略变为：

$$
\pi_\theta(a_t \mid
o^{\text{vision}}_{\le t},
h^{\text{tactile}}_{\le t},
x,
s_{\le t}
) .
$$

触觉 token 与视觉、语言、状态 token 一起送入 VLA 主干 Transformer。

## 8.3 “知识指导”从何而来？

“知识指导”大致可以分成三种来源：

1. **显式知识（symbolic / 文本）**
   - 工程手册、维修说明、材料属性文档；
   - 例如：
     - “插 USB 时不要用力硬插，应先对齐后轻推”；
     - “玻璃制品不能施加过大力”。

2. **隐式知识（LLM 内部语义）**
   - LLM 在大规模预训练中“读”过的大量文本；
   - 包含关于力、材料、安全等常识；
   - 可以通过 prompt 或知识蒸馏的方式注入到 VLA 策略中。

3. **经验知识（episodic memory）**
   - 过往的成功/失败执行轨迹；
   - 可以结构化存储，并通过检索（retrieval）提供给当前策略；
   - 例如：“之前插线时卡住的位置和处理方式”。

## 8.4 知识指导触觉 VLA 的示意架构

可以设想一个模块化的架构：

1. **底层多模态编码层**
   - 视觉编码器 $f_{\text{vision}}$；
   - 触觉编码器 $f_{\text{tactile}}$；
   - 状态编码器 $f_{\text{state}}$；
   - 文本编码器 $f_{\text{text}}$（任务指令、对话）。

2. **知识模块 $g_\phi$**
   - 可以是一个 LLM 或结构化知识图谱接口；
   - 输入：
     - 当前任务描述 $x$；
     - 历史执行摘要；
     - 相关文档/说明书片段；
   - 输出：
     - 高层 code token（VLAC），如 `INSERT_CABLE_WITH_FORCE_LIMIT(5N)`；
     - 或“知识 token”：表示一些约束或建议（例如最大允许力、接触策略等）。

3. **VLA 主干（Transformer）**
   - 输入：
     - 视觉 token；
     - 触觉 token；
     - 状态 token；
     - 文本 token；
     - 知识 token；
   - 输出：
     - 动作 token（可以是 VLAKP + VLADP 的组合）。

4. **控制执行层**
   - 把动作 token 解码成具体控制信号；
   - 对触觉信号进行实时监控：
     - 若检测到异常力峰值或异常接触模式，则触发安全策略或 replanning。

在数学上，可以把知识 token 视作额外条件 $k$：

$$
\pi_\theta(a_t \mid
o^{\text{vision}}_{\le t},
h^{\text{tactile}}_{\le t},
x,
s_{\le t},
k
) ,
$$

其中 $k = g_\phi(x, \text{knowledge base})$ 是知识模块生成的特征。

## 8.5 训练方式设想

### 8.5.1 多源数据混合

- **真实机器人数据**：
  - 包含视觉、触觉、状态、动作；
  - 带成功/失败标签，或回报信息。
- **仿真数据**：
  - 易于生成大量高质量触觉与力信息；
  - 可以用于预训练触觉编码器和策略的初始版本。
- **文本知识数据**：
  - 手册、文档、网络教程；
  - 用于预训练知识模块和语言 encoder。

### 8.5.2 分阶段训练

一个合理的训练流程可能是：

1. 阶段 1：预训练视觉/语言/触觉 encoder 和 LLM（知识模块）；
2. 阶段 2：在大量带触觉的机器人轨迹上做行为克隆；
3. 阶段 3：引入知识模块产生的知识 token，联合微调策略，使其遵循知识约束；
4. 阶段 4：在真实机器人上进行少量在线 RL 或偏好学习，进一步强化任务成功率和安全性。

### 8.5.3 损失设计

除了标准的 BC 损失外，可引入：

- **触觉安全损失**：
  - 对超出安全阈值的力/力矩施加惩罚；
- **知识一致性损失**：
  - 当知识模块说“最大力 5N”，而策略产生了需要 >5N 的动作时，加入惩罚；
- **偏好/奖励相关损失**：
  - 通过人类评价或 LLM 评分去调整策略偏好。

示意性写法：

$$
\mathcal{L}
= \mathcal{L}_{\text{BC}}
+ \lambda_{\text{tactile}} \mathcal{L}_{\text{tactile-safety}}
+ \lambda_{\text{knowledge}} \mathcal{L}_{\text{knowledge-consistency}} + \dots
$$

## 8.6 潜在优势与挑战

**潜在优势：**

- 能处理需要精细力控和接触感知的复杂任务；
- 可以利用显式知识减少“瞎试”的次数：
  - 比如插线先对齐再施力，而不是硬推；
- 更好的可解释性：
  - 知识模块能够用自然语言解释当前策略和失败原因；
- 安全性提升：
  - 通过知识约束和触觉安全损失减少危险行为。

**关键挑战：**

- 数据昂贵：
  - 真实触觉数据难采集、难标注；
- 多模态对齐难：
  - 视觉、触觉、语言、知识之间的对齐需要精心设计；
- 算力与时延：
  - 引入触觉与知识模块后，模型规模和推理延迟增加；
- 安全保证：
  - 即使有知识指导，仍需额外的安全监控与冗余机制。

---

# 9. 总结与展望

本文从动机、整体架构、数学推导和工程细节的角度，系统梳理了 VLA（Vision-Language-Action）模型的基本框架，并重点讨论了一个前沿方向：**知识指导的触觉 VLA**。

关键要点回顾：

- **VLA 的本质**：把机器人决策问题转写为一个多模态条件序列生成问题，让一个大模型同时理解视觉、语言、状态，并输出动作 token；
- **动作层次化表示**：通过 VLAC（代码）、VLAKP（关键姿态）、VLADP（稠密控制）等多层次动作 token，使模型既可解释又能精细控制；
- **训练与推理**：在行为克隆基础上，结合 RL、安全约束、多任务训练和分层控制，实现跨任务跨机器人的统一策略；
- **知识指导的触觉 VLA**：在视觉和语言之外引入触觉模态与显式知识，有望显著提升机器人在复杂接触任务上的表现与安全性。

展望未来几年，比较值得期待的方向包括：

- 用更大规模的多机器人、多任务、多模态数据训练真正通用的 VLA；
- 将网络知识、工程经验和安全规范系统性地注入 VLA 策略；
- 推动从“看图说话”到“看图做事，再到摸得懂、想得对”的整体飞跃。

在具体工程实践中，一个可能的路线是：

1. 从视觉+语言 VLA 起步，在相对简单的抓取/移动任务上验证；
2. 逐步引入力/触觉信号，开启触觉 VLA 的探索；
3. 在关键任务上引入知识模块，形成小规模试点系统；
4. 逐渐扩展到更丰富的任务和更复杂的机器人形态。

