---
title: "第20章：GPT-3与In-Context Learning"
subtitle: "When Scale Brings Emergence: The Birth of In-Context Learning"
author: "Ying Zha"
date: "2026-01-28"
categories: [NLP, Deep Learning, LLM, GPT-3, In-Context Learning]
tags: [GPT-3, In-Context Learning, Few-Shot, Zero-Shot, Prompt Engineering, 涌现能力, 大语言模型, Brown, OpenAI]
description: "前三章（Ch17–Ch19）回答了'如何训练大模型'的理论和工程问题——Scaling Laws告诉我们规模与性能的数学关系，训练稳定性技术让百亿参数的训练不崩溃，分布式系统让千亿参数模型在万卡集群上跑起来。但一个更本质的问题悬而未决：这些大模型到底能做什么'小模型做不到的事'？2020年，OpenAI的GPT-3用175B参数给出了一个令人震惊的答案——In-Context Learning：不需要任何梯度更新，仅通过在输入中提供几个示例，模型就能学会新任务。这种能力不是被训练出来的，而是在规模达到一定阈值后自发涌现的。本章系统讲述GPT-3的架构、训练、ICL的现象与机制，以及由此诞生的Prompt Engineering范式。"
image: "figures/chapter-20/original/fig1-gpt3-zero-one-few-shot.png"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    fig-cap-location: bottom
bibliography: references.bib
---

> **核心问题**：当语言模型的规模从十亿增长到千亿参数时，是否会涌现出小模型不具备的全新能力？In-Context Learning——不经过梯度更新就能从少量示例中学会新任务——是如何发生的？
>
> **历史坐标**：2020 | Brown et al. "Language Models are Few-Shot Learners" (GPT-3) | 从规模到涌现、从微调到提示

::: {.callout-tip collapse="true"}
## 本章参考来源

### 论文
- **Brown et al. (2020)** "Language Models are Few-Shot Learners" (arXiv:2005.14165, GPT-3) — 参考了 Section 2（Approach: zero/one/few-shot 定义）、Section 3（全部42个benchmark实验结果）、Figure 2.1（ICL范式对比图）、Figure 1.2（ICL学习曲线）、Figure 1.3（聚合性能随规模的变化）、Table 2.1（8个模型规模配置）；提取了 5 张论文原图
- **Xie et al. (2022)** "An Explanation of In-context Learning as Implicit Bayesian Inference" (arXiv:2111.02080) — 参考了 ICL 作为隐式贝叶斯推断的理论框架
- **Dai et al. (2023)** "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers" (arXiv:2212.10559) — 参考了 ICL 与隐式梯度下降的等价性分析
- **Olsson et al. (2022)** "In-context Learning and Induction Heads" (arXiv:2209.11895, Anthropic) — 参考了归纳头（Induction Heads）作为 ICL 机制单元的发现
- **Min et al. (2022)** "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?" (arXiv:2202.12837) — 参考了"标签可能不重要，格式才重要"的反直觉发现
- **Zhao et al. (2021)** "Calibrate Before Use: Improving Few-Shot Performance of Language Models" (arXiv:2102.09690) — 参考了 ICL 的不稳定性分析（对示例选择、顺序、格式的敏感性）
- **Lu et al. (2022)** "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity" (arXiv:2104.08786) — 参考了示例顺序敏感性的量化分析

### 教材
- SLP3 Chapter 7 (Large Language Models) — 参考了 GPT-3 架构、In-Context Learning 定义和 scaling 讨论
- D2L Chapter 15 — 参考了预训练语言模型的教学组织

### 课程
- Stanford CS224N Lecture 10-11 (Winter 2025) — 参考了 GPT-3 和 In-Context Learning 的教学框架
- CMU 11-711 ANLP Lecture "Prompting & In-context Learning" (Fall 2024) — 参考了 Prompting 技术的系统化讲解
:::

---

## 从上一章说起

前三章我们系统地解决了大语言模型训练的三大基础问题。第17章揭示了规模与性能之间的幂律关系，Scaling Laws 将大模型训练从炼金术变成了可预测的工程科学。第18章讨论了如何在百亿参数的规模下保持训练稳定——从 Adam 到 BF16，从 warmup 到梯度裁剪，这些数值工程技巧构成了大模型训练的"安全网"。第19章则解决了最根本的硬件约束：通过数据并行、张量并行、流水线并行和 ZeRO 内存优化的组合，让千亿参数的模型能在成千上万张 GPU 上训练起来。

至此，"如何训练一个超大模型"的问题已经有了完整的答案。但正如第19章结尾所提到的：分布式训练解决了工程问题，但训练出来的大模型是否真的比小模型更有用？规模的增长带来的是量变还是质变？

这个问题的答案比任何人预期的都更加戏剧性。2020年5月，OpenAI发表了GPT-3论文——一个拥有1750亿参数的语言模型。GPT-3不仅在困惑度上符合Scaling Laws的预测（量变），它还展示了一种小模型完全不具备的全新能力：**In-Context Learning**（上下文学习）。你不需要对模型做任何微调，不需要更新一个参数，只需要在输入中给出几个任务示例，GPT-3就能"学会"执行这个任务——翻译、摘要、问答、甚至简单的算术。

这种能力不是被特意训练出来的。OpenAI只是训练了一个更大的语言模型来预测下一个词——和GPT-2做的事情完全一样。但当模型规模跨过某个阈值时，一种全新的能力似乎"自发涌现"了。这个发现震动了整个AI领域，因为它暗示着一种可能性：或许通过不断扩大规模，语言模型能够获得越来越多我们事先未预料到的能力。

> 💡 **本章核心洞察**：GPT-3的核心贡献不是"更大的语言模型"（那只是Scaling Laws的延续），而是发现了 **In-Context Learning**——一种无需梯度更新、仅通过输入示例就能完成新任务的能力。这种能力随规模增长而增强，改变了NLP的使用范式：从"预训练+微调"转向"预训练+提示"（prompting）。

---

## 问题的本质是什么？

### 从"微调范式"的局限说起

在GPT-3出现之前，使用预训练语言模型的标准流程是**预训练+微调**（pre-train then fine-tune）。第13章讲述的BERT就是这个范式的代表：先在大规模语料上做掩码语言模型预训练，然后为每个下游任务收集标注数据、添加任务特定的分类头、更新全部参数。

这个范式虽然有效，但存在三个根本性的限制。

第一个限制是**任务专用性**。每个任务都需要一个独立微调的模型。如果你有10个不同的NLP任务，就需要训练、部署和维护10个不同的模型。当模型规模达到百亿或千亿参数时，这意味着你需要存储数十个TB级别的模型权重——光是存储成本就令人望而却步。

第二个限制是**对标注数据的依赖**。微调需要下游任务的标注数据，通常至少数千到数万条。对于很多实际任务（如特定领域的信息抽取、新语言的翻译），获取高质量标注数据既昂贵又耗时。更尴尬的是，标注数据的获取速度远远跟不上人们对AI应用的需求。

第三个限制更加微妙：**微调可能损害模型的通用能力**。当你在一个小数据集上微调一个大模型时，模型往往会"忘记"预训练阶段学到的很多知识——这被称为灾难性遗忘（catastrophic forgetting）。微调后的BERT在目标任务上表现很好，但它不再是一个"通用的语言理解系统"。

这些限制共同指向一个更深层的问题：**我们能否找到一种使用方式，让一个模型不经修改就能适应各种任务？**

### 一个理想中的"万能模型"

让我们想象一下理想的场景：你有一个经过大规模预训练的语言模型，你想用它来做情感分析。在微调范式下，你需要收集情感标注数据、设计分类头、训练几个epoch、调参、评估。但如果模型足够"聪明"，你能不能直接告诉它："这是一个情感分类任务。'这部电影太棒了' → 正面。'演技很烂' → 负面。现在请分类：'剧情引人入胜'"——然后模型直接给出答案？

这正是In-Context Learning的核心思想：**将任务说明和示例直接放在模型的输入中，让模型通过"阅读"这些信息来完成任务，而不是通过梯度更新来"学习"任务。**

但这听起来像是天方夜谭。一个仅仅被训练来预测"下一个词"的模型，怎么可能仅凭输入中的几个示例就"学会"一个全新的任务？要回答这个问题，我们首先需要理解GPT-3本身。

---

## 核心思想与直觉

### 关键洞察：规模带来的不仅是"更好"，而是"不同"

GPT-3的核心洞察可以用一句话概括：**当语言模型足够大时，它不仅能更好地预测下一个词，还能从输入的上下文中"推断"出你想让它做什么任务，并直接执行。**

这不是一个连续的量变过程。GPT-2（1.5B参数）只能勉强做一些零样本任务，效果远不如微调模型。但GPT-3（175B参数）在很多任务上的few-shot表现已经接近甚至超过了微调的BERT。从1.5B到175B，参数量增加了约100倍，但ICL能力的提升远超100倍——这更像是一种相变（phase transition），而非线性增长。

为什么规模会带来这种质变？一个直觉性的解释是：语言模型在预训练时看到的文本中，天然地包含了大量"从示例中学习"的模式。互联网上充满了这样的文本结构：

```
问：法国的首都是哪里？
答：巴黎。

问：日本的首都是哪里？
答：东京。

问：巴西的首都是哪里？
答：
```

当模型在海量文本上训练时，它不仅学会了"东京是日本的首都"这样的知识，还学会了"如果前面有几个问答对，下一个应该按同样的格式回答"这种**元模式**（meta-pattern）。小模型可能只学到了浅层的文本模式（续写一段通顺的句子），而足够大的模型则学到了更深层的结构——包括"根据示例推断任务规则"的能力。

### In-Context Learning 的三种模式

GPT-3论文定义了三种使用模式，从简单到复杂：

**Zero-shot**（零样本）：只给任务描述，不给任何示例。

```
Translate English to French:
cheese =>
```

模型需要仅凭"Translate English to French"这个指令来理解任务并执行。这要求模型在预训练中就已经"知道"翻译是什么，以及如何从英文翻译成法文。

**One-shot**（单样本）：给一个示例。

```
Translate English to French:
sea otter => loutre de mer
cheese =>
```

一个示例帮助模型"锁定"了任务的格式（英文 => 法文）和风格。

**Few-shot**（少样本）：给几个到几十个示例。

```
Translate English to French:
sea otter => loutre de mer
peppermint => menthe poivrée
plush giraffe => girafe en peluche
cheese =>
```

更多的示例让模型更准确地理解任务边界和预期输出格式。

![GPT-3的四种任务范式对比：传统微调（需要梯度更新）vs Zero-shot / One-shot / Few-shot（仅需前向推理）。](figures/chapter-20/original/fig1-gpt3-zero-one-few-shot.png){#fig-gpt3-paradigms width=85%}

::: {.figure-caption}
*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 2.1*
:::

@fig-gpt3-paradigms 是GPT-3论文中最具标志性的图之一。它清晰地展示了传统微调与ICL三种模式的根本区别：微调需要大量标注数据和梯度更新来调整模型参数，而零样本、单样本和少样本学习完全不修改模型——所有"学习"都发生在前向推理的输入中。这不仅是技术方法的差异，更是**使用范式**的根本转变。

### 一个类比：从"培训员工"到"给指令"

传统的微调范式就像是**培训一个新员工**：你给他教材（标注数据），让他练习（训练），经过几周的训练（epoch），他终于能胜任这个特定岗位。但如果你需要他做另一项工作，就得重新培训。

In-Context Learning则像是**给一个经验丰富的通才下指令**：你不需要培训他，只需要告诉他"这是任务要求，这是几个例子"，他就能立刻理解并执行。他之所以能做到这一点，是因为他在过去的工作中已经积累了足够丰富的经验和能力——你不是在教他新技能，而是在**激活**他已有的能力。

GPT-3正是这样一个"通才"：它在预训练阶段看过了互联网上几乎所有类型的文本，积累了海量的"经验"。当你给它few-shot示例时，你不是在教它学翻译、学分类——这些能力它已经"知道"了。你做的只是**告诉它你想用哪种能力**。

---

## 技术细节

### GPT-3的模型架构

GPT-3的架构本身并无太大创新——它基本上就是一个更大的GPT-2。核心架构仍然是Decoder-only Transformer，使用因果（causal）自注意力和自回归语言建模目标。与GPT-2的主要区别在于：GPT-3采用了交替使用 dense 和 locally banded sparse attention pattern（类似 Sparse Transformer），但论文中并没有详细讨论这一细节的影响。

GPT-3论文训练了8个不同规模的模型，从1.25亿参数到1750亿参数：

| 模型名 | 参数量 $N$ | 层数 $n_{\text{layers}}$ | 隐藏维度 $d_{\text{model}}$ | 注意力头数 $n_{\text{heads}}$ | 头维度 $d_{\text{head}}$ | Batch Size | 学习率 |
|--------|-----------|----------|-----------|-----------|---------|-----------|--------|
| GPT-3 Small | 125M | 12 | 768 | 12 | 64 | 0.5M | 6.0×10⁻⁴ |
| GPT-3 Medium | 350M | 24 | 1024 | 16 | 64 | 0.5M | 3.0×10⁻⁴ |
| GPT-3 Large | 760M | 24 | 1536 | 16 | 96 | 0.5M | 2.5×10⁻⁴ |
| GPT-3 XL | 1.3B | 24 | 2048 | 24 | 128 | 1M | 2.0×10⁻⁴ |
| GPT-3 2.7B | 2.7B | 32 | 2560 | 32 | 80 | 1M | 1.6×10⁻⁴ |
| GPT-3 6.7B | 6.7B | 32 | 4096 | 32 | 128 | 2M | 1.2×10⁻⁴ |
| GPT-3 13B | 13B | 40 | 5140 | 40 | 128 | 2M | 1.0×10⁻⁴ |
| **GPT-3 175B** | **175B** | **96** | **12288** | **96** | **128** | **3.2M** | **0.6×10⁻⁴** |

: GPT-3 系列模型的架构配置。数据源自 Brown et al. (2020) Table 2.1。 {#tbl-gpt3-models}

这个表格包含了几个值得关注的设计细节。首先，所有模型的上下文窗口长度都是 $n_{\text{ctx}} = 2048$ 个token（与GPT-2相同）。其次，学习率随模型规模增大而减小——这与第18章讨论的训练稳定性直觉一致：更大的模型需要更温和的学习率来避免训练不稳定。第三，batch size随模型增大而增大（从0.5M到3.2M tokens），这是因为更大的模型能更高效地利用更大的batch（第17章的Scaling Laws预测了这一点）。

### 训练数据与计算量

GPT-3的训练数据由五个来源混合而成：

| 数据集 | Token数量 | 训练中权重 | 训练中的Epoch数 |
|--------|-----------|-----------|----------------|
| Common Crawl (过滤后) | 410B | 60% | 0.44 |
| WebText2 | 19B | 22% | 2.9 |
| Books1 | 12B | 8% | 1.9 |
| Books2 | 55B | 8% | 0.43 |
| Wikipedia | 3B | 3% | 3.4 |

: GPT-3 训练数据构成。注意训练权重并不正比于数据大小——高质量数据(WebText2, Wikipedia)被过采样。 {#tbl-gpt3-data}

一个有趣的观察是：虽然Common Crawl占据了绝大多数token（410B），但高质量数据集被显著过采样——WebText2只有19B token却占了22%的训练权重，Wikipedia只有3B token却被训练了3.4个epoch。这反映了一个现在被广泛接受的观点：**数据质量比数据数量更重要**，这在后来LLaMA等模型的训练中被进一步强化。

GPT-3 175B的总计算量约为 $3.14 \times 10^{23}$ FLOPs。按照第17章介绍的 $C \approx 6ND$ 公式，$6 \times 175 \times 10^9 \times 300 \times 10^9 = 3.15 \times 10^{23}$，与论文报告一致。然而，按照Chinchilla法则（$D^* \approx 20N$），175B参数应配 $3.5 \times 10^{12}$ tokens，而GPT-3只用了 $300 \times 10^9$ tokens——训练数据不到"最优值"的十分之一。换言之，**GPT-3是一个严重欠训练的模型**。但即便如此，它仍然展示了惊人的ICL能力——这让人不禁想象，如果GPT-3按照Chinchilla法则充分训练，效果会有多好。

![GPT-3系列模型的训练计算量对比。尽管GPT-3 175B比RoBERTa-Large大近500倍，但由于在远少于典型规模的token数上训练，其计算量的增长幅度远小于参数量的增长。](figures/chapter-20/original/fig5-gpt3-scaling-compute.png){#fig-gpt3-compute width=85%}

::: {.figure-caption}
*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 2.2*
:::

### In-Context Learning 的实验结果

GPT-3论文在42个benchmark上进行了系统评估，覆盖了传统NLP任务、翻译、常识推理、阅读理解等多个领域。

![GPT-3在42个accuracy-denominated benchmarks上的聚合性能。Zero-shot性能随规模稳步提升，但few-shot性能提升更快，表明更大的模型更擅长In-Context Learning。](figures/chapter-20/original/fig3-gpt3-aggregate-performance.png){#fig-gpt3-aggregate width=85%}

::: {.figure-caption}
*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 1.3*
:::

@fig-gpt3-aggregate 传达了本章最重要的实验发现：**ICL能力与模型规模之间存在超线性的关系**。随着模型从125M增长到175B：

- **Zero-shot** 性能稳步提升——这并不太令人惊讶，因为更大的模型存储了更多知识。
- **Few-shot** 性能提升得**更快**——这才是关键发现。小模型即使给了示例也无法有效利用，而大模型能够从少量示例中快速"学会"任务。

这个差异的含义是深远的：ICL不仅仅是"模型知道更多知识"的体现，它是一种随规模涌现的**元能力**——利用上下文信息来适应新任务的能力。

#### 完整数值示例：In-Context Learning的工作过程

**设定**：使用GPT-3对一段文本做情感分类。模型的输入就是一个文本序列，我们通过精心构造输入来"告诉"模型做什么。

**Step 1: 构造 Few-shot 输入**

```text
Classify the sentiment of the review.

Review: "The food was amazing and the service was excellent!"
Sentiment: Positive

Review: "Terrible experience. The room was dirty and staff was rude."
Sentiment: Negative

Review: "The hotel was okay, nothing special but not bad either."
Sentiment: Neutral

Review: "I absolutely loved this movie, the acting was superb!"
Sentiment:
```

**Step 2: 模型处理输入**

GPT-3将整个输入视为一个token序列，通过自回归方式逐token处理。当它到达最后一行的"Sentiment:"时，它需要预测下一个token。

**Step 3: 模型内部发生了什么？**

在传统的语言模型视角下，模型只是在"续写"一段文本。但在这个特定的上下文中，模型观察到了一个清晰的模式：

- 每段"Review: ... Sentiment: ..."构成一个样本
- 前三个样本建立了"正面评价→Positive，负面→Negative，中性→Neutral"的映射
- 最后一个Review的内容（"absolutely loved", "superb"）强烈暗示正面情感

**Step 4: 模型输出**

模型生成的下一个token（以最高概率）是"Positive"。

**解读**：从外部观察，GPT-3"学会了"做情感分类——它正确地将"I absolutely loved this movie"分类为Positive。但在模型内部，**没有发生任何参数更新**。模型所做的一切就是：阅读输入文本，识别出其中的任务模式，然后按照这个模式生成最合理的续写。

这个例子揭示了ICL的一个深刻特性：**它模糊了"学习"和"推理"的边界**。模型到底是在"学习"一个新任务，还是在"推理"出给定上下文中最合理的输出？这个问题至今仍是活跃的研究话题。

### 标志性实验结果

让我们看几个具体的benchmark结果，以量化ICL的效果。

**LAMBADA**（最后一词预测任务）：

| 模型 | 设定 | 准确率 |
|------|------|--------|
| GPT-3 175B | Zero-shot | 76.2% |
| GPT-3 175B | Few-shot | **86.4%** |
| 之前SOTA（微调模型） | 有监督 | 68.0% |

GPT-3的few-shot结果不仅超越了之前所有微调模型的SOTA，甚至zero-shot性能就已经大幅领先。这是ICL最令人印象深刻的展示之一。

**TriviaQA**（开放域问答）：

| 模型 | 设定 | 准确率 |
|------|------|--------|
| GPT-3 175B | Zero-shot | 64.3% |
| GPT-3 175B | One-shot | 68.0% |
| GPT-3 175B | Few-shot | **71.2%** |
| 微调的T5-11B+SSM | 有监督 | 60.5% |

在TriviaQA上，GPT-3的few-shot结果也超越了精心微调的T5-11B。这里需要注意的是，GPT-3使用的是闭卷设置（不访问外部知识库），完全依靠参数中存储的知识来回答问题。

**翻译**（En→Fr, En→De, En→Ro）：

GPT-3在英法翻译上达到了接近有监督SOTA的水平（few-shot BLEU 32.6 vs 有监督SOTA 35.0），这一点格外值得注意——GPT-3的训练数据中非英语文本只占约7%，但它仍然"学会了"翻译。

然而，GPT-3在某些任务上的表现并不令人满意。在自然语言推理（NLI）任务如 SuperGLUE 的 RTE 上，GPT-3 few-shot 只达到约 72%，而微调的BERT已经超过 85%。在需要精确理解句子间语义关系的任务上，ICL的局限性开始显现。

![GPT-3在不同规模下的ICL学习曲线。更大的模型不仅绝对性能更高，还展示出更陡峭的"学习曲线"——即从上下文示例中获益更多。](figures/chapter-20/original/fig2-gpt3-icl-learning-curves.png){#fig-gpt3-icl-curves width=85%}

::: {.figure-caption}
*Source: Brown et al. (2020) "Language Models are Few-Shot Learners", Figure 1.2*
:::

### ICL的规模效应：量化"越大越好"

@fig-gpt3-icl-curves 展示了一个精巧的实验：在一个简单的符号操作任务上（从单词中去除随机符号），不同规模的模型如何利用上下文示例。图中最引人注目的发现是：**大模型的ICL学习曲线显著更陡**。

这意味着什么？小模型即使给了100个示例，性能提升也很有限——它们缺乏从示例中"提取规则"的能力。而175B模型只需要10-15个示例就能接近完美表现。这不是因为大模型"记住了更多"，而是因为它们具备了一种更高层次的能力：**从少量示例中归纳规则**。

我们可以将不同规模下的ICL能力做一个定性总结：

| 规模 | Zero-shot | Few-shot | ICL行为 |
|------|-----------|----------|---------|
| ~125M | 接近随机 | 几乎无提升 | 无法有效利用上下文 |
| ~1.3B | 略好于随机 | 有一定提升 | 能识别简单模式 |
| ~13B | 有意义的性能 | 显著提升 | 能执行中等复杂度任务 |
| ~175B | 接近SOTA | 经常超越微调 | 能从示例推断复杂规则 |

---

## 工程实践

### GPT-3 API：LLM即服务的诞生

2020年6月，在论文发表后不久，OpenAI发布了GPT-3的API——这是历史上第一个大语言模型的商业化接口。用户不需要下载模型、不需要GPU，只需要发送一个HTTP请求，就能使用175B参数的语言模型。

这个商业决策的影响远超技术层面。它催生了一种全新的软件开发范式：**Prompt Programming**（提示编程）。开发者不再需要收集数据、训练模型、部署模型，而是通过设计合适的prompt来完成任务。一个人、一台笔记本、一个API key，就能构建之前需要整个ML团队才能开发的功能。

### ICL推理流程

::: {.callout-note}
## Algorithm 1: Few-Shot In-Context Learning Inference（改编自 Brown et al., 2020）

```
Input:  预训练语言模型 LM，任务描述 T，
        K 个示例 {(x₁,y₁), ..., (xₖ,yₖ)}，查询输入 x_query
Output: 预测标签 ŷ

1. 构造 prompt：
     prompt ← T + "\n\n"
     for k = 1 to K:
         prompt ← prompt + format(xₖ, yₖ) + "\n\n"
     prompt ← prompt + format_query(x_query)

2. 前向推理（无梯度更新）：
     token_probs ← LM.forward(prompt)

3. 提取预测：
     ŷ ← argmax token_probs  # 或采样

4. return ŷ
```

*改编自 Brown et al. (2020) "Language Models are Few-Shot Learners", Section 2.1。注意：整个过程没有任何参数更新，所有"学习"都编码在 prompt 的构造中。*
:::

::: {.callout-note}
## Algorithm 2: Contextual Calibration（Zhao et al., 2021）

```
Input:  语言模型 LM，prompt 模板 P，
        标签空间 Y = {y₁, ..., yₘ}，查询输入 x_query
Output: 校准后的预测 ŷ

1. 测量先验偏差：
     p̂_bias ← LM(P + "N/A")   # 用空内容输入测量偏差
     W ← diag(p̂_bias)⁻¹        # 构造校准矩阵

2. 对查询进行推理：
     p̂_raw ← LM(P + x_query)   # 原始预测概率

3. 校准：
     p̂_cal ← normalize(W · p̂_raw)  # 减去先验偏差

4. return ŷ ← argmax p̂_cal
```

*Source: Zhao et al. (2021) "Calibrate Before Use: Improving Few-Shot Performance of Language Models", Section 3. [arXiv:2102.09690](https://arxiv.org/abs/2102.09690)*
:::

### Prompt设计的核心原则

ICL的效果高度依赖于prompt的设计。以下是从GPT-3论文和后续研究中总结的关键原则：

**原则1：格式一致性**

Few-shot示例应该保持一致的格式。模型从示例中学习的不仅是任务内容，更重要的是**输入-输出的映射格式**。

```python
# 好的 prompt：格式一致
prompt = """
Q: What is the capital of France?
A: Paris

Q: What is the capital of Japan?
A: Tokyo

Q: What is the capital of Brazil?
A: """

# 差的 prompt：格式不一致
prompt = """
The capital of France is Paris.
Q: What is the capital of Japan?
Answer: Tokyo
What about Brazil's capital?
"""
```

**原则2：示例的代表性**

选择的示例应该覆盖任务的不同方面：

```python
# 情感分类：覆盖正面、负面、中性
examples = [
    ("Great product!", "Positive"),
    ("Terrible quality.", "Negative"),
    ("It's okay.", "Neutral"),
]
```

**原则3：任务描述的清晰性**

一个清晰的任务描述（task instruction）可以显著提升zero-shot性能：

```python
# 有明确任务描述
prompt = "Classify the following movie review as Positive or Negative.\n\n"

# 无任务描述（仅靠示例推断）
prompt = ""  # 直接给示例
```

### 一个完整的Few-shot实现

```python
import openai

def few_shot_classify(text, examples, task_desc, model="gpt-3"):
    """
    使用 Few-shot ICL 进行文本分类。

    Args:
        text: 待分类的文本
        examples: list of (input, label) 示例对
        task_desc: 任务描述字符串
        model: 模型名称

    Returns:
        模型预测的标签
    """
    # 构造 prompt
    prompt = task_desc + "\n\n"

    # 添加 few-shot 示例
    for inp, label in examples:
        prompt += f"Input: {inp}\nLabel: {label}\n\n"

    # 添加待分类样本
    prompt += f"Input: {text}\nLabel:"

    # 调用模型（生成 1 个 token 即可）
    response = openai.Completion.create(
        model=model,
        prompt=prompt,
        max_tokens=5,
        temperature=0,  # 贪心解码，确保确定性
        stop=["\n"]      # 遇到换行停止
    )

    return response.choices[0].text.strip()

# 使用示例
examples = [
    ("The food was delicious!", "Positive"),
    ("Worst service ever.", "Negative"),
    ("It was fine, nothing special.", "Neutral"),
]

result = few_shot_classify(
    text="I absolutely loved the atmosphere!",
    examples=examples,
    task_desc="Classify the sentiment of the following review."
)
print(f"Prediction: {result}")  # Prediction: Positive
```

这段代码虽然简单，但它包含了ICL的全部核心逻辑：构造一个包含任务描述和示例的prompt，将其送入模型，从生成的文本中提取答案。没有训练循环、没有损失函数、没有梯度计算——"学习"完全发生在输入构造阶段。

### 复现细节与工程注意事项

**Token预算管理**：GPT-3的上下文窗口只有2048个token，few-shot示例占据的token越多，留给输入文本和输出的空间就越少。在实践中需要在"示例数量"和"输入长度"之间权衡：

$$
n_{\text{ctx}} = n_{\text{instruction}} + K \cdot n_{\text{example}} + n_{\text{input}} + n_{\text{output}}
$$

其中 $K$ 是示例数量。如果每个示例平均占50个token，任务描述占20个token，那么在2048的窗口下，即使留200个token给输入和输出，也最多只能放约36个示例。这解释了为什么GPT-3论文中的few-shot通常只用10-100个示例。

**Temperature设置**：对于分类等确定性任务，应使用 $\text{temperature} = 0$（等价于贪心解码）；对于创意生成任务，可以使用 $0.7 - 1.0$ 的温度。

**Stop tokens**：在分类任务中，设置合适的停止token（如换行符`\n`）可以防止模型生成多余的文本。

---

## 深入理解

> **研究者必读**：这一节探讨In-Context Learning的理论机制、边界条件和开放问题。

### 为什么有效？——理论视角

In-Context Learning为什么能work？这是一个看似简单、实则深刻的问题。一个仅被训练来预测下一个词的模型，为什么能从输入中的几个示例"学会"新任务？自2020年GPT-3论文发表以来，多个研究团队从不同角度提出了理论解释。

#### 理论1：ICL作为隐式贝叶斯推断

Xie et al. (2022) 提出了一个优雅的理论框架：In-Context Learning本质上是在做**隐式贝叶斯推断**（Implicit Bayesian Inference）。

核心思想是这样的：假设预训练数据来自多个"概念"（concept）的混合分布。每个概念决定了文本的生成规则——比如"情感分类"是一个概念，"翻译"是另一个概念。当模型在预训练中学习了这个混合分布后，给它few-shot示例等价于给它观测数据，让它推断当前的"概念"是什么。

形式化地，设 $\theta$ 表示潜在概念，$x_{1:K}$ 是few-shot示例，$x_{\text{query}}$ 是查询输入。模型的预测可以理解为：

$$
p(y \mid x_{\text{query}}, x_{1:K}) = \int p(y \mid x_{\text{query}}, \theta) \cdot p(\theta \mid x_{1:K}) \, d\theta
$$

其中 $p(\theta \mid x_{1:K})$ 是模型从示例中推断出的概念后验分布。更多的示例提供更多"证据"，使后验更加集中，从而提高预测准确性。

这个理论的美妙之处在于它解释了两个实验现象：第一，为什么更多示例能提高性能（更多证据→更好的后验）；第二，为什么示例的标签可能不那么重要（示例的主要作用是帮助推断概念/任务，而非学习输入-输出映射）。

#### 理论2：ICL作为隐式梯度下降

Dai et al. (2023) 提出了另一个令人惊讶的视角：Transformer的注意力机制在处理few-shot示例时，其行为**类似于梯度下降**。

他们的核心观察是：考虑一个线性注意力层 $f(q) = W_V X^\top \cdot \text{softmax}(X W_K^\top q)$。当few-shot示例被加入上下文时，这等价于对一个线性模型做了一步梯度更新。具体地，他们证明了在某些简化假设下：

$$
\text{ICL}_{\text{Attention}}(x_{\text{query}}) \approx \text{ICL}_{\text{GD}}(x_{\text{query}}) = W_0 x_{\text{query}} - \eta \nabla_{W} \mathcal{L}(W_0) \cdot x_{\text{query}}
$$

其中 $W_0$ 是预训练权重，$\mathcal{L}$ 是基于few-shot示例的损失函数，$\eta$ 是隐式"学习率"。

换言之，Transformer的一层注意力计算等价于一步梯度下降——前向传播中隐含了一个微型的"训练过程"。这个理论的力量在于它将ICL和传统的基于梯度的学习统一到了同一个框架下。

#### 理论3：归纳头（Induction Heads）

Olsson et al. (2022, Anthropic) 从更微观的层面发现了ICL的一种具体机制：**归纳头**（Induction Heads）。归纳头是一种由两个注意力头协作形成的电路，其功能是：如果在上下文中出现了模式 "[A][B] ... [A]"，归纳头会预测下一个token是 "[B]"。

这看起来只是一个简单的"复制"模式，但它是ICL的基础。当few-shot示例中包含 "positive → Positive" 的映射，而查询也是一个positive的文本时，归纳头帮助模型"复制"对应的标签。

Olsson et al. 的关键发现是：

1. 归纳头在训练过程中会突然出现（phase transition），这与ICL能力的突然涌现一致。
2. 消除归纳头会导致ICL能力大幅下降。
3. 归纳头的形成与模型规模有关——更大的模型能形成更复杂的归纳头。

### 为什么有效？——实证视角

Min et al. (2022) 的工作提供了关于ICL的最反直觉的实证发现之一。他们系统地测试了few-shot示例中各个成分的重要性：

**实验**：在情感分类任务中，将few-shot示例的标签**随机替换**（正面文本标注为"Negative"，反面文本标注为"Positive"），然后观察模型性能的变化。

**结果**：令人震惊的是，**使用随机标签的few-shot性能与使用正确标签几乎相同**。

这意味着什么？模型从few-shot示例中"学到"的主要不是输入-输出的映射（这是我们直觉上认为的），而是：

1. **输入的分布**：什么样的文本是有效输入
2. **输出的格式和空间**：标签应该是"Positive"或"Negative"（而不是自由文本）
3. **输入-输出的映射模式**：一个输入对应一个标签（而不是一段解释）

这个发现与Xie et al. 的贝叶斯推断理论高度一致：few-shot示例的主要作用是帮助模型**锁定任务类型**（推断概念 $\theta$），而非学习具体的映射规则。

然而，需要注意的是，这个结论并非在所有任务和规模上都成立。在某些需要学习新映射的任务上（如将非标准标签映射到类别），正确标签确实重要。这个发现的价值在于它迫使我们重新思考ICL"学到了什么"。

### 方法的边界条件

ICL并非万能的。以下是其已知的失效条件和局限：

**失效条件1：任务复杂度超过模型容量**。ICL在需要多步推理的任务上表现不佳。例如，多位数加法（如 "1234 + 5678 = ?"）即使给了很多示例，GPT-3也经常出错。这表明ICL能做的"学习"有其复杂度上限。这个局限直接催生了下一章将讨论的Chain-of-Thought。

**失效条件2：任务需要精确的格式对齐**。如果目标任务的输入-输出格式与预训练数据中常见的模式差异很大，ICL效果会显著下降。

**失效条件3：示例数量受限于上下文窗口**。GPT-3只有2048个token的窗口，这严重限制了可以提供的示例数量。对于需要大量示例才能学会的复杂任务，ICL的表现会受到上下文长度的瓶颈限制。

**失效条件4：对示例选择和顺序高度敏感**。这一点值得特别展开。

### ICL的不稳定性问题

Zhao et al. (2021) 和 Lu et al. (2022) 系统地量化了ICL的不稳定性，发现了几个令人不安的现象。

**示例顺序敏感性**：Zhao et al. (2021) 在 SST-2 情感分类任务上用 GPT-3 2.7B 做了一个触目惊心的实验——使用相同的4个few-shot示例，仅仅改变它们的**排列顺序**，准确率就从 **54.3%（接近随机猜测）波动到 93.4%（接近SOTA）**。更极端的是，在2-shot设置下，仅仅将两个示例的顺序**对调**，准确率就从 88.5% 骤降到 51.3%。Lu et al. (2022) 进一步证实，这种顺序敏感性在**所有模型规模**上都存在，增加模型大小或示例数量都**无法消除**这一问题。

**三种系统性偏差**：Zhao et al. 识别出了ICL中的三种系统性偏差来源。**多数标签偏差**（Majority Label Bias）：如果few-shot示例中某个标签出现得更频繁，模型会倾向于预测多数标签。**近因偏差**（Recency Bias）：模型倾向于预测与最后几个示例相同的标签。**常见token偏差**（Common Token Bias）：模型更倾向于预测在预训练数据中频繁出现的token（如预测"America"而非"Saint Lucia"）。

Zhao et al. (2021) 提出了一种名为**Contextual Calibration**的缓解方法：先用一个"空输入"（content-free input，如 "N/A"、"[MASK]" 或空字符串）来测量模型的先验偏差，然后将其从预测中减去。这种方法简单但效果显著——在某些任务上可以带来高达 **30个百分点** 的绝对提升。

### 开放研究问题

如果你要在这个方向写一篇论文，以下几个问题仍然值得深入探索：

第一个方向是**ICL的机制理解**。尽管已有贝叶斯推断、隐式梯度下降、归纳头等多种理论解释，但这些理论之间的关系尚不清楚。它们是同一现象的不同视角，还是描述了不同层次的机制？一个统一的理论框架仍然缺失。

第二个方向是**ICL能力的涌现条件**。为什么ICL需要如此大的模型规模才能出现？存在某个最小规模阈值吗？通过改进训练方法或架构设计，能否让更小的模型也获得强ICL能力？

第三个方向是**ICL与微调的关系**。ICL和微调是两种完全不同的"学习"吗？还是它们是同一连续体上的两个端点？如果ICL确实等价于隐式梯度下降，那它与显式微调的精确差异是什么？

---

## 局限性与未解决的问题

### 本方法的局限

GPT-3和In-Context Learning虽然代表了NLP范式的重大转变，但它们面临着几个根本性的局限。

第一个局限是**推理能力的缺失**。ICL擅长的是模式匹配和知识检索，但在需要多步推理、逻辑推导或数学计算的任务上表现很差。给GPT-3一个需要四步推理的数学题，即使提供了类似问题的示例，它也经常在中间步骤出错。这不是因为模型"不够大"——即使是175B参数，直接的ICL也无法可靠地执行复杂推理。

第二个局限是**不可控性**。ICL的输出高度依赖prompt的措辞、示例的选择和顺序，而这些因素的影响难以预测和控制。在需要稳定、可靠输出的生产环境中，这种不可控性是一个严重问题。

第三个局限是**计算成本**。每次推理都需要处理整个prompt（包括所有few-shot示例），这意味着ICL的推理成本远高于微调模型。175B参数模型的单次推理就需要大量计算，再加上几十个示例的长prompt，成本进一步攀升。

### 这些局限导向了什么？

ICL的推理缺陷引出了一个自然的问题：**如果模型无法直接"跳到"答案，能否让它像人类一样展示中间推理步骤？**如果给模型的示例中不仅包含最终答案，还包含详细的推理过程（"首先...，然后...，因此..."），模型能否学会这种"展示工作过程"的模式？

这正是下一章将讨论的 **Chain-of-Thought (CoT) Prompting** 的核心思想——它不是一种全新的能力，而是对ICL的一种巧妙利用：通过在few-shot示例中加入推理链条，引导模型也生成类似的中间推理步骤。CoT的出现进一步释放了大模型的潜力，但也引发了更深层的问题：LLM真的在"推理"吗？

> 下一章预告：第21章将聚焦**涌现能力与思维链推理**。Chain-of-Thought如何让GPT-3级别的模型在数学和逻辑推理上获得飞跃式提升？涌现能力是真实的相变还是度量方式的假象？Zero-shot CoT的神奇咒语"Let's think step by step"为什么有效？——这些问题将引导我们更深入地理解大语言模型的能力边界。

---

## 本章小结

### 核心要点回顾

本章围绕GPT-3和In-Context Learning，讲述了大语言模型从"基础设施"到"涌现能力"的关键跨越。

GPT-3（2020）以175B参数——96层Transformer，12288维隐藏状态，96个注意力头——成为当时最大的语言模型。它的架构本身并无太大创新（仍然是Decoder-only Transformer + 自回归语言建模），真正的突破在于规模带来的涌现：In-Context Learning。

In-Context Learning的核心发现是：足够大的语言模型不需要梯度更新就能从输入中的少量示例"学会"新任务。这种能力有三种模式——zero-shot（仅描述任务）、one-shot（一个示例）和few-shot（多个示例）——它们的效果与模型规模呈超线性关系：大模型不仅绝对性能更高，还更擅长利用上下文示例。

关于ICL为什么work，目前有三种主要的理论解释：隐式贝叶斯推断（Xie et al.，模型从示例推断潜在概念）、隐式梯度下降（Dai et al.，注意力计算等价于一步梯度更新）、以及归纳头机制（Olsson et al.，特定的注意力头模式实现了模式匹配）。Min et al.的实验进一步表明，ICL的主要作用可能是帮助模型"锁定任务类型"，而非学习具体的输入-输出映射。

然而，ICL也有明显的局限：对prompt设计高度敏感（示例顺序可导致30%到90%的准确率波动）、无法可靠执行多步推理、计算成本高昂。

### 关键公式速查

| 内容 | 公式/数值 |
|------|----------|
| GPT-3 参数量 | $N = 175 \times 10^9$ |
| 总计算量 | $C \approx 3.14 \times 10^{23}$ FLOPs |
| 上下文窗口 | $n_{\text{ctx}} = 2048$ tokens |
| Token预算约束 | $n_{\text{ctx}} = n_{\text{instr}} + K \cdot n_{\text{example}} + n_{\text{input}} + n_{\text{output}}$ |
| ICL贝叶斯推断 | $p(y \mid x_q, x_{1:K}) = \int p(y \mid x_q, \theta) \cdot p(\theta \mid x_{1:K}) d\theta$ |
| ICL隐式梯度下降 | $\text{ICL}(x_q) \approx W_0 x_q - \eta \nabla_W \mathcal{L}(W_0) \cdot x_q$ |
| 按Chinchilla法则的最优数据量 | $D^* = 20 \times 175B = 3.5T$ tokens（而GPT-3仅用300B） |

### 思考题

1. **[概念理解]** 为什么GPT-3的few-shot性能与模型规模之间是"超线性"关系（即大模型从示例中获益更多），而非简单的线性关系？提示：考虑ICL需要什么样的"元能力"，以及这种能力与模型容量的关系。

2. **[数学推导]** 根据Dai et al.的理论，线性注意力层处理few-shot示例等价于一步梯度下降。请推导：对于一个线性模型 $y = Wx$，如果使用MSE损失 $\mathcal{L} = \frac{1}{K}\sum_{k=1}^K \|Wx_k - y_k\|^2$，一步梯度下降后的权重更新 $W' = W - \eta \nabla_W \mathcal{L}$ 的显式形式是什么？它如何对应到注意力机制的Key-Value结构？

3. **[工程实践]** 使用OpenAI API（或其他LLM API），在SST-2情感分类数据集上比较以下设置的准确率：(a) zero-shot，(b) 4-shot随机示例，(c) 4-shot但标签随机打乱，(d) 4-shot相同示例但不同顺序（至少5种排列）。你的结果是否验证了Min et al.和Lu et al.的发现？

4. **[研究思考]** ICL的三种理论解释（贝叶斯推断、隐式梯度下降、归纳头）各自有什么假设和局限？它们是互斥的还是互补的？如果你要设计实验来区分这三种理论，你会怎么做？

5. **[开放思考]** GPT-3展示了"规模带来涌现"的现象，但这是否意味着"只要足够大就能解决一切"？考虑以下反例：(a) GPT-3在简单算术上的失败，(b) ICL的不稳定性不随规模完全消失，(c) 某些任务（如精确推理）可能需要根本不同的方法。你认为ICL的能力边界在哪里？

---

## 延伸阅读

### 核心论文（必读）

**Brown, T. et al. (2020). "Language Models are Few-Shot Learners"**。GPT-3原始论文，本章最核心的参考。重点阅读：Section 2（zero/one/few-shot的精确定义）、Section 3.1-3.5（关键benchmark结果）、Figure 1.2-1.3（ICL学习曲线和聚合性能）、Figure 2.1（ICL范式图）。可快速浏览：Section 4（数据污染分析）、Appendix中的详细实验表格。注意：论文长达75页，其中大量是实验结果，核心内容集中在前20页。[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)

### 理论基础

**Xie, S. et al. (2022). "An Explanation of In-context Learning as Implicit Bayesian Inference"**。将ICL理解为贝叶斯推断的优雅理论框架。重点阅读：Section 3（理论推导）。[arXiv:2111.02080](https://arxiv.org/abs/2111.02080)

**Dai, D. et al. (2023). "Why Can GPT Learn In-Context?"**。证明ICL等价于隐式梯度下降。重点阅读：Section 3（双重形式对比）。[arXiv:2212.10559](https://arxiv.org/abs/2212.10559)

**Olsson, C. et al. (2022). "In-context Learning and Induction Heads"**。Anthropic的研究，发现了ICL的微观机制。重点阅读：Section 2-3（归纳头的定义和实验验证）。[arXiv:2209.11895](https://arxiv.org/abs/2209.11895)

**von Oswald, J. et al. (2023). "Transformers Learn In-Context by Gradient Descent"**。严格证明单层线性自注意力可以实现一步梯度下降，N层可近似N步GD。与Dai et al.互补，提供了更形式化的理论保证。ICML 2023。[arXiv:2212.07677](https://arxiv.org/abs/2212.07677)

### 后续发展

**Min, S. et al. (2022). "Rethinking the Role of Demonstrations"**。反直觉发现：ICL中标签可能不如格式重要。[arXiv:2202.12837](https://arxiv.org/abs/2202.12837)

**Zhao, Z. et al. (2021). "Calibrate Before Use"**。系统分析ICL不稳定性并提出校准方法。[arXiv:2102.09690](https://arxiv.org/abs/2102.09690)

**Lu, Y. et al. (2022). "Fantastically Ordered Prompts and Where to Find Them"**。量化了示例顺序对ICL的影响。[arXiv:2104.08786](https://arxiv.org/abs/2104.08786)

**Wei, J. et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"**。下一章的核心论文，展示了如何通过ICL的变体解决推理问题。[arXiv:2201.11903](https://arxiv.org/abs/2201.11903)

### 综述与教程

**Dong, Q. et al. (2023). "A Survey on In-Context Learning"**。ICL领域最全面的综述。[arXiv:2301.00234](https://arxiv.org/abs/2301.00234)

### 代码资源

- **OpenAI API文档**：[platform.openai.com/docs](https://platform.openai.com/docs)（ICL的主要使用方式）
- **HELM**（Holistic Evaluation of Language Models）：[crfm.stanford.edu/helm](https://crfm.stanford.edu/helm/)（系统性评估LLM的ICL能力）

---

## 历史注脚

GPT-3的发布是AI历史上一个标志性的时刻，其影响远超技术领域。

2020年5月28日，Brown et al. 的论文在arXiv上发布，迅速引发了学术界的轰动。仅仅两周后的6月11日，OpenAI就宣布了GPT-3的API——这个速度本身就说明了论文和商业化是同步推进的。训练一次GPT-3 175B估计需要约355个GPU年的V100计算时间，成本约为**460万美元**。在此之前，使用大语言模型需要这种级别的投入和深厚的ML工程能力；API的出现让任何一个会写Python的人都能以每1000个token几美分的价格使用175B参数的模型。这催生了一波"prompt programming"的浪潮——Twitter上充斥着人们用GPT-3做各种令人惊叹（或令人担忧）的事情的演示：写代码、写诗、模拟对话、生成法律文书……

一个有趣的细节是：GPT-3论文有31位作者，但只有一小部分人参与了核心的模型训练和实验。论文中的许多benchmark实验实际上是由不同的小组分别完成的，这反映了大模型研究日益工业化的趋势——不再是一两个研究者的工作，而是需要整个团队的协作。

GPT-3也标志着一个概念的诞生：**基础模型**（Foundation Model）。2021年，Stanford HAI发布了著名的"基础模型"报告，其核心论点正是基于GPT-3的示范：一个足够大的预训练模型可以通过ICL适配几乎任何下游任务，因此它不再是"一个模型解决一个问题"，而是"一个基础模型支撑一整个应用生态"。这个概念深刻地重塑了AI行业的商业模式和研究方向。

讽刺的是，GPT-3虽然以"Few-Shot Learners"为标题强调了ICL，但后来最成功的GPT系列产品（ChatGPT、GPT-4）反而不是主要依靠ICL来使用的——它们通过指令微调（Instruction Tuning）和RLHF让模型学会了直接理解用户意图，不再需要few-shot示例。从这个角度看，GPT-3发现的ICL能力更像是一个**过渡性的里程碑**：它证明了大模型具有惊人的潜力，但释放这种潜力的最终方式不是ICL，而是更精细的对齐技术——这正是第23-25章将要讲述的故事。
