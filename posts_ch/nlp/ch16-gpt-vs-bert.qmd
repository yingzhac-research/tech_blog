---
title: "第16章：GPT vs BERT——两条路线的分化与融合"
subtitle: "Encoder-only, Decoder-only, or Encoder-Decoder: The Great Architecture Debate"
author: "Ying Zha"
date: "2026-01-27"
categories: [NLP, Deep Learning, Pre-training, GPT, BERT, T5, BART]
tags: [GPT, BERT, T5, BART, UniLM, Encoder-only, Decoder-only, Encoder-Decoder, 架构对比, 范式转变]
description: "BERT用双向注意力在理解任务上一骑绝尘，GPT用因果注意力在生成任务上独领风骚，T5和BART尝试用Encoder-Decoder统一两者。三种架构的差异本质上是Attention Mask的不同——全可见、因果三角、还是混合模式。但到2020年，一个令人意外的趋势正在形成：Decoder-only架构开始在几乎所有任务上追平甚至超越其他两种架构。这不是因为Decoder-only在架构上更优——T5的控制实验甚至表明Encoder-Decoder在同等条件下更强——而是因为自回归语言建模目标在规模化时展现出了压倒性的工程优势。从「预训练+微调」到「预训练+提示」的范式转变，彻底改写了我们对预训练架构的选择逻辑。"
image: "figures/chapter-16/original/fig-t5-architecture-variants.png"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    fig-cap-location: bottom
bibliography: references.bib
---

> **核心问题**：三种预训练架构——Encoder-only、Decoder-only、Encoder-Decoder——各有什么根本的优劣？为什么Decoder-only路线最终成为大语言模型时代的主流选择？
>
> **历史坐标**：2019–2020 | T5 (Raffel et al.), BART (Lewis et al.), UniLM (Dong et al.) | 预训练架构的系统性对比与范式转变

::: {.callout-tip collapse="true"}
## 本章参考来源

### 论文
- **Raffel et al. (2020)** "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (arXiv:1910.10683) — 参考了 Section 3.2（三种架构的系统性对比）、Figure 3（Attention Mask矩阵对比）、Figure 4（三种架构变体示意图）、Table 2（架构对比实验数据）。本章最核心的参考来源
- **Lewis et al. (2020)** "BART: Denoising Sequence-to-Sequence Pre-training" (arXiv:1910.13461, ACL 2020) — 参考了 Figure 1（BART vs BERT vs GPT架构对比）、Section 2（BART模型设计）、Table 1（预训练目标对比）
- **Dong et al. (2019)** "Unified Language Model Pre-training for Natural Language Understanding and Generation" (arXiv:1905.03197, NeurIPS 2019) — 参考了 Figure 1（统一注意力掩码示意图）；从论文中提取了UniLM通过Attention Mask统一三种范式的核心洞察
- **Wang et al. (2022)** "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?" (arXiv:2204.05832, ICML 2022) — 参考了零样本泛化实验结论：纯无监督预训练下因果解码器最强，多任务微调后Encoder-Decoder反超
- **Radford et al. (2019)** "Language Models are Unsupervised Multitask Learners" (GPT-2 paper, OpenAI) — 参考了零样本任务迁移的关键发现
- **Brown et al. (2020)** "Language Models are Few-Shot Learners" (arXiv:2005.14165, GPT-3 paper) — 参考了In-Context Learning范式和规模与能力的关系

### 教材
- **D2L** Section 11.9 — 参考了 Encoder-only / Encoder-Decoder / Decoder-only 三种架构的并列对比组织方式，以及 Figure 11.9.1、11.9.3、11.9.6 的 Attention Pattern 可视化
- **D2L** Section 15.8 — 参考了 Figure 15.8.1（ELMo vs GPT vs BERT对比图）及"Combining the Best of Both Worlds"的教学框架
- **SLP3** Chapters 7, 10 — 参考了解码器和编码器两种范式的分离讲解方式，以及Chapter 10开篇对双向与因果注意力的精确对比

### 课程
- **Stanford CS224N** Lecture 9 "Pretraining" (Winter 2025) — 参考了三种架构的分类框架和"All the biggest pretrained models are Decoders"的核心论断
- **CMU 11-711 ANLP** Lecture 5/6 "Pre-training" (Fall 2024 / Spring 2025) — 参考了实践导向的现代视角：从BERT时代到Decoder-only主导的演进叙事
- **Stanford CS324** "Training" lecture — 参考了三种架构的技术细节对比（MLM 80/10/10策略、[CLS] token vs 自然语言分类的对比）
:::

---

## 从上一章说起

上一章我们深入研究了预训练模型的三条工程优化路线。RoBERTa证明了"训练更充分"就能释放BERT的巨大潜力——不改一行架构就追平了XLNet。ALBERT通过参数共享将模型瘦身到原来的九分之一。DistilBERT通过知识蒸馏在保留97%性能的同时实现了60%的加速。这些工作各有成就，但它们共享一个根本前提：**BERT的Encoder-only架构是值得优化的起点**。

然而，上一章结尾我们已经指出了这个前提本身的脆弱性。RoBERTa、ALBERT、DistilBERT，乃至整个BERT家族——XLNet、ELECTRA、SpanBERT——都是在Encoder-only框架内做文章。无论训练策略多么精妙、参数共享多么高效、蒸馏损失多么巧妙，它们都无法回避一个根本性的问题：**Encoder-only架构天然不擅长生成任务**。你无法用BERT来写一首诗、翻译一段话、或者回答一个开放式问题——因为BERT的双向注意力机制意味着每个位置都能"看到"未来，而生成任务要求模型一个词一个词地往后写，不能偷看答案。

与此同时，GPT系列正在走一条截然不同的道路。2019年2月，OpenAI发布了GPT-2——一个15亿参数的Decoder-only模型，它仅仅通过"预测下一个词"这个朴素到几乎可以称为"无聊"的目标来训练，却展示了令人惊讶的零样本任务迁移能力：不做任何微调，GPT-2就能翻译、摘要、回答问题，只要你在输入前面加上正确的"提示"（prompt）。这与BERT的"预训练+微调"范式形成了鲜明的对比——BERT每做一个新任务，都需要一个有标注的数据集和一轮微调。

这就引出了一个本章要深入讨论的核心问题：**Encoder-only（BERT路线）、Decoder-only（GPT路线）、Encoder-Decoder（T5/BART路线）——这三种架构到底各有什么根本的优劣？**它们的差异仅仅是"双向 vs 单向"这么简单吗？为什么到了大模型时代，Decoder-only路线几乎一统天下？

2019到2020年间，几项关键工作为我们提供了系统性的回答。

> 💡 **本章核心洞察**：三种架构的差异本质上是**Attention Mask**的不同——全可见（Encoder-only）、因果三角（Decoder-only）、混合模式（Encoder-Decoder）。T5的控制实验甚至表明Encoder-Decoder在同等条件下表现最好。但Decoder-only的最终胜出不是因为架构更优，而是因为自回归语言建模目标在**规模化**和**通用性**上展现出了压倒性的工程优势。从"预训练+微调"到"预训练+提示"的范式转变，彻底改写了架构选择的逻辑。

---

## 问题的本质是什么？

### 一个架构，三种面孔

表面上看，Encoder-only（BERT）、Decoder-only（GPT）、Encoder-Decoder（原始Transformer/T5）是三种不同的模型架构。但如果我们剥开表面，深入到Attention层的实现细节，会发现一个优雅的统一视角：**它们的区别不在于网络结构本身，而在于Attention Mask的模式**。

让我们用一个具体的例子来说明。假设输入是四个token：$[x_1, x_2, x_3, x_4]$。在Self-Attention中，每个token需要计算它对所有其他token的注意力权重。Attention Mask决定了"谁能看到谁"：

**Encoder（双向/全可见）**：每个token都能看到所有其他token，包括它前面和后面的。Mask矩阵全为1：

$$
M_{\text{enc}} = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \end{bmatrix}
$$

这就是BERT的注意力模式。$x_3$ 在理解自己时，可以同时利用 $x_1$（过去）和 $x_4$（未来）的信息。代价是：你不能用它来做自回归生成，因为生成 $x_3$ 时你还没有 $x_4$。

**Decoder（因果/下三角）**：每个token只能看到它自己和它前面的token。Mask矩阵是下三角矩阵：

$$
M_{\text{dec}} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 1 & 1 & 1 & 1 \end{bmatrix}
$$

这就是GPT的注意力模式。$x_3$ 只能利用 $x_1$ 和 $x_2$ 的信息来理解自己，完全看不到 $x_4$。这种约束使得模型可以自回归地生成文本——每一步只依赖已经生成的内容。代价是：理解 $x_3$ 时少了"来自未来的线索"。

**Encoder-Decoder（混合模式）**：编码器内部全可见，解码器内部因果，解码器对编码器的注意力（Cross-Attention）全可见：

$$
M_{\text{enc-dec}} = \begin{bmatrix} M_{\text{enc}} & \mathbf{0} \\ \mathbf{1} & M_{\text{dec}} \end{bmatrix}
$$

这就是T5和BART的模式。编码器可以双向理解输入，解码器在生成时既能看到完整的输入（通过Cross-Attention），又保持了自回归的因果约束。

UniLM（Dong et al., 2019）最早明确地展示了这个统一视角——它用一个共享的Transformer，仅通过切换Attention Mask就实现了三种模式。这个发现意义深远：**三种"架构"的区别不是结构性的，而是掩码模式的选择**。

![UniLM通过统一的注意力掩码实现三种预训练模式。左：双向LM（BERT式），掩码全为0，所有token互相可见。中：单向LM（GPT式），上三角设为 $-\infty$，每个token只能看到过去。右：Seq2Seq LM，源端全可见，目标端因果。](figures/chapter-16/original/fig-unilm-attention-masks.png){#fig-unilm-masks width=85%}

::: {.figure-caption}
*Source: Dong et al. (2019) "Unified Language Model Pre-training for Natural Language Understanding and Generation", Figure 1. [arXiv:1905.03197](https://arxiv.org/abs/1905.03197)*
:::

### 但差异不止于Mask

如果三种架构只是Mask的不同，那选择就简单了——用能看到最多信息的那个（Encoder，全可见）。但实际情况远比这复杂。

**预训练目标的差异**。Mask模式决定了你能用什么预训练目标。因果Mask自然地对应自回归语言建模（预测下一个词），这只需要无标注文本。全可见Mask无法做自回归预测（因为答案已经可见了），所以需要人为制造预测任务——如BERT的MLM（遮住一些词，让模型预测）。Encoder-Decoder可以结合两者：编码器用全可见理解输入，解码器用因果生成输出。

**下游适配方式的差异**。BERT的下游适配方式是微调：在预训练模型上加一个任务特定的头（如分类层），然后在有标注数据上更新所有参数。GPT的适配方式逐渐从微调转向了提示（prompting）：不更新任何参数，只在输入前面加上任务描述和少量示例。T5的适配方式是将所有任务都转化为"文本到文本"的格式——输入一段文本，输出一段文本——统一而优雅。

**规模化行为的差异**。这是最关键的区别，也是Decoder-only最终胜出的核心原因。当模型从亿级参数扩展到千亿级时，三种架构的表现发生了戏剧性的分化——但这个故事需要先理解每种架构的设计哲学。

### 为什么这个问题重要？

在2019年之前，架构选择更多是一个"品味"问题：做NLU（自然语言理解）用BERT，做NLG（自然语言生成）用GPT，做Seq2Seq任务用Transformer Encoder-Decoder。大家各选各的，相安无事。

但到2019-2020年间，两个趋势让这个问题变得尖锐起来。第一个趋势是**统一的野心**：研究者们开始追问——能否有一个单一模型同时擅长理解和生成？T5的text-to-text框架和UniLM的统一Mask就是这种野心的体现。第二个趋势是**规模的诱惑**：随着GPT-2和GPT-3的成功，人们意识到模型规模的扩展可能带来质变而非量变——但千亿级的训练极其昂贵，你只能选择一种架构来赌，选错了意味着数千万美元的沉没成本。

理解三种架构的根本优劣，不只是学术兴趣——它直接决定了整个行业的技术路线。

---

## 核心思想与直觉

### 三种设计哲学

三种架构背后隐含着三种不同的"语言理解"哲学，理解这些哲学比记住技术细节更重要。

**Encoder-only（BERT）的哲学："理解先于表达"**。BERT相信，语言理解的核心是对上下文的充分编码。一个词的含义由它的整个上下文决定——"bank"在"river bank"和"bank account"中意思完全不同，只有同时看到前后文才能消歧。因此BERT让每个token都能看到所有其他token，最大化上下文信息的利用。至于下游任务（分类、问答、NER），只需要在编码之上加一个简单的头。这种哲学的潜台词是：**如果你把语言"理解"透了，任何具体任务都是简单的**。

**Decoder-only（GPT）的哲学："生成即理解"**。GPT相信，语言能力最自然的体现是"预测下一个词"。如果你能准确地预测下一个词，说明你已经理解了之前的所有内容——语法、语义、逻辑、常识、推理，全都蕴含在"下一个词是什么"这个问题中。因此GPT不需要双向注意力，因果遮蔽就足够了。至于下游任务，可以通过自然语言的方式来"描述"任务——"将以下英文翻译成法语：..."——模型只需要继续往下写就行。这种哲学的潜台词是：**语言建模是最终极的无监督学习任务，其他一切都是它的副产品**。

**Encoder-Decoder（T5/BART）的哲学："理解和表达各司其职"**。T5相信，理解和生成是两种不同的认知操作，应该由不同的模块来处理。编码器负责"读懂"输入（用双向注意力最大化理解），解码器负责"说出"输出（用因果注意力保证生成的自回归性），两者通过Cross-Attention桥接。这种哲学的潜台词是：**语言处理天然是一个"输入→处理→输出"的过程，像人一样先理解再表达**。

### 一个类比：三种阅读方式

如果把这三种架构比作人阅读文章的方式：

**Encoder-only像标注者**。她把整篇文章从头到尾读完，然后对每个词、每个句子做标注——这个词是什么词性？这句话表达了什么情感？这段话的主题是什么？她的理解是全局性的，但她不擅长"写"——你让她根据文章写一个摘要，她会手足无措。

**Decoder-only像讲故事的人**。他从第一个词开始，每次只看到自己已经写下的内容，然后决定下一个词是什么。他的理解是随着叙述而构建的——当他写到"他走进银行..."时，下一个词是"柜台"还是"河岸"取决于前面的上下文。他不仅能"理解"，还能"创作"，而且理解和创作是同一个过程。但他有一个弱点：他不能回头修改——每个词一旦写下就无法更改，因为后面的词已经基于它来预测了。

**Encoder-Decoder像翻译官**。她先完整地阅读源语言的文章（编码器），形成对整体含义的理解，然后一字一句地用目标语言重新表达（解码器）。她的优势是"先理解后表达"——翻译的质量取决于对原文的深度理解。但她需要两个独立的"脑区"，在参数量固定的情况下，每个脑区能分到的容量就少了。

### T5的统一框架：所有NLP都是"Text-to-Text"

T5（Text-to-Text Transfer Transformer）提出了一个极其优雅的统一框架：**将所有NLP任务都转化为"输入文本→输出文本"的格式**。

翻译？输入"translate English to German: That is good."，输出"Das ist gut."。分类？输入"mnli premise: ... hypothesis: ..."，输出"entailment"。摘要？输入"summarize: [文章]"，输出摘要文本。问答？输入"question: ... context: ..."，输出答案文本。

![T5的text-to-text框架：所有NLP任务统一为"输入一段文本，输出一段文本"。翻译、摘要、分类、回归等不同类型的任务共享相同的模型、损失函数和解码过程。](figures/chapter-16/original/fig-t5-text-to-text-framework.png){#fig-t5-text2text width=85%}

::: {.figure-caption}
*Source: Raffel et al. (2020) "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", Figure 1. [arXiv:1910.10683](https://arxiv.org/abs/1910.10683)*
:::

这个设计的巧妙之处在于：**模型不需要知道自己在做什么任务**。不需要任务特定的头（classification head、span extraction head等），不需要特殊的输入格式（`[CLS]`、`[SEP]`），不需要不同的损失函数——所有任务共用同一个Encoder-Decoder模型和同一个交叉熵损失。任务的"身份"完全由输入文本中的前缀来指定。

这不仅简化了工程实现，更重要的是，它暗示了一种可能性：**如果语言足够丰富，那么所有认知任务都可以用语言来描述和执行**。这个想法后来被GPT-3的In-Context Learning发扬光大——只不过GPT-3用Decoder-only架构实现了类似的效果。

---

## 技术细节

### T5的架构对比实验

T5论文中最有价值的贡献之一，是对三种架构的**控制变量实验**。Raffel et al. 不只是提出了T5模型本身，他们还系统地比较了不同架构在相同条件下的表现。这在当时是罕见的——大多数论文只展示自己提出的方法，很少做如此公正的横向对比。

**实验设置**。所有模型使用相同数量的参数（约2.2亿）和相同的训练数据（C4语料库的子集），在相同的下游任务上评测。唯一的变量是架构和预训练目标。Raffel et al. 比较了以下变体：

| 架构变体 | 结构 | 注意力模式 | 参数分配 |
|----------|------|-----------|----------|
| **Encoder-Decoder** | 双栈：编码器 + 解码器 | 编码器全可见 + 解码器因果 + 交叉注意力 | 各约$L$层 |
| **Decoder-only (Language Model)** | 单栈：仅解码器 | 因果（下三角） | $2L$层 |
| **Prefix LM** | 单栈：共享参数 | 输入部分全可见 + 输出部分因果 | $2L$层 |

![T5论文中对三种架构变体的对比示意图。左：标准Encoder-Decoder，编码器和解码器各自是独立的Transformer栈，通过Cross-Attention连接。中：Language Model（Decoder-only），单栈，因果注意力。右：Prefix LM，单栈，输入部分全可见，输出部分因果。](figures/chapter-16/original/fig-t5-architecture-variants.png){#fig-t5-arch width=85%}

::: {.figure-caption}
*Source: Raffel et al. (2020) "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", Figure 4. [arXiv:1910.10683](https://arxiv.org/abs/1910.10683)*
:::

**一个关键的实验设计细节**值得特别注意：Encoder-Decoder模型的编码器有$L$层、解码器有$L$层，总共$2L$层，但编码器和解码器不共享参数。Decoder-only模型只有一个$2L$层的栈。这意味着两者的总参数量大致相同。但Raffel et al. 还测试了一个"参数共享"的Encoder-Decoder变体——编码器和解码器共享所有参数，使得参数量减半到约$L$层的水平。即便在这种"不公平"的设置下，参数共享的Encoder-Decoder仍然表现出色。

### T5的注意力掩码对比

![T5论文中三种Attention Mask模式的对比。左：全可见（Encoder式），所有位置互相可见。中：因果（Decoder式），每个位置只能看到自己和过去的位置。右：因果+前缀（Prefix LM），输入部分全可见，输出部分因果。深色方块表示该位置的注意力被允许，浅色方块表示被遮蔽。](figures/chapter-16/original/fig-t5-attention-masks.png){#fig-t5-masks width=80%}

::: {.figure-caption}
*Source: Raffel et al. (2020) "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", Figure 3. [arXiv:1910.10683](https://arxiv.org/abs/1910.10683)*
:::

### 数值实验：三种Mask下的Attention分布

为了建立对三种Mask模式的直觉，让我们用一个具体的数值例子来感受它们的差异。

**设定**：输入序列为 $[\text{The}, \text{cat}, \text{sat}, \text{down}]$，$d_k = 2$。假设经过Q、K投影后：

$$
Q = K = \begin{bmatrix} 1.0 & 0.5 \\ 0.5 & 1.0 \\ 0.8 & 0.2 \\ 0.3 & 0.9 \end{bmatrix}
$$

**Step 1：计算原始注意力分数**

$$
QK^\top = \begin{bmatrix} 1.25 & 1.00 & 0.90 & 0.75 \\ 1.00 & 1.25 & 0.60 & 1.05 \\ 0.90 & 0.60 & 0.68 & 0.42 \\ 0.75 & 1.05 & 0.42 & 0.90 \end{bmatrix}
$$

缩放后（除以 $\sqrt{d_k} = \sqrt{2} \approx 1.41$）：

$$
\frac{QK^\top}{\sqrt{d_k}} \approx \begin{bmatrix} 0.88 & 0.71 & 0.64 & 0.53 \\ 0.71 & 0.88 & 0.43 & 0.74 \\ 0.64 & 0.43 & 0.48 & 0.30 \\ 0.53 & 0.74 & 0.30 & 0.64 \end{bmatrix}
$$

**Step 2：应用不同的Mask后做Softmax**

**全可见（Encoder模式）**——不遮蔽任何位置，对每行做完整的softmax：

$$
A_{\text{enc}} \approx \begin{bmatrix} \mathbf{0.30} & 0.26 & 0.24 & 0.22 \\ 0.24 & \mathbf{0.28} & 0.18 & 0.25 \\ \mathbf{0.29} & 0.23 & 0.24 & 0.20 \\ 0.22 & \mathbf{0.27} & 0.17 & 0.24 \end{bmatrix}
$$

每个位置的注意力分布在所有4个token上，对角线（自注意力）略高但不占主导。"sat"对"The"的注意力为0.29——它能同时利用前面和后面的上下文。

**因果（Decoder模式）**——遮蔽上三角（设为 $-\infty$），只对可见位置做softmax：

$$
A_{\text{dec}} \approx \begin{bmatrix} \mathbf{1.00} & 0 & 0 & 0 \\ 0.47 & \mathbf{0.53} & 0 & 0 \\ \mathbf{0.39} & 0.30 & 0.31 & 0 \\ 0.26 & \mathbf{0.31} & 0.19 & 0.28 \end{bmatrix}
$$

第一个位置"The"的全部注意力集中在自己身上——它没有其他可看的。"sat"（第3行）只能看到"The"、"cat"、"sat"三个位置，对"down"的注意力为0——信息完全被截断。

**解读差异**：在Encoder模式下，"sat"对4个位置的注意力分布是 $[0.29, 0.23, 0.24, 0.20]$，能利用"down"的信息（0.20）。在Decoder模式下变成 $[0.39, 0.30, 0.31, 0]$——"down"被完全遮蔽，剩余的注意力重新分配到前三个位置。这就是"单向 vs 双向"的具体体现：Encoder可以利用"未来信息"来理解"过去"，Decoder做不到。

### BART：用去噪预训练统一理解与生成

BART（Lewis et al., 2020）从一个不同的角度尝试统一BERT和GPT：**将BERT的去噪思想应用到Encoder-Decoder架构上**。

BART的设计思路非常直觉：如果把预训练看作"去噪自编码"（denoising autoencoding），那么编码器负责理解被破坏的输入，解码器负责重建原始文本。破坏方式越多样，模型学到的表示就越丰富。

![BART架构与BERT、GPT的对比。BERT使用双向编码器，随机遮蔽输入token后预测被遮蔽的位置。GPT使用因果解码器，自回归地预测下一个token。BART使用编码器-解码器结构，先对输入施加任意噪声（如token遮蔽、token删除、文本填充、句子排列、文档旋转），然后让解码器自回归地重建原始文本。](figures/chapter-16/original/fig-bart-comparison.png){#fig-bart width=85%}

::: {.figure-caption}
*Source: Lewis et al. (2020) "BART: Denoising Sequence-to-Sequence Pre-training", Figure 1. [arXiv:1910.13461](https://arxiv.org/abs/1910.13461)*
:::

BART探索了五种噪声策略：

| 噪声策略 | 操作 | 直觉 |
|----------|------|------|
| **Token Masking** | 随机用 `[MASK]` 替换token | 类似BERT的MLM |
| **Token Deletion** | 随机删除token | 模型需要判断"少了什么"和"少在哪里" |
| **Text Infilling** | 随机用单个 `[MASK]` 替换长度随机的span | 模型需要判断span的长度——比单token更难 |
| **Sentence Permutation** | 随机打乱句子顺序 | 模型需要理解文档级结构 |
| **Document Rotation** | 随机选择一个token作为文档开头 | 模型需要找到真正的开头 |

BART的实验发现，**Text Infilling + Sentence Permutation** 的组合效果最好。这个结论是合理的：Text Infilling迫使模型理解局部语义（填充被遮蔽的span），Sentence Permutation迫使模型理解全局结构（恢复句子的正确顺序）。

### 三种架构的对比总结

综合T5、BART、UniLM和后续研究的实验结果，我们可以对三种架构做一个系统性的对比。

| 维度 | Encoder-only (BERT) | Decoder-only (GPT) | Encoder-Decoder (T5/BART) |
|------|---------------------|---------------------|---------------------------|
| **注意力模式** | 全可见（双向） | 因果（单向） | 编码器双向 + 解码器因果 |
| **预训练目标** | MLM（遮蔽语言模型） | Causal LM（下一词预测） | Span Corruption / 去噪 |
| **下游适配** | 微调（加任务头） | 微调 或 提示 | Text-to-Text（统一格式） |
| **理解任务（NLU）** | ⭐⭐⭐⭐⭐ 最强 | ⭐⭐⭐ 较弱（单向限制） | ⭐⭐⭐⭐ 强 |
| **生成任务（NLG）** | ⭐ 天然不适合 | ⭐⭐⭐⭐⭐ 最强 | ⭐⭐⭐⭐ 强 |
| **Seq2Seq任务** | ⭐⭐ 需要特殊适配 | ⭐⭐⭐ 可以做但非最优 | ⭐⭐⭐⭐⭐ 天然适合 |
| **零样本泛化** | ⭐ 几乎不具备 | ⭐⭐⭐⭐⭐ 最强 | ⭐⭐⭐ 需要微调后才有 |
| **训练数据要求** | 需要配对数据做NSP | 只需无标注文本 | 只需无标注文本 |
| **参数效率** | 参数全用于编码 | 参数全用于单栈 | 参数分配给两个栈 |
| **规模化友好度** | ⭐⭐ 难以扩展 | ⭐⭐⭐⭐⭐ 最友好 | ⭐⭐⭐ 中等 |

这个表格值得仔细品味。注意两个特别之处：第一，**没有一种架构在所有维度上都最优**——BERT在理解任务上最强，GPT在生成和零样本上最强，T5在Seq2Seq上最强。第二，**"规模化友好度"是一个后来才被认识到的关键维度**——而这个维度恰恰是Decoder-only的最大优势。

### 关键设计决策深度分析

#### 决策1：为什么BERT不能做生成？

这个问题的答案比"因为BERT是双向的"更微妙。让我们精确地分析。

**表面原因**：BERT的全可见Mask意味着每个位置都能看到未来的token。如果你让BERT自回归地生成文本——先生成 $x_1$，再生成 $x_2$，...——那么在生成 $x_t$ 时，模型在训练时的"经验"是可以看到 $x_{t+1}, x_{t+2}, ...$，但在推理时这些位置还不存在。训练和推理之间的这种不一致（exposure bias）会导致生成质量严重退化。

**深层原因**：BERT的预训练目标MLM只训练模型预测被遮蔽的15%的token，其余85%的token不产生任何训练信号。这意味着BERT只学会了"填空"（完形填空式的理解），而没有学会"续写"（从左到右的连贯生成）。即使你强行给BERT加上因果Mask做微调，它缺乏大量自回归训练带来的流畅生成能力。

**一个有趣的反例**：UniLM证明了如果你在预训练时就包含自回归目标（通过因果Mask的模式），同一个Transformer权重也能做生成。这说明"不能生成"不是Transformer架构的限制，而是BERT预训练目标的限制。

#### 决策2：为什么GPT在理解任务上不如BERT？

GPT的因果Mask意味着处理位置 $t$ 时只能看到 $x_1, ..., x_t$，而BERT能看到整个序列。直觉上，"看到更多信息"应该导致更好的理解，但为什么？

一个精确的解释来自信息论。对于位置 $t$ 的表示 $h_t$：

$$
I_{\text{enc}}(h_t; \mathbf{x}) \geq I_{\text{dec}}(h_t; \mathbf{x})
$$

其中 $I$ 是互信息，$\mathbf{x}$ 是完整序列。Encoder的 $h_t$ 包含了来自整个序列的信息（因为全可见），而Decoder的 $h_t$ 只包含来自 $x_{1:t}$ 的信息。按照数据处理不等式（Data Processing Inequality），丢弃信息只会减少互信息，不会增加。

但这个论证有一个微妙的漏洞。虽然Encoder的单个 $h_t$ 确实比Decoder的包含更多信息，但Decoder有一个补偿机制：**它的自回归训练迫使每一层的表示都编码了丰富的预测信息**。在足够大的模型和足够多的数据下，Decoder的 $h_t$ 虽然只基于过去的上下文，但可能已经学会了对未来的强大预测——这本身就是一种"隐式的双向理解"。

这就是为什么GPT-3在足够大的规模下能在很多NLU基准上追平甚至超越BERT——**规模可以部分弥补信息的不完整**。

#### 决策3：Encoder-Decoder的参数效率问题

当总参数量固定时，Encoder-Decoder需要把参数分给两个栈。假设总参数预算为 $P$：

- Decoder-only：全部 $P$ 用于一个栈，可以做到 $L$ 层
- Encoder-Decoder：编码器得到 $P/2$，解码器得到 $P/2$，各 $L/2$ 层

在相同参数下，Decoder-only的单栈更深（$L$ 层 vs $L/2$ 层），更深的网络通常意味着更强的表达能力。T5论文的一个重要发现是：尽管如此，**Encoder-Decoder在相同参数下仍然优于Decoder-only**。原因可能是编码器的双向注意力提供了质量更高的输入表示，抵消了深度的劣势。

但这个优势有一个前提：**任务有明确的"输入"和"输出"之分**。对于翻译、摘要、问答等Seq2Seq任务，Encoder-Decoder可以把输入分配给编码器、输出分配给解码器，充分利用两个栈的专业化分工。但对于开放式生成（写故事、聊天）、或者输入和输出边界模糊的任务，这种分工的优势就不那么明显了。

---

## 工程实践

### 用代码理解三种Attention Mask

下面的代码直观地展示了三种Mask的构造和应用：

```{python}
#| code-fold: false
#| code-summary: "三种 Attention Mask 的构造与可视化"

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

def create_masks(seq_len, prefix_len=None):
    """构造三种 Attention Mask"""
    # 1. Encoder: 全可见
    encoder_mask = torch.ones(seq_len, seq_len)

    # 2. Decoder: 因果（下三角）
    decoder_mask = torch.tril(torch.ones(seq_len, seq_len))

    # 3. Prefix LM: 前 prefix_len 个位置全可见，之后因果
    if prefix_len is None:
        prefix_len = seq_len // 2
    prefix_mask = torch.zeros(seq_len, seq_len)
    # 前缀部分：所有位置都能看到前缀
    prefix_mask[:, :prefix_len] = 1
    # 后缀部分：因果
    prefix_mask[prefix_len:, prefix_len:] = torch.tril(
        torch.ones(seq_len - prefix_len, seq_len - prefix_len)
    )
    # 前缀内部：互相可见
    prefix_mask[:prefix_len, :prefix_len] = 1

    return encoder_mask, decoder_mask, prefix_mask

# 构造并可视化
seq_len = 8
enc_mask, dec_mask, prefix_mask = create_masks(seq_len, prefix_len=4)

fig, axes = plt.subplots(1, 3, figsize=(14, 4))
titles = [
    'Encoder\n(Fully Visible)',
    'Decoder\n(Causal)',
    'Prefix LM\n(Prefix Visible + Causal)'
]
masks = [enc_mask, dec_mask, prefix_mask]

for ax, mask, title in zip(axes, masks, titles):
    ax.imshow(mask.numpy(), cmap='Blues', vmin=0, vmax=1, aspect='equal')
    ax.set_title(title, fontsize=12, fontweight='bold')
    ax.set_xlabel('Key position')
    ax.set_ylabel('Query position')
    # 标注前缀边界（Prefix LM）
    if 'Prefix' in title:
        ax.axhline(y=3.5, color='red', linestyle='--', alpha=0.7)
        ax.axvline(x=3.5, color='red', linestyle='--', alpha=0.7)
    ax.set_xticks(range(seq_len))
    ax.set_yticks(range(seq_len))

plt.tight_layout()
plt.savefig('figures/chapter-16/fig-attention-masks-comparison.png',
            dpi=200, bbox_inches='tight', facecolor='white')
plt.show()
print("深色 = 注意力被允许，浅色 = 注意力被遮蔽")
print(f"Encoder: 每个位置可看到 {seq_len} 个位置（全部）")
print(f"Decoder: 位置 i 可看到 i+1 个位置（自身及之前）")
print(f"Prefix LM: 前缀（前4个）互相全可见，后缀（后4个）因果")
```

### 三种架构的推理差异

```{python}
#| code-fold: false
#| code-summary: "三种架构在推理时的行为差异"

def encoder_inference(model_fn, input_ids):
    """Encoder-only 推理：一次前向传播，得到所有位置的表示"""
    # 输入: [batch, seq_len]
    # 输出: [batch, seq_len, hidden_size] — 每个位置的上下文表示
    hidden_states = model_fn(input_ids, mask='fully_visible')
    # 用于分类: 取 [CLS] token 的表示
    cls_repr = hidden_states[:, 0, :]
    return cls_repr  # → 分类头

def decoder_inference(model_fn, prompt_ids, max_new_tokens):
    """Decoder-only 推理：自回归生成，一次生成一个 token"""
    generated = prompt_ids.clone()
    for _ in range(max_new_tokens):
        # 每步只需要最后一个 token 的 logits（但需要所有历史的 KV Cache）
        logits = model_fn(generated, mask='causal')[:, -1, :]
        next_token = logits.argmax(dim=-1, keepdim=True)
        generated = torch.cat([generated, next_token], dim=-1)
    return generated

def encoder_decoder_inference(model_fn_enc, model_fn_dec, input_ids, max_new_tokens):
    """Encoder-Decoder 推理：先编码，再自回归解码"""
    # Step 1: 编码器一次性处理整个输入（只做一次！）
    encoder_output = model_fn_enc(input_ids, mask='fully_visible')
    # Step 2: 解码器自回归生成，每步通过 Cross-Attention 查看编码器输出
    decoder_input = torch.tensor([[BOS_TOKEN]])
    for _ in range(max_new_tokens):
        logits = model_fn_dec(decoder_input, encoder_output, mask='causal')[:, -1, :]
        next_token = logits.argmax(dim=-1, keepdim=True)
        decoder_input = torch.cat([decoder_input, next_token], dim=-1)
    return decoder_input

# 关键区别总结
print("=== 推理时的关键差异 ===")
print()
print("Encoder-only (BERT):")
print("  - 输入整个序列 → 一次前向传播 → 得到所有位置的表示")
print("  - 不能自回归生成")
print("  - 推理速度: 最快（一次前向）")
print()
print("Decoder-only (GPT):")
print("  - 自回归生成: 每步一个 token")
print("  - 使用 KV Cache 避免重复计算")
print("  - 推理速度: 较慢（生成 N 个 token 需要 N 步）")
print()
print("Encoder-Decoder (T5):")
print("  - 编码器: 一次前向（与 BERT 相同）")
print("  - 解码器: 自回归 + Cross-Attention 到编码器")
print("  - 推理速度: 中等（编码一次 + 解码 N 步）")
print("  - 优势: 编码器不需要在每步解码时重新计算")
```

### 使用Hugging Face对比三种架构

```{python}
#| code-fold: true
#| code-summary: "用 Hugging Face 实际对比三种架构的行为"
#| eval: false

from transformers import (
    BertTokenizer, BertForSequenceClassification,  # Encoder-only
    GPT2Tokenizer, GPT2LMHeadModel,                 # Decoder-only
    T5Tokenizer, T5ForConditionalGeneration,         # Encoder-Decoder
)
import torch

# === 任务: 情感分类 "This movie is great!" ===
text = "This movie is great!"

# --- BERT (Encoder-only): 用 [CLS] token + 分类头 ---
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', num_labels=2
)
inputs = bert_tokenizer(text, return_tensors='pt')
with torch.no_grad():
    outputs = bert_model(**inputs)
    # outputs.logits: [batch, 2] → positive / negative
print(f"BERT: logits = {outputs.logits}")
print("→ BERT 用 [CLS] 表示 + 线性分类头，输出离散类别")

# --- GPT-2 (Decoder-only): 将分类转化为生成 ---
gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')
# 将分类任务转化为生成: "Review: This movie is great! Sentiment:"
prompt = f"Review: {text} Sentiment:"
inputs = gpt_tokenizer(prompt, return_tensors='pt')
with torch.no_grad():
    outputs = gpt_model.generate(
        **inputs, max_new_tokens=3, do_sample=False
    )
generated = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"GPT-2: '{generated}'")
print("→ GPT-2 通过续写生成答案（如 'positive'），不需要分类头")

# --- T5 (Encoder-Decoder): text-to-text ---
t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')
# T5 的输入格式: "sst2 sentence: This movie is great!"
t5_input = f"sst2 sentence: {text}"
inputs = t5_tokenizer(t5_input, return_tensors='pt')
with torch.no_grad():
    outputs = t5_model.generate(**inputs, max_new_tokens=5)
result = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"T5: '{result}'")
print("→ T5 将分类转化为 text-to-text，输出文本标签（如 'positive'）")
```

这段代码清晰地展示了三种架构处理同一个任务的不同方式：BERT用特殊的`[CLS]` token加分类头，GPT用自然语言提示加续写，T5用text-to-text格式。注意GPT和T5的方式更"通用"——不需要任务特定的头，只需要改变输入格式。这种通用性在后来被证明是至关重要的。

---

## 深入理解

> **研究者必读**：这一节探讨三种架构的理论基础、Decoder-only胜出的深层原因、以及这个"胜出"结论的局限性。

### 历史转折：为什么Decoder-only最终胜出？

这是本章最核心的问题，也是理解整个大语言模型时代的关键。答案不是单一的，而是多个因素的交汇。

**因素一：训练目标的简洁性**

自回归语言建模（预测下一个词）是所有预训练目标中最简单的：

$$
\mathcal{L}_{\text{CLM}} = -\sum_{t=1}^{T} \log P(x_t | x_1, ..., x_{t-1}; \theta)
$$

这个目标不需要任何人为的设计决策——不需要决定遮蔽比例（BERT的15%是最优的吗？）、不需要决定噪声策略（T5的Span Corruption长度怎么选？）、不需要构造负样本。每一个token都产生一个训练信号，不像MLM只有15%的token参与训练。这种简洁性在工程上带来了巨大的优势：更少的超参数意味着更少的调优成本，尤其是在千亿级模型的训练中——每次实验可能花费数百万美元。

**因素二：训练数据的自然性**

自回归目标只需要无标注文本——把互联网上的文章一篇接一篇地拼起来就行。不需要配对数据（翻译需要平行语料）、不需要标注数据（分类需要标签）、甚至不需要精心的数据预处理（MLM需要决定哪些位置遮蔽、用什么替换）。这种数据需求的朴素性使得Decoder-only模型可以利用最大规模的训练数据——整个互联网。

**因素三：规模化时的"涌现"**

这是最戏剧性的因素。GPT-2（15亿参数）展示了零样本任务迁移的雏形，GPT-3（1750亿参数）将其推向了令人震惊的水平。In-Context Learning（上下文学习）——在输入前面放几个示例，模型就能"学会"新任务——是一种在Encoder-only和Encoder-Decoder模型中从未被观察到的能力。

为什么只有Decoder-only展现出了这种涌现能力？一个可能的解释是：**自回归预训练天然就是在做"In-Context Learning"**。在训练时，模型不断地从上文推断下文的规律（语法、语义、逻辑、风格），而In-Context Learning只不过是把"上文"换成了任务描述和示例。换句话说，Decoder-only模型的预训练目标和下游使用方式之间存在完美的一致性——它在训练时做的事情，和你在使用时让它做的事情，本质上是同一件事。

BERT则没有这种一致性。BERT训练时做"填空"，使用时做"分类"或"抽取"——两者之间存在gap，需要微调来弥补。

**因素四：KV Cache带来的推理优势**

在自回归生成中，Decoder-only架构可以利用KV Cache：生成第 $t$ 个token时，前 $t-1$ 个token的Key和Value已经在之前的步骤中计算过了，不需要重新计算。这使得每步生成的计算成本是 $O(d^2)$ 而非 $O(t \cdot d^2)$。

Encoder-Decoder架构也有Cache，但它需要维护两份：编码器的KV（用于Cross-Attention）和解码器自身的KV。更重要的是，在批量推理（batch inference）中，不同输入的编码器输出长度可能不同，这给内存管理带来了额外的复杂性。

**因素五：工程生态的自我强化**

一旦GPT-2/3证明了Decoder-only路线的潜力，整个社区的工程投入——优化工具（vLLM、TGI）、训练框架（Megatron-LM）、部署基础设施——都开始向Decoder-only倾斜。这种生态优势进一步加速了Decoder-only的主导地位。后来者选择Decoder-only不仅是因为架构好，更是因为"现成的工具更多"。

### 但T5的实验表明Encoder-Decoder在受控条件下更强

这里必须诚实地指出一个经常被忽视的事实。T5论文（Raffel et al., 2020）的受控实验表明，**在相同参数量和相同训练数据下，Encoder-Decoder架构在几乎所有下游任务上都优于Decoder-only**。

Wang et al. (2022) 的大规模实验进一步验证了这一点：在纯无监督预训练后做零样本评测，因果Decoder确实最强；但在多任务微调之后，非因果架构（包括Encoder-Decoder）反而更强。

这意味着Decoder-only的"胜出"更多是**实际工程和使用范式的胜利**，而非架构本身的优越性。Decoder-only赢在：
- 训练目标更简单（不需要调遮蔽策略）
- 数据准备更容易（只需要拼接文本）
- 零样本/少样本能力更强（与自回归训练天然一致）
- 推理优化更成熟（KV Cache、连续批处理）

如果我们只关心"在充分微调后的最优性能"，Encoder-Decoder可能仍然是更好的选择。但大语言模型时代的使用范式已经从"微调"转向了"提示"——而这正是Decoder-only的主场。

### 从"预训练+微调"到"预训练+提示"

这是一个范式层面的转变，其影响远超架构选择本身。

**微调范式（2018-2020主流）**：

$$
\text{预训练模型} \xrightarrow{\text{有标注数据}} \text{微调} \xrightarrow{} \text{任务特定模型}
$$

- 每个任务需要一个单独的模型副本
- 需要有标注数据（通常数千到数万条）
- 性能上限高，但部署成本高（N个任务 = N个模型）

**提示范式（2020至今主流）**：

$$
\text{大语言模型} \xrightarrow{\text{自然语言提示}} \text{直接输出}
$$

- 一个模型处理所有任务
- 不需要（或只需要少量）标注数据
- 通过改变输入的"提示"（prompt）来切换任务
- 性能可能略低于充分微调，但通用性极强

这种范式转变从根本上改变了架构选择的评判标准。在微调范式下，最重要的是"模型在有标注数据微调后的峰值性能"——BERT系列在这个维度上表现最好。但在提示范式下，最重要的是"模型在零样本/少样本下的泛化能力"——GPT系列在这个维度上独占鳌头。

### 方法的边界条件

**Decoder-only并非在所有场景下都最优**。对于有明确"输入-输出"结构的任务（如翻译、摘要），Encoder-Decoder仍然可能更高效——因为编码器对输入的双向处理可以产生更好的表示。Meta的最新研究（2025）表明，在推理FLOPs相同的条件下，Encoder-Decoder模型几乎主导了质量-计算的Pareto前沿。

**规模是Decoder-only优势的前提**。在小规模（< 1B参数）下，BERT式的Encoder-only模型在NLU任务上仍然显著优于同规模的Decoder-only模型。Decoder-only的零样本能力需要大量参数来"涌现"——GPT-2的1.5B参数只展示了粗糙的零样本能力，GPT-3的175B参数才真正令人信服。对于资源受限的场景，Encoder-only仍然是务实的选择。

**"提示"范式的不稳定性**。与微调相比，提示的结果对提示措辞高度敏感。"Classify this review as positive or negative"和"Is this review positive or negative?"可能给出截然不同的结果。这种不稳定性是提示范式的固有局限，也是后来指令微调（Instruction Tuning）和RLHF试图解决的问题。

### 开放研究问题

**Encoder-Decoder是否被过早放弃了？** 大多数最大的语言模型（GPT-4、Claude、Gemini、LLaMA）都采用Decoder-only架构。但T5的实验表明Encoder-Decoder在受控条件下更优。如果社区在Encoder-Decoder上投入同等规模的工程优化，结果会不同吗？UL2（Tay et al., 2022）和最新的研究试图回答这个问题，但目前仍然缺乏千亿级参数的公平对比。

**最优注意力模式是否是数据依赖的？** 也许不同类型的数据和任务适合不同的Mask模式。代码生成可能更适合因果Mask（代码天然是从上到下写的），而文档理解可能更适合全可见Mask。自适应地选择Mask模式——甚至在同一个模型的不同层使用不同的Mask——是一个有趣的研究方向。

**双向能力能否被"注入"到Decoder-only模型中？** 一些最新的工作尝试在Decoder-only模型的训练中引入双向目标（如BERT式的MLM），以弥补单向注意力在理解任务上的劣势。这种"混合训练"能否兼得两种架构的优势？

---

## 局限性与未解决的问题

### 本章讨论的局限

本章将三种架构的对比框定在"预训练"范式内——通过大规模语料库上的自监督学习获得通用的语言能力。但这个框架本身有一个根本性的局限：**预训练只教会了模型"语言"，没有教会它"对齐"**。

一个175B参数的GPT-3可以流畅地生成文本，但它可能输出有害内容、编造事实、或者不遵循用户的指令。这不是架构的问题，而是训练目标的问题——"预测下一个词"并不等于"生成有帮助且无害的回复"。

### 架构选择之外的问题

到2020年末，NLP社区面临的最紧迫问题已经不再是"选择什么架构"，而是：

**规模的问题**：从亿级到千亿级参数，训练成本从数万美元跳升到数千万美元。如何高效地训练和部署这些庞然大物？Scaling Laws告诉我们什么？这是下一章的主题。

**对齐的问题**：如何让大模型不仅"能力强"，还"行为好"？如何让它遵循指令、拒绝有害请求、承认不确定性？这是后续RLHF和对齐章节的主题。

**效率的问题**：千亿级模型的推理成本高昂，如何让更多人用得起？量化、蒸馏、投机解码等技术试图回答这个问题。

### 这些局限导向了什么？

本章确立了Decoder-only作为大语言模型时代主流架构的地位，但也清楚地表明：**架构选择只是起点，不是终点**。真正改变游戏规则的是"规模"——当模型从GPT-2的15亿参数扩展到GPT-3的1750亿参数时，不只是量变，还出现了涌现能力的质变。

下一章将深入探讨这个"规模的力量"：Scaling Laws揭示了模型性能与规模之间令人惊讶的幂律关系，Chinchilla修正了我们对最优训练配置的理解，而"涌现能力"的发现彻底改变了我们对大模型的期待。

> 下一章预告：第17章将聚焦**规模的力量——Scaling Laws**。损失函数与模型大小、数据量、计算量之间遵循怎样的幂律关系？Chinchilla的"训练数据比模型参数更重要"的发现如何改变了整个行业的训练策略？从量变到质变的"涌现能力"是真实的现象，还是度量的假象？

---

## 本章小结

### 核心要点回顾

本章系统地对比了三种预训练架构——Encoder-only（BERT）、Decoder-only（GPT）、Encoder-Decoder（T5/BART）——并深入分析了Decoder-only路线最终胜出的原因。

三种架构的差异本质上是Attention Mask的不同：全可见（Encoder）让每个位置都能利用完整的上下文信息，代价是无法自回归生成；因果（Decoder）让每个位置只看到过去，代价是失去未来信息，收获是天然的生成能力；混合模式（Encoder-Decoder）试图兼得两者，代价是需要将参数分配给两个栈。

T5的系统性实验提供了最公正的横向对比：在受控条件下，Encoder-Decoder在几乎所有任务上都表现最好。但Decoder-only最终胜出不是因为架构更优，而是因为五个相互强化的因素：训练目标的简洁性（只需预测下一个词）、训练数据的自然性（只需无标注文本）、涌现能力的独特性（In-Context Learning）、推理优化的成熟性（KV Cache）、以及工程生态的自我强化。

从"预训练+微调"到"预训练+提示"的范式转变是理解架构选择的关键。微调范式下BERT最强，提示范式下GPT最强——不是模型变了，是评判标准变了。这个范式转变不可逆地重塑了整个NLP领域的技术路线。

### 关键公式速查

| 公式 | 含义 |
|------|------|
| $M_{\text{enc}} = \mathbf{1}_{n \times n}$ | Encoder全可见Mask |
| $M_{\text{dec}} = \text{tril}(\mathbf{1}_{n \times n})$ | Decoder因果Mask（下三角） |
| $\mathcal{L}_{\text{CLM}} = -\sum_t \log P(x_t \mid x_{<t})$ | 因果语言建模损失 |
| $\mathcal{L}_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log P(x_i \mid \mathbf{x}_{\backslash \mathcal{M}})$ | 遮蔽语言建模损失 |
| $I_{\text{enc}}(h_t; \mathbf{x}) \geq I_{\text{dec}}(h_t; \mathbf{x})$ | Encoder表示包含更多信息 |

### 思考题

1. **[概念理解]** UniLM证明了一个共享参数的Transformer可以通过切换Attention Mask来实现三种模式。如果三种"架构"的区别只是Mask的不同，为什么不让所有模型都用全可见Mask（Encoder模式）——毕竟它提供了最多的信息？你的回答应该涉及预训练目标和下游使用方式之间的关系。

2. **[实验分析]** T5的实验表明Encoder-Decoder在受控条件下优于Decoder-only，但GPT-3在实际使用中展现了更强的能力。如何解释这个矛盾？提示：考虑实验规模（T5实验在~220M参数，GPT-3在175B参数）和评测方式（微调 vs 零样本）的差异。

3. **[工程实践]** 使用Hugging Face加载BERT-base、GPT-2和T5-small，对同一个情感分类任务（如SST-2的100条样本）做推理。比较：(a) 三种模型的推理延迟；(b) 三种模型处理相同任务所需的不同输入格式；(c) 哪种模型的"适配成本"最低？

4. **[研究思考]** 如果你在2020年初拥有无限计算资源，你会选择哪种架构来训练一个通用大模型？请给出你的选择和理由，并考虑以下约束：(a) 你希望模型能同时擅长理解和生成；(b) 你希望模型规模能扩展到1000亿参数以上；(c) 你希望用户不需要微调就能使用模型。

5. **[开放思考]** "Decoder-only的胜出"是历史的必然还是路径依赖的偶然？如果2018年BERT不是用Encoder-only而是用Encoder-Decoder（像T5那样），整个领域的发展路径会不同吗？生态效应（工具、社区、投资）在技术路线选择中扮演了多大的角色？

---

## 延伸阅读

### 核心论文（必读）

**Raffel, C. et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**。T5论文，本章最核心的参考。重点阅读：Section 3.2（三种架构的系统性对比）、Table 2（架构对比数据）、Figure 3-4（Attention Mask和架构示意图）。可快速浏览：Section 3.3-3.7（训练细节实验）。这篇论文的价值不仅在于T5模型本身，更在于它作为"NLP领域最大规模的受控实验"为后来的研究提供了宝贵的实证基础。[arXiv:1910.10683](https://arxiv.org/abs/1910.10683)

**Lewis, M. et al. (2020). "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"**。BART论文，Encoder-Decoder去噪预训练。重点阅读：Figure 1（与BERT/GPT的对比）、Table 1（预训练目标消融）、Section 3（微调策略）。[arXiv:1910.13461](https://arxiv.org/abs/1910.13461)

**Dong, L. et al. (2019). "Unified Language Model Pre-training for Natural Language Understanding and Generation"**。UniLM论文，首次展示了通过Attention Mask统一三种范式。重点阅读：Figure 1（统一掩码）、Section 2（统一预训练方法）。[arXiv:1905.03197](https://arxiv.org/abs/1905.03197)

### 理论基础

**Wang, T. et al. (2022). "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"**。最大规模的架构对比实验（5B参数级别），发现纯无监督预训练下因果Decoder最强，但多任务微调后Encoder-Decoder反超。[arXiv:2204.05832](https://arxiv.org/abs/2204.05832)

### 后续发展

**Tay, Y. et al. (2023). "UL2: Unifying Language Learning Paradigms"**。尝试在一个模型中统一自回归和去噪目标，使用"混合去噪器"（Mixture of Denoisers）策略。可以看作UniLM理念在大规模模型上的延续。[arXiv:2205.05131](https://arxiv.org/abs/2205.05131)

**Brown, T. et al. (2020). "Language Models are Few-Shot Learners"**。GPT-3论文，Decoder-only路线的里程碑，首次大规模展示了In-Context Learning的威力。重点阅读：Section 1（核心发现）、Section 3（任务设定：零样本/单样本/少样本）。[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)

### 综述与教程

**Sebastian Raschka (2023). "Understanding Encoder and Decoder LLMs"**。对三种架构最清晰的科普解释之一，配有直观的图示。[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)

### 代码资源

- **Hugging Face T5**: [huggingface.co/t5-base](https://huggingface.co/t5-base)
- **Hugging Face BART**: [huggingface.co/facebook/bart-base](https://huggingface.co/facebook/bart-base)
- **T5 官方实现**: [github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)

---

## 历史注脚

T5、BART和UniLM几乎同时出现在2019年下半年到2020年初，但它们代表的不只是三个模型——而是三种关于"语言AI应该怎么做"的哲学碰撞。

T5来自Google Brain，它最引人注目的特征不是模型本身，而是那篇长达67页的论文——这可能是NLP历史上最"详尽"的实验论文之一。Raffel et al. 对架构、预训练目标、数据量、微调策略等维度做了穷举式的消融实验，仅训练消耗的计算量就相当于数百万美元。这篇论文的精神是"不猜测，用实验说话"——一种在深度学习研究中日益稀缺的严谨态度。

BART来自Facebook AI（现Meta AI），它的贡献更多是工程性的：证明了Encoder-Decoder架构配合去噪预训练可以同时擅长理解和生成。BART在摘要任务上的表现尤其出色——直到今天，很多对话摘要和文档摘要系统仍然基于BART的变体。

UniLM来自微软研究院，它的洞察最为优雅：三种看似不同的架构，本质上可以用同一个Transformer通过不同的Mask来实现。这个统一视角后来深刻地影响了人们对Transformer的理解——它不是一种特定的架构，而是一个"通用的注意力计算框架"，具体行为由Mask来定义。

但历史的吊诡之处在于：在2019年的这场"架构大辩论"中，最终获胜的既不是最优雅的设计（UniLM的统一框架），也不是实验证明最强的架构（T5的Encoder-Decoder），而是最朴素的方案——GPT的Decoder-only加上"预测下一个词"。这个结果本身就是一个深刻的教训：**在足够大的规模下，简单性比最优性更重要**。当你训练一个千亿参数的模型时，你能承受的复杂性是有限的——更简单的训练目标意味着更少的超参数、更少的工程难题、更稳定的训练过程。

从某种意义上说，GPT路线的胜出是"less is more"哲学在AI领域的又一次胜利——就像生物进化中，并非最复杂的物种存活下来，而是最适应环境的物种。当环境变成了"大规模训练"时，适应性意味着简单性。
