---
title: "ç¬¬6ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„è¯ç”Ÿä¸æ¼”è¿›"
subtitle: "ä»ä¿¡æ¯ç“¶é¢ˆåˆ°åŠ¨æ€èšç„¦ï¼šBahdanauåŠ æ€§æ³¨æ„åŠ›ã€Luongä¹˜æ€§æ³¨æ„åŠ›ï¼Œä»¥åŠæ³¨æ„åŠ›è®¾è®¡ç©ºé—´çš„ç³»ç»Ÿæ¢ç´¢"
author: "Ying Zha"
date: "2026-01-25"
categories: [NLP, Attention, Seq2Seq, æœºå™¨ç¿»è¯‘, Bahdanau, Luong]
tags: [æ³¨æ„åŠ›æœºåˆ¶, åŠ æ€§æ³¨æ„åŠ›, ä¹˜æ€§æ³¨æ„åŠ›, ç‚¹ç§¯æ³¨æ„åŠ›, å¯¹é½æ¨¡å‹, ä¸Šä¸‹æ–‡å‘é‡, ç¥ç»æœºå™¨ç¿»è¯‘, Global Attention, Local Attention, Hard Attention, Soft Attention]
description: "æ³¨æ„åŠ›æœºåˆ¶çš„å®Œæ•´æ•…äº‹ï¼šä»Bahdanauæ‰“ç ´Seq2Seqä¿¡æ¯ç“¶é¢ˆï¼Œåˆ°Luongçš„ç³»ç»Ÿæ€§æ¢ç´¢ï¼ˆåŠ æ€§vsä¹˜æ€§ã€å…¨å±€vså±€éƒ¨ã€è½¯vsç¡¬ï¼‰ï¼Œå†åˆ°æ³¨æ„åŠ›ç‹¬ç«‹äºRNNçš„å‰å¥ã€‚"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    css: styles.css
    fig-cap-location: bottom
---

> **æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•è®©è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œèƒ½å¤Ÿè®¿é—®è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯åªä¾èµ–ä¸€ä¸ªå‹ç¼©åçš„å‘é‡ï¼Ÿè¿›ä¸€æ­¥åœ°ï¼Œè®¡ç®—"æ³¨æ„åŠ›"çš„æœ€ä½³æ–¹å¼æ˜¯ä»€ä¹ˆâ€”â€”åŠ æ€§ã€ä¹˜æ€§ã€å…¨å±€è¿˜æ˜¯å±€éƒ¨ï¼Ÿ
>
> **å†å²åæ ‡**ï¼š2014-2015 | Bahdanau, Cho, Bengio â†’ Luong, Pham, Manning | ä»æ³¨æ„åŠ›çš„è¯ç”Ÿåˆ°ç³»ç»Ÿæ€§æ¢ç´¢

---

## ä»ä¸Šä¸€ç« è¯´èµ·

ä¸Šä¸€ç« æˆ‘ä»¬è§è¯äº†RNNçš„è¾‰ç…Œä¸å›°å¢ƒã€‚LSTMå’ŒGRUé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒSeq2Seqæ¶æ„è®©ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤„ç†ç¿»è¯‘ã€æ‘˜è¦ç­‰åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ã€‚

ä½†Seq2Seqæœ‰ä¸€ä¸ªè‡´å‘½çš„è®¾è®¡ç¼ºé™·ï¼š**ä¿¡æ¯ç“¶é¢ˆ**ã€‚

å›é¡¾Seq2Seqçš„å·¥ä½œæ–¹å¼ï¼šç¼–ç å™¨è¯»å–æ•´ä¸ªè¾“å…¥åºåˆ—ï¼Œå°†æ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ ä¸­ï¼›è§£ç å™¨ä»…å‡­è¿™ä¸ªå‘é‡ï¼Œé€è¯ç”Ÿæˆè¾“å‡ºã€‚è¿™æ„å‘³ç€ï¼Œæ— è®ºè¾“å…¥æ˜¯5ä¸ªè¯è¿˜æ˜¯50ä¸ªè¯ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½è¦å¡è¿›åŒä¸€ä¸ªç»´åº¦çš„å‘é‡ã€‚

Sutskeverç­‰äºº(2014)çš„å®éªŒæ¸…æ¥šåœ°å±•ç¤ºäº†è¿™ä¸ªé—®é¢˜ï¼šå½“è¾“å…¥å¥å­è¶…è¿‡20ä¸ªè¯æ—¶ï¼Œç¿»è¯‘è´¨é‡æ€¥å‰§ä¸‹é™ã€‚æ›´é•¿çš„å¥å­åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œè€Œå›ºå®šå¤§å°çš„å‘é‡æ— æ³•æ‰¿è½½ã€‚

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ„Ÿå—è¿™ä¸ªé—®é¢˜ã€‚è€ƒè™‘ç¿»è¯‘ä»»åŠ¡ï¼š

> **è‹±è¯­**ï¼šThe agreement on the European Economic Area was signed in August 1992.
>
> **æ³•è¯­**ï¼šL'accord sur la zone Ã©conomique europÃ©enne a Ã©tÃ© signÃ© en aoÃ»t 1992.

å½“è§£ç å™¨ç”Ÿæˆ"aoÃ»t"ï¼ˆå…«æœˆï¼‰æ—¶ï¼Œå®ƒéœ€è¦çŸ¥é“åŸæ–‡ä¸­çš„"August"ã€‚ä½†åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œ"August"è¿™ä¸ªè¯é¦–å…ˆè¢«ç¼–ç è¿›éšè—çŠ¶æ€ï¼Œç„¶åä¸å…¶ä»–æ‰€æœ‰è¯çš„ä¿¡æ¯æ··åˆåœ¨ä¸€èµ·ï¼Œæœ€ç»ˆå‹ç¼©æˆä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ã€‚è§£ç å™¨è¦ä»è¿™ä¸ªå‹ç¼©åçš„å‘é‡ä¸­"æŒ–å‡º"Augustçš„ä¿¡æ¯â€”â€”è¿™å°±åƒä»ä¸€é”…æ±¤é‡Œæ‰¾å›åŸæ¥çš„é£Ÿæã€‚

æ›´ç³Ÿç³•çš„æ˜¯ï¼Œå¥å­ä¸­çš„æŸäº›è¯å¯¹å½“å‰ç”Ÿæˆçš„è¯æ›´é‡è¦ã€‚ç¿»è¯‘"August"æ—¶ï¼Œæ¨¡å‹æœ€éœ€è¦å…³æ³¨çš„æ˜¯åŸæ–‡ä¸­çš„"August"ï¼Œè€Œä¸æ˜¯"The"æˆ–"was"ã€‚ä½†æ ‡å‡†Seq2Seqå¯¹æ‰€æœ‰è¾“å…¥ä½ç½®ä¸€è§†åŒä»â€”â€”å®ƒä»¬éƒ½è¢«åŒç­‰åœ°å‹ç¼©è¿›äº† $\mathbf{c}$ã€‚

> ğŸ’¡ **æœ¬ç« æ ¸å¿ƒæ´å¯Ÿ**ï¼šè§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œåº”è¯¥èƒ½å¤Ÿ**æœ‰é€‰æ‹©åœ°å…³æ³¨**è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®ã€‚ä¸åŒçš„è¾“å‡ºè¯éœ€è¦å…³æ³¨ä¸åŒçš„è¾“å…¥è¯â€”â€”è¿™å°±æ˜¯"æ³¨æ„åŠ›"çš„æœ¬è´¨ã€‚æœ¬ç« å°†è¿½æº¯æ³¨æ„åŠ›æœºåˆ¶ä»è¯ç”Ÿï¼ˆBahdanau, 2014ï¼‰åˆ°ç³»ç»Ÿæ€§æ¢ç´¢ï¼ˆLuong, 2015ï¼‰çš„å®Œæ•´å†ç¨‹ï¼Œæ­ç¤ºä¸åŒè®¾è®¡é€‰æ‹©èƒŒåçš„æƒè¡¡ã€‚

---

## é—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ

### é—®é¢˜çš„ç²¾ç¡®å®šä¹‰

è®©æˆ‘ä»¬å½¢å¼åŒ–åœ°æè¿°Seq2Seqçš„ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ã€‚

åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œç¼–ç å™¨äº§ç”Ÿä¸€ç³»åˆ—éšè—çŠ¶æ€ $\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, \ldots, \mathbf{h}_T^{enc}$ï¼Œä½†åªæœ‰æœ€åä¸€ä¸ªçŠ¶æ€ $\mathbf{h}_T^{enc}$ è¢«ä¼ é€’ç»™è§£ç å™¨ä½œä¸ºä¸Šä¸‹æ–‡å‘é‡ï¼š

$$
\mathbf{c} = \mathbf{h}_T^{enc}
$$

è§£ç å™¨çš„æ¯ä¸€æ­¥éƒ½ä½¿ç”¨è¿™åŒä¸€ä¸ª $\mathbf{c}$ï¼š

$$
\mathbf{h}_t^{dec} = f(\mathbf{h}_{t-1}^{dec}, y_{t-1}, \mathbf{c})
$$

é—®é¢˜åœ¨äºï¼š$\mathbf{c}$ æ˜¯ä¸€ä¸ª**é™æ€**çš„ã€**å…¨å±€**çš„è¡¨ç¤ºã€‚å®ƒåœ¨è§£ç çš„æ¯ä¸€æ­¥éƒ½ä¿æŒä¸å˜ï¼Œæ— æ³•æ ¹æ®å½“å‰ç”Ÿæˆçš„è¯åŠ¨æ€è°ƒæ•´ã€‚

ä»ä¿¡æ¯è®ºçš„è§’åº¦çœ‹ï¼Œå¦‚æœè¾“å…¥åºåˆ— $\mathbf{x}$ çš„ä¿¡æ¯ç†µæ˜¯ $H(\mathbf{x})$ï¼Œè€Œ $\mathbf{c}$ çš„ç»´åº¦æ˜¯ $d$ï¼Œé‚£ä¹ˆ $\mathbf{c}$ æœ€å¤šèƒ½æºå¸¦ $O(d)$ çš„ä¿¡æ¯é‡ã€‚å½“ $H(\mathbf{x}) > O(d)$ æ—¶ï¼Œä¿¡æ¯ä¸¢å¤±æ˜¯ä¸å¯é¿å…çš„ã€‚

### ä¹‹å‰çš„å°è¯•ä¸ºä½•å¤±è´¥ï¼Ÿ

åœ¨Attentionå‡ºç°ä¹‹å‰ï¼Œç ”ç©¶è€…å°è¯•è¿‡ä¸€äº›ç¼“è§£ä¿¡æ¯ç“¶é¢ˆçš„æ–¹æ³•ï¼š

**å¢åŠ ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦**ï¼šç›´è§‰ä¸Šï¼Œæ›´å¤§çš„ $\mathbf{c}$ å¯ä»¥æºå¸¦æ›´å¤šä¿¡æ¯ã€‚ä½†è¿™åªæ˜¯å»¶ç¼“é—®é¢˜ï¼Œè€Œéè§£å†³é—®é¢˜ã€‚è€Œä¸”æ›´å¤§çš„å‘é‡æ„å‘³ç€æ›´å¤šå‚æ•°ï¼Œæ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

**ä½¿ç”¨åŒå‘RNN**ï¼šè®©ç¼–ç å™¨åŒæ—¶ä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦è¯»å–è¾“å…¥ï¼Œç„¶åæ‹¼æ¥ä¸¤ä¸ªæ–¹å‘çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚è¿™ç¡®å®èƒ½æ•è·æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†ä»ç„¶æ˜¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå‘é‡â€”â€”åªæ˜¯è¿™ä¸ªå‘é‡ç¨å¾®å¤§äº†ä¸€ç‚¹ã€‚

**è¾“å…¥åè½¬**ï¼šSutskeverç­‰äººå‘ç°ï¼Œå°†è¾“å…¥åºåˆ—åè½¬åå†è¾“å…¥ç¼–ç å™¨ï¼Œç¿»è¯‘æ•ˆæœæ›´å¥½ã€‚è¿™æ˜¯å› ä¸ºè¾“å…¥çš„æœ€åå‡ ä¸ªè¯ï¼ˆåè½¬åå˜æˆæœ€å…ˆè¾“å…¥çš„è¯ï¼‰ä¸è¾“å‡ºçš„æœ€å…ˆå‡ ä¸ªè¯å¾€å¾€æœ‰æ›´å¼ºçš„å¯¹åº”å…³ç³»ã€‚ä½†è¿™åªæ˜¯ä¸€ä¸ªå¯å‘å¼æŠ€å·§ï¼Œä¸èƒ½ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜ã€‚

è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è§¦åŠé—®é¢˜çš„æ ¸å¿ƒï¼š**è§£ç å™¨åªèƒ½çœ‹åˆ°ä¸€ä¸ªå›ºå®šçš„ã€å…¨å±€çš„è¡¨ç¤ºï¼Œæ— æ³•åŠ¨æ€åœ°è®¿é—®è¾“å…¥çš„ä¸åŒéƒ¨åˆ†**ã€‚

### æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„è§£å†³æ–¹æ¡ˆï¼Ÿ

ç†æƒ³çš„è§£å†³æ–¹æ¡ˆåº”è¯¥å…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

1. **åŠ¨æ€æ€§**ï¼šè§£ç å™¨åœ¨ç”Ÿæˆä¸åŒè¯æ—¶ï¼Œåº”è¯¥èƒ½å¤Ÿå…³æ³¨è¾“å…¥çš„ä¸åŒä½ç½®
2. **è½¯é€‰æ‹©**ï¼šä¸æ˜¯ç¡¬æ€§åœ°é€‰æ‹©æŸä¸€ä¸ªä½ç½®ï¼Œè€Œæ˜¯å¯¹æ‰€æœ‰ä½ç½®è®¡ç®—ä¸€ä¸ªé‡è¦æ€§åˆ†å¸ƒ
3. **ç«¯åˆ°ç«¯å¯è®­ç»ƒ**ï¼šæ•´ä¸ªæœºåˆ¶åº”è¯¥å¯ä»¥é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–
4. **å¯è§£é‡Šæ€§**ï¼šæ¨¡å‹å…³æ³¨å“ªäº›ä½ç½®åº”è¯¥æ˜¯å¯ä»¥è§‚å¯Ÿå’Œç†è§£çš„

è¿™äº›ç‰¹æ€§æ­£æ˜¯Attentionæœºåˆ¶æ‰€æä¾›çš„ã€‚

---

## æ ¸å¿ƒæ€æƒ³ä¸ç›´è§‰

### å…³é”®æ´å¯Ÿï¼šåŠ¨æ€çš„ã€åŸºäºå†…å®¹çš„å¯»å€

Attentionçš„æ ¸å¿ƒæ´å¯Ÿå¯ä»¥ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼š

> **è®©è§£ç å™¨åœ¨æ¯ä¸€æ­¥éƒ½èƒ½"å›å¤´çœ‹"ç¼–ç å™¨çš„æ‰€æœ‰ä½ç½®ï¼Œå¹¶æ ¹æ®å½“å‰éœ€è¦åŠ¨æ€å†³å®šå…³æ³¨å“ªäº›ä½ç½®ã€‚**

è¿™ä¸ªæƒ³æ³•å¬èµ·æ¥ç®€å•ï¼Œä½†å®ƒå½»åº•æ”¹å˜äº†åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„èŒƒå¼ã€‚

### ç›´è§‰è§£é‡Šï¼šèšå…‰ç¯ä¸å›¾ä¹¦é¦†

æƒ³è±¡ä½ åœ¨ä¸€ä¸ªé»‘æš—çš„å›¾ä¹¦é¦†é‡Œæ‰¾ä¹¦ã€‚ä¼ ç»ŸSeq2Seqå°±åƒæ˜¯ï¼šä½ å…ˆç”¨æ‰‹ç”µç­’å¿«é€Ÿæ‰«è¿‡æ‰€æœ‰ä¹¦æ¶ï¼Œç„¶åå…³æ‰æ‰‹ç”µç­’ï¼Œä»…å‡­è®°å¿†å»å–ä¹¦ã€‚ä½ å¯¹æ•´ä¸ªå›¾ä¹¦é¦†æœ‰ä¸€ä¸ªæ¨¡ç³Šçš„æ•´ä½“å°è±¡ï¼Œä½†ç»†èŠ‚å¾ˆå®¹æ˜“é—å¿˜ã€‚

Attentionæœºåˆ¶åˆ™åƒæ˜¯ï¼šä½ æ‰‹é‡Œæœ‰ä¸€ä¸ª**å¯è°ƒèŠ‚çš„èšå…‰ç¯**ã€‚å½“ä½ éœ€è¦æ‰¾æŸæœ¬ä¹¦æ—¶ï¼Œä½ å¯ä»¥æŠŠèšå…‰ç¯ç…§å‘ç›¸å…³çš„ä¹¦æ¶ï¼Œä»”ç»†æŸ¥çœ‹é‚£é‡Œçš„ä¹¦åã€‚ä¸åŒçš„æŸ¥è¯¢éœ€æ±‚ä¼šè®©ä½ æŠŠå…‰ç…§å‘ä¸åŒçš„ä½ç½®ã€‚

æ›´å…·ä½“åœ°è¯´ï¼Œå½“è§£ç å™¨ç”Ÿæˆ"aoÃ»t"ï¼ˆå…«æœˆï¼‰è¿™ä¸ªè¯æ—¶ï¼ŒAttentionæœºåˆ¶ä¼šï¼š

1. æŸ¥çœ‹ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼ˆå›¾ä¹¦é¦†çš„æ‰€æœ‰ä¹¦æ¶ï¼‰
2. è®¡ç®—æ¯ä¸ªä½ç½®ä¸å½“å‰ä»»åŠ¡çš„ç›¸å…³æ€§ï¼ˆåˆ¤æ–­æ¯ä¸ªä¹¦æ¶æ˜¯å¦å¯èƒ½æœ‰ä½ è¦çš„ä¹¦ï¼‰
3. æŠŠ"èšå…‰ç¯"ä¸»è¦ç…§å‘ç›¸å…³çš„ä½ç½®ï¼ˆ"August"å¯¹åº”çš„ç¼–ç å™¨çŠ¶æ€ï¼‰
4. ä»è¿™äº›ä½ç½®æ±‡æ€»ä¿¡æ¯ï¼Œè¾…åŠ©ç”Ÿæˆå½“å‰è¯

### å¦ä¸€ä¸ªç±»æ¯”ï¼šåŠ æƒæŠ•ç¥¨

ä½ ä¹Ÿå¯ä»¥æŠŠAttentionç†è§£ä¸ºä¸€ç§åŠ æƒæŠ•ç¥¨æœºåˆ¶ã€‚

æƒ³è±¡è§£ç å™¨æ˜¯ä¸€ä¸ªé¢†å¯¼ï¼Œéœ€è¦åšä¸€ä¸ªå†³å®šï¼ˆç”Ÿæˆä¸‹ä¸€ä¸ªè¯ï¼‰ã€‚å®ƒæœ‰ä¸€ä¸ªé¡¾é—®å›¢é˜Ÿï¼ˆç¼–ç å™¨çš„å„ä¸ªéšè—çŠ¶æ€ï¼‰ï¼Œæ¯ä¸ªé¡¾é—®æŒæ¡è¾“å…¥åºåˆ—ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯ã€‚

ä¼ ç»ŸSeq2Seqï¼šåªå¬ä¸€ä¸ª"æ€»é¡¾é—®"çš„æ„è§ï¼ˆä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ï¼‰ï¼Œè¿™ä¸ªæ€»é¡¾é—®è¦ç»¼åˆæ‰€æœ‰äººçš„ä¿¡æ¯ã€‚

Attentionæœºåˆ¶ï¼šç›´æ¥å¾è¯¢æ¯ä¸ªé¡¾é—®çš„æ„è§ï¼Œç„¶åæ ¹æ®è®®é¢˜ç›¸å…³æ€§ç»™ä¸åŒé¡¾é—®çš„æ„è§èµ‹äºˆä¸åŒæƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—å‡ºæœ€ç»ˆå†³å®šã€‚

### è®¾è®¡åŠ¨æœºï¼šä¸ºä»€ä¹ˆé€‰æ‹©è½¯æ³¨æ„åŠ›ï¼Ÿ

Attentionæœºåˆ¶æœ‰ä¸¤ç§åŸºæœ¬å˜ä½“ï¼š

- **è½¯æ³¨æ„åŠ›ï¼ˆSoft Attentionï¼‰**ï¼šå¯¹æ‰€æœ‰ä½ç½®è®¡ç®—æ¦‚ç‡åˆ†å¸ƒï¼ŒåŠ æƒæ±‚å’Œ
- **ç¡¬æ³¨æ„åŠ›ï¼ˆHard Attentionï¼‰**ï¼šé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼Œåªçœ‹é‚£é‡Œçš„ä¿¡æ¯

Bahdanauç­‰äººé€‰æ‹©äº†è½¯æ³¨æ„åŠ›ï¼ŒåŸå› æ˜¯ï¼š

1. **å¯å¾®åˆ†**ï¼šè½¯æ³¨æ„åŠ›çš„åŠ æƒæ±‚å’Œæ˜¯å¯å¾®çš„ï¼Œå¯ä»¥ç”¨æ ‡å‡†çš„åå‘ä¼ æ’­è®­ç»ƒ
2. **ç¨³å®š**ï¼šç¡¬æ³¨æ„åŠ›éœ€è¦é‡‡æ ·æˆ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®­ç»ƒï¼Œæ–¹å·®å¤§ï¼Œä¸ç¨³å®š
3. **ä¿¡æ¯æ›´ä¸°å¯Œ**ï¼šè½¯æ³¨æ„åŠ›å¯ä»¥åŒæ—¶åˆ©ç”¨å¤šä¸ªä½ç½®çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯éæ­¤å³å½¼

ç¡¬æ³¨æ„åŠ›ä¹Ÿæœ‰å…¶ä¼˜åŠ¿ï¼ˆè®¡ç®—æ›´é«˜æ•ˆï¼Œæ›´ç¨€ç–ï¼‰ï¼Œä½†åœ¨å®è·µä¸­ï¼Œè½¯æ³¨æ„åŠ›å› å…¶ç®€å•å’Œæœ‰æ•ˆæˆä¸ºäº†ä¸»æµã€‚æˆ‘ä»¬å°†åœ¨åé¢çš„å°èŠ‚ä¸­æ›´è¯¦ç»†åœ°å¯¹æ¯”ä¸¤è€…ã€‚

---

## æŠ€æœ¯ç»†èŠ‚

### Bahdanau Attentionï¼šåŠ æ€§æ³¨æ„åŠ›

2014å¹´ï¼ŒBahdanauã€Choå’ŒBengioæå‡ºäº†ç¬¬ä¸€ä¸ªæˆåŠŸçš„æ³¨æ„åŠ›æœºåˆ¶ç”¨äºæœºå™¨ç¿»è¯‘ã€‚è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

é¦–å…ˆï¼Œç¼–ç å™¨ä½¿ç”¨**åŒå‘RNN**ï¼Œåœ¨æ¯ä¸ªä½ç½® $j$ äº§ç”Ÿä¸€ä¸ªéšè—çŠ¶æ€ï¼š

$$
\mathbf{h}_j = [\overrightarrow{\mathbf{h}}_j; \overleftarrow{\mathbf{h}}_j]
$$

å…¶ä¸­ $\overrightarrow{\mathbf{h}}_j$ æ˜¯å‰å‘RNNçš„éšè—çŠ¶æ€ï¼Œ$\overleftarrow{\mathbf{h}}_j$ æ˜¯åå‘RNNçš„éšè—çŠ¶æ€ã€‚æ‹¼æ¥åï¼Œ$\mathbf{h}_j$ åŒæ—¶åŒ…å«äº†ä½ç½® $j$ çš„å·¦ä¾§å’Œå³ä¾§ä¸Šä¸‹æ–‡ã€‚

åœ¨è§£ç çš„ç¬¬ $i$ æ­¥ï¼Œæˆ‘ä»¬è®¡ç®—ä¸€ä¸ª**åŠ¨æ€çš„ä¸Šä¸‹æ–‡å‘é‡** $\mathbf{c}_i$ï¼ˆæ³¨æ„ï¼šä¸å†æ˜¯å›ºå®šçš„ $\mathbf{c}$ï¼Œè€Œæ˜¯æ¯ä¸€æ­¥éƒ½ä¸åŒçš„ $\mathbf{c}_i$ï¼‰ï¼š

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

å…¶ä¸­ $\alpha_{ij}$ æ˜¯ç¬¬ $i$ æ­¥è§£ç æ—¶ï¼Œå¯¹è¾“å…¥ä½ç½® $j$ çš„æ³¨æ„åŠ›æƒé‡ã€‚

é‚£ä¹ˆ $\alpha_{ij}$ æ˜¯æ€ä¹ˆè®¡ç®—çš„å‘¢ï¼Ÿè¿™æ˜¯Attentionæœºåˆ¶çš„æ ¸å¿ƒã€‚

é¦–å…ˆï¼Œè®¡ç®—ä¸€ä¸ª**å¯¹é½åˆ†æ•°ï¼ˆalignment scoreï¼‰** $e_{ij}$ï¼Œè¡¡é‡è§£ç å™¨å½“å‰çŠ¶æ€ä¸ç¼–ç å™¨ä½ç½® $j$ çš„ç›¸å…³æ€§ï¼š

$$
e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)
$$

å…¶ä¸­ $\mathbf{s}_{i-1}$ æ˜¯è§£ç å™¨åœ¨ç¬¬ $i-1$ æ­¥çš„éšè—çŠ¶æ€ï¼Œ$a$ æ˜¯ä¸€ä¸ª**å¯¹é½æ¨¡å‹ï¼ˆalignment modelï¼‰**ã€‚

Bahdanauä½¿ç”¨äº†ä¸€ä¸ªå•å±‚å‰é¦ˆç½‘ç»œä½œä¸ºå¯¹é½æ¨¡å‹ï¼š

$$
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
$$

è¿™è¢«ç§°ä¸º**åŠ æ€§æ³¨æ„åŠ›ï¼ˆadditive attentionï¼‰**ï¼Œå› ä¸º $\mathbf{s}_{i-1}$ å’Œ $\mathbf{h}_j$ æ˜¯é€šè¿‡åŠ æ³•ç»“åˆçš„ã€‚

ç„¶åï¼Œå¯¹æ‰€æœ‰ä½ç½®çš„åˆ†æ•°åšsoftmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼š

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$

softmaxç¡®ä¿äº†ï¼š

- æ‰€æœ‰æƒé‡éƒ½æ˜¯æ­£æ•°ï¼š$\alpha_{ij} > 0$
- æƒé‡ä¹‹å’Œä¸º1ï¼š$\sum_j \alpha_{ij} = 1$

è¿™æ ·ï¼Œ$\alpha_{ij}$ å¯ä»¥è§£é‡Šä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒâ€”â€”è§£ç å™¨åœ¨ç¬¬ $i$ æ­¥"å…³æ³¨"è¾“å…¥ä½ç½® $j$ çš„æ¦‚ç‡ã€‚

::: {.callout-note}
## Algorithm: Bahdanau Attention (Bahdanau et al., 2015)

```python
def bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):
    """
    Bahdanau (åŠ æ€§) æ³¨æ„åŠ›æœºåˆ¶

    å‚æ•°:
        s_prev: è§£ç å™¨ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ [batch, dec_hidden]
        encoder_outputs: ç¼–ç å™¨æ‰€æœ‰éšè—çŠ¶æ€ [batch, src_len, enc_hidden]
        W_a, U_a, v_a: å¯å­¦ä¹ å‚æ•°

    è¿”å›:
        context: ä¸Šä¸‹æ–‡å‘é‡ [batch, enc_hidden]
        attention_weights: æ³¨æ„åŠ›æƒé‡ [batch, src_len]
    """
    # Step 1: è®¡ç®—å¯¹é½åˆ†æ•°
    # s_prev å¹¿æ’­åˆ°æ‰€æœ‰æºä½ç½®
    scores = v_a @ tanh(W_a @ s_prev + U_a @ encoder_outputs)  # [batch, src_len]

    # Step 2: Softmax å½’ä¸€åŒ–
    attention_weights = softmax(scores, dim=-1)  # [batch, src_len]

    # Step 3: åŠ æƒæ±‚å’Œ
    context = attention_weights @ encoder_outputs  # [batch, enc_hidden]

    return context, attention_weights
```

*Source: Bahdanau, Cho, & Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", ICLR 2015. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*
:::

ä¸‹å›¾å±•ç¤ºäº†å¸¦Bahdanau Attentionçš„RNN Encoder-Decoderæ¶æ„ï¼š

![å¸¦Bahdanau Attentionçš„RNN Encoder-Decoderæ¶æ„ã€‚ç¼–ç å™¨ï¼ˆåº•éƒ¨ï¼‰ä½¿ç”¨åŒå‘RNNå¤„ç†è¾“å…¥åºåˆ—ï¼Œäº§ç”Ÿéšè—çŠ¶æ€åºåˆ—ã€‚è§£ç å™¨ï¼ˆé¡¶éƒ¨ï¼‰åœ¨æ¯ä¸€æ­¥é€šè¿‡Attentionæœºåˆ¶åŠ¨æ€è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼šå°†å½“å‰è§£ç å™¨çŠ¶æ€ä¸æ‰€æœ‰ç¼–ç å™¨çŠ¶æ€æ¯”è¾ƒï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡$c_t$ï¼Œè¾…åŠ©ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ã€‚](figures/chapter-6/original/fig-bahdanau-attention-d2l.svg){#fig-attention-mechanism width=70%}

::: {.figure-caption}
*Source: Dive into Deep Learning, Figure 11.4.2. [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)*
:::

### å®Œæ•´æ•°å€¼ç¤ºä¾‹ï¼šBahdanau Attentionè®¡ç®—

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå°ä¾‹å­èµ°ä¸€éå®Œæ•´çš„Attentionè®¡ç®—è¿‡ç¨‹ã€‚

**è®¾å®š**ï¼š

- è¾“å…¥åºåˆ—ï¼š3ä¸ªè¯ï¼ˆ"I love NLP"ï¼‰ï¼Œç¼–ç åå¾—åˆ°3ä¸ªéšè—çŠ¶æ€
- è§£ç å™¨éšè—çŠ¶æ€ç»´åº¦ï¼š$d_s = 4$
- ç¼–ç å™¨éšè—çŠ¶æ€ç»´åº¦ï¼š$d_h = 4$
- æ³¨æ„åŠ›ä¸­é—´ç»´åº¦ï¼š$d_a = 3$

**ç¼–ç å™¨è¾“å‡º**ï¼ˆå‡è®¾å·²ç»è®¡ç®—å¥½ï¼‰ï¼š

$$
\mathbf{h}_1 = [0.2, 0.5, -0.3, 0.8]^\top \quad \text{("I")}
$$

$$
\mathbf{h}_2 = [0.7, -0.2, 0.4, 0.1]^\top \quad \text{("love")}
$$

$$
\mathbf{h}_3 = [-0.1, 0.6, 0.5, -0.4]^\top \quad \text{("NLP")}
$$

**è§£ç å™¨å½“å‰çŠ¶æ€**ï¼ˆæ­£åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªç›®æ ‡è¯ï¼‰ï¼š

$$
\mathbf{s}_0 = [0.1, -0.3, 0.4, 0.2]^\top
$$

**å‚æ•°**ï¼ˆç®€åŒ–çš„éšæœºå€¼ï¼‰ï¼š

$$
\mathbf{W}_a = \begin{bmatrix} 0.1 & -0.2 & 0.3 & 0.1 \\ 0.2 & 0.1 & -0.1 & 0.2 \\ -0.1 & 0.3 & 0.2 & -0.2 \end{bmatrix}, \quad
\mathbf{U}_a = \begin{bmatrix} 0.2 & 0.1 & -0.2 & 0.3 \\ -0.1 & 0.2 & 0.1 & 0.1 \\ 0.3 & -0.1 & 0.2 & -0.1 \end{bmatrix}
$$

$$
\mathbf{v}_a = [0.5, -0.3, 0.4]^\top
$$

**Step 1ï¼šè®¡ç®— $\mathbf{W}_a \mathbf{s}_0$**

$$
\mathbf{W}_a \mathbf{s}_0 = \begin{bmatrix} 0.1 \cdot 0.1 + (-0.2) \cdot (-0.3) + 0.3 \cdot 0.4 + 0.1 \cdot 0.2 \\ \vdots \end{bmatrix} = \begin{bmatrix} 0.21 \\ 0.03 \\ 0.04 \end{bmatrix}
$$

**Step 2ï¼šå¯¹æ¯ä¸ªç¼–ç å™¨çŠ¶æ€è®¡ç®— $\mathbf{U}_a \mathbf{h}_j$**

$$
\mathbf{U}_a \mathbf{h}_1 = [0.33, 0.14, -0.05]^\top
$$

$$
\mathbf{U}_a \mathbf{h}_2 = [0.13, 0.06, 0.29]^\top
$$

$$
\mathbf{U}_a \mathbf{h}_3 = [-0.15, 0.17, 0.05]^\top
$$

**Step 3ï¼šè®¡ç®—å¯¹é½åˆ†æ•° $e_{1j}$**

$$
e_{11} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_0 + \mathbf{U}_a \mathbf{h}_1) = \mathbf{v}_a^\top \tanh([0.54, 0.17, -0.01]^\top)
$$

$$
= [0.5, -0.3, 0.4] \cdot [\tanh(0.54), \tanh(0.17), \tanh(-0.01)]^\top
$$

$$
= [0.5, -0.3, 0.4] \cdot [0.49, 0.17, -0.01]^\top = 0.24 - 0.05 - 0.004 \approx 0.19
$$

ç±»ä¼¼åœ°è®¡ç®— $e_{12}$ å’Œ $e_{13}$ï¼š

$$
e_{12} \approx 0.25, \quad e_{13} \approx 0.08
$$

**Step 4ï¼šSoftmaxå½’ä¸€åŒ–**

$$
\alpha_{11} = \frac{\exp(0.19)}{\exp(0.19) + \exp(0.25) + \exp(0.08)} = \frac{1.21}{1.21 + 1.28 + 1.08} = \frac{1.21}{3.57} \approx 0.34
$$

$$
\alpha_{12} = \frac{1.28}{3.57} \approx 0.36, \quad \alpha_{13} = \frac{1.08}{3.57} \approx 0.30
$$

**Step 5ï¼šè®¡ç®—ä¸Šä¸‹æ–‡å‘é‡**

$$
\mathbf{c}_1 = \alpha_{11} \mathbf{h}_1 + \alpha_{12} \mathbf{h}_2 + \alpha_{13} \mathbf{h}_3
$$

$$
= 0.34 \cdot [0.2, 0.5, -0.3, 0.8]^\top + 0.36 \cdot [0.7, -0.2, 0.4, 0.1]^\top + 0.30 \cdot [-0.1, 0.6, 0.5, -0.4]^\top
$$

$$
\approx [0.29, 0.18, 0.09, 0.19]^\top
$$

**è§£è¯»**ï¼šåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¨¡å‹å¯¹"love"çš„å…³æ³¨æœ€å¤šï¼ˆ0.36ï¼‰ï¼Œå…¶æ¬¡æ˜¯"I"ï¼ˆ0.34ï¼‰å’Œ"NLP"ï¼ˆ0.30ï¼‰ã€‚æ³¨æ„åŠ›æƒé‡ç›¸å¯¹å‡åŒ€ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯éšæœºå‚æ•°ã€‚åœ¨è®­ç»ƒåçš„çœŸå®æ¨¡å‹ä¸­ï¼Œæƒé‡åˆ†å¸ƒä¼šæ›´åŠ å°–é”â€”â€”æ¨¡å‹ä¼šå­¦ä¼šåœ¨éœ€è¦æ—¶èšç„¦äºç‰¹å®šä½ç½®ã€‚

### è§£ç å™¨çš„å®Œæ•´æµç¨‹

æœ‰äº†Attentionæœºåˆ¶ï¼Œè§£ç å™¨çš„æ¯ä¸€æ­¥å·¥ä½œæµç¨‹å˜ä¸ºï¼š

1. **è®¡ç®—æ³¨æ„åŠ›æƒé‡** $\alpha_{ij}$ï¼šåŸºäºå½“å‰è§£ç å™¨çŠ¶æ€å’Œæ‰€æœ‰ç¼–ç å™¨çŠ¶æ€
2. **è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡** $\mathbf{c}_i$ï¼šå¯¹ç¼–ç å™¨çŠ¶æ€åŠ æƒæ±‚å’Œ
3. **æ›´æ–°è§£ç å™¨çŠ¶æ€**ï¼šç»“åˆä¸Šä¸‹æ–‡å‘é‡ã€å‰ä¸€æ­¥è¾“å‡ºã€å‰ä¸€æ­¥çŠ¶æ€

$$
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)
$$

4. **ç”Ÿæˆè¾“å‡º**ï¼šåŸºäºæ–°çš„è§£ç å™¨çŠ¶æ€

$$
P(y_i | y_{<i}, \mathbf{x}) = g(\mathbf{s}_i, y_{i-1}, \mathbf{c}_i)
$$

å…³é”®åŒºåˆ«æ˜¯ï¼š**æ¯ä¸€æ­¥éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„ä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}_i$**ï¼Œå®ƒæ˜¯æ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€è®¡ç®—çš„ã€‚

### Luong Attentionï¼šä¹˜æ€§æ³¨æ„åŠ›çš„ç³»ç»Ÿæ€§æ¢ç´¢

Bahdanauçš„åŠ æ€§æ³¨æ„åŠ›è™½ç„¶æœ‰æ•ˆï¼Œä½†å®ƒçš„è®¡ç®—é‡ä¸å°â€”â€”æ¯æ¬¡è®¡ç®—å¯¹é½åˆ†æ•°éƒ½éœ€è¦ä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼ˆä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ã€ä¸€ä¸ªéçº¿æ€§æ¿€æ´»ã€ä¸€ä¸ªå‘é‡ç‚¹ç§¯ï¼‰ã€‚ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜éšä¹‹æµ®ç°ï¼šèƒ½å¦ç”¨æ›´ç®€å•çš„æ“ä½œæ¥è¡¡é‡ä¸¤ä¸ªçŠ¶æ€çš„ç›¸å…³æ€§ï¼Ÿ

2015å¹´ï¼Œè·Bahdanauè®ºæ–‡å‘è¡¨ä»…ä¸€å¹´ï¼Œæ–¯å¦ç¦å¤§å­¦çš„Luongã€Phamå’ŒManningå‘è¡¨äº†ä¸€ç¯‡ç³»ç»Ÿæ€§çš„ç ”ç©¶ï¼Œå›ç­”äº†è¿™ä¸ªé—®é¢˜ã€‚ä»–ä»¬æå‡ºäº†è®¡ç®—æ•ˆç‡æ›´é«˜çš„**ä¹˜æ€§æ³¨æ„åŠ›ï¼ˆmultiplicative attentionï¼‰**ï¼Œå¹¶ç³»ç»Ÿæ¯”è¾ƒäº†ä¸‰ç§å¯¹é½å‡½æ•°ï¼š

| åç§° | å…¬å¼ | å‚æ•° | è®¡ç®—æ•ˆç‡ |
|------|------|------|----------|
| **Dot** | $\mathbf{s}^\top \mathbf{h}$ | æ—  | æœ€å¿« |
| **General** | $\mathbf{s}^\top \mathbf{W}_a \mathbf{h}$ | $\mathbf{W}_a$ | ä¸­ç­‰ |
| **Concat** | $\mathbf{v}_a^\top \tanh(\mathbf{W}_a [\mathbf{s}; \mathbf{h}])$ | $\mathbf{v}_a, \mathbf{W}_a$ | æœ€æ…¢ |

**Dotï¼ˆç‚¹ç§¯ï¼‰**ï¼šæœ€ç®€å•çš„å½¢å¼ï¼Œç›´æ¥è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å†…ç§¯ $e_{ij} = \mathbf{s}_i^\top \mathbf{h}_j$ã€‚æ²¡æœ‰ä»»ä½•å¯å­¦ä¹ å‚æ•°ï¼Œè®¡ç®—æœ€å¿«ï¼Œä½†è¦æ±‚è§£ç å™¨å’Œç¼–ç å™¨çš„éšè—ç»´åº¦å¿…é¡»ç›¸åŒã€‚ç›´è§‰ä¸Šï¼Œç‚¹ç§¯è¡¡é‡çš„æ˜¯ä¸¤ä¸ªå‘é‡çš„"ç›¸ä¼¼åº¦"â€”â€”å¦‚æœå®ƒä»¬æŒ‡å‘ç›¸ä¼¼çš„æ–¹å‘ï¼Œç‚¹ç§¯å°±å¤§ï¼›å¦‚æœæ­£äº¤ï¼Œç‚¹ç§¯ä¸ºé›¶ã€‚

**Generalï¼ˆä¸€èˆ¬å½¢å¼ï¼‰**ï¼šå¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çŸ©é˜µ $\mathbf{W}_a$ï¼Œå…è®¸ä¸åŒç»´åº¦çš„çŠ¶æ€ç›¸äº’æ¯”è¾ƒï¼Œä¹Ÿå¢åŠ äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚å®è´¨ä¸Šæ˜¯é—®ï¼š"$\mathbf{s}$ å’Œ $\mathbf{W}_a \mathbf{h}$ï¼ˆ$\mathbf{h}$ çš„ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼‰æœ‰å¤šç›¸ä¼¼ï¼Ÿ"

**Concatï¼ˆæ‹¼æ¥ï¼‰**ï¼šè¿™å°±æ˜¯Bahdanauçš„åŠ æ€§æ³¨æ„åŠ›ã€‚è¡¨è¾¾èƒ½åŠ›æœ€å¼ºâ€”â€”å®ƒå¯ä»¥å­¦ä¹ ä»»æ„éçº¿æ€§çš„å¯¹é½å‡½æ•°â€”â€”ä½†è®¡ç®—æœ€æ…¢ã€‚

é™¤äº†å¯¹é½å‡½æ•°ï¼ŒLuongè¿˜å‘ç°äº†å¦ä¸€ä¸ªé‡è¦çš„è®¾è®¡å·®å¼‚ï¼š**æ³¨æ„åŠ›åœ¨è§£ç å™¨ä¸­çš„ä½¿ç”¨ä½ç½®**ã€‚

Bahdanauçš„æ–¹å¼æ˜¯å…ˆç®—æ³¨æ„åŠ›å†æ›´æ–°RNNçŠ¶æ€ï¼š$\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)$ã€‚Luongçš„æ–¹å¼åˆ™æ›´åŠ æ¨¡å—åŒ–â€”â€”å…ˆç”¨RNNè®¡ç®—æ–°çŠ¶æ€ï¼Œå†åŸºäºæ–°çŠ¶æ€è®¡ç®—æ³¨æ„åŠ›ï¼š

$$
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1})
$$
$$
\mathbf{c}_i = \text{Attention}(\mathbf{s}_i, \mathbf{H})
$$
$$
\tilde{\mathbf{s}}_i = \tanh(\mathbf{W}_c [\mathbf{c}_i; \mathbf{s}_i])
$$

è¿™ç§è®¾è®¡è®©RNNå’ŒAttentionè§£è€¦ï¼Œä¾¿äºåˆ†æå’Œè°ƒè¯•ã€‚

::: {.callout-note}
## Algorithm: Luong Attention Variants (Luong et al., 2015)

```python
def luong_attention(decoder_state, encoder_outputs, method='dot', W_a=None, v_a=None):
    """
    Luong æ³¨æ„åŠ›æœºåˆ¶çš„ä¸‰ç§å˜ä½“

    å‚æ•°:
        decoder_state: è§£ç å™¨å½“å‰éšè—çŠ¶æ€ [batch, dec_hidden]
        encoder_outputs: ç¼–ç å™¨æ‰€æœ‰éšè—çŠ¶æ€ [batch, src_len, enc_hidden]
        method: 'dot', 'general', æˆ– 'concat'
        W_a: å¯å­¦ä¹ å‚æ•°ï¼ˆgeneralå’Œconcatéœ€è¦ï¼‰
        v_a: å¯å­¦ä¹ å‚æ•°ï¼ˆconcatéœ€è¦ï¼‰

    è¿”å›:
        context: ä¸Šä¸‹æ–‡å‘é‡ [batch, enc_hidden]
        attention_weights: æ³¨æ„åŠ›æƒé‡ [batch, src_len]
    """
    if method == 'dot':
        # ç‚¹ç§¯: s^T h
        # [batch, dec_hidden] @ [batch, enc_hidden, src_len] -> [batch, src_len]
        scores = torch.bmm(decoder_state.unsqueeze(1),
                          encoder_outputs.transpose(1, 2)).squeeze(1)

    elif method == 'general':
        # ä¸€èˆ¬å½¢å¼: s^T W h
        # å…ˆè®¡ç®— W @ h: [batch, src_len, dec_hidden]
        transformed = encoder_outputs @ W_a.T
        scores = torch.bmm(decoder_state.unsqueeze(1),
                          transformed.transpose(1, 2)).squeeze(1)

    elif method == 'concat':
        # æ‹¼æ¥å½¢å¼: v^T tanh(W [s; h])
        # æ‰©å±• decoder_state åˆ°æ‰€æœ‰ä½ç½®
        s_expanded = decoder_state.unsqueeze(1).expand(-1, encoder_outputs.size(1), -1)
        concat = torch.cat([s_expanded, encoder_outputs], dim=-1)
        scores = v_a @ torch.tanh(concat @ W_a.T).transpose(1, 2)
        scores = scores.squeeze(1)

    # Softmax å½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)

    # åŠ æƒæ±‚å’Œ
    context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

    return context, attention_weights
```

*Source: Luong, Pham, & Manning (2015) "Effective Approaches to Attention-based Neural Machine Translation", EMNLP 2015. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*
:::

### ä¸‰ç§å¯¹é½å‡½æ•°çš„æ•°å€¼å¯¹æ¯”

è®©æˆ‘ä»¬ç”¨åŒä¸€ç»„æ•°æ®ï¼Œæ¯”è¾ƒç‚¹ç§¯å¯¹é½å’ŒåŠ æ€§å¯¹é½çš„å·®å¼‚ã€‚

**è®¾å®š**ï¼š

- è§£ç å™¨çŠ¶æ€ï¼š$\mathbf{s} = [0.5, -0.3, 0.8, 0.2]^\top$
- ç¼–ç å™¨çŠ¶æ€ï¼ˆ3ä¸ªä½ç½®ï¼‰ï¼š
  - $\mathbf{h}_1 = [0.2, 0.4, 0.1, -0.3]^\top$
  - $\mathbf{h}_2 = [0.6, -0.1, 0.7, 0.3]^\top$
  - $\mathbf{h}_3 = [-0.2, 0.5, 0.3, 0.1]^\top$

**Dot-Product è®¡ç®—**ï¼š

$$
e_1 = \mathbf{s}^\top \mathbf{h}_1 = 0.5 \times 0.2 + (-0.3) \times 0.4 + 0.8 \times 0.1 + 0.2 \times (-0.3)
$$
$$
= 0.10 - 0.12 + 0.08 - 0.06 = 0.00
$$

$$
e_2 = \mathbf{s}^\top \mathbf{h}_2 = 0.5 \times 0.6 + (-0.3) \times (-0.1) + 0.8 \times 0.7 + 0.2 \times 0.3
$$
$$
= 0.30 + 0.03 + 0.56 + 0.06 = 0.95
$$

$$
e_3 = \mathbf{s}^\top \mathbf{h}_3 = 0.5 \times (-0.2) + (-0.3) \times 0.5 + 0.8 \times 0.3 + 0.2 \times 0.1
$$
$$
= -0.10 - 0.15 + 0.24 + 0.02 = 0.01
$$

**Softmax å½’ä¸€åŒ–**ï¼š

$$
\alpha_1 = \frac{e^{0.00}}{e^{0.00} + e^{0.95} + e^{0.01}} = \frac{1.00}{1.00 + 2.59 + 1.01} = \frac{1.00}{4.60} \approx 0.22
$$

$$
\alpha_2 = \frac{e^{0.95}}{4.60} = \frac{2.59}{4.60} \approx 0.56
$$

$$
\alpha_3 = \frac{e^{0.01}}{4.60} = \frac{1.01}{4.60} \approx 0.22
$$

**è§£è¯»**ï¼šä½¿ç”¨ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œæ¨¡å‹å°†56%çš„æ³¨æ„åŠ›æ”¾åœ¨ä½ç½®2ï¼Œè¿™æ˜¯å› ä¸º $\mathbf{h}_2$ ä¸ $\mathbf{s}$ åœ¨å‘é‡ç©ºé—´ä¸­æœ€"ç›¸ä¼¼"â€”â€”å®ƒä»¬çš„æ–¹å‘æœ€ä¸ºä¸€è‡´ã€‚æ³¨æ„ï¼Œç‚¹ç§¯æ³¨æ„åŠ›ä¸éœ€è¦ä»»ä½•å¯å­¦ä¹ å‚æ•°ï¼Œä»…é å‘é‡çš„å‡ ä½•å…³ç³»å°±èƒ½åŒºåˆ†å‡ºæœ€ç›¸å…³çš„ä½ç½®ã€‚è€Œå½“ $\mathbf{W}_a = \mathbf{I}$ï¼ˆå•ä½çŸ©é˜µï¼‰æ—¶ï¼ŒGeneralé€€åŒ–ä¸ºDot-Productï¼›åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ$\mathbf{W}_a$ å…è®¸æ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„ç›¸å…³æ€§æ¨¡å¼ã€‚

### Global vs Local Attention

Luongè¿˜æå‡ºäº†å¦ä¸€ä¸ªé‡è¦çš„è®¾è®¡ç»´åº¦ï¼š**æ³¨æ„åŠ›çš„èŒƒå›´**ã€‚

**Global Attention** å…³æ³¨æºåºåˆ—çš„æ‰€æœ‰ä½ç½®ï¼Œè¿™æ˜¯Bahdanauçš„åšæ³•ï¼Œä¹Ÿæ˜¯å‰é¢è®¨è®ºçš„é»˜è®¤æ–¹å¼ï¼š

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

**Local Attention** åªå…³æ³¨æºåºåˆ—çš„ä¸€ä¸ª**çª—å£**ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨æ¯ä¸ªè§£ç æ­¥ï¼Œå…ˆé¢„æµ‹ä¸€ä¸ªå¯¹é½ä½ç½® $p_i$ï¼Œç„¶ååªè®¡ç®—ä»¥ $p_i$ ä¸ºä¸­å¿ƒã€å®½åº¦ä¸º $2D+1$ çš„çª—å£å†…çš„æ³¨æ„åŠ›ï¼š

$$
\mathbf{c}_i = \sum_{j=p_i-D}^{p_i+D} \alpha_{ij} \mathbf{h}_j
$$

å¯¹é½ä½ç½® $p_i$ å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼ç¡®å®šï¼š

**Local-mï¼ˆå•è°ƒï¼‰**ï¼šå‡è®¾æºå’Œç›®æ ‡å¤§è‡´å¯¹é½ï¼Œç®€å•è®¾ç½® $p_i = i$ã€‚

**Local-pï¼ˆé¢„æµ‹ï¼‰**ï¼šå­¦ä¹ ä¸€ä¸ªå‡½æ•°æ¥é¢„æµ‹ $p_i$ï¼š

$$
p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))
$$

å…¶ä¸­ $\sigma$ æ˜¯sigmoidå‡½æ•°ï¼Œç¡®ä¿ $p_i \in [0, T_x]$ã€‚ä¸ºäº†è®©æ³¨æ„åŠ›åœ¨çª—å£ä¸­å¿ƒé™„è¿‘æ›´é›†ä¸­ï¼ŒLocal Attentionè¿˜å¼•å…¥äº†ä¸€ä¸ªé«˜æ–¯åç½®ï¼š

$$
\alpha_{ij} = \text{align}(\mathbf{s}_i, \mathbf{h}_j) \cdot \exp\left(-\frac{(j - p_i)^2}{2\sigma^2}\right)
$$

![Luongè®ºæ–‡ä¸­çš„Global vs Local Attentionå¯¹æ¯”ã€‚å·¦è¾¹æ˜¯Global Attentionï¼šè§£ç å™¨çŠ¶æ€ $h_t$ ä¸æ‰€æœ‰æºä½ç½®è®¡ç®—æ³¨æ„åŠ›ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ $c_t$ã€‚å³è¾¹æ˜¯Local Attentionï¼šå…ˆé¢„æµ‹å¯¹é½ä½ç½® $p_t$ï¼Œåªè®¡ç®—çª—å£ $[p_t - D, p_t + D]$ å†…çš„æ³¨æ„åŠ›ã€‚](figures/chapter-6/original/fig-global-local-attention.png){#fig-global-local width=80%}

::: {.figure-caption}
*Source: Luong, Pham, & Manning (2015) "Effective Approaches to Attention-based Neural Machine Translation", Figure 2 & 3. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*
:::

### Hard vs Soft Attention

é™¤äº†Global/Localçš„ç»´åº¦ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªåŸºæœ¬åŒºåˆ†ï¼š**è½¯æ³¨æ„åŠ›ï¼ˆSoft Attentionï¼‰** vs **ç¡¬æ³¨æ„åŠ›ï¼ˆHard Attentionï¼‰**ã€‚

**Soft Attention** è®¡ç®—æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ï¼ˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼Œç„¶ååŠ æƒæ±‚å’Œï¼š

$$
\mathbf{c}_i = \sum_j \alpha_{ij} \mathbf{h}_j = \mathbb{E}_{p(j | \mathbf{s}_i, \mathbf{H})}[\mathbf{h}_j]
$$

**Hard Attention** ä»æ³¨æ„åŠ›åˆ†å¸ƒä¸­**é‡‡æ ·**ä¸€ä¸ªä½ç½® $j^*$ï¼Œåªä½¿ç”¨é‚£ä¸ªä½ç½®çš„ä¿¡æ¯ï¼š

$$
j^* \sim \text{Categorical}(\alpha_{i1}, \alpha_{i2}, \ldots, \alpha_{iT_x})
$$
$$
\mathbf{c}_i = \mathbf{h}_{j^*}
$$

ä¸¤è€…çš„æ ¸å¿ƒåŒºåˆ«åœ¨äº**å¯å¾®åˆ†æ€§**ã€‚Soft Attentionæ˜¯å¯å¾®åˆ†çš„â€”â€”åŠ æƒæ±‚å’Œæ˜¯ä¸€ä¸ªè¿ç»­æ“ä½œï¼Œæ¢¯åº¦å¯ä»¥é€šè¿‡ $\alpha_{ij}$ æµå‘å¯¹é½å‡½æ•°çš„å‚æ•°ã€‚Hard Attentionä¸å¯å¾®åˆ†â€”â€”é‡‡æ ·æ“ä½œæ˜¯ç¦»æ•£çš„ï¼Œéœ€è¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚REINFORCEï¼‰æ¥è®­ç»ƒï¼Œå¸¦æ¥é«˜æ–¹å·®å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚

::: {.callout-warning}
## Hard Attentionçš„è®­ç»ƒå›°éš¾

Hard Attentionè™½ç„¶åœ¨æ¦‚å¿µä¸Šæ›´æ¥è¿‘äººç±»çš„"æ³¨æ„"ï¼ˆæˆ‘ä»¬çœŸçš„åªçœ‹ä¸€ä¸ªåœ°æ–¹ï¼Œè€Œä¸æ˜¯æ¨¡ç³Šåœ°çœ‹æ‰€æœ‰åœ°æ–¹ï¼‰ï¼Œä½†å®ƒçš„è®­ç»ƒéœ€è¦å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼š

$$
\nabla_\theta J = \mathbb{E}_{j^* \sim p(j|\theta)} \left[ \nabla_\theta \log p(j^* | \theta) \cdot R(j^*) \right]
$$

å…¶ä¸­ $R(j^*)$ æ˜¯é€‰æ‹©ä½ç½® $j^*$ å¸¦æ¥çš„"å¥–åŠ±"ã€‚è¿™ä¸ªæ¢¯åº¦ä¼°è®¡çš„æ–¹å·®å¾ˆå¤§ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ‰èƒ½ç¨³å®šã€‚

å®è·µä¸­ï¼Œ**Soft Attentionå‡ ä¹æ€»æ˜¯æ›´å¥½çš„é€‰æ‹©**ï¼Œå› ä¸ºï¼š

1. ç«¯åˆ°ç«¯å¯å¾®åˆ†ï¼Œè®­ç»ƒç®€å•
2. æ¢¯åº¦ä¼°è®¡æ²¡æœ‰æ–¹å·®é—®é¢˜
3. å¯ä»¥åŒæ—¶åˆ©ç”¨å¤šä¸ªä½ç½®çš„ä¿¡æ¯
:::

### å¤æ‚åº¦åˆ†æ

ä¸åŒæ³¨æ„åŠ›å˜ä½“çš„è®¡ç®—å¤æ‚åº¦æ±‡æ€»å¦‚ä¸‹ï¼š

| å˜ä½“ | å¯¹é½è®¡ç®— | æ€»å¤æ‚åº¦ |
|------|----------|----------|
| **Global + Dot** | $O(T_x \cdot d)$ per step | $O(T_x \cdot T_y \cdot d)$ |
| **Global + General** | $O(T_x \cdot d^2)$ per step | $O(T_x \cdot T_y \cdot d^2)$ |
| **Global + Concat** | $O(T_x \cdot d^2)$ per step | $O(T_x \cdot T_y \cdot d^2)$ |
| **Local** | $O(D \cdot d)$ per step | $O(D \cdot T_y \cdot d)$ |

å…¶ä¸­ $T_x$ æ˜¯æºåºåˆ—é•¿åº¦ï¼Œ$T_y$ æ˜¯ç›®æ ‡åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯éšè—ç»´åº¦ï¼Œ$D$ æ˜¯å±€éƒ¨çª—å£å¤§å°ã€‚

ä¸æ ‡å‡†Seq2Seqç›¸æ¯”ï¼ŒGlobal Attentionå¢åŠ äº† $O(T_x \cdot T_y)$ çš„è®¡ç®—é‡ï¼ŒåŒæ—¶éœ€è¦å­˜å‚¨æ‰€æœ‰ç¼–ç å™¨éšè—çŠ¶æ€ï¼ˆ$O(T_x \cdot d)$ ç©ºé—´ï¼‰ã€‚è¿™æ˜¯ç”¨ç©ºé—´æ¢å–æ€§èƒ½çš„å…¸å‹ä¾‹å­ã€‚Local Attentionçš„ä¼˜åŠ¿åœ¨é•¿åºåˆ—æ—¶å°¤ä¸ºæ˜æ˜¾ï¼šå½“ $T_x = 1000$ è€Œ $D = 50$ æ—¶ï¼Œè®¡ç®—é‡å‡å°‘äº†20å€ã€‚

---

## æ³¨æ„åŠ›å¯è§†åŒ–ï¼šæ¨¡å‹åœ¨"çœ‹"ä»€ä¹ˆï¼Ÿ

### å¯¹é½çŸ©é˜µ

Attentionæœºåˆ¶çš„ä¸€ä¸ªç¾å¦™ç‰¹æ€§æ˜¯**å¯è§£é‡Šæ€§**ã€‚æ³¨æ„åŠ›æƒé‡ $\alpha_{ij}$ ç›´æ¥å‘Šè¯‰æˆ‘ä»¬ï¼šåœ¨ç”Ÿæˆç¬¬ $i$ ä¸ªç›®æ ‡è¯æ—¶ï¼Œæ¨¡å‹å…³æ³¨äº†å“ªäº›æºè¯ã€‚

æˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰çš„æ³¨æ„åŠ›æƒé‡æ’åˆ—æˆä¸€ä¸ªçŸ©é˜µï¼Œæ¨ªè½´æ˜¯æºåºåˆ—ï¼Œçºµè½´æ˜¯ç›®æ ‡åºåˆ—ã€‚è¿™ä¸ªçŸ©é˜µè¢«ç§°ä¸º**å¯¹é½çŸ©é˜µï¼ˆalignment matrixï¼‰**ã€‚

![å¯¹é½å¯è§†åŒ–ï¼šå››ä¸ªè‹±æ³•ç¿»è¯‘ä¾‹å­çš„æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ã€‚æ¨ªè½´æ˜¯è‹±è¯­æºå¥å­ï¼Œçºµè½´æ˜¯æ³•è¯­ç›®æ ‡å¥å­ã€‚ç™½è‰²è¡¨ç¤ºé«˜æ³¨æ„åŠ›æƒé‡ï¼Œé»‘è‰²è¡¨ç¤ºä½æƒé‡ã€‚æ³¨æ„å¯¹è§’çº¿æ¨¡å¼ï¼ˆå•è¯ä¸€ä¸€å¯¹åº”ï¼‰å’Œåç¦»å¯¹è§’çº¿çš„åŒºåŸŸï¼ˆè¯­åºè°ƒæ•´ï¼‰ã€‚ä¾‹å¦‚(a)ä¸­"August"å¯¹åº”"aoÃ»t"ï¼Œ"European Economic Area"å¯¹åº”"zone Ã©conomique europÃ©enne"ã€‚](figures/chapter-6/original/fig3-alignment-visualization.png){#fig-alignment-visualization width=95%}

::: {.figure-caption}
*Source: Bahdanau, Cho, & Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", Figure 3. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*
:::

### å¯¹é½æ¨¡å¼çš„è¯­è¨€å­¦æ„ä¹‰

é€šè¿‡è§‚å¯Ÿå¯¹é½çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸€äº›æœ‰è¶£çš„è¯­è¨€å­¦æ¨¡å¼ï¼š

**å•è°ƒå¯¹é½**ï¼šå¯¹äºè¯­åºç›¸ä¼¼çš„è¯­è¨€å¯¹ï¼ˆå¦‚è‹±è¯­åˆ°å¾·è¯­çš„æŸäº›ç»“æ„ï¼‰ï¼Œå¯¹é½çŸ©é˜µæ¥è¿‘å¯¹è§’çº¿â€”â€”ç¬¬1ä¸ªæºè¯å¯¹åº”ç¬¬1ä¸ªç›®æ ‡è¯ï¼Œç¬¬2ä¸ªå¯¹åº”ç¬¬2ä¸ªï¼Œä¾æ­¤ç±»æ¨ã€‚

**è¯­åºè°ƒæ•´**ï¼šå½“æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„è¯åºä¸åŒæ—¶ï¼Œå¯¹é½çŸ©é˜µä¼šåç¦»å¯¹è§’çº¿ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„"red car"ç¿»è¯‘æˆæ³•è¯­æ˜¯"voiture rouge"ï¼ˆè½¦ çº¢ï¼‰ï¼Œå¯¹é½çŸ©é˜µä¼šæ˜¾ç¤ºäº¤å‰æ¨¡å¼ã€‚

**ä¸€å¯¹å¤šå’Œå¤šå¯¹ä¸€**ï¼šæŸäº›è¯æ²¡æœ‰ç›´æ¥å¯¹åº”ï¼Œæˆ–ä¸€ä¸ªè¯å¯¹åº”å¤šä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„"going to"å¯èƒ½å¯¹åº”æ³•è¯­çš„å•ä¸ªè¯"va"ã€‚

**ç©ºå¯¹é½**ï¼šæŸäº›ç›®æ ‡è¯ï¼ˆå¦‚å† è¯ï¼‰å¯èƒ½æ²¡æœ‰æ˜ç¡®çš„æºè¯å¯¹åº”ï¼Œå®ƒä»¬çš„æ³¨æ„åŠ›æƒé‡ä¼šåˆ†æ•£åœ¨å¤šä¸ªä½ç½®ã€‚

### å¯è§†åŒ–çš„å±€é™æ€§

è™½ç„¶æ³¨æ„åŠ›å¯è§†åŒ–å¾ˆå¸å¼•äººï¼Œä½†æˆ‘ä»¬è¦è°¨æ…è§£è¯»ï¼š

1. **æ³¨æ„åŠ›ä¸ç­‰äºè§£é‡Š**ï¼šé«˜æ³¨æ„åŠ›æƒé‡ä¸ä¸€å®šæ„å‘³ç€æ¨¡å‹"ç†è§£"äº†é‚£ä¸ªä½ç½®çš„å†…å®¹
2. **å¯èƒ½æœ‰å¤šé‡å› ç´ **ï¼šæ¨¡å‹å¯èƒ½é€šè¿‡å…¶ä»–æœºåˆ¶ï¼ˆå¦‚ä½ç½®ä¿¡æ¯ï¼‰åšå‡ºå†³å®š
3. **è®­ç»ƒç›®æ ‡çš„å½±å“**ï¼šæ³¨æ„åŠ›æƒé‡æ˜¯ä¸ºäº†æœ€å°åŒ–ç¿»è¯‘æŸå¤±è€Œå­¦ä¹ çš„ï¼Œä¸ä¸€å®šåæ˜ äººç±»çš„å¯¹é½ç›´è§‰

åæ¥çš„ç ”ç©¶ï¼ˆå¦‚Jain & Wallace, 2019ï¼‰å¯¹æ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æå‡ºäº†è´¨ç–‘ã€‚ä½†ä½œä¸ºä¸€ä¸ªè¯Šæ–­å·¥å…·ï¼Œæ³¨æ„åŠ›å¯è§†åŒ–ä»ç„¶éå¸¸æœ‰ä»·å€¼ã€‚

---

## å·¥ç¨‹å®è·µï¼šå¸¦Attentionçš„Seq2Seq

è®©æˆ‘ä»¬ç”¨PyTorchå®ç°ä¸€ä¸ªå®Œæ•´çš„å¸¦Attentionçš„Seq2Seqæ¨¡å‹ï¼Œæ¶µç›–Bahdanauå’ŒLuongä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶ã€‚

### ç¼–ç å™¨

```{python}
#| code-fold: false
import torch
import torch.nn as nn
import torch.nn.functional as F

class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.rnn = nn.GRU(
            embed_dim, hidden_dim,
            num_layers=num_layers,
            bidirectional=True,  # åŒå‘
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        # å°†åŒå‘çš„éšè—çŠ¶æ€å‹ç¼©åˆ°å•å‘ç»´åº¦
        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_lengths=None):
        # src: [batch_size, src_len]
        embedded = self.dropout(self.embedding(src))  # [batch, src_len, embed_dim]

        if src_lengths is not None:
            packed = nn.utils.rnn.pack_padded_sequence(
                embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            packed_outputs, hidden = self.rnn(packed)
            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)
        else:
            outputs, hidden = self.rnn(embedded)

        # outputs: [batch, src_len, hidden_dim * 2] (åŒå‘æ‹¼æ¥)
        # hidden: [num_layers * 2, batch, hidden_dim]

        # åˆå¹¶å‰å‘å’Œåå‘çš„æœ€ç»ˆéšè—çŠ¶æ€
        # hidden[-2] æ˜¯æœ€åä¸€å±‚å‰å‘ï¼Œhidden[-1] æ˜¯æœ€åä¸€å±‚åå‘
        hidden = torch.tanh(self.fc(torch.cat([hidden[-2], hidden[-1]], dim=1)))
        # hidden: [batch, hidden_dim]

        return outputs, hidden
```

### Bahdanau Attentionå±‚

```{python}
#| code-fold: false
class BahdanauAttention(nn.Module):
    def __init__(self, enc_hidden_dim, dec_hidden_dim, attention_dim):
        super().__init__()
        # åŠ æ€§æ³¨æ„åŠ›çš„å‚æ•°
        self.W_a = nn.Linear(dec_hidden_dim, attention_dim, bias=False)
        self.U_a = nn.Linear(enc_hidden_dim * 2, attention_dim, bias=False)  # åŒå‘ç¼–ç å™¨
        self.v_a = nn.Linear(attention_dim, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs, mask=None):
        """
        decoder_hidden: [batch, dec_hidden]
        encoder_outputs: [batch, src_len, enc_hidden * 2]
        mask: [batch, src_len], Trueè¡¨ç¤ºéœ€è¦maskçš„ä½ç½®ï¼ˆpaddingï¼‰
        """
        batch_size, src_len, _ = encoder_outputs.shape

        # decoder_hidden æ‰©å±•åˆ°æ‰€æœ‰æºä½ç½®
        # [batch, dec_hidden] -> [batch, src_len, dec_hidden]
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)

        # è®¡ç®—å¯¹é½åˆ†æ•°
        # [batch, src_len, attention_dim]
        energy = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))
        # [batch, src_len, 1] -> [batch, src_len]
        attention_scores = self.v_a(energy).squeeze(-1)

        # åº”ç”¨maskï¼ˆå°†paddingä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå°çš„è´Ÿæ•°ï¼‰
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask, -1e10)

        # Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(attention_scores, dim=1)  # [batch, src_len]

        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        # [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -> [batch, 1, enc_hidden*2]
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)  # [batch, enc_hidden * 2]

        return context, attention_weights
```

### è§£ç å™¨

```{python}
#| code-fold: false
class AttentionDecoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,
                 attention_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)

        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # GRUè¾“å…¥æ˜¯ï¼šembedded + context
        self.rnn = nn.GRU(
            embed_dim + enc_hidden_dim * 2,  # åŒå‘ç¼–ç å™¨
            dec_hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(dec_hidden_dim + enc_hidden_dim * 2 + embed_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_token, hidden, encoder_outputs, mask=None):
        """
        å•æ­¥è§£ç 
        input_token: [batch] - ä¸Šä¸€æ­¥çš„è¾“å‡ºtoken
        hidden: [1, batch, dec_hidden] - ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€
        encoder_outputs: [batch, src_len, enc_hidden * 2]
        """
        # Embedding
        embedded = self.dropout(self.embedding(input_token))  # [batch, embed_dim]

        # Attention
        # hidden[-1] å–æœ€åä¸€å±‚ï¼Œ[batch, dec_hidden]
        context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)

        # æ‹¼æ¥embeddedå’Œcontextä½œä¸ºRNNè¾“å…¥
        rnn_input = torch.cat([embedded, context], dim=1).unsqueeze(1)  # [batch, 1, embed+ctx]

        # RNN
        output, hidden = self.rnn(rnn_input, hidden)
        output = output.squeeze(1)  # [batch, dec_hidden]

        # è¾“å‡ºå±‚
        prediction = self.fc(torch.cat([output, context, embedded], dim=1))

        return prediction, hidden, attention_weights
```

### å®Œæ•´çš„Seq2Seqæ¨¡å‹

```{python}
#| code-fold: false
class Seq2SeqAttention(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        src: [batch, src_len]
        trg: [batch, trg_len]
        """
        batch_size = src.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.vocab_size

        # å­˜å‚¨è¾“å‡º
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        attentions = []

        # ç¼–ç 
        encoder_outputs, hidden = self.encoder(src)
        # hidden: [batch, dec_hidden] -> [1, batch, dec_hidden]
        hidden = hidden.unsqueeze(0)

        # ç¬¬ä¸€ä¸ªè§£ç è¾“å…¥æ˜¯ <sos> token
        input_token = trg[:, 0]

        for t in range(1, trg_len):
            prediction, hidden, attention = self.decoder(
                input_token, hidden, encoder_outputs
            )
            outputs[:, t] = prediction
            attentions.append(attention)

            # Teacher forcing
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = prediction.argmax(1)
            input_token = trg[:, t] if teacher_force else top1

        return outputs, torch.stack(attentions, dim=1)

# åˆ›å»ºæ¨¡å‹ç¤ºä¾‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

encoder = Encoder(vocab_size=10000, embed_dim=256, hidden_dim=512)
decoder = AttentionDecoder(
    vocab_size=10000, embed_dim=256,
    enc_hidden_dim=512, dec_hidden_dim=512, attention_dim=256
)
model = Seq2SeqAttention(encoder, decoder, device).to(device)

print(f"ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in encoder.parameters()):,}")
print(f"è§£ç å™¨å‚æ•°: {sum(p.numel() for p in decoder.parameters()):,}")
print(f"æ€»å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
```

### Luong Attentionå®ç°

```{python}
#| code-fold: false
class LuongAttention(nn.Module):
    """
    Luong æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒä¸‰ç§å¯¹é½æ–¹å¼
    """
    def __init__(self, enc_hidden_dim, dec_hidden_dim, method='dot'):
        super().__init__()
        self.method = method
        self.enc_hidden_dim = enc_hidden_dim
        self.dec_hidden_dim = dec_hidden_dim

        if method == 'general':
            self.W_a = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)
        elif method == 'concat':
            self.W_a = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim, bias=False)
            self.v_a = nn.Linear(dec_hidden_dim, 1, bias=False)

    def forward(self, decoder_state, encoder_outputs, mask=None):
        """
        decoder_state: [batch, dec_hidden]
        encoder_outputs: [batch, src_len, enc_hidden]
        mask: [batch, src_len], Trueè¡¨ç¤ºpaddingä½ç½®
        """
        batch_size, src_len, _ = encoder_outputs.shape

        if self.method == 'dot':
            # ç‚¹ç§¯: s^T h
            # éœ€è¦ dec_hidden == enc_hidden
            scores = torch.bmm(
                decoder_state.unsqueeze(1),  # [batch, 1, dec_hidden]
                encoder_outputs.transpose(1, 2)  # [batch, enc_hidden, src_len]
            ).squeeze(1)  # [batch, src_len]

        elif self.method == 'general':
            # ä¸€èˆ¬å½¢å¼: s^T W h
            # W å°† enc_hidden æ˜ å°„åˆ° dec_hidden
            transformed = self.W_a(encoder_outputs)  # [batch, src_len, dec_hidden]
            scores = torch.bmm(
                decoder_state.unsqueeze(1),
                transformed.transpose(1, 2)
            ).squeeze(1)

        elif self.method == 'concat':
            # æ‹¼æ¥å½¢å¼: v^T tanh(W [s; h])
            decoder_expanded = decoder_state.unsqueeze(1).expand(-1, src_len, -1)
            concat = torch.cat([decoder_expanded, encoder_outputs], dim=-1)
            energy = torch.tanh(self.W_a(concat))  # [batch, src_len, dec_hidden]
            scores = self.v_a(energy).squeeze(-1)  # [batch, src_len]

        # åº”ç”¨ mask
        if mask is not None:
            scores = scores.masked_fill(mask, -1e10)

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # ä¸Šä¸‹æ–‡å‘é‡
        context = torch.bmm(
            attention_weights.unsqueeze(1),
            encoder_outputs
        ).squeeze(1)

        return context, attention_weights
```

### Local Attentionå®ç°

```{python}
#| code-fold: false
class LocalAttention(nn.Module):
    """
    Luong çš„ Local Attentionï¼ˆé¢„æµ‹å‹ï¼‰
    """
    def __init__(self, enc_hidden_dim, dec_hidden_dim, window_size=10):
        super().__init__()
        self.window_size = window_size  # D: çª—å£åŠå¾„
        self.enc_hidden_dim = enc_hidden_dim

        # ä½ç½®é¢„æµ‹ç½‘ç»œ
        self.W_p = nn.Linear(dec_hidden_dim, dec_hidden_dim)
        self.v_p = nn.Linear(dec_hidden_dim, 1)

        # å¯¹é½å‡½æ•°ï¼ˆä½¿ç”¨ generalï¼‰
        self.W_a = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)

        # é«˜æ–¯æ ‡å‡†å·®
        self.sigma = window_size / 2

    def forward(self, decoder_state, encoder_outputs, mask=None):
        """
        decoder_state: [batch, dec_hidden]
        encoder_outputs: [batch, src_len, enc_hidden]
        """
        batch_size, src_len, _ = encoder_outputs.shape
        device = decoder_state.device

        # Step 1: é¢„æµ‹å¯¹é½ä½ç½® p
        # p = S * sigmoid(v^T tanh(W_p s))
        p = src_len * torch.sigmoid(
            self.v_p(torch.tanh(self.W_p(decoder_state)))
        ).squeeze(-1)  # [batch]

        # Step 2: è®¡ç®—æ‰€æœ‰ä½ç½®çš„å¯¹é½åˆ†æ•°
        transformed = self.W_a(encoder_outputs)  # [batch, src_len, dec_hidden]
        scores = torch.bmm(
            decoder_state.unsqueeze(1),
            transformed.transpose(1, 2)
        ).squeeze(1)  # [batch, src_len]

        # Step 3: åº”ç”¨é«˜æ–¯çª—å£
        # ç”Ÿæˆä½ç½®ç´¢å¼• [0, 1, 2, ..., src_len-1]
        positions = torch.arange(src_len, device=device).float()
        positions = positions.unsqueeze(0).expand(batch_size, -1)  # [batch, src_len]

        # é«˜æ–¯æƒé‡: exp(-(j - p)^2 / (2 * sigma^2))
        gaussian = torch.exp(-((positions - p.unsqueeze(1)) ** 2) / (2 * self.sigma ** 2))

        # Step 4: çª—å£maskï¼ˆåªä¿ç•™ [p-D, p+D] èŒƒå›´å†…çš„ä½ç½®ï¼‰
        window_mask = (positions >= (p.unsqueeze(1) - self.window_size)) & \
                      (positions <= (p.unsqueeze(1) + self.window_size))

        # åº”ç”¨çª—å£mask
        scores = scores.masked_fill(~window_mask, -1e10)

        # Step 5: Softmax + é«˜æ–¯åŠ æƒ
        attention_weights = F.softmax(scores, dim=-1) * gaussian

        # é‡æ–°å½’ä¸€åŒ–
        attention_weights = attention_weights / (attention_weights.sum(dim=-1, keepdim=True) + 1e-10)

        # ä¸Šä¸‹æ–‡å‘é‡
        context = torch.bmm(
            attention_weights.unsqueeze(1),
            encoder_outputs
        ).squeeze(1)

        return context, attention_weights, p
```

### å¯¹æ¯”å®éªŒ

```{python}
#| code-fold: false
# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
src_len = 10
enc_hidden = 64
dec_hidden = 64

encoder_outputs = torch.randn(batch_size, src_len, enc_hidden)
decoder_state = torch.randn(batch_size, dec_hidden)

# æµ‹è¯•ä¸‰ç§ Luong Attention
for method in ['dot', 'general', 'concat']:
    attn = LuongAttention(enc_hidden, dec_hidden, method=method)
    context, weights = attn(decoder_state, encoder_outputs)
    print(f"{method:8s}: context shape = {context.shape}, weights sum = {weights.sum(dim=-1)}")

# æµ‹è¯• Local Attention
local_attn = LocalAttention(enc_hidden, dec_hidden, window_size=3)
context, weights, p = local_attn(decoder_state, encoder_outputs)
print(f"{'local':8s}: context shape = {context.shape}, predicted p = {p.tolist()}")
```

### å…³é”®å®ç°ç»†èŠ‚

**Maskå¤„ç†**ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œbatchä¸­çš„åºåˆ—é•¿åº¦ä¸åŒï¼Œéœ€è¦paddingã€‚è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œpaddingä½ç½®ä¸åº”è¯¥è·å¾—ä»»ä½•æƒé‡ã€‚æˆ‘ä»¬é€šè¿‡maskå°†è¿™äº›ä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå¤§çš„è´Ÿæ•°ï¼Œsoftmaxåå®ƒä»¬çš„æƒé‡è¶‹è¿‘äº0ã€‚

**Teacher Forcing**ï¼šè®­ç»ƒæ—¶ï¼Œè§£ç å™¨çš„è¾“å…¥å¯ä»¥æ˜¯çœŸå®çš„ä¸Šä¸€ä¸ªè¯ï¼ˆteacher forcingï¼‰æˆ–æ¨¡å‹é¢„æµ‹çš„è¯ã€‚`teacher_forcing_ratio` æ§åˆ¶ä¸¤è€…çš„æ··åˆæ¯”ä¾‹ã€‚è¾ƒé«˜çš„æ¯”ä¾‹åŠ é€Ÿè®­ç»ƒï¼Œä½†å¯èƒ½å¯¼è‡´exposure biasã€‚

**åŒå‘ç¼–ç å™¨**ï¼šæˆ‘ä»¬ä½¿ç”¨åŒå‘GRUï¼Œç¼–ç å™¨è¾“å‡ºçš„ç»´åº¦æ˜¯ `hidden_dim * 2`ã€‚è¿™è®©æ¯ä¸ªä½ç½®éƒ½åŒ…å«å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

---

## æ·±å…¥ç†è§£

### ä¸ºä»€ä¹ˆAttentionæœ‰æ•ˆï¼Ÿâ€”â€”ç†è®ºè§†è§’

**ä¿¡æ¯è®ºè§†è§’**ï¼šæ ‡å‡†Seq2Seqçš„ä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ æ˜¯è¾“å…¥ $\mathbf{x}$ çš„ä¸€ä¸ª**å……åˆ†ç»Ÿè®¡é‡ï¼ˆsufficient statisticï¼‰**â€”â€”å¦‚æœ $\mathbf{c}$ å®Œç¾ï¼Œå®ƒåº”è¯¥åŒ…å«å…³äº $\mathbf{y}$ çš„æ‰€æœ‰å¿…è¦ä¿¡æ¯ã€‚ä½†åœ¨å®è·µä¸­ï¼Œæœ‰é™ç»´åº¦çš„ $\mathbf{c}$ æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚Attentioné€šè¿‡è®©è§£ç å™¨è®¿é—®æ‰€æœ‰çš„ $\mathbf{h}_j$ï¼Œå®é™…ä¸Šæ˜¯åœ¨è¯´ï¼š**ä¸è¦æ±‚ä¸€ä¸ªå……åˆ†ç»Ÿè®¡é‡ï¼Œè€Œæ˜¯è®©æ¨¡å‹åœ¨éœ€è¦æ—¶ç›´æ¥æŸ¥è¯¢åŸå§‹ä¿¡æ¯**ã€‚è¿™ç»•è¿‡äº†ä¿¡æ¯ç“¶é¢ˆã€‚

**è®°å¿†å¯»å€è§†è§’**ï¼šå¯ä»¥æŠŠç¼–ç å™¨çš„éšè—çŠ¶æ€çœ‹ä½œä¸€ä¸ª**å¤–éƒ¨è®°å¿†ï¼ˆexternal memoryï¼‰**ï¼Œæ¯ä¸ª $\mathbf{h}_j$ æ˜¯ä¸€ä¸ªè®°å¿†æ§½ã€‚Attentionæœºåˆ¶å®ç°äº†**åŸºäºå†…å®¹çš„è½¯å¯»å€ï¼ˆcontent-based soft addressingï¼‰**â€”â€”æ ¹æ®å½“å‰æŸ¥è¯¢ï¼ˆè§£ç å™¨çŠ¶æ€ï¼‰æ£€ç´¢ç›¸å…³çš„è®°å¿†ã€‚è¿™ä¸ªè§†è§’åæ¥è¢«æ˜¾å¼åŒ–ä¸ºMemory Networkså’ŒNeural Turing Machineã€‚

**æ¢¯åº¦æµè§†è§’**ï¼šä»ä¼˜åŒ–è§’åº¦ï¼ŒAttentionæä¾›äº†ä¸€æ¡ä»è§£ç å™¨åˆ°ç¼–ç å™¨ç‰¹å®šä½ç½®çš„**ç›´æ¥è·¯å¾„**ã€‚åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œæ¢¯åº¦è¦ä»è§£ç å™¨æµå›ç¼–ç å™¨ï¼Œå¿…é¡»ç»è¿‡ $\mathbf{c}$ï¼Œå†ç»è¿‡æ•´ä¸ªç¼–ç è¿‡ç¨‹ã€‚Attentionåˆ›é€ äº†"æ·å¾„"â€”â€”æ¢¯åº¦å¯ä»¥é€šè¿‡æ³¨æ„åŠ›æƒé‡ç›´æ¥ä¼ åˆ°ç›¸å…³çš„ç¼–ç å™¨ä½ç½®ã€‚

### ä¸ºä»€ä¹ˆç‚¹ç§¯æ³¨æ„åŠ›èƒ½å·¥ä½œï¼Ÿ

ç‚¹ç§¯æ³¨æ„åŠ›çš„æœ‰æ•ˆæ€§å¯ä»¥ä»å¤šä¸ªè§’åº¦ç†è§£ã€‚

**ä½™å¼¦ç›¸ä¼¼åº¦è§†è§’**ï¼šå½“å‘é‡è¢«å½’ä¸€åŒ–åï¼Œç‚¹ç§¯å°±æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼š

$$
\mathbf{s}^\top \mathbf{h} = \|\mathbf{s}\| \|\mathbf{h}\| \cos(\theta)
$$

ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯è¡¡é‡ä¸¤ä¸ªå‘é‡"æ–¹å‘ä¸€è‡´æ€§"çš„ç»å…¸æŒ‡æ ‡ã€‚ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šå­¦ä¹ è®©ç›¸å…³çš„çŠ¶æ€æŒ‡å‘ç›¸ä¼¼çš„æ–¹å‘ã€‚

**æ ¸æ–¹æ³•è§†è§’**ï¼šç‚¹ç§¯å¯ä»¥çœ‹ä½œä¸€ä¸ªçº¿æ€§æ ¸ï¼ˆlinear kernelï¼‰ã€‚åœ¨æ ¸æ–¹æ³•çš„æ¡†æ¶ä¸‹ï¼Œæ³¨æ„åŠ›æƒé‡å®é™…ä¸Šæ˜¯åœ¨ä¸€ä¸ªç‰¹å¾ç©ºé—´ä¸­è®¡ç®—ç›¸ä¼¼åº¦ã€‚General Attentionå¼•å…¥çš„å¯å­¦ä¹ çŸ©é˜µ $\mathbf{W}_a$ ç›¸å½“äºå­¦ä¹ ä¸€ä¸ªMahalanobisè·ç¦»ã€‚

**ä¿¡æ¯æ£€ç´¢è§†è§’**ï¼šç‚¹ç§¯æ³¨æ„åŠ›å¯ä»¥ç±»æ¯”ä¸ºå‘é‡ç©ºé—´æ¨¡å‹ä¸­çš„æŸ¥è¯¢-æ–‡æ¡£åŒ¹é…ã€‚è§£ç å™¨çŠ¶æ€æ˜¯"æŸ¥è¯¢"ï¼Œç¼–ç å™¨çŠ¶æ€æ˜¯"æ–‡æ¡£"ï¼Œç‚¹ç§¯è¡¡é‡æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸å…³æ€§ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾ï¼Ÿ

Luongçš„è®ºæ–‡æ²¡æœ‰è®¨è®ºè¿™ä¸ªé—®é¢˜ï¼Œä½†åæ¥çš„Transformerè®ºæ–‡ï¼ˆVaswani et al., 2017ï¼‰æŒ‡å‡ºäº†ç‚¹ç§¯æ³¨æ„åŠ›çš„ä¸€ä¸ªæ½œåœ¨é—®é¢˜ã€‚

å½“å‘é‡ç»´åº¦ $d$ å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯çš„æ–¹å·®ä¼šå¾ˆå¤§ã€‚å‡è®¾ $\mathbf{s}$ å’Œ $\mathbf{h}$ çš„æ¯ä¸ªåˆ†é‡éƒ½æ˜¯ç‹¬ç«‹çš„ã€å‡å€¼ä¸º0ã€æ–¹å·®ä¸º1çš„éšæœºå˜é‡ï¼Œé‚£ä¹ˆï¼š

$$
\text{Var}(\mathbf{s}^\top \mathbf{h}) = d
$$

å½“ $d = 512$ æ—¶ï¼Œç‚¹ç§¯çš„æ ‡å‡†å·®æ˜¯ $\sqrt{512} \approx 22.6$ã€‚è¿™æ„å‘³ç€ç‚¹ç§¯å¯èƒ½äº§ç”Ÿå¾ˆå¤§çš„æ­£å€¼æˆ–è´Ÿå€¼ï¼Œå¯¼è‡´softmaxè¾“å‡ºæ¥è¿‘one-hotåˆ†å¸ƒï¼Œæ¢¯åº¦å˜å¾—å¾ˆå°ã€‚

è§£å†³æ–¹æ¡ˆæ˜¯**ç¼©æ”¾**ï¼š

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \frac{\mathbf{s}^\top \mathbf{h}}{\sqrt{d}}
$$

è¿™å°±æ˜¯Transformerä¸­çš„**Scaled Dot-Product Attention**ã€‚Luongçš„è®ºæ–‡ä½¿ç”¨çš„ç»´åº¦è¾ƒå°ï¼ˆ500å·¦å³ï¼‰ï¼Œé—®é¢˜ä¸å¤ªæ˜æ˜¾ï¼›ä½†åœ¨Transformerçš„å¤§ç»´åº¦è®¾ç½®ä¸‹ï¼Œç¼©æ”¾å˜å¾—å¿…è¦ã€‚è¿™ä¸ªè®¾è®¡é€‰æ‹©å°†åœ¨ç¬¬8ç« Transformerä¸­å‘æŒ¥æ ¸å¿ƒä½œç”¨ã€‚

### è¾¹ç•Œæ¡ä»¶ä¸å¤±æ•ˆæ¨¡å¼

**å•è°ƒå¯¹é½å‡è®¾**ï¼šBahdanau Attentionéšå«å‡è®¾æºå’Œç›®æ ‡ä¹‹é—´å­˜åœ¨æŸç§å¯¹é½å…³ç³»ã€‚å¯¹äºç¿»è¯‘ä»»åŠ¡è¿™é€šå¸¸æˆç«‹ï¼Œä½†å¯¹äºæŸäº›ä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ï¼‰ï¼Œè¿™ä¸ªå‡è®¾å¯èƒ½ä¸æˆç«‹â€”â€”æ‘˜è¦éœ€è¦æ•´åˆåˆ†æ•£åœ¨å„å¤„çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯"å¯¹é½"åˆ°ç‰¹å®šä½ç½®ã€‚

**å¤æ‚åº¦é™åˆ¶**ï¼šå½“æºåºåˆ—å¾ˆé•¿æ—¶ï¼ˆå¦‚æ–‡æ¡£çº§ç¿»è¯‘ï¼‰ï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å˜å¾—æ˜‚è´µã€‚$O(T_x \cdot T_y)$ çš„å¤æ‚åº¦åœ¨ $T_x = 10000$ æ—¶æ˜¯ä¸å¯æ¥å—çš„ã€‚Local Attentionæ˜¯ä¸€ç§ç¼“è§£æ–¹æ¡ˆï¼Œä½†çª—å£å¤§å°çš„é€‰æ‹©éœ€è¦æ ¹æ®ä»»åŠ¡è°ƒæ•´ã€‚

**åˆ†å¸ƒåç§»**ï¼šè®­ç»ƒæ—¶ï¼Œè§£ç å™¨çœ‹åˆ°çš„ä¸Šä¸‹æ–‡å‘é‡åˆ†å¸ƒä¸æ¨ç†æ—¶å¯èƒ½ä¸åŒï¼ˆå› ä¸ºteacher forcingï¼‰ã€‚è¿™å¯èƒ½å¯¼è‡´æ³¨æ„åŠ›æƒé‡åœ¨æ¨ç†æ—¶ä¸å¤Ÿå‡†ç¡®ã€‚

**Dot-Productçš„ç»´åº¦é™åˆ¶**ï¼šè§£ç å™¨å’Œç¼–ç å™¨çš„éšè—ç»´åº¦å¿…é¡»ç›¸åŒï¼Œå¦åˆ™æ— æ³•è®¡ç®—ç‚¹ç§¯ã€‚General Attentioné€šè¿‡å¼•å…¥ $\mathbf{W}_a$ è§£é™¤äº†è¿™ä¸ªé™åˆ¶ï¼Œä½†å¢åŠ äº†å‚æ•°é‡ã€‚

**Local Attentionçš„ä½ç½®é¢„æµ‹é”™è¯¯**ï¼šå¦‚æœä½ç½®é¢„æµ‹å‡½æ•°å­¦ä¹ ä¸å¥½ï¼Œä¼šç³»ç»Ÿæ€§åœ°é”™è¿‡é‡è¦ä¿¡æ¯ã€‚å¯¹äºè¯­åºå·®å¼‚å¤§çš„è¯­è¨€å¯¹ï¼Œå±€éƒ¨çª—å£å¯èƒ½è¦†ç›–ä¸åˆ°æ­£ç¡®ä½ç½®ã€‚

---

## å±€é™æ€§ä¸å±•æœ›

ç»è¿‡Bahdanauï¼ˆ2014ï¼‰å’ŒLuongï¼ˆ2015ï¼‰çš„å·¥ä½œï¼Œæ³¨æ„åŠ›æœºåˆ¶å·²ç»åœ¨æœºå™¨ç¿»è¯‘ä¸­ç¡®ç«‹äº†æ ¸å¿ƒåœ°ä½ã€‚ä½†ä¸¤ä¸ªæ ¹æœ¬æ€§çš„å±€é™é€æ¸æµ®ç°ï¼ŒæŒ‡å‘æ›´æ·±å±‚çš„é—®é¢˜ã€‚

**æ³¨æ„åŠ›ä»ç„¶æ˜¯RNNçš„"é™„å±å“"**ã€‚æ— è®ºæ˜¯Bahdanauè¿˜æ˜¯Luongçš„æ³¨æ„åŠ›ï¼Œéƒ½æ˜¯Seq2Seqæ¶æ„çš„å¢å¼ºç»„ä»¶ã€‚ç¼–ç å’Œè§£ç çš„æ ¸å¿ƒä»ç„¶ä¾èµ–RNNã€‚è¿™æ„å‘³ç€é¡ºåºè®¡ç®—æ— æ³•é¿å…â€”â€”RNNå¿…é¡»é€æ­¥å¤„ç†åºåˆ—ï¼Œæ— æ³•å¹¶è¡Œã€‚è™½ç„¶Attentionç¼“è§£äº†é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼Œä½†RNNæœ¬èº«çš„æ¢¯åº¦æµå›°éš¾å¹¶æ²¡æœ‰æ ¹æœ¬è§£å†³ã€‚

**æ³¨æ„åŠ›åªåœ¨ç¼–ç å™¨-è§£ç å™¨ä¹‹é—´**ã€‚å½“å‰çš„Attentionåªè®©è§£ç å™¨å…³æ³¨ç¼–ç å™¨ã€‚ä½†ä¸€ä¸ªæ›´è‡ªç„¶çš„é—®é¢˜æ˜¯ï¼š**ç¼–ç å™¨å†…éƒ¨çš„å„ä¸ªä½ç½®èƒ½å¦ç›¸äº’å…³æ³¨ï¼Ÿ** ä¸€ä¸ªè¯çš„ç†è§£å¯èƒ½ä¾èµ–äºåŒä¸€å¥è¯ä¸­çš„å…¶ä»–è¯ï¼ˆä¾‹å¦‚ä»£è¯çš„æŒ‡ä»£æ¶ˆè§£ï¼‰ï¼Œè€Œå½“å‰çš„æ¶æ„æ²¡æœ‰æä¾›è¿™ç§"è‡ªæˆ‘å…³æ³¨"çš„æœºåˆ¶ã€‚

**ä½ç½®ä¿¡æ¯æ˜¯éšå¼çš„**ã€‚Attentionæœ¬èº«ä¸åŒ…å«ä½ç½®ä¿¡æ¯ã€‚ä½ç½®ä¿¡æ¯å®Œå…¨ä¾èµ–RNNçš„é¡ºåºå¤„ç†æ¥éšå¼ç¼–ç ã€‚å¦‚æœæŠ›å¼ƒRNNï¼Œä½ç½®ä¿¡æ¯å°†å®Œå…¨ä¸¢å¤±â€”â€”è¿™æš—ç¤ºæœªæ¥çš„æ¶æ„éœ€è¦æŸç§æ˜¾å¼çš„ä½ç½®ç¼–ç æ–¹æ¡ˆã€‚

è¿™äº›å±€é™å¼•å‘äº†ä¸¤ä¸ªæ·±åˆ»çš„é—®é¢˜ã€‚ç¬¬ä¸€ï¼š**èƒ½å¦è®©Attentionç‹¬ç«‹äºRNNï¼Ÿ** å¦‚æœAttentionå¦‚æ­¤æœ‰æ•ˆï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦RNNä½œä¸ºè½½ä½“ï¼Ÿç¬¬äºŒï¼š**èƒ½å¦è®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ç›¸äº’å…³æ³¨ï¼Ÿ** å¦‚æœæ³¨æ„åŠ›ä¸å±€é™äºç¼–ç å™¨-è§£ç å™¨ä¹‹é—´ï¼Œè€Œæ˜¯æ‰©å±•åˆ°åŒä¸€åºåˆ—å†…â€”â€”è¿™å°±æ˜¯**Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ã€‚

ä¸‹ä¸€ç« å°†æ­æ™“ç­”æ¡ˆã€‚Self-Attentionè®©æ¯ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥ä¸å…¶ä»–æ‰€æœ‰ä½ç½®äº¤äº’ï¼Œä¸å†éœ€è¦RNNä½œä¸ºä¸­ä»‹ã€‚è€Œåœ¨æ­¤ä¹‹åçš„Transformeræ¶æ„ï¼ˆç¬¬8ç« ï¼‰å°†å½»åº•å…‘ç°"Attention Is All You Need"çš„æ‰¿è¯ºâ€”â€”å®Œå…¨æŠ›å¼ƒå¾ªç¯ç»“æ„ï¼Œç”¨çº¯ç²¹çš„æ³¨æ„åŠ›æœºåˆ¶æ„å»ºæ•´ä¸ªæ¨¡å‹ã€‚

> ä»åŠ æ€§åˆ°ä¹˜æ€§ï¼Œä»å…¨å±€åˆ°å±€éƒ¨ï¼Œä»RNNçš„é™„å±åˆ°ç‹¬ç«‹çš„æ ¸å¿ƒâ€”â€”æ³¨æ„åŠ›æœºåˆ¶èµ°è¿‡äº†ä»"è¾…åŠ©å·¥å…·"åˆ°"æ¶æ„åŸºçŸ³"çš„å®Œæ•´å†ç¨‹ã€‚Bahdanauæ‰“ç ´äº†ä¿¡æ¯ç“¶é¢ˆï¼ŒLuongæ‹“å±•äº†è®¾è®¡ç©ºé—´ï¼Œè€ŒçœŸæ­£çš„é©å‘½è¿˜åœ¨åé¢ã€‚

---

## æœ¬ç« å°ç»“

::: {.callout-important}
## æ ¸å¿ƒè¦ç‚¹

- **é—®é¢˜**ï¼šSeq2Seqçš„ä¿¡æ¯ç“¶é¢ˆâ€”â€”æ‰€æœ‰è¾“å…¥ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå‘é‡ï¼Œå¯¼è‡´é•¿åºåˆ—ä¿¡æ¯ä¸¢å¤±
- **æ´å¯Ÿ**ï¼šè§£ç å™¨åº”è¯¥èƒ½å¤ŸåŠ¨æ€åœ°ã€æœ‰é€‰æ‹©åœ°å…³æ³¨è¾“å…¥çš„ä¸åŒä½ç½®ï¼›è¿›ä¸€æ­¥åœ°ï¼Œæ³¨æ„åŠ›çš„è®¡ç®—æ–¹å¼å¯ä»¥å¤šæ ·åŒ–
- **Bahdanau (2014)**ï¼šåŠ æ€§æ³¨æ„åŠ›æ‰“ç ´äº†å›ºå®šå‘é‡çš„é™åˆ¶ï¼Œå¤§å¹…æå‡äº†é•¿åºåˆ—ç¿»è¯‘è´¨é‡
- **Luong (2015)**ï¼šç³»ç»Ÿæ¯”è¾ƒäº†dot/general/concatä¸‰ç§å¯¹é½å‡½æ•°å’Œglobal/localä¸¤ç§èŒƒå›´ç­–ç•¥ï¼Œç¡®ç«‹äº†ç‚¹ç§¯æ³¨æ„åŠ›çš„æ•ˆç‡ä¼˜åŠ¿
- **æ„ä¹‰**ï¼šæ³¨æ„åŠ›ä»Seq2Seqçš„"è¡¥ä¸"å‘å±•ä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œä¸ºåç»­çš„Self-Attentionå’ŒTransformerå¥ å®šäº†åŸºç¡€
:::

### å…³é”®å…¬å¼é€ŸæŸ¥

**BahdanauåŠ æ€§æ³¨æ„åŠ›**ï¼š

$$
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
$$

**Dot-Product Attention**ï¼š

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{h}
$$

**General Attention**ï¼š

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{W}_a \mathbf{h}
$$

**æ³¨æ„åŠ›æƒé‡**ï¼š

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$

**ä¸Šä¸‹æ–‡å‘é‡**ï¼š

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

**Local Attentionä½ç½®é¢„æµ‹**ï¼š

$$
p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))
$$

---

## æ€è€ƒé¢˜

1. **[æ¦‚å¿µç†è§£]** ä¸ºä»€ä¹ˆè¯´Attentionå®ç°äº†"è½¯å¯»å€"ï¼Ÿå®ƒä¸è®¡ç®—æœºå†…å­˜çš„ç¡¬å¯»å€æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿè¿™ç§è½¯å¯»å€çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

2. **[æ•°å­¦æ¨å¯¼]** ç‚¹ç§¯æ³¨æ„åŠ›å’ŒåŠ æ€§æ³¨æ„åŠ›åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šæœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿè®¾è®¡ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œå±•ç¤ºåŠ æ€§æ³¨æ„åŠ›èƒ½å­¦ä¹ è€Œç‚¹ç§¯æ³¨æ„åŠ›æ— æ³•å­¦ä¹ çš„ç›¸å…³æ€§æ¨¡å¼ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œå¦‚æœGeneral Attentionä¸­ $\mathbf{W}_a = \mathbf{U}\mathbf{V}^\top$ï¼ˆç§©-$r$åˆ†è§£ï¼‰ï¼Œè¿™å¯¹æ³¨æ„åŠ›æ¨¡å¼æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

3. **[å·¥ç¨‹å®è·µ]** åœ¨å®ç°Attentionæ—¶ï¼Œä¸ºä»€ä¹ˆè¦å¯¹paddingä½ç½®åº”ç”¨maskï¼Ÿå¦‚æœä¸åšmaskä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿå¦‚ä½•æ­£ç¡®å®ç°maskï¼ˆè€ƒè™‘æ•°å€¼ç¨³å®šæ€§ï¼‰ï¼Ÿ

4. **[æ‰¹åˆ¤æ€è€ƒ]** Local Attentionå‡è®¾å¯¹é½æ˜¯å¤§è‡´å•è°ƒçš„ï¼ˆæºå’Œç›®æ ‡çš„ä½ç½®å¯¹åº”ï¼‰ã€‚å¯¹äºå“ªäº›è¯­è¨€å¯¹æˆ–ä»»åŠ¡ï¼Œè¿™ä¸ªå‡è®¾ä¼šä¸¥é‡å¤±æ•ˆï¼Ÿèƒ½å¦è®¾è®¡ä¸€ç§"éå•è°ƒå±€éƒ¨æ³¨æ„åŠ›"ï¼Ÿ

5. **[å¼€æ”¾é—®é¢˜]** Hard Attentionè™½ç„¶è®­ç»ƒå›°éš¾ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªä¼˜åŠ¿ï¼šç¨€ç–æ€§å¯ä»¥æé«˜å¯è§£é‡Šæ€§ã€‚æœ‰æ²¡æœ‰æ–¹æ³•æ—¢ä¿æŒSoft Attentionçš„å¯å¾®åˆ†æ€§ï¼Œåˆèƒ½è·å¾—æ¥è¿‘Hard Attentionçš„ç¨€ç–æ€§ï¼Ÿï¼ˆæç¤ºï¼šè€ƒè™‘ç¨€ç–softmaxã€Gumbel-softmaxï¼‰

---

## å»¶ä¼¸é˜…è¯»

### æ ¸å¿ƒè®ºæ–‡ï¼ˆå¿…è¯»ï¼‰

- **[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**
  - Attentionæœºåˆ¶åœ¨NMTä¸­çš„å¼€åˆ›æ€§å·¥ä½œ
  - é‡ç‚¹è¯»ï¼šSection 3ï¼ˆæ¨¡å‹æ¶æ„ï¼‰ã€Section 5ï¼ˆå¯è§†åŒ–åˆ†æï¼‰
  - arXiv: [1409.0473](https://arxiv.org/abs/1409.0473)

- **[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**
  - ç³»ç»Ÿæ¯”è¾ƒä¸åŒæ³¨æ„åŠ›å˜ä½“ï¼Œç¡®ç«‹è®¾è®¡å‡†åˆ™
  - é‡ç‚¹è¯»ï¼šSection 3ï¼ˆGlobal vs Localï¼‰ã€Section 4ï¼ˆå®éªŒå¯¹æ¯”ï¼‰
  - arXiv: [1508.04025](https://arxiv.org/abs/1508.04025)

### ç†è®ºåŸºç¡€

- **[Graves et al., 2014] Neural Turing Machines**
  - æå‡ºäº†åŸºäºå†…å®¹çš„è½¯å¯»å€ï¼Œæ˜¯Attentionçš„ç†è®ºå…ˆé©±
  - é‡ç‚¹è¯»ï¼šSection 3.1ï¼ˆAttentionæœºåˆ¶ï¼‰

### åç»­å‘å±•

- **[Vaswani et al., 2017] Attention Is All You Need**
  - æå‡ºScaled Dot-Product Attentionå’ŒMulti-Head Attentionï¼Œå®Œå…¨æŠ›å¼ƒRNN
  - æœ¬æ•™æç¬¬8ç« çš„æ ¸å¿ƒå†…å®¹
  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)

- **[Xu et al., 2015] Show, Attend and Tell**
  - Hard Attentionåœ¨å›¾åƒæè¿°ä¸­çš„åº”ç”¨ï¼Œå¯¹æ¯”Softå’ŒHard Attentionçš„æ•ˆæœ
  - arXiv: [1502.03044](https://arxiv.org/abs/1502.03044)

### æ³¨æ„åŠ›å¯è§£é‡Šæ€§

- **[Jain & Wallace, 2019] Attention is not Explanation**
  - è´¨ç–‘Attentionæƒé‡ä½œä¸ºè§£é‡Šçš„å¯é æ€§
  - arXiv: [1902.10186](https://arxiv.org/abs/1902.10186)

- **[Wiegreffe & Pinter, 2019] Attention is not not Explanation**
  - å¯¹ä¸Šè¿°è®ºæ–‡çš„å›åº”ï¼Œæ›´ç»†è‡´åœ°è®¨è®ºAttentionçš„è§£é‡Šæ€§
  - arXiv: [1908.04626](https://arxiv.org/abs/1908.04626)

### å¤§è§„æ¨¡å®è¯å¯¹æ¯”

- **[Britz et al., 2017] Massive Exploration of Neural Machine Translation Architectures**
  - å¤§è§„æ¨¡å¯¹æ¯”NMTçš„å„ç§è®¾è®¡é€‰æ‹©ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›å˜ä½“çš„å®è¯å¯¹æ¯”
  - arXiv: [1703.03906](https://arxiv.org/abs/1703.03906)

---

## å†å²æ³¨è„š

Attentionæœºåˆ¶çš„çµæ„Ÿéƒ¨åˆ†æ¥è‡ªäººç±»è§†è§‰ç³»ç»Ÿã€‚å½“æˆ‘ä»¬çœ‹ä¸€å¹…å¤æ‚çš„å›¾åƒæ—¶ï¼Œæˆ‘ä»¬ä¸ä¼šåŒæ—¶å¤„ç†æ‰€æœ‰åƒç´ ï¼Œè€Œæ˜¯ä¼š"èšç„¦"åœ¨æ„Ÿå…´è¶£çš„åŒºåŸŸã€‚è¿™ç§é€‰æ‹©æ€§æ³¨æ„ï¼ˆselective attentionï¼‰æ˜¯è®¤çŸ¥ç§‘å­¦ç ”ç©¶çš„ç»å…¸è¯¾é¢˜ã€‚

Bahdanauåœ¨2014å¹´å°†è¿™ä¸ªæ€æƒ³å¼•å…¥ç¥ç»æœºå™¨ç¿»è¯‘æ—¶ï¼Œå¹¶æ²¡æœ‰é¢„æ–™åˆ°å®ƒä¼šæˆä¸ºæ·±åº¦å­¦ä¹ æœ€æ ¸å¿ƒçš„ç»„ä»¶ä¹‹ä¸€ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä»–ä»¬è°¦è™šåœ°ç§°ä¹‹ä¸º"å¯¹é½æ¨¡å‹"ï¼ˆalignment modelï¼‰ï¼Œè€Œä¸æ˜¯"æ³¨æ„åŠ›"ã€‚"Attention"è¿™ä¸ªæœ¯è¯­æ˜¯åæ¥è¢«ç¤¾åŒºå¹¿æ³›é‡‡ç”¨çš„ã€‚

ä»…ä¸€å¹´åï¼Œæ–¯å¦ç¦çš„Luongã€Phamå’ŒManningå‘è¡¨äº†ç³»ç»Ÿæ€§çš„æ¢ç´¢å·¥ä½œã€‚ä»–ä»¬ä¸ä»…æå‡ºäº†è®¡ç®—æ›´é«˜æ•ˆçš„ä¹˜æ€§æ³¨æ„åŠ›ï¼Œæ›´é‡è¦çš„æ˜¯**ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒå’Œæ€»ç»“**äº†å½“æ—¶çš„å„ç§æ–¹æ³•ï¼Œä¸ºåæ¥è€…æä¾›äº†æ¸…æ™°çš„è®¾è®¡æŒ‡å—ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒLuongè®ºæ–‡ä¸­æåˆ°çš„ç‚¹ç§¯æ³¨æ„åŠ›å› ä¸ºè¿‡äºç®€å•è€Œåœ¨å½“æ—¶æ²¡æœ‰å—åˆ°å¤ªå¤šå…³æ³¨â€”â€”ä¸»æµä»ç„¶åå¥½å‚æ•°åŒ–çš„å¯¹é½å‡½æ•°ã€‚ä½†ä¸¤å¹´åï¼Œå½“Transformerè®ºæ–‡æå‡º"Scaled Dot-Product Attention"æ—¶ï¼Œç‚¹ç§¯æ³¨æ„åŠ›ç»ˆäºç™»ä¸Šäº†å†å²èˆå°çš„ä¸­å¤®ã€‚

ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œè¿™ä¸¤ç¯‡è®ºæ–‡å…±åŒå®Œæˆäº†Attentionå‘å±•çš„ç¬¬ä¸€é˜¶æ®µï¼šBahdanauå¼€åˆ›äº†æ³¨æ„åŠ›æœºåˆ¶ï¼ŒLuongæ‹“å±•äº†è®¾è®¡ç©ºé—´å¹¶å»ºç«‹äº†ç³»ç»Ÿæ€§çš„ç†è§£ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ›´å¤§èƒ†çš„æƒ³æ³•æ­£åœ¨é…é…¿ï¼šå¦‚æœAttentionå¦‚æ­¤å¼ºå¤§ï¼Œæˆ‘ä»¬æ˜¯å¦è¿˜éœ€è¦RNNï¼Ÿä¸‹ä¸€ç« å°†è®²è¿°Self-Attentionçš„è¯ç”Ÿï¼Œè€Œä¸ä¹…ä¹‹åï¼ŒVaswaniç­‰äººå°†ç»™å‡ºå†³å®šæ€§çš„å›ç­”â€”â€”"Attention Is All You Need"ã€‚è¿™ç¯‡è®ºæ–‡ä¸ä»…åœ¨æŠ€æœ¯ä¸Šé©æ–°äº†åºåˆ—å»ºæ¨¡ï¼Œå…¶æ ‡é¢˜æœ¬èº«ä¹Ÿæˆä¸ºäº†æ·±åº¦å­¦ä¹ å†å²ä¸Šæœ€å…·å½±å“åŠ›çš„é‡‘å¥ä¹‹ä¸€ã€‚
