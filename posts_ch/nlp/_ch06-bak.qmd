---
title: "第6章：注意力机制的变体演进"
subtitle: "从加性到乘性、从全局到局部、从软到硬"
author: "Ying Zha"
date: "2026-01-25"
categories: [NLP, Attention, Luong, 机器翻译, Seq2Seq]
tags: [乘性注意力, 点积注意力, Global Attention, Local Attention, Hard Attention, Soft Attention]
description: "注意力机制的设计空间：从加性到乘性、从全局到局部、从软到硬，Luong的系统性探索与效率-表达力权衡。"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    css: styles.css
    fig-cap-location: bottom
---

> **核心问题**：Bahdanau的加性注意力虽然有效，但是否有更简洁、更高效的注意力计算方式？不同的设计选择会带来怎样的权衡？
>
> **历史坐标**：2015 | Luong, Pham, Manning | 注意力机制的系统性探索

---

## 从上一章说起

上一章我们见证了Attention机制的诞生。Bahdanau等人通过让解码器在每一步都能"回头看"编码器的所有位置，彻底打破了Seq2Seq的信息瓶颈。这个突破性的想法迅速在机器翻译领域引发了连锁反应——如果Attention如此有效，是否还有其他方式来计算"注意力"？

Bahdanau的设计使用了一个**加性**的对齐函数：先将解码器状态和编码器状态分别经过线性变换，然后相加，最后通过一个单层网络得到标量分数。这个设计有效，但计算量不小——每次计算对齐分数都需要一个前馈网络。一个自然的问题浮现：能否用更简单的操作，比如直接计算向量的点积，来衡量两个状态的相关性？

2015年，斯坦福大学的Luong、Pham和Manning发表了一篇系统性的研究，探索了多种注意力机制的变体。他们不仅提出了计算效率更高的**乘性注意力（multiplicative attention）**，还探讨了**全局注意力**与**局部注意力**的权衡、不同对齐函数的对比，以及注意力在解码器中的最佳使用位置。

> 💡 **本章核心洞察**：注意力机制的设计空间远比想象中丰富。**加性 vs 乘性**决定了表达能力与计算效率的权衡；**全局 vs 局部**决定了长序列处理的策略；**软 vs 硬**决定了端到端可训练性。理解这些设计选择，是理解后来Transformer中Scaled Dot-Product Attention的关键。

---

## 问题的本质是什么？

### 问题的精确定义

Bahdanau Attention虽然有效，但在实践中面临几个设计问题：

**计算效率问题**：加性注意力需要为每对（解码器状态，编码器状态）计算一个前馈网络的输出：

$$
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
$$

这涉及两个矩阵乘法（$\mathbf{W}_a \mathbf{s}$ 和 $\mathbf{U}_a \mathbf{h}$）、一个非线性激活（$\tanh$）、一个向量点积（$\mathbf{v}_a^\top (\cdot)$）。当序列很长时，这个计算量是可观的。

**设计空间未充分探索**：Bahdanau做了一系列设计选择——使用加性对齐、在每个解码步之前计算注意力、关注所有源位置——但这些选择是否最优？有没有更好的替代方案？

**长序列的挑战**：对于很长的源序列，计算对所有位置的注意力可能是浪费的。直觉上，在翻译某个词时，我们只需要关注源序列的一小部分，而不是全部。能否只计算"局部"的注意力？

### 我们需要什么样的解决方案？

一个系统性的探索应该回答以下问题：

1. **对齐函数**：除了加性，还有哪些方式计算两个向量的相关性？它们的表达能力和计算效率如何权衡？
2. **注意力范围**：是否需要关注所有位置？能否只关注一个局部窗口？
3. **使用位置**：注意力应该在解码的哪个阶段使用？是在计算解码器状态之前，还是之后？
4. **软 vs 硬**：是否可以用确定性的"硬"选择替代概率分布的"软"选择？

---

## 核心思想与直觉

### Luong Attention：乘性替代加性

Luong等人提出的核心改进是用**乘性（multiplicative）**操作替代加性操作来计算对齐分数。最简单的形式是直接计算点积：

$$
e_{ij} = \mathbf{s}_i^\top \mathbf{h}_j
$$

这被称为**点积注意力（dot-product attention）**。与Bahdanau的加性注意力相比，它没有任何可学习参数——只是两个向量的内积。

直觉上，点积衡量的是两个向量的"相似度"。如果解码器状态 $\mathbf{s}_i$ 和编码器状态 $\mathbf{h}_j$ 指向相似的方向，点积就大；如果它们正交，点积就是零。这正是我们想要的：找到与当前解码状态最"相关"的编码位置。

### 三种对齐函数的对比

Luong的论文系统比较了三种对齐函数：

| 名称 | 公式 | 参数 | 计算效率 |
|------|------|------|----------|
| **Dot** | $\mathbf{s}^\top \mathbf{h}$ | 无 | 最快 |
| **General** | $\mathbf{s}^\top \mathbf{W}_a \mathbf{h}$ | $\mathbf{W}_a$ | 中等 |
| **Concat** | $\mathbf{v}_a^\top \tanh(\mathbf{W}_a [\mathbf{s}; \mathbf{h}])$ | $\mathbf{v}_a, \mathbf{W}_a$ | 最慢 |

**Dot（点积）**：最简单，计算最快，但要求解码器和编码器的隐藏维度必须相同。

**General（一般形式）**：引入一个可学习的矩阵 $\mathbf{W}_a$，可以处理不同维度的状态，也增加了模型的表达能力。这实际上是在问："$\mathbf{s}$ 和 $\mathbf{W}_a \mathbf{h}$（$\mathbf{h}$ 的一个线性变换）有多相似？"

**Concat（拼接）**：这就是Bahdanau的加性注意力，将两个向量拼接后通过一个单层网络。表达能力最强，但计算最慢。

### 另一个关键差异：注意力的使用位置

除了对齐函数的不同，Luong还指出了另一个重要差异：**注意力应该在解码器的什么位置使用？**

**Bahdanau的方式**：先计算注意力，得到上下文向量 $\mathbf{c}_i$，然后将 $\mathbf{c}_i$ 和上一步输出 $y_{i-1}$ 一起输入RNN，计算新的隐藏状态 $\mathbf{s}_i$。

$$
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)
$$

**Luong的方式**：先用RNN计算新的隐藏状态 $\mathbf{s}_i$，然后基于 $\mathbf{s}_i$ 计算注意力，得到上下文向量 $\mathbf{c}_i$，最后将两者结合生成输出。

$$
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1})
$$
$$
\mathbf{c}_i = \text{Attention}(\mathbf{s}_i, \mathbf{H})
$$
$$
\tilde{\mathbf{s}}_i = \tanh(\mathbf{W}_c [\mathbf{c}_i; \mathbf{s}_i])
$$

这看起来是个细节差异，但Luong的方式更加模块化——RNN和Attention是分离的，便于分析和调试。

---

## 技术细节

### Luong Attention的三种变体

让我们详细看看三种对齐函数的数学形式和实现。

**变体1：Dot-Product Attention**

$$
\text{score}(\mathbf{s}_i, \mathbf{h}_j) = \mathbf{s}_i^\top \mathbf{h}_j
$$

这是最简单的形式。两个向量的点积可以用矩阵乘法高效实现：如果我们有所有编码器状态组成的矩阵 $\mathbf{H} \in \mathbb{R}^{T_x \times d}$，那么所有对齐分数可以一次计算：

$$
\mathbf{e}_i = \mathbf{H} \mathbf{s}_i \in \mathbb{R}^{T_x}
$$

**变体2：General Attention**

$$
\text{score}(\mathbf{s}_i, \mathbf{h}_j) = \mathbf{s}_i^\top \mathbf{W}_a \mathbf{h}_j
$$

其中 $\mathbf{W}_a \in \mathbb{R}^{d_s \times d_h}$ 是可学习参数。这允许解码器和编码器有不同的隐藏维度，同时让模型学习一个"相似度度量"。

**变体3：Concat Attention（与Bahdanau类似）**

$$
\text{score}(\mathbf{s}_i, \mathbf{h}_j) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a [\mathbf{s}_i; \mathbf{h}_j])
$$

其中 $[\mathbf{s}_i; \mathbf{h}_j]$ 表示向量拼接。这是最具表达能力的形式，因为它可以学习任意的对齐函数。

::: {.callout-note}
## Algorithm: Luong Attention Variants (Luong et al., 2015)

```python
def luong_attention(decoder_state, encoder_outputs, method='dot', W_a=None, v_a=None):
    """
    Luong 注意力机制的三种变体

    参数:
        decoder_state: 解码器当前隐藏状态 [batch, dec_hidden]
        encoder_outputs: 编码器所有隐藏状态 [batch, src_len, enc_hidden]
        method: 'dot', 'general', 或 'concat'
        W_a: 可学习参数（general和concat需要）
        v_a: 可学习参数（concat需要）

    返回:
        context: 上下文向量 [batch, enc_hidden]
        attention_weights: 注意力权重 [batch, src_len]
    """
    if method == 'dot':
        # 点积: s^T h
        # [batch, dec_hidden] @ [batch, enc_hidden, src_len] -> [batch, src_len]
        scores = torch.bmm(decoder_state.unsqueeze(1),
                          encoder_outputs.transpose(1, 2)).squeeze(1)

    elif method == 'general':
        # 一般形式: s^T W h
        # 先计算 W @ h: [batch, src_len, dec_hidden]
        transformed = encoder_outputs @ W_a.T
        scores = torch.bmm(decoder_state.unsqueeze(1),
                          transformed.transpose(1, 2)).squeeze(1)

    elif method == 'concat':
        # 拼接形式: v^T tanh(W [s; h])
        # 扩展 decoder_state 到所有位置
        s_expanded = decoder_state.unsqueeze(1).expand(-1, encoder_outputs.size(1), -1)
        concat = torch.cat([s_expanded, encoder_outputs], dim=-1)
        scores = v_a @ torch.tanh(concat @ W_a.T).transpose(1, 2)
        scores = scores.squeeze(1)

    # Softmax 归一化
    attention_weights = F.softmax(scores, dim=-1)

    # 加权求和
    context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

    return context, attention_weights
```

*Source: Luong, Pham, & Manning (2015) "Effective Approaches to Attention-based Neural Machine Translation", EMNLP 2015. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*
:::

### 完整数值示例：对比三种对齐函数

让我们用同一组数据，比较三种对齐函数计算出的分数。

**设定**：

- 解码器状态：$\mathbf{s} = [0.5, -0.3, 0.8, 0.2]^\top$
- 编码器状态（3个位置）：
  - $\mathbf{h}_1 = [0.2, 0.4, 0.1, -0.3]^\top$
  - $\mathbf{h}_2 = [0.6, -0.1, 0.7, 0.3]^\top$
  - $\mathbf{h}_3 = [-0.2, 0.5, 0.3, 0.1]^\top$

**Dot-Product 计算**：

$$
e_1 = \mathbf{s}^\top \mathbf{h}_1 = 0.5 \times 0.2 + (-0.3) \times 0.4 + 0.8 \times 0.1 + 0.2 \times (-0.3)
$$
$$
= 0.10 - 0.12 + 0.08 - 0.06 = 0.00
$$

$$
e_2 = \mathbf{s}^\top \mathbf{h}_2 = 0.5 \times 0.6 + (-0.3) \times (-0.1) + 0.8 \times 0.7 + 0.2 \times 0.3
$$
$$
= 0.30 + 0.03 + 0.56 + 0.06 = 0.95
$$

$$
e_3 = \mathbf{s}^\top \mathbf{h}_3 = 0.5 \times (-0.2) + (-0.3) \times 0.5 + 0.8 \times 0.3 + 0.2 \times 0.1
$$
$$
= -0.10 - 0.15 + 0.24 + 0.02 = 0.01
$$

**Softmax 归一化**：

$$
\alpha_1 = \frac{e^{0.00}}{e^{0.00} + e^{0.95} + e^{0.01}} = \frac{1.00}{1.00 + 2.59 + 1.01} = \frac{1.00}{4.60} \approx 0.22
$$

$$
\alpha_2 = \frac{e^{0.95}}{4.60} = \frac{2.59}{4.60} \approx 0.56
$$

$$
\alpha_3 = \frac{e^{0.01}}{4.60} = \frac{1.01}{4.60} \approx 0.22
$$

**解读**：使用点积注意力，模型将56%的注意力放在位置2，这是因为 $\mathbf{h}_2$ 与 $\mathbf{s}$ 的点积最大——它们在向量空间中最"相似"。

**General Attention 计算**（假设 $\mathbf{W}_a$ 是单位矩阵）：

当 $\mathbf{W}_a = \mathbf{I}$ 时，General退化为Dot-Product。在一般情况下，$\mathbf{W}_a$ 可以学习一个变换，使得模型能够发现更复杂的相关性模式。

### Global vs Local Attention

Luong还提出了另一个重要的设计选择：**注意力的范围**。

**Global Attention**：关注所有源位置。这是Bahdanau的做法，也是上面讨论的默认方式。

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

**Local Attention**：只关注源序列的一个**窗口**。

核心思想是：在每个解码步，先预测一个对齐位置 $p_i$，然后只计算以 $p_i$ 为中心、宽度为 $2D+1$ 的窗口内的注意力。

$$
\mathbf{c}_i = \sum_{j=p_i-D}^{p_i+D} \alpha_{ij} \mathbf{h}_j
$$

对齐位置 $p_i$ 可以通过两种方式确定：

**Local-m（单调）**：假设源和目标大致对齐，简单设置 $p_i = i$。

**Local-p（预测）**：学习一个函数来预测 $p_i$：

$$
p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))
$$

其中 $\sigma$ 是sigmoid函数，确保 $p_i \in [0, T_x]$。

为了让注意力在窗口中心附近更集中，Local Attention还引入了一个高斯偏置：

$$
\alpha_{ij} = \text{align}(\mathbf{s}_i, \mathbf{h}_j) \cdot \exp\left(-\frac{(j - p_i)^2}{2\sigma^2}\right)
$$

![Luong论文中的Global vs Local Attention对比。左边是Global Attention：解码器状态 $h_t$ 与所有源位置计算注意力，生成上下文向量 $c_t$。右边是Local Attention：先预测对齐位置 $p_t$，只计算窗口 $[p_t - D, p_t + D]$ 内的注意力。](figures/chapter-6/original/fig-global-local-attention.png){#fig-global-local width=80%}

::: {.figure-caption}
*Source: Luong, Pham, & Manning (2015) "Effective Approaches to Attention-based Neural Machine Translation", Figure 2 & 3. [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)*
:::

### Hard Attention vs Soft Attention

除了Global/Local的区分，还有另一个重要维度：**软注意力（Soft Attention）** vs **硬注意力（Hard Attention）**。

**Soft Attention**：计算所有位置的注意力权重（一个概率分布），然后加权求和。这是我们一直在讨论的方式。

$$
\mathbf{c}_i = \sum_j \alpha_{ij} \mathbf{h}_j = \mathbb{E}_{p(j | \mathbf{s}_i, \mathbf{H})}[\mathbf{h}_j]
$$

**Hard Attention**：从注意力分布中**采样**一个位置 $j^*$，只使用那个位置的信息。

$$
j^* \sim \text{Categorical}(\alpha_{i1}, \alpha_{i2}, \ldots, \alpha_{iT_x})
$$
$$
\mathbf{c}_i = \mathbf{h}_{j^*}
$$

两者的核心区别在于**可微分性**。

**Soft Attention 是可微分的**：加权求和是一个连续操作，梯度可以通过 $\alpha_{ij}$ 流向对齐函数的参数。这意味着我们可以用标准的反向传播进行端到端训练。

**Hard Attention 不可微分**：采样操作是离散的，梯度无法直接通过。要训练Hard Attention，需要使用强化学习方法（如REINFORCE），用期望梯度的蒙特卡洛估计来近似。这带来了高方差和训练不稳定的问题。

::: {.callout-warning}
## Hard Attention的训练困难

Hard Attention虽然在概念上更接近人类的"注意"（我们真的只看一个地方，而不是模糊地看所有地方），但它的训练需要强化学习技术：

$$
\nabla_\theta J = \mathbb{E}_{j^* \sim p(j|\theta)} \left[ \nabla_\theta \log p(j^* | \theta) \cdot R(j^*) \right]
$$

其中 $R(j^*)$ 是选择位置 $j^*$ 带来的"奖励"。这个梯度估计的方差很大，需要大量采样才能稳定。

实践中，**Soft Attention几乎总是更好的选择**，因为：

1. 端到端可微分，训练简单
2. 梯度估计没有方差问题
3. 可以同时利用多个位置的信息
:::

### 复杂度分析

不同注意力变体的计算复杂度：

| 变体 | 对齐计算 | 总复杂度 |
|------|----------|----------|
| **Global + Dot** | $O(T_x \cdot d)$ per step | $O(T_x \cdot T_y \cdot d)$ |
| **Global + General** | $O(T_x \cdot d^2)$ per step | $O(T_x \cdot T_y \cdot d^2)$ |
| **Global + Concat** | $O(T_x \cdot d^2)$ per step | $O(T_x \cdot T_y \cdot d^2)$ |
| **Local** | $O(D \cdot d)$ per step | $O(D \cdot T_y \cdot d)$ |

其中 $T_x$ 是源序列长度，$T_y$ 是目标序列长度，$d$ 是隐藏维度，$D$ 是局部窗口大小。

Local Attention的优势在长序列时尤为明显：当 $T_x = 1000$ 而 $D = 50$ 时，计算量减少了20倍。

---

## 工程实践

### 实现Luong Attention

```{python}
#| code-fold: false
import torch
import torch.nn as nn
import torch.nn.functional as F

class LuongAttention(nn.Module):
    """
    Luong 注意力机制，支持三种对齐方式
    """
    def __init__(self, enc_hidden_dim, dec_hidden_dim, method='dot'):
        super().__init__()
        self.method = method
        self.enc_hidden_dim = enc_hidden_dim
        self.dec_hidden_dim = dec_hidden_dim

        if method == 'general':
            self.W_a = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)
        elif method == 'concat':
            self.W_a = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim, bias=False)
            self.v_a = nn.Linear(dec_hidden_dim, 1, bias=False)

    def forward(self, decoder_state, encoder_outputs, mask=None):
        """
        decoder_state: [batch, dec_hidden]
        encoder_outputs: [batch, src_len, enc_hidden]
        mask: [batch, src_len], True表示padding位置
        """
        batch_size, src_len, _ = encoder_outputs.shape

        if self.method == 'dot':
            # 点积: s^T h
            # 需要 dec_hidden == enc_hidden
            scores = torch.bmm(
                decoder_state.unsqueeze(1),  # [batch, 1, dec_hidden]
                encoder_outputs.transpose(1, 2)  # [batch, enc_hidden, src_len]
            ).squeeze(1)  # [batch, src_len]

        elif self.method == 'general':
            # 一般形式: s^T W h
            # W 将 enc_hidden 映射到 dec_hidden
            transformed = self.W_a(encoder_outputs)  # [batch, src_len, dec_hidden]
            scores = torch.bmm(
                decoder_state.unsqueeze(1),
                transformed.transpose(1, 2)
            ).squeeze(1)

        elif self.method == 'concat':
            # 拼接形式: v^T tanh(W [s; h])
            decoder_expanded = decoder_state.unsqueeze(1).expand(-1, src_len, -1)
            concat = torch.cat([decoder_expanded, encoder_outputs], dim=-1)
            energy = torch.tanh(self.W_a(concat))  # [batch, src_len, dec_hidden]
            scores = self.v_a(energy).squeeze(-1)  # [batch, src_len]

        # 应用 mask
        if mask is not None:
            scores = scores.masked_fill(mask, -1e10)

        # Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # 上下文向量
        context = torch.bmm(
            attention_weights.unsqueeze(1),
            encoder_outputs
        ).squeeze(1)

        return context, attention_weights
```

### 实现Local Attention

```{python}
#| code-fold: false
class LocalAttention(nn.Module):
    """
    Luong 的 Local Attention（预测型）
    """
    def __init__(self, enc_hidden_dim, dec_hidden_dim, window_size=10):
        super().__init__()
        self.window_size = window_size  # D: 窗口半径
        self.enc_hidden_dim = enc_hidden_dim

        # 位置预测网络
        self.W_p = nn.Linear(dec_hidden_dim, dec_hidden_dim)
        self.v_p = nn.Linear(dec_hidden_dim, 1)

        # 对齐函数（使用 general）
        self.W_a = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)

        # 高斯标准差
        self.sigma = window_size / 2

    def forward(self, decoder_state, encoder_outputs, mask=None):
        """
        decoder_state: [batch, dec_hidden]
        encoder_outputs: [batch, src_len, enc_hidden]
        """
        batch_size, src_len, _ = encoder_outputs.shape
        device = decoder_state.device

        # Step 1: 预测对齐位置 p
        # p = S * sigmoid(v^T tanh(W_p s))
        p = src_len * torch.sigmoid(
            self.v_p(torch.tanh(self.W_p(decoder_state)))
        ).squeeze(-1)  # [batch]

        # Step 2: 计算所有位置的对齐分数
        transformed = self.W_a(encoder_outputs)  # [batch, src_len, dec_hidden]
        scores = torch.bmm(
            decoder_state.unsqueeze(1),
            transformed.transpose(1, 2)
        ).squeeze(1)  # [batch, src_len]

        # Step 3: 应用高斯窗口
        # 生成位置索引 [0, 1, 2, ..., src_len-1]
        positions = torch.arange(src_len, device=device).float()
        positions = positions.unsqueeze(0).expand(batch_size, -1)  # [batch, src_len]

        # 高斯权重: exp(-(j - p)^2 / (2 * sigma^2))
        gaussian = torch.exp(-((positions - p.unsqueeze(1)) ** 2) / (2 * self.sigma ** 2))

        # Step 4: 窗口mask（只保留 [p-D, p+D] 范围内的位置）
        window_mask = (positions >= (p.unsqueeze(1) - self.window_size)) & \
                      (positions <= (p.unsqueeze(1) + self.window_size))

        # 应用窗口mask
        scores = scores.masked_fill(~window_mask, -1e10)

        # Step 5: Softmax + 高斯加权
        attention_weights = F.softmax(scores, dim=-1) * gaussian

        # 重新归一化
        attention_weights = attention_weights / (attention_weights.sum(dim=-1, keepdim=True) + 1e-10)

        # 上下文向量
        context = torch.bmm(
            attention_weights.unsqueeze(1),
            encoder_outputs
        ).squeeze(1)

        return context, attention_weights, p
```

### 对比实验

```{python}
#| code-fold: false
# 创建测试数据
batch_size = 2
src_len = 10
enc_hidden = 64
dec_hidden = 64

encoder_outputs = torch.randn(batch_size, src_len, enc_hidden)
decoder_state = torch.randn(batch_size, dec_hidden)

# 测试三种 Luong Attention
for method in ['dot', 'general', 'concat']:
    attn = LuongAttention(enc_hidden, dec_hidden, method=method)
    context, weights = attn(decoder_state, encoder_outputs)
    print(f"{method:8s}: context shape = {context.shape}, weights sum = {weights.sum(dim=-1)}")

# 测试 Local Attention
local_attn = LocalAttention(enc_hidden, dec_hidden, window_size=3)
context, weights, p = local_attn(decoder_state, encoder_outputs)
print(f"{'local':8s}: context shape = {context.shape}, predicted p = {p.tolist()}")
```

---

## 深入理解

### 为什么点积注意力能工作？——理论视角

点积注意力的有效性可以从多个角度理解。

**余弦相似度视角**：当向量被归一化后，点积就是余弦相似度：

$$
\mathbf{s}^\top \mathbf{h} = \|\mathbf{s}\| \|\mathbf{h}\| \cos(\theta)
$$

余弦相似度是衡量两个向量"方向一致性"的经典指标。神经网络在训练过程中，会学习让相关的状态指向相似的方向。

**核方法视角**：点积可以看作一个线性核（linear kernel）。在核方法的框架下，注意力权重实际上是在一个特征空间中计算相似度。General Attention引入的可学习矩阵 $\mathbf{W}_a$ 相当于学习一个Mahalanobis距离。

**信息检索视角**：点积注意力可以类比为向量空间模型中的查询-文档匹配。解码器状态是"查询"，编码器状态是"文档"，点积衡量查询与文档的相关性。

### 为什么需要缩放？——Scaled Dot-Product的预兆

Luong的论文没有讨论这个问题，但后来的Transformer论文（Vaswani et al., 2017）指出了点积注意力的一个潜在问题：

当向量维度 $d$ 很大时，点积的方差会很大。假设 $\mathbf{s}$ 和 $\mathbf{h}$ 的每个分量都是独立的、均值为0、方差为1的随机变量，那么：

$$
\text{Var}(\mathbf{s}^\top \mathbf{h}) = d
$$

当 $d = 512$ 时，点积的标准差是 $\sqrt{512} \approx 22.6$。这意味着点积可能产生很大的正值或负值，导致softmax输出接近one-hot分布，梯度变得很小。

解决方案是**缩放**：

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \frac{\mathbf{s}^\top \mathbf{h}}{\sqrt{d}}
$$

这就是Transformer中的**Scaled Dot-Product Attention**。Luong的论文使用的维度较小（500左右），问题不太明显；但在Transformer的大维度设置下，缩放变得必要。

### 方法的边界条件

**Dot-Product Attention的局限**：

1. **维度必须匹配**：解码器和编码器的隐藏维度必须相同，否则无法计算点积
2. **表达能力有限**：无法学习复杂的相关性模式，只能捕捉"方向相似"的关系

**Local Attention的局限**：

1. **需要预测对齐位置**：如果位置预测错误，会错过重要信息
2. **不适合非单调对齐**：对于语序差异大的语言对，局部窗口可能覆盖不到正确位置
3. **窗口大小是超参数**：选择不当会影响性能

**General/Concat的局限**：

1. **计算开销**：额外的矩阵乘法增加了计算量
2. **过拟合风险**：更多参数可能导致在小数据集上过拟合

### 开放研究问题

1. **对齐函数的最优选择**：在什么条件下应该选择哪种对齐函数？是否有理论指导？

2. **动态窗口**：Local Attention使用固定窗口大小，能否根据内容动态调整？

3. **多粒度注意力**：能否同时使用全局和局部注意力，在不同粒度上捕获信息？

---

## 局限性与展望

### 本章方法的核心局限

**1. 注意力仍然是RNN的"附属品"**

无论是Bahdanau还是Luong的注意力，都是Seq2Seq架构的增强组件。编码和解码的核心仍然依赖RNN。这意味着：

- 顺序计算无法避免：RNN必须逐步处理序列
- 长距离依赖仍然困难：虽然Attention提供了捷径，但RNN本身的问题没有解决

**2. 注意力只在编码器-解码器之间**

当前的Attention只让解码器关注编码器。但一个自然的问题是：**编码器内部的各个位置能否相互关注？** 一个词的理解可能依赖于同一句话中的其他词，而当前的架构没有提供这种机制。

**3. 位置信息是隐式的**

Attention本身不包含位置信息。位置信息完全依赖RNN的顺序处理来隐式编码。如果抛弃RNN，位置信息将完全丢失。

### 这些局限指向什么？

Luong的工作完成了对Seq2Seq Attention的系统性探索，确立了一些最佳实践（如点积注意力的效率优势）。但更深层的问题浮现：

**能否让Attention独立于RNN？**

如果Attention如此有效，为什么还需要RNN？能否设计一个完全基于Attention的架构？

**能否让序列中的每个位置相互关注？**

当前的Attention是"跨序列"的（解码器关注编码器）。如果让同一序列内的位置相互关注——这就是**Self-Attention**——会发生什么？

这些问题的答案，将在接下来的两章揭晓：第7章将介绍Self-Attention的诞生，第8章将介绍革命性的Transformer架构——"Attention Is All You Need"。

> 从加性到乘性，从全局到局部，Luong的探索为Attention机制建立了理论和实践的基础。但真正的革命还在后面：当研究者意识到**注意力本身就可以成为架构的核心**，深度学习的历史翻开了新的一页。

---

## 本章小结

::: {.callout-important}
## 核心要点

- **问题**：Bahdanau的加性注意力有效但计算较慢，注意力机制的设计空间还有很多未探索的选择
- **洞察**：点积注意力可以用简单的向量内积计算相关性，大幅提高计算效率；局部注意力可以减少长序列的计算开销
- **方法**：Luong系统比较了dot/general/concat三种对齐函数，以及global/local两种范围策略
- **意义**：确立了点积注意力的效率优势，为后来Transformer的Scaled Dot-Product Attention奠定基础
:::

### 关键公式速查

**Dot-Product Attention**：

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{h}
$$

**General Attention**：

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{s}^\top \mathbf{W}_a \mathbf{h}
$$

**Concat (Additive) Attention**：

$$
\text{score}(\mathbf{s}, \mathbf{h}) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a [\mathbf{s}; \mathbf{h}])
$$

**Local Attention位置预测**：

$$
p_i = T_x \cdot \sigma(\mathbf{v}_p^\top \tanh(\mathbf{W}_p \mathbf{s}_i))
$$

---

## 思考题

1. **[概念理解]** 点积注意力和加性注意力在表达能力上有什么本质区别？设计一个简单的例子，展示加性注意力能学习而点积注意力无法学习的相关性模式。

2. **[数学推导]** 证明：当 $\mathbf{W}_a$ 是单位矩阵时，General Attention退化为Dot-Product Attention。更一般地，如果 $\mathbf{W}_a = \mathbf{U}\mathbf{V}^\top$（秩-$r$分解），这对注意力模式有什么影响？

3. **[工程实践]** 实现一个支持多头（multi-head）的Luong Attention。每个头使用不同的 $\mathbf{W}_a$，最后拼接所有头的输出。对比单头和多头在翻译任务上的表现。

4. **[批判思考]** Local Attention假设对齐是大致单调的（源和目标的位置对应）。对于哪些语言对或任务，这个假设会严重失效？能否设计一种"非单调局部注意力"？

5. **[开放问题]** Hard Attention虽然训练困难，但它有一个优势：稀疏性可以提高可解释性。有没有方法既保持Soft Attention的可微分性，又能获得接近Hard Attention的稀疏性？（提示：考虑稀疏softmax、Gumbel-softmax）

---

## 延伸阅读

### 核心论文（必读）

- **[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**
  - 本章的核心论文，系统比较不同注意力变体
  - 重点读：Section 3（Global vs Local）、Section 4（实验对比）
  - arXiv: [1508.04025](https://arxiv.org/abs/1508.04025)

### 理论基础

- **[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**
  - 上一章的核心，Attention的开山之作
  - arXiv: [1409.0473](https://arxiv.org/abs/1409.0473)

### 后续发展

- **[Vaswani et al., 2017] Attention Is All You Need**
  - 提出Scaled Dot-Product Attention和Multi-Head Attention
  - 完全抛弃RNN，只用Attention构建模型
  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)

- **[Xu et al., 2015] Show, Attend and Tell**
  - Hard Attention在图像描述中的应用
  - 对比Soft和Hard Attention的效果
  - arXiv: [1502.03044](https://arxiv.org/abs/1502.03044)

### 技术报告

- **[Britz et al., 2017] Massive Exploration of Neural Machine Translation Architectures**
  - 大规模对比NMT的各种设计选择
  - 包括注意力变体的实证对比
  - arXiv: [1703.03906](https://arxiv.org/abs/1703.03906)

---

## 历史注脚

Luong的论文发表于2015年EMNLP，距离Bahdanau的论文仅一年。在这短短一年里，Attention迅速成为NMT的标准配置，各种变体层出不穷。Luong的工作之所以重要，不仅在于提出了新的变体，更在于它**系统性地比较和总结**了当时的各种方法，为后来者提供了清晰的设计指南。

有趣的是，Luong论文中提到的点积注意力（Dot-Product）因为过于简单而没有受到太多关注。当时的主流仍然是参数化的对齐函数。但两年后，当Transformer论文提出"Scaled Dot-Product Attention"时，点积注意力终于登上了历史舞台的中央——它不仅简单高效，而且在大规模设置下与更复杂的对齐函数表现相当。

从某种意义上说，Luong的论文是Attention发展史上的一个"中场休息"——它总结了第一阶段的探索，为第二阶段（Self-Attention和Transformer）的革命铺平了道路。
