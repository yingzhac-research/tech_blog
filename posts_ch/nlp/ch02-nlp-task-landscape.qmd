---
title: "第2章：NLP核心任务全景"
subtitle: "先看清问题，再看模型如何解题"
author: "Ying Zha"
date: "2026-01-27"
categories: [NLP, Tasks, Evaluation, Benchmarks]
tags: [分类, 序列标注, NER, 机器翻译, 问答, 信息抽取, 句法分析, 评测]
description: "NLP核心任务的全景地图：从文本分类到机器翻译，从序列标注到问答系统。理解模型在解决什么问题，是理解模型演进的前提。"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    css: styles.css
    fig-cap-location: bottom
---

> **核心问题**：NLP 到底在解决什么问题？模型演进的背后，是哪些任务在驱动技术进步？
>
> **历史坐标**：1990s-2020s | 从任务专用模型到通用大模型 | 任务定义驱动模型设计

---

::: {.callout-tip}
## 本章参考来源

### 教材
- SLP3 Ch 4 (Text Classification), Ch 12 (Machine Translation), Ch 17 (Sequence Labeling for POS and NER), Ch 18-19 (Parsing), Ch 20 (Information Extraction), Ch 22 (Sentiment), Ch 23 (Coreference Resolution) — 各任务的经典定义和评测方法
- D2L Ch 16 (Natural Language Processing Applications) — 情感分析、NLI 的实践示例

### 课程
- CS224N (Stanford) Lecture 1-2 — NLP 任务全景概述
- CMU 11-711 ANLP Lecture 1-2 — NLP 任务分类框架

### 综述
- Sun et al. (2022) "Paradigm Shift in Natural Language Processing" — 七大建模范式分类框架（Classification, Matching, SeqLab, MRC, Seq2Seq, Seq2ASeq, (M)LM）
:::

---

## 从上一章说起

上一章我们回顾了前深度学习时代的 NLP。我们看到了符号主义的辉煌与幻灭、统计方法的崛起与瓶颈，最终落脚于一个核心困境：**特征工程的诅咒**——每个任务都需要专家手工设计特征，这不仅耗费大量人力，更无法扩展到复杂问题。

但在急于寻找解决方案之前，我们需要退一步，问一个更基本的问题：**NLP 到底在解决什么问题？**

本书的叙事是 model-centered 的——每章聚焦一个模型或技术。但如果只看模型，不看模型在解决什么问题，我们的理解就是悬空的。Attention 为什么诞生于机器翻译？BERT 为什么首先在自然语言推理上展示实力？GPT-3 的 few-shot 能力为什么让人震惊？这些问题的答案都藏在 NLP 的任务版图中。

本章提供一张"任务地图"。后续每讲一个模型，你都能在这张地图上找到它解决的问题。这不是一个需要从头到尾细读的章节，更像是一个随时可以翻回来查阅的参考——当你在第 8 章读到 Transformer 在 WMT 上的 BLEU 分数时，可以回到这里了解什么是 BLEU、什么是 WMT。

> 💡 **本章核心洞察**：NLP 的任务多样性远超想象——从判断一句话的情感极性，到翻译整篇文档，从识别文本中的人名，到理解"他"指的是谁。但所有这些任务都可以归结为少数几种输入/输出模式，而这些模式直接决定了模型架构的设计。

---

## 为什么需要一张任务地图？

"自然语言处理"这个词听起来像是一个统一的领域，但实际上，它包含了一系列差异巨大的任务。判断一条影评是正面还是负面，和把一段英文翻译成中文，虽然都是"处理语言"，但在输入输出形式、难度、评测方式上完全不同。理解这种多样性，是理解模型演进的前提。

### 从输入/输出看任务分类

尽管 NLP 任务千差万别，但从输入/输出的角度来看，它们可以归纳为少数几种基本模式。

第一种是**分类**（Classification）：输入一段文本，输出一个类别标签。情感分析（"这部电影好看吗？"）、主题分类（"这篇新闻属于什么领域？"）、自然语言推理（"前提是否蕴含假设？"）都属于这个模式。模型的输出空间是有限的、离散的，通常用 softmax 层实现。

第二种是**序列标注**（Sequence Labeling）：输入一个词序列，为每个词分配一个标签。词性标注（"这个词是名词还是动词？"）和命名实体识别（"这个词是人名、地名还是组织名？"）是典型代表。输入和输出等长，每个位置独立预测但彼此相关。

第三种是**序列到序列**（Sequence-to-Sequence）：输入一个序列，输出另一个序列，两者长度可以不同。机器翻译（英文句子 → 中文句子）、文本摘要（长文档 → 短摘要）、对话生成（用户输入 → 系统回复）都是这种模式。这也是催生 Encoder-Decoder 架构和 Attention 机制的核心任务类型。

第四种是**结构预测**（Structure Prediction）：输入一个序列，输出一个结构化对象——树、图或其他复杂结构。句法分析（输出依存树或短语结构树）和语义角色标注（识别"谁对谁做了什么"）属于这个模式。搜索空间巨大，需要结构化的推理算法。

第五种是**信息获取**（Information Seeking）：输入一段文本加上一个问题或查询，输出答案。阅读理解（"根据这篇文章，谁发明了电话？"）和信息抽取（"从这篇新闻中提取所有公司之间的收购关系"）属于这个模式。

这五种模式并不是相互排斥的。同一个任务可以用不同的模式建模——比如命名实体识别既可以用序列标注方式（BIO 标签），也可以用阅读理解方式（"文中有哪些人名？"），甚至可以用序列到序列方式（直接生成实体列表）。Sun et al. (2022) 的综述 "Paradigm Shift in Natural Language Processing" 系统研究了这种建模范式的迁移现象，发现大语言模型时代正在把越来越多的任务统一到生成式（Seq2Seq / 自回归 LM）范式下。

理解这些模式的价值在于：它们直接决定了模型架构和损失函数的设计。分类任务用交叉熵损失和 softmax 输出层；序列标注用 CRF 层或逐位置分类；序列到序列用 Encoder-Decoder 架构加自回归解码。后续章节介绍的每个模型，都可以追溯到它要解决的是哪种模式的任务。

---

## 文本分类与语义匹配

文本分类是 NLP 中最基础也最广泛的任务类型：给定一段文本，将其归入预定义的类别。虽然"分类"听起来简单，但它覆盖了从一句话的情感判断到复杂的语义推理。

### 情感分析

情感分析（Sentiment Analysis）是 NLP 最经典的应用之一。任务定义很直接：给定一段文本（一条影评、一条推文、一篇产品评论），判断作者表达的情感极性——正面、负面，或中性。

这个任务之所以重要，不仅因为它有直接的商业价值（品牌监控、市场分析），更因为它是 NLP 研究的"果蝇"——足够简单、数据充足、容易评测，成为新方法的第一块试金石。从朴素贝叶斯到逻辑回归，从 CNN 到 BERT，几乎每种 NLP 方法都首先在情感分析上证明自己。

代表性 benchmark 包括 **SST-2**（Stanford Sentiment Treebank，二分类，67k 条影评）和 **IMDb**（50k 条长影评），评测指标通常是准确率（Accuracy）。在 pre-BERT 时代，SST-2 上的最优准确率约为 90%；BERT 发布后迅速突破 95%，到 2024 年大语言模型已接近人类水平。

### 自然语言推理

自然语言推理（Natural Language Inference, NLI）要求模型判断两个句子之间的逻辑关系：给定一个"前提"（premise）和一个"假设"（hypothesis），判断假设相对于前提是**蕴含**（entailment）、**矛盾**（contradiction）还是**中立**（neutral）。

举个例子：前提是"一只猫在阳台上睡觉"，假设是"有一只动物在休息"——这是蕴含关系。如果假设是"猫在追老鼠"——这是矛盾。如果假设是"猫的主人在做饭"——这是中立（既不能推出也不能否定）。

NLI 之所以在 NLP 研究中占据核心位置，是因为它被认为是衡量"语言理解"的一个好的代理任务（proxy task）。正确判断蕴含关系需要词汇知识、句法理解、逻辑推理甚至世界知识，几乎涵盖了语言理解的所有方面。因此，NLI 成为 GLUE 和 SuperGLUE 评测基准的核心组成部分，也是 BERT 展示其理解能力的第一个舞台。

代表性 benchmarks 包括 **SNLI**（Stanford NLI，570k 样本）和 **MNLI**（Multi-Genre NLI，433k 样本，覆盖多种文体），评测指标是准确率。

### 主题分类与其他分类任务

除了情感和推理，文本分类还包括主题分类（"这篇新闻是关于体育、政治还是科技？"）、意图识别（"用户这句话是在询问、投诉还是下单？"）、有害内容检测（"这条评论是否包含仇恨言论？"）等。代表性 benchmarks 如 **AG News**（4 类新闻分类）和 **DBpedia**（14 类主题分类）。

所有这些分类任务共享一个 pre-DL 时代的痛点：模型的表现严重依赖特征工程的质量。研究者需要手工设计文本特征——词袋、TF-IDF 加权、n-gram 组合、情感词典匹配——然后喂给逻辑回归或 SVM 等分类器。每个任务、每个领域都需要不同的特征，而且特征的上限就是模型的上限。这正是第 1 章所说的"特征工程的诅咒"在分类任务上的具体体现。

---

## 序列标注任务

序列标注是另一类基础 NLP 任务：给定一个词序列，为序列中的每个位置分配一个标签。输入和输出等长，标签之间往往存在依赖关系——前一个词的标签会影响后一个词的标签。

### 词性标注

词性标注（Part-of-Speech Tagging, POS Tagging）是 NLP 最古老的任务之一：为句子中的每个词标注其语法角色——名词、动词、形容词、副词等。例如：

$$
\text{The/DT} \quad \text{cat/NN} \quad \text{sat/VBD} \quad \text{on/IN} \quad \text{the/DT} \quad \text{mat/NN}
$$

词性标注看似简单，但歧义无处不在。"book"既可以是名词（"一本书"）也可以是动词（"预订"）。"running"既可以是动词的现在分词也可以是形容词。消解这些歧义需要利用上下文信息，这正是 HMM 和 CRF 在第 1 章中解决的问题。

词性标注的评测指标是 **token-level accuracy**——所有词中被正确标注的比例。在 Penn Treebank 数据集上，当前最优方法的准确率已经超过 97%，接近人类标注者之间的一致性（约 97%）。从某种意义上说，词性标注已经被"解决"了。

### 命名实体识别

命名实体识别（Named Entity Recognition, NER）是序列标注中更有实用价值的任务：从文本中识别出人名（PER）、地名（LOC）、组织名（ORG）等命名实体。例如：

$$
\text{[Barack Obama]}_{\text{PER}} \text{ was born in } \text{[Honolulu]}_{\text{LOC}} \text{, } \text{[Hawaii]}_{\text{LOC}}
$$

NER 通常使用 **BIO 标注方案**：B（Begin）表示实体的开始，I（Inside）表示实体的内部，O（Outside）表示非实体。例如 "Barack Obama" 标注为 "B-PER I-PER"。这种方案把 NER 转化为了标准的序列标注问题。

NER 的评测比词性标注更严格：使用 **span-level F1**——只有实体的边界和类型都完全正确才算预测正确。代表性 benchmark 是 **CoNLL-2003**（英语和德语新闻语料），以及更大规模的 **OntoNotes 5.0**（覆盖 18 种实体类型）。

值得注意的是，真实世界的 NER 比 benchmark 上看起来要困难得多。嵌套实体（"中国科学院上海分院"中"上海"本身也是一个 LOC 实体）、不连续实体、新出现的实体（新公司、新人物）、领域迁移（医疗文本中的实体与新闻文本完全不同），这些挑战至今没有完美的解决方案。

### 序列标注的传统痛点

在第 1 章中，我们已经看到 HMM 和 CRF 是序列标注的主流方法。CRF 的优势在于可以整合丰富的特征——当前词的词形、前缀后缀、是否大写、是否包含数字、前后词的特征、甚至外部词典——并通过全局归一化建模标签之间的依赖。

但问题在于：**这些特征全部需要手工设计**。NER 领域的研究者花了数年时间打磨特征模板（feature template），每个语言、每个领域都需要不同的特征集。一个为英语新闻设计的特征模板，搬到中文医疗文本上就完全不管用了。这正是深度学习方法（BiLSTM-CRF、BERT-CRF）后来之所以革命性的原因——它们自动学习特征表示，不再依赖手工设计。

---

## 序列到序列任务

序列到序列（Sequence-to-Sequence, Seq2Seq）任务是 NLP 中最复杂也最激动人心的一类：输入一个序列，生成另一个可能不等长的序列。这类任务直接催生了 Encoder-Decoder 架构（第 5 章）、Attention 机制（第 6 章）和 Transformer（第 8 章），可以说是推动现代 NLP 架构演进的最重要力量。

### 机器翻译

机器翻译（Machine Translation, MT）是 NLP 历史上最重要的应用之一，也是推动技术进步最强劲的引擎。从 1950 年代的基于规则的翻译，到 1990 年代的统计机器翻译（SMT），再到 2014 年的神经机器翻译（NMT），每一次 NLP 的范式转变都首先在翻译任务上显现。

Attention 机制诞生于 Bahdanau et al. (2014) 对 NMT 的改进。Transformer 架构诞生于 Vaswani et al. (2017) 的论文 "Attention Is All You Need"，其实验全部在翻译任务上进行。可以说，如果没有机器翻译这个"磨刀石"，现代 NLP 的面貌会完全不同。

MT 的标准评测指标是 **BLEU**（Bilingual Evaluation Understudy），它通过计算机器翻译输出与人类参考翻译之间的 n-gram 重叠度来评估翻译质量。BLEU 虽然有诸多局限（不考虑语义、对同义表达不敏感），但因其自动化和可复现性，长期是 MT 领域的标准指标。近年来，**COMET** 等基于预训练模型的评测指标开始取代 BLEU，因为它们与人类判断的相关性更高。

代表性 benchmark 是每年举办的 **WMT**（Workshop on Machine Translation）共享任务，覆盖数十种语言对，是 MT 研究的"奥运会"。

### 文本摘要

文本摘要（Summarization）是另一个重要的 Seq2Seq 任务：将一篇长文档压缩为简短的摘要，保留核心信息。

这个任务有两种范式。**抽取式摘要**（Extractive）从原文中选择最重要的句子组成摘要，不生成新的文字——可以看作一个句子级别的二分类问题（选 vs 不选）。**生成式摘要**（Abstractive）用自己的语言重新表述核心信息，可能产生原文中没有出现过的词句——这是一个真正的 Seq2Seq 问题，也更接近人类写摘要的方式。

评测指标 **ROUGE**（Recall-Oriented Understudy for Gisting Evaluation）衡量生成摘要与参考摘要之间的 n-gram 重叠度，其中 ROUGE-1（unigram）、ROUGE-2（bigram）和 ROUGE-L（最长公共子序列）是最常用的变体。代表性 benchmarks 包括 **CNN/DailyMail**（新闻摘要）和 **XSum**（极端摘要，要求一句话概括全文）。

### 对话系统

对话系统是 Seq2Seq 任务的另一个重要分支。**任务型对话**（Task-oriented Dialogue）有明确的目标（订机票、查天气），系统需要理解用户意图、管理对话状态、调用后端服务。**开放域对话**（Open-domain Dialogue）没有固定目标，系统需要像人一样自然地闲聊。

对话系统的评测是出了名的困难。BLEU 和 ROUGE 都不适合评估对话质量——一个好的回复可以完全不包含参考回复中的词汇。人类评估（流畅性、相关性、信息量）目前仍然是最可靠的评测方式，但成本高昂且难以复现。这也是为什么第 22 章会专门讨论评测方法论。

### 序列到序列的传统痛点

在深度学习之前，机器翻译使用的是统计机器翻译（SMT）框架，它包含多个独立训练的组件：词对齐模型、短语翻译表、语言模型、重排序模型。每个组件都需要专门设计，整个系统的训练和调优极其复杂。IBM Model 1-5 的词对齐算法、基于短语的翻译模型（Phrase-based SMT）——这些工作推动了 NLP 的发展，但也展示了 pipeline 系统的固有局限：每个组件独立优化，整体未必最优。

2014 年 Sutskever et al. 提出的 Seq2Seq 模型用一个端到端（end-to-end）的神经网络替代了整个 SMT pipeline，这是一次根本性的简化。但新的问题随之而来——所有信息被压缩到一个固定长度的向量中，这就是第 5 章将讨论的"信息瓶颈"问题。

---

## 结构预测与信息抽取

并非所有 NLP 任务的输出都是标签或文本序列。有些任务需要输出结构化对象——树、图、关系三元组——这类任务对模型的结构化推理能力提出了更高要求。

### 句法分析

句法分析（Parsing）是理解语言结构的核心任务。它有两种主要形式。

**依存句法分析**（Dependency Parsing）揭示词与词之间的支配关系。在"猫追老鼠"中，"追"是根节点，"猫"是"追"的主语依存（nsubj），"老鼠"是"追"的宾语依存（dobj）。输出是一棵以谓词为根的依存树。评测指标是 **UAS**（Unlabeled Attachment Score，只看依存方向是否正确）和 **LAS**（Labeled Attachment Score，还要求依存类型正确），标准 benchmark 是 Penn Treebank 和 Universal Dependencies（UD）。

**成分句法分析**（Constituency Parsing）将句子分解为嵌套的短语结构。"[S [NP The cat] [VP [V sat] [PP on [NP the mat]]]]"。评测指标是 Parseval 的 Precision/Recall/F1，衡量预测的短语边界与参考标注的匹配度。

句法分析为什么重要？因为语言的很多现象——长距离依赖、歧义消解、语义角色——都建立在句法结构之上。"I saw the man with the telescope"（我用望远镜看到了那个人？还是我看到了拿望远镜的人？）的歧义本质上是句法结构的歧义。

### 信息抽取

信息抽取（Information Extraction, IE）是从非结构化文本中提取结构化知识。

**关系抽取**（Relation Extraction）识别实体之间的语义关系。例如从"乔布斯创办了苹果公司"中提取三元组（乔布斯, 创办, 苹果公司）。代表性 benchmark 是 **TACRED**（42 种关系类型，106k 样本）。

**事件抽取**（Event Extraction）更进一步，识别事件及其参与者。例如从"特斯拉上海工厂于 2019 年 1 月奠基"中提取：事件类型=建设，地点=上海，时间=2019 年 1 月，主体=特斯拉。代表性 benchmark 是 **ACE 2005**。

### 共指消解

共指消解（Coreference Resolution）解决的是"他/她/它/这/那"指的是谁的问题。在"玛丽把书还给了约翰。**她**说这本书很好看"中，模型需要判断"她"指的是"玛丽"。这个任务需要跨越句子边界的推理，对模型的上下文理解能力要求很高。代表性 benchmark 是 **CoNLL-2012 Shared Task**。

### 结构预测的传统痛点

结构预测任务的搜索空间远比分类或序列标注任务大得多。一个长度为 $n$ 的句子，可能的依存树数量是指数级的。传统方法需要精心设计搜索算法（如 Eisner 算法、CKY 算法）和复杂的特征模板。每种结构预测任务都有自己的一套算法和特征体系，知识很难在任务之间共享。

---

## 问答与阅读理解

问答系统（Question Answering, QA）是最直观地体现"语言理解"的任务：给模型一个问题，让它给出答案。这个任务之所以重要，不仅因为它有直接的应用价值（搜索引擎、智能助手），更因为它被认为是衡量机器语言理解能力的"终极测试"——如果一个模型能正确回答各种问题，它一定在某种程度上"理解"了语言。

### 抽取式问答

抽取式 QA 是最常见的形式化方式：给定一篇文档和一个问题，从文档中找到包含答案的最小文本片段（span）。例如：

> **文档**：Nikola Tesla（1856-1943）是一位塞尔维亚裔美国发明家...他于 1856 年 7 月 10 日出生于...
>
> **问题**：Tesla 出生于哪一年？
>
> **答案**：1856（即文档中从"1856"开始、到"1856"结束的 span）

这个形式化的优雅之处在于：它把开放式的"回答问题"转化为一个定义明确的"找起止位置"问题。模型只需预测答案的起始和结束位置，这大大简化了问题。

代表性 benchmarks 包括 **SQuAD**（Stanford Question Answering Dataset，100k+ 问答对）和 **Natural Questions**（Google 搜索日志中的真实问题）。评测指标是 **Exact Match (EM)**（答案是否与参考完全一致）和 **F1**（答案与参考之间的 token 级别 F1）。

SQuAD 在 2016 年发布后迅速成为 NLP 最热门的 benchmark——BERT 在 2018 年发布时，其在 SQuAD 上超越人类水平的结果震动了整个社区。

### 生成式问答与开放域问答

抽取式 QA 的局限是：答案必须是文档中的原文。但很多问题的答案需要推理、整合甚至计算，无法直接从原文中提取。例如"如果 A 比 B 高，B 比 C 高，那么 A 和 C 谁更高？"——答案"A 更高"可能不出现在任何文档中。

生成式 QA 允许模型自由生成答案文本，但评测变得更加困难——如何判断一个自由生成的答案是否正确？这个问题至今没有完美的解决方案。

开放域 QA 则更进一步：没有给定的文档，模型需要自己检索相关信息并生成答案。这直接催生了第 31 章将讨论的检索增强生成（RAG）架构。

### 评测基准的演进

QA 和阅读理解领域的 benchmark 演进，折射出 NLP 能力评测的整体变迁。

早期的 **GLUE**（2018）和 **SuperGLUE**（2019）是多任务评测基准，包含 NLI、QA、共指消解等多种任务，旨在全面评估模型的语言理解能力。BERT 发布后不到两年，GLUE 上的最优分数已经超越人类水平，SuperGLUE 也在 2021 年被"刷满"。

**MMLU**（Massive Multitask Language Understanding，2021）通过 57 个学科（从法律到医学到数学）的多选题来评估大语言模型的知识广度。**BigBench**（2022）和 **HELM**（2022）尝试探测涌现能力和多维度表现。评测的详细讨论将在第 22 章展开，这里只需记住一个趋势：**benchmark 越来越难、越来越综合，但也越来越容易被数据污染和过度优化**。

---

## 从任务专用到通用模型：一条演进主线

回顾上述所有任务，一个贯穿始终的痛点浮出水面。

在 pre-DL 时代，每个任务都需要独立的解决方案。情感分析用词袋+逻辑回归，NER 用手工特征+CRF，机器翻译用对齐模型+短语翻译表+语言模型，句法分析用 Eisner 算法+结构化感知器。任务之间的方法几乎不可复用。一个在 NER 上积累的经验，对翻译没有什么帮助。

这与人类的语言能力形成鲜明对比——一个人学会了阅读，就自然获得了理解、推理、翻译、总结的基础能力。知识在任务之间自由迁移。

深度学习的革命性正在于逐步打破了任务之间的壁垒。本书后续的章节将展示这一过程是如何发生的：

**第一步：学习表示**（第 3-5 章）。Word2Vec 学到的词向量可以作为各种任务的通用输入特征，不再需要手工设计。但每个任务仍然需要独立的模型架构。

**第二步：学习注意力**（第 6-9 章）。Attention 和 Transformer 提供了一种通用的序列建模架构，适用于翻译、分类、标注等多种任务。但每个任务仍然需要从头训练。

**第三步：学习预训练**（第 10-16 章）。ELMo、BERT、GPT 通过在大规模语料上预训练，学到了通用的语言表示。下游任务只需要少量标注数据进行微调。"一次预训练，到处微调"成为新范式。

**第四步：学习规模**（第 17 章及之后）。GPT-3 展示了一个足够大的语言模型可以零样本或少样本完成几乎所有 NLP 任务，连微调都不需要了。"一个模型解所有问题"不再是梦想。

这条演进主线——从任务专用到通用——是理解本书后续所有章节的核心线索。下一章将迈出第一步：从手工特征走向自动学习的分布式表示。

### 任务-模型-评测速查表

下表汇总了本章介绍的主要 NLP 任务，供后续章节参考。

| 任务 | I/O 模式 | 经典方法 | 深度学习方法 | 评测指标 | 代表性 Benchmark | 本书章节 |
|------|----------|----------|-------------|---------|-----------------|---------|
| 情感分析 | Text → Label | BoW + SVM | CNN, BERT Fine-tune | Accuracy | SST-2, IMDb | Ch1, Ch13 |
| 自然语言推理 | Text Pair → Label | 特征 + 分类器 | BERT, RoBERTa | Accuracy | SNLI, MNLI | Ch13, Ch15 |
| 词性标注 | Seq → Label/token | HMM, CRF | BiLSTM-CRF, BERT | Token Accuracy | Penn Treebank | Ch1, Ch5 |
| 命名实体识别 | Seq → Label/token | CRF + 特征模板 | BiLSTM-CRF, BERT | Span F1 | CoNLL-2003 | Ch1, Ch5, Ch13 |
| 机器翻译 | Seq → Seq | SMT (短语) | Seq2Seq + Attn, Transformer | BLEU, COMET | WMT | Ch5, Ch6, Ch8 |
| 文本摘要 | Seq → Seq | 抽取式 | Pointer-Gen, BART, T5 | ROUGE | CNN/DM, XSum | Ch14, Ch16 |
| 依存句法分析 | Seq → Tree | MST, Eisner | BiLSTM, BERT | UAS/LAS | UD, PTB | Ch1 |
| 关系抽取 | Text → Triplet | 规则, SVM | CNN, BERT | F1 | TACRED, ACE | — |
| 共指消解 | Doc → Clusters | 规则, 统计 | SpanBERT, Coref Model | CoNLL F1 | CoNLL-2012 | — |
| 阅读理解 | Text + Q → Span | 规则 | BiDAF, BERT | EM, F1 | SQuAD, NQ | Ch13 |
| 开放域 QA | Q → Answer | IR + Extract | RAG, LLM | EM, F1 | NQ, TriviaQA | Ch20, Ch31 |
| 对话 | Context → Response | 模板, 检索 | Seq2Seq, LLM | 人类评估 | — | Ch20, Ch23 |

---

## 本章小结

### 核心要点回顾

本章勾勒了 NLP 任务的全景地图。我们看到，"处理自然语言"这个笼统的目标背后，是一系列输入/输出模式截然不同的任务。文本分类将一段文本映射到离散标签，序列标注为每个 token 分配标签，序列到序列把一个序列转化为另一个序列，结构预测输出树或图，问答从文本中定位答案。

这些任务的共同痛点是：在深度学习之前，每个任务都需要独立的特征工程、独立的模型架构、独立的算法设计。这种"任务孤岛"模式严重限制了 NLP 的发展速度和可扩展性。

接下来几章将展示深度学习如何一步步打破这些壁垒——从通用的词向量（第 3 章），到通用的序列建模架构（第 5-8 章），到通用的预训练表示（第 10-16 章），最终到通用的大语言模型（第 17 章及之后）。

### 思考题

1. **[概念理解]** 为什么 NLI 被认为是衡量"语言理解"的好代理任务？它能衡量哪些方面的理解能力？又有什么局限？

2. **[批判性思考]** BLEU 通过 n-gram 重叠度来评估翻译质量。这种评测方式有什么根本性的局限？你能举一个 BLEU 分数高但翻译质量差（或反过来）的例子吗？

3. **[研究思考]** Sun et al. (2022) 发现同一个 NLP 任务可以用不同的建模范式（分类、序列标注、生成等）来解决。在大语言模型时代，所有任务是否都应该统一到生成范式下？这样做有什么优势和代价？

4. **[开放思考]** 本章的"任务-模型-评测速查表"隐含了一个假设：每个任务都有一个（或几个）"正确的"评测指标。但 BLEU、ROUGE、F1 真的能衡量模型在这些任务上的"真实能力"吗？什么才是好的评测？

---

## 延伸阅读

### 核心教材

- **Speech and Language Processing, 3rd Edition (Jurafsky & Martin)**：NLP 领域的权威教材，按任务组织章节，每个任务都有深入覆盖
  - 重点读：Ch 4（分类）、Ch 17（序列标注）、Ch 12（翻译）
  - [web.stanford.edu/~jurafsky/slp3](https://web.stanford.edu/~jurafsky/slp3/)

### 综述论文

- **"Paradigm Shift in Natural Language Processing" (Sun et al., 2022)**：系统研究了 NLP 任务的建模范式演变，提出了七大范式分类框架
  - 重点读：Section 3（各任务的范式迁移分析）

### 评测基准

- **GLUE / SuperGLUE**：多任务 NLU 评测基准，包含本章介绍的多种任务
  - [gluebenchmark.com](https://gluebenchmark.com/) / [super.gluebenchmark.com](https://super.gluebenchmark.com/)
- **SQuAD**：阅读理解的标准 benchmark
  - [rajpurkar.github.io/SQuAD-explorer](https://rajpurkar.github.io/SQuAD-explorer/)

### 课程资源

- **Stanford CS224N Lecture 1-2**：NLP 全景概述和基础任务介绍
  - [web.stanford.edu/class/cs224n](https://web.stanford.edu/class/cs224n/)
- **CMU 11-711 ANLP Lecture 1-2**：NLP 任务分类和现代方法概述
  - [phontron.com/class/anlp-fall2024](https://phontron.com/class/anlp-fall2024/)
