---
title: "ç¬¬7ç« ï¼šSelf-Attentionçš„çªç ´"
subtitle: "å½“åºåˆ—å¼€å§‹å®¡è§†è‡ªå·±"
author: "Ying Zha"
date: "2026-01-25"
categories: [NLP, Attention, Self-Attention, Memory Networks, ä½ç½®ç¼–ç ]
tags: [è‡ªæ³¨æ„åŠ›, Q-K-V, ä½ç½®ç¼–ç , æ­£å¼¦ç¼–ç , ç½®æ¢ä¸å˜æ€§, Decomposable Attention]
description: "ä»è·¨åºåˆ—æ³¨æ„åŠ›åˆ°è‡ªæ³¨æ„åŠ›ï¼šåºåˆ—å¦‚ä½•å®¡è§†è‡ªå·±ï¼ŒQ-K-Væ¡†æ¶çš„å»ºç«‹ï¼Œä»¥åŠä½ç½®ä¿¡æ¯ç¼ºå¤±çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    css: styles.css
    fig-cap-location: bottom
---

> **æ ¸å¿ƒé—®é¢˜**ï¼šåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®èƒ½å¦ç›´æ¥å…³æ³¨åŒä¸€åºåˆ—ä¸­çš„å…¶ä»–ä½ç½®ï¼Ÿå¦‚æœå¯ä»¥ï¼Œè¿™ç§"è‡ªæˆ‘å®¡è§†"çš„æœºåˆ¶ä¼šå¸¦æ¥ä»€ä¹ˆçªç ´ï¼Œåˆä¼šä¸¢å¤±ä»€ä¹ˆï¼Ÿ
>
> **å†å²åæ ‡**ï¼š2015-2017 | Memory Networks, Decomposable Attention | ä»è¾…åŠ©æœºåˆ¶åˆ°æ ¸å¿ƒæ¶æ„çš„è½¬å˜

---

## ä»ä¸Šä¸€ç« è¯´èµ·

ä¸Šä¸€ç« æˆ‘ä»¬ç³»ç»Ÿæ¢ç´¢äº†Attentionæœºåˆ¶çš„è®¾è®¡ç©ºé—´ï¼šåŠ æ€§ä¸ä¹˜æ€§å¯¹é½å‡½æ•°çš„æƒè¡¡ã€å…¨å±€ä¸å±€éƒ¨èŒƒå›´çš„å–èˆã€è½¯æ³¨æ„åŠ›ä¸ç¡¬æ³¨æ„åŠ›çš„å·®å¼‚ã€‚Luongçš„å·¥ä½œç¡®ç«‹äº†ç‚¹ç§¯æ³¨æ„åŠ›çš„æ•ˆç‡ä¼˜åŠ¿ï¼Œä¸ºåæ¥çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚ä½†æ— è®ºæ˜¯Bahdanauè¿˜æ˜¯Luongï¼Œä»–ä»¬çš„Attentionéƒ½æœ‰ä¸€ä¸ªå…±åŒçš„ç‰¹ç‚¹â€”â€”å®ƒæ˜¯**è·¨åºåˆ—**çš„ï¼Œè®©è§£ç å™¨å…³æ³¨ç¼–ç å™¨ã€‚

ç„¶è€Œï¼Œä¸Šä¸€ç« çš„ç»“å°¾ç•™ä¸‹äº†ä¸¤ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚

ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼š**Attentionèƒ½å¦ç‹¬ç«‹äºRNNï¼Ÿ** å½“å‰çš„Attentionåªæ˜¯RNNæ¶æ„çš„å¢å¼ºç»„ä»¶ã€‚ç¼–ç å™¨æ˜¯RNNï¼Œè§£ç å™¨ä¹Ÿæ˜¯RNNï¼ŒAttentionåªæ˜¯åœ¨å®ƒä»¬ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚é¡ºåºè®¡ç®—çš„ç“¶é¢ˆã€é•¿è·ç¦»ä¾èµ–çš„æ¢¯åº¦é—®é¢˜ï¼Œè¿™äº›RNNçš„å›ºæœ‰ç¼ºé™·å¹¶æ²¡æœ‰è¢«Attentionè§£å†³ã€‚

ç¬¬äºŒä¸ªé—®é¢˜æ›´åŠ æ ¹æœ¬ï¼š**åŒä¸€åºåˆ—å†…éƒ¨çš„ä½ç½®èƒ½å¦ç›¸äº’å…³æ³¨ï¼Ÿ** è€ƒè™‘è¿™å¥è¯ï¼š"The animal didn't cross the street because it was too tired." è¦ç†è§£"it"æŒ‡ä»£ä»€ä¹ˆï¼Œéœ€è¦å»ºç«‹"it"ä¸"animal"ä¹‹é—´çš„è”ç³»ã€‚åœ¨å½“å‰çš„RNNç¼–ç å™¨ä¸­ï¼Œè¿™ç§è”ç³»åªèƒ½é€šè¿‡é€æ­¥ä¼ é€’æ¥éšå¼å»ºç«‹ã€‚ä½†å¦‚æœ"it"èƒ½å¤Ÿç›´æ¥"çœ‹åˆ°"å¥å­ä¸­çš„å…¶ä»–è¯ï¼Œç›´æ¥è®¡ç®—ä¸"animal"å’Œ"street"çš„ç›¸å…³æ€§ï¼Œç†è§£ä¸æ˜¯ä¼šæ›´åŠ ç›´æ¥å—ï¼Ÿ

è¿™å°±å¼•å‡ºäº†æœ¬ç« çš„ä¸»è§’ï¼š**Self-Attention**â€”â€”è®©åºåˆ—"å®¡è§†è‡ªå·±"çš„æœºåˆ¶ã€‚

> ğŸ’¡ **æœ¬ç« æ ¸å¿ƒæ´å¯Ÿ**ï¼šSelf-Attentionè®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®å¯ä»¥ç›´æ¥å…³æ³¨åŒä¸€åºåˆ—çš„æ‰€æœ‰å…¶ä»–ä½ç½®ï¼Œä¸€æ­¥åˆ°ä½åœ°å»ºç«‹ä»»æ„è·ç¦»çš„ä¾èµ–å…³ç³»ã€‚è¿™ä¸ä»…çªç ´äº†RNNçš„é¡ºåºç“¶é¢ˆï¼Œè¿˜ä¸ºå®Œå…¨å¹¶è¡Œçš„è®¡ç®—æ‰“å¼€äº†å¤§é—¨ã€‚ä½†è¿™ç§èƒ½åŠ›æ˜¯æœ‰ä»£ä»·çš„â€”â€”Self-Attentionå¤©ç”Ÿä¸¢å¤±äº†ä½ç½®ä¿¡æ¯ï¼Œå¦‚ä½•å¼¥è¡¥è¿™ä¸€ç¼ºé™·å°†æˆä¸ºå…³é”®çš„è®¾è®¡æŒ‘æˆ˜ã€‚

---

## é—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ

### é—®é¢˜çš„ç²¾ç¡®å®šä¹‰

æˆ‘ä»¬è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼š**å¦‚ä½•è®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®éƒ½èƒ½è·å¾—å…³äºæ•´ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Ÿ**

æ›´æ­£å¼åœ°è¯´ï¼Œç»™å®šè¾“å…¥åºåˆ— $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)$ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºæ¯ä¸ªä½ç½® $i$ ç”Ÿæˆä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤º $\mathbf{z}_i$ï¼Œä½¿å¾— $\mathbf{z}_i$ èƒ½å¤Ÿç¼–ç æ¥è‡ªæ•´ä¸ªåºåˆ—çš„ç›¸å…³ä¿¡æ¯ã€‚

åœ¨RNNä¸­ï¼Œè¿™æ˜¯é€šè¿‡é¡ºåºè®¡ç®—å®ç°çš„ï¼š

$$
\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)
$$

éšè—çŠ¶æ€ $\mathbf{h}_t$ éšå¼åœ°"å‹ç¼©"äº†ä½ç½® $1$ åˆ° $t$ çš„æ‰€æœ‰ä¿¡æ¯ã€‚åŒå‘RNNåˆ™åŒæ—¶è€ƒè™‘å‰å‘å’Œåå‘çš„ä¿¡æ¯ã€‚ä½†è¿™ç§æ–¹å¼æœ‰ä¸¤ä¸ªæ ¹æœ¬é—®é¢˜ï¼š

**é¡ºåºä¾èµ–**ï¼š$\mathbf{h}_t$ å¿…é¡»ç­‰ $\mathbf{h}_{t-1}$ è®¡ç®—å®Œæˆï¼Œæ— æ³•å¹¶è¡Œã€‚

**é—´æ¥è¿æ¥**ï¼šä½ç½® $1$ çš„ä¿¡æ¯è¦åˆ°è¾¾ä½ç½® $n$ï¼Œå¿…é¡»ç»è¿‡ $n-1$ æ¬¡éçº¿æ€§å˜æ¢ã€‚æ¯æ¬¡å˜æ¢éƒ½ä¼šå¸¦æ¥ä¿¡æ¯æŸå¤±ã€‚

### ä¸€ä¸ªå…³é”®çš„æ€æƒ³å®éªŒ

è®©æˆ‘ä»¬åšä¸€ä¸ªæ€æƒ³å®éªŒã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¥å­ "I love natural language processing"ï¼ŒåŒ…å«5ä¸ªè¯ã€‚ç°åœ¨è€ƒè™‘ä¸¤ç§æ–¹å¼æ¥å»ºç«‹è¯ä¸è¯ä¹‹é—´çš„è”ç³»ï¼š

**RNNæ–¹å¼**ï¼š

```
I â†’ love â†’ natural â†’ language â†’ processing
```

"I" çš„ä¿¡æ¯è¦åˆ°è¾¾ "processing"ï¼Œéœ€è¦ç»è¿‡ 4 æ­¥ä¼ é€’ã€‚æ¯ä¸€æ­¥ï¼Œä¿¡æ¯éƒ½ä¸æ–°çš„è¾“å…¥æ··åˆï¼Œç»è¿‡éçº¿æ€§å˜æ¢ã€‚æœ€ç»ˆåˆ°è¾¾ "processing" æ—¶ï¼Œå…³äº "I" çš„ä¿¡æ¯å·²ç»è¢«å¤§é‡ç¨€é‡Šã€‚

**ç†æƒ³æ–¹å¼**ï¼š

```
     I â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ processing
       â†–                       â†—
         love â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ language
               â†–       â†—
                 natural
```

æ¯ä¸ªè¯éƒ½å¯ä»¥ç›´æ¥"çœ‹åˆ°"å…¶ä»–æ‰€æœ‰è¯ã€‚"processing" æƒ³è¦çŸ¥é“ "I" çš„ä¿¡æ¯ï¼Ÿç›´æ¥æŸ¥è¯¢ï¼Œä¸€æ­¥åˆ°ä½ã€‚

è¿™å°±æ˜¯Self-Attentionçš„æ ¸å¿ƒæ€æƒ³ï¼š**æŠŠ $O(n)$ çš„é—´æ¥è·¯å¾„å˜æˆ $O(1)$ çš„ç›´æ¥è·¯å¾„**ã€‚

### æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„è§£å†³æ–¹æ¡ˆï¼Ÿ

ä»ä¸Šè¿°åˆ†æå¯ä»¥çœ‹å‡ºï¼Œç†æƒ³çš„åºåˆ—å»ºæ¨¡æœºåˆ¶åº”è¯¥æ»¡è¶³ä»¥ä¸‹ç‰¹æ€§ï¼š

1. **ç›´æ¥è¿æ¥**ï¼šä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’æ˜¯ä¸€æ­¥åˆ°ä½çš„ï¼Œè€Œéé€æ­¥ä¼ é€’
2. **å¹¶è¡Œè®¡ç®—**ï¼šä¸åŒä½ç½®çš„è¡¨ç¤ºå¯ä»¥åŒæ—¶è®¡ç®—ï¼Œä¸éœ€è¦ç­‰å¾…å‰åºè®¡ç®—å®Œæˆ
3. **åŠ¨æ€æƒé‡**ï¼šå…³æ³¨çš„å¼ºåº¦åº”è¯¥å–å†³äºå†…å®¹çš„ç›¸å…³æ€§ï¼Œè€Œéå›ºå®šçš„ä½ç½®å…³ç³»
4. **å…¨å±€è§†é‡**ï¼šæ¯ä¸ªä½ç½®éƒ½èƒ½çœ‹åˆ°æ•´ä¸ªåºåˆ—ï¼Œè€Œéåªæœ‰å±€éƒ¨çª—å£

Self-Attentionæ­£æ˜¯ä¸ºäº†åŒæ—¶æ»¡è¶³è¿™å››ä¸ªéœ€æ±‚è€Œè®¾è®¡çš„ã€‚

---

## æ ¸å¿ƒæ€æƒ³ä¸ç›´è§‰

### ä»Cross-Attentionåˆ°Self-Attention

å›é¡¾ä¸€ä¸‹Seq2Seqä¸­çš„Attentionæœºåˆ¶ï¼š

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j^{(enc)}
$$

å…¶ä¸­ $\alpha_{ij}$ è¡¡é‡è§£ç å™¨ä½ç½® $i$ å¯¹ç¼–ç å™¨ä½ç½® $j$ çš„å…³æ³¨ç¨‹åº¦ã€‚è¿™é‡Œçš„å…³é”®æ˜¯ï¼šQueryæ¥è‡ªè§£ç å™¨ï¼ŒKeyå’ŒValueæ¥è‡ªç¼–ç å™¨â€”â€”è¿™æ˜¯**è·¨åºåˆ—**çš„æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰ã€‚

Self-Attentionçš„æƒ³æ³•æå…¶ç®€å•ï¼š**è®©Queryã€Keyã€Valueéƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—**ã€‚

$$
\mathbf{z}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{x}_j
$$

å…¶ä¸­ $\alpha_{ij}$ è¡¡é‡ä½ç½® $i$ å¯¹ä½ç½® $j$ çš„å…³æ³¨ç¨‹åº¦ã€‚ç°åœ¨ï¼Œåºåˆ—åœ¨"å®¡è§†è‡ªå·±"â€”â€”æ¯ä¸ªä½ç½®éƒ½åœ¨é—®ï¼š"æˆ‘åº”è¯¥å…³æ³¨åºåˆ—ä¸­çš„å“ªäº›å…¶ä»–ä½ç½®ï¼Ÿ"

### Query-Key-Valueçš„ä¸‰å…ƒç»„è§†è§’

ä¸ºäº†è®©Self-Attentionæ›´åŠ çµæ´»ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ç”¨åŸå§‹è¡¨ç¤ºè®¡ç®—æ³¨æ„åŠ›ï¼Œè€Œæ˜¯å…ˆè¿›è¡Œçº¿æ€§å˜æ¢ï¼š

$$
\mathbf{q}_i = \mathbf{W}_Q \mathbf{x}_i, \quad \mathbf{k}_i = \mathbf{W}_K \mathbf{x}_i, \quad \mathbf{v}_i = \mathbf{W}_V \mathbf{x}_i
$$

ç„¶åï¼š

$$
\alpha_{ij} = \text{softmax}_j\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right)
$$

$$
\mathbf{z}_i = \sum_{j=1}^{n} \alpha_{ij} \mathbf{v}_j
$$

è¿™ç§Query-Key-Valueçš„ä¸‰å…ƒç»„ç»“æ„æœ‰æ·±åˆ»çš„ç›´è§‰æ„ä¹‰ï¼š

- **Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼šä»£è¡¨"æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ"â€”â€”ä½ç½® $i$ æƒ³è¦è·å–ä»€ä¹ˆæ ·çš„ä¿¡æ¯
- **Keyï¼ˆé”®ï¼‰**ï¼šä»£è¡¨"æˆ‘æ˜¯ä»€ä¹ˆï¼Ÿ"â€”â€”ä½ç½® $j$ çš„èº«ä»½æ ‡è¯†ï¼Œç”¨äºåŒ¹é…æŸ¥è¯¢
- **Valueï¼ˆå€¼ï¼‰**ï¼šä»£è¡¨"æˆ‘èƒ½æä¾›ä»€ä¹ˆï¼Ÿ"â€”â€”ä½ç½® $j$ å®é™…è´¡çŒ®çš„å†…å®¹

è¿™ä¸ªè®¾è®¡å…è®¸åŒä¸€ä¸ªä½ç½®ä»¥ä¸åŒçš„"èº«ä»½"å‚ä¸è®¡ç®—ï¼šå½“å®ƒä½œä¸ºæŸ¥è¯¢è€…æ—¶ï¼Œå®ƒåœ¨å¯»æ‰¾ç›¸å…³ä¿¡æ¯ï¼›å½“å®ƒä½œä¸ºè¢«æŸ¥è¯¢è€…æ—¶ï¼Œå®ƒçš„Keyå†³å®šæ˜¯å¦è¢«é€‰ä¸­ï¼Œå®ƒçš„Valueå†³å®šè´¡çŒ®ä»€ä¹ˆå†…å®¹ã€‚

### ä¸€ä¸ªå…·ä½“çš„ä¾‹å­

è€ƒè™‘å¥å­ "The cat sat on the mat"ï¼Œæˆ‘ä»¬æƒ³ç†è§£è¯ "sat" çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚

åœ¨Self-Attentionä¸­ï¼Œ"sat" ä¼šï¼š

1. ç”Ÿæˆä¸€ä¸ªQueryå‘é‡ï¼Œç¼–ç "åŠ¨è¯'sat'åœ¨å¯»æ‰¾å®ƒçš„ä¸»è¯­å’Œå®¾è¯­"
2. ä¸æ‰€æœ‰è¯çš„Keyå‘é‡è®¡ç®—ç›¸ä¼¼åº¦
3. ä¸ "cat" çš„Keyé«˜åº¦åŒ¹é…ï¼ˆ"cat"æ˜¯ä¸»è¯­ï¼‰ï¼Œä¸ "mat" ä¹Ÿæœ‰ä¸€å®šåŒ¹é…ï¼ˆ"mat"æ˜¯ä»‹è¯å®¾è¯­çš„å¯¹è±¡ï¼‰
4. æœ€ç»ˆçš„è¡¨ç¤ºæ˜¯æ‰€æœ‰è¯çš„Valueçš„åŠ æƒå’Œï¼Œ"cat" å’Œ "mat" è´¡çŒ®è¾ƒå¤š

è¿™ä¸ªè¿‡ç¨‹å®Œå…¨æ˜¯åŸºäºå†…å®¹çš„â€”â€”æ¨¡å‹å­¦ä¹ åˆ°"åŠ¨è¯é€šå¸¸éœ€è¦å…³æ³¨å®ƒçš„ä¸»è¯­å’Œå®¾è¯­"ï¼Œè€Œè¿™ç§å­¦ä¹ æ˜¯ä»æ•°æ®ä¸­è‡ªåŠ¨è·å¾—çš„ï¼Œæ— éœ€äººå·¥å®šä¹‰è¯­æ³•è§„åˆ™ã€‚

---

## Memory Networksï¼šSelf-Attentionçš„æ€æƒ³å…ˆé©±

### é—®ç­”ç³»ç»Ÿçš„æŒ‘æˆ˜

åœ¨Self-Attentionè¢«æ˜ç¡®æå‡ºä¹‹å‰ï¼Œæœ‰ä¸€æ¡å¹³è¡Œçš„ç ”ç©¶è„‰ç»œåœ¨æ¢ç´¢ç±»ä¼¼çš„æƒ³æ³•ï¼š**Memory Networks**ã€‚

2014-2015å¹´ï¼ŒFacebook AI Researchï¼ˆSukhbaatar, Szegedy, Westonç­‰äººï¼‰æå‡ºäº†Memory Networksæ¥è§£å†³éœ€è¦å¤šè·³æ¨ç†çš„é—®ç­”ä»»åŠ¡ã€‚è€ƒè™‘ä»¥ä¸‹ä¾‹å­ï¼š

**æ•…äº‹**ï¼š
> Mary moved to the bathroom.
> John went to the hallway.
> Mary traveled to the office.

**é—®é¢˜**ï¼šWhere is Mary?

**ç­”æ¡ˆ**ï¼šoffice

è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ï¼š
1. ä»è®°å¿†ä¸­æ‰¾åˆ°ä¸"Mary"ç›¸å…³çš„ä¿¡æ¯
2. è¯†åˆ«å‡ºæœ€æ–°çš„ä½ç½®ä¿¡æ¯
3. ç»¼åˆè¿™äº›ä¿¡æ¯å¾—å‡ºç­”æ¡ˆ

è¿™æ¯”ç®€å•çš„æ–‡æœ¬åŒ¹é…å¤æ‚å¾—å¤šâ€”â€”éœ€è¦åœ¨å¤šæ¡ä¿¡æ¯ä¹‹é—´å»ºç«‹è”ç³»ï¼Œè¿›è¡Œæ¨ç†ã€‚

### Memory Networksçš„è®¾è®¡

Memory Networksçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå°†ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ªå¤–éƒ¨è®°å¿†ä¸­ï¼Œç„¶åé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ¥è¯»å–ç›¸å…³ä¿¡æ¯ã€‚

![End-To-End Memory Networksçš„æ¶æ„ã€‚è¾“å…¥å¥å­è¢«ç¼–ç ä¸ºè®°å¿†å‘é‡ï¼ˆçº¢è‰²ï¼‰ï¼Œé—®é¢˜è¢«ç¼–ç ä¸ºæŸ¥è¯¢å‘é‡ï¼ˆè“è‰²ï¼‰ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä»è®°å¿†ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚](figures/chapter-7/original/fig-memory-network.png){#fig-memory-network width=70%}

::: {.figure-caption}
*Source: Sukhbaatar et al. (2015) "End-To-End Memory Networks", Figure 1. [arXiv:1503.08895](https://arxiv.org/abs/1503.08895)*
:::

å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ç»„è®°å¿† $\{\mathbf{m}_1, \mathbf{m}_2, \ldots, \mathbf{m}_n\}$ å’Œä¸€ä¸ªæŸ¥è¯¢ $\mathbf{q}$ï¼š

**Step 1: è®¡ç®—æ³¨æ„åŠ›æƒé‡**

$$
p_i = \text{softmax}(\mathbf{q}^\top \mathbf{m}_i)
$$

**Step 2: è¯»å–è®°å¿†**

$$
\mathbf{o} = \sum_{i=1}^{n} p_i \mathbf{c}_i
$$

å…¶ä¸­ $\mathbf{c}_i$ æ˜¯è®°å¿† $i$ çš„è¾“å‡ºè¡¨ç¤ºï¼ˆå¯ä»¥ä¸ $\mathbf{m}_i$ ä¸åŒï¼‰ã€‚

**Step 3: æ›´æ–°æŸ¥è¯¢å¹¶è¿­ä»£**

$$
\mathbf{q}' = \mathbf{q} + \mathbf{o}
$$

ç„¶åç”¨ $\mathbf{q}'$ å†æ¬¡æŸ¥è¯¢è®°å¿†ï¼Œè¿›è¡Œå¤šè·³æ¨ç†ã€‚

### Memory Networksä¸Self-Attentionçš„è”ç³»

çœ‹å‡ºæ¥äº†å—ï¼ŸMemory Networksä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¸Self-AttentionæƒŠäººåœ°ç›¸ä¼¼ï¼š

| Memory Networks | Self-Attention |
|-----------------|----------------|
| æŸ¥è¯¢ $\mathbf{q}$ | Query $\mathbf{Q}$ |
| è®°å¿† $\mathbf{m}_i$ | Key $\mathbf{K}$ |
| è¾“å‡º $\mathbf{c}_i$ | Value $\mathbf{V}$ |
| $p_i = \text{softmax}(\mathbf{q}^\top \mathbf{m}_i)$ | $\alpha = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d})$ |
| $\mathbf{o} = \sum_i p_i \mathbf{c}_i$ | $\mathbf{Z} = \alpha \mathbf{V}$ |

å…³é”®çš„è”ç³»åœ¨äºï¼š**Self-Attentionå¯ä»¥çœ‹ä½œæ˜¯åºåˆ—å¯¹è‡ªå·±è¿›è¡ŒMemory Networkå¼çš„æŸ¥è¯¢**ã€‚

æ¯ä¸ªä½ç½®éƒ½æ˜¯ä¸€ä¸ª"è®°å¿†æ§½"ï¼Œæ¯ä¸ªä½ç½®ä¹Ÿéƒ½æ˜¯ä¸€ä¸ª"æŸ¥è¯¢è€…"ã€‚å½“ä½ç½® $i$ è®¡ç®—å®ƒçš„è¾“å‡ºæ—¶ï¼Œå®ƒå°±åƒåœ¨ç”¨è‡ªå·±çš„Queryå»æ£€ç´¢å…¶ä»–ä½ç½®ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰å­˜å‚¨çš„ä¿¡æ¯ã€‚

è¿™ä¸ªè§†è§’è§£é‡Šäº†ä¸ºä»€ä¹ˆSelf-Attentionå¦‚æ­¤å¼ºå¤§ï¼šå®ƒæœ¬è´¨ä¸Šæ˜¯åœ¨åš**åŸºäºå†…å®¹çš„å…³è”è®°å¿†æ£€ç´¢**ï¼Œè€Œè¿™æ­£æ˜¯æ¨ç†å’Œç†è§£çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

### ä»Memory Networksåˆ°Transformer

Memory Networksçš„å·¥ä½œå¯¹åæ¥çš„Transformeræœ‰æ·±è¿œå½±å“ï¼š

1. **ç«¯åˆ°ç«¯å¯å¾®åˆ†**ï¼šæ—©æœŸçš„Memory Networkséœ€è¦å¼ºç›‘ç£ï¼ˆå‘Šè¯‰æ¨¡å‹åº”è¯¥å…³æ³¨å“ªäº›è®°å¿†ï¼‰ï¼Œåæ¥çš„End-to-End Memory Networksé€šè¿‡è½¯æ³¨æ„åŠ›å®ç°äº†ç«¯åˆ°ç«¯è®­ç»ƒâ€”â€”è¿™æ­£æ˜¯Transformeræ‰€é‡‡ç”¨çš„æ–¹å¼ã€‚

2. **å¤šè·³æ¨ç†**ï¼šMemory Networksé€šè¿‡å¤šæ¬¡æŸ¥è¯¢è®°å¿†æ¥å®ç°å¤æ‚æ¨ç†ã€‚Transformerä¸­çš„å¤šå±‚å †å ä¹Ÿå¯ä»¥ç†è§£ä¸ºå¤šè·³æ¨ç†â€”â€”æ¯ä¸€å±‚çš„Self-Attentionéƒ½æ˜¯ä¸€æ¬¡"æŸ¥è¯¢"ã€‚

3. **Key-Valueåˆ†ç¦»**ï¼šMemory Networksä¸­ $\mathbf{m}_i$ï¼ˆç”¨äºè®¡ç®—æ³¨æ„åŠ›ï¼‰å’Œ $\mathbf{c}_i$ï¼ˆç”¨äºè¾“å‡ºï¼‰å¯ä»¥ä¸åŒã€‚è¿™å¯å‘äº†Self-Attentionä¸­Keyå’ŒValueçš„åˆ†ç¦»è®¾è®¡ã€‚

---

## æŠ€æœ¯ç»†èŠ‚

### Self-Attentionçš„æ•°å­¦å½¢å¼

è®©æˆ‘ä»¬æ­£å¼å®šä¹‰Self-Attentionçš„è®¡ç®—è¿‡ç¨‹ã€‚

ç»™å®šè¾“å…¥åºåˆ— $\mathbf{X} = [\mathbf{x}_1; \mathbf{x}_2; \ldots; \mathbf{x}_n]^\top \in \mathbb{R}^{n \times d}$ï¼ŒSelf-Attentionçš„è®¡ç®—å¦‚ä¸‹ï¼š

**Step 1: çº¿æ€§æŠ•å½±**

$$
\mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_V
$$

å…¶ä¸­ $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d \times d_k}$ï¼Œ$\mathbf{W}_V \in \mathbb{R}^{d \times d_v}$ã€‚

**Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**

$$
\mathbf{S} = \mathbf{Q} \mathbf{K}^\top \in \mathbb{R}^{n \times n}
$$

çŸ©é˜µ $\mathbf{S}$ çš„å…ƒç´  $S_{ij} = \mathbf{q}_i^\top \mathbf{k}_j$ è¡¡é‡ä½ç½® $i$ å¯¹ä½ç½® $j$ çš„"å…³æ³¨ç¨‹åº¦"ã€‚

**Step 3: ç¼©æ”¾å’Œå½’ä¸€åŒ–**

$$
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{S}}{\sqrt{d_k}}\right) \in \mathbb{R}^{n \times n}
$$

æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¡¨ç¤ºè¯¥ä½ç½®å¯¹æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ã€‚

**Step 4: åŠ æƒèšåˆ**

$$
\mathbf{Z} = \mathbf{A} \mathbf{V} \in \mathbb{R}^{n \times d_v}
$$

è¾“å‡º $\mathbf{z}_i = \sum_{j=1}^{n} A_{ij} \mathbf{v}_j$ æ˜¯æ‰€æœ‰Valueå‘é‡çš„åŠ æƒå’Œã€‚

::: {.callout-note}
## Algorithm: Scaled Dot-Product Self-Attention (Vaswani et al., 2017)

```python
def self_attention(X, W_Q, W_K, W_V):
    """
    Self-Attention è®¡ç®—

    å‚æ•°:
        X: è¾“å…¥åºåˆ— [batch, seq_len, d_model]
        W_Q, W_K: æŠ•å½±çŸ©é˜µ [d_model, d_k]
        W_V: æŠ•å½±çŸ©é˜µ [d_model, d_v]

    è¿”å›:
        Z: è¾“å‡ºåºåˆ— [batch, seq_len, d_v]
        A: æ³¨æ„åŠ›æƒé‡ [batch, seq_len, seq_len]
    """
    # Step 1: çº¿æ€§æŠ•å½±
    Q = X @ W_Q  # [batch, seq_len, d_k]
    K = X @ W_K  # [batch, seq_len, d_k]
    V = X @ W_V  # [batch, seq_len, d_v]

    # Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    d_k = Q.shape[-1]
    scores = Q @ K.transpose(-2, -1)  # [batch, seq_len, seq_len]

    # Step 3: ç¼©æ”¾ + Softmax
    scores = scores / math.sqrt(d_k)
    A = F.softmax(scores, dim=-1)

    # Step 4: åŠ æƒèšåˆ
    Z = A @ V  # [batch, seq_len, d_v]

    return Z, A
```

*Adapted from: Vaswani, A. et al. (2017). "Attention Is All You Need". NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*
:::

### å®Œæ•´æ•°å€¼ç¤ºä¾‹ï¼šä»è¾“å…¥åˆ°è¾“å‡º

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå°è§„æ¨¡çš„ä¾‹å­å®Œæ•´æ¼”ç¤ºSelf-Attentionçš„è®¡ç®—è¿‡ç¨‹ã€‚

**è®¾å®š**ï¼šå¥å­ "I love NLP"ï¼Œå…±3ä¸ªè¯ã€‚$d_{model} = 4$ï¼Œ$d_k = d_v = 4$ã€‚

**Step 1: è¾“å…¥è¡¨ç¤º**ï¼ˆå‡è®¾å·²ç»é€šè¿‡embeddingè·å¾—ï¼‰

$$
\mathbf{X} = \begin{bmatrix}
\mathbf{x}_I \\
\mathbf{x}_{love} \\
\mathbf{x}_{NLP}
\end{bmatrix} = \begin{bmatrix}
1.0 & 0.0 & 1.0 & 0.0 \\
0.0 & 1.0 & 0.5 & 0.5 \\
1.0 & 1.0 & 0.0 & 0.0
\end{bmatrix}
$$

**Step 2: æŠ•å½±çŸ©é˜µ**ï¼ˆä¸ºç®€åŒ–ï¼Œä½¿ç”¨æ¥è¿‘å•ä½çŸ©é˜µçš„å€¼ï¼‰

$$
\mathbf{W}_Q = \mathbf{W}_K = \mathbf{W}_V = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
$$

å› æ­¤ $\mathbf{Q} = \mathbf{K} = \mathbf{V} = \mathbf{X}$ã€‚

**Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° $\mathbf{Q}\mathbf{K}^\top$**

$$
\mathbf{S} = \mathbf{X} \mathbf{X}^\top = \begin{bmatrix}
\mathbf{x}_I^\top \mathbf{x}_I & \mathbf{x}_I^\top \mathbf{x}_{love} & \mathbf{x}_I^\top \mathbf{x}_{NLP} \\
\mathbf{x}_{love}^\top \mathbf{x}_I & \mathbf{x}_{love}^\top \mathbf{x}_{love} & \mathbf{x}_{love}^\top \mathbf{x}_{NLP} \\
\mathbf{x}_{NLP}^\top \mathbf{x}_I & \mathbf{x}_{NLP}^\top \mathbf{x}_{love} & \mathbf{x}_{NLP}^\top \mathbf{x}_{NLP}
\end{bmatrix}
$$

é€ä¸ªè®¡ç®—ï¼š
- $\mathbf{x}_I^\top \mathbf{x}_I = 1^2 + 0^2 + 1^2 + 0^2 = 2.0$
- $\mathbf{x}_I^\top \mathbf{x}_{love} = 1 \times 0 + 0 \times 1 + 1 \times 0.5 + 0 \times 0.5 = 0.5$
- $\mathbf{x}_I^\top \mathbf{x}_{NLP} = 1 \times 1 + 0 \times 1 + 1 \times 0 + 0 \times 0 = 1.0$
- $\mathbf{x}_{love}^\top \mathbf{x}_{love} = 0^2 + 1^2 + 0.5^2 + 0.5^2 = 1.5$
- $\mathbf{x}_{love}^\top \mathbf{x}_{NLP} = 0 \times 1 + 1 \times 1 + 0.5 \times 0 + 0.5 \times 0 = 1.0$
- $\mathbf{x}_{NLP}^\top \mathbf{x}_{NLP} = 1^2 + 1^2 + 0^2 + 0^2 = 2.0$

$$
\mathbf{S} = \begin{bmatrix}
2.0 & 0.5 & 1.0 \\
0.5 & 1.5 & 1.0 \\
1.0 & 1.0 & 2.0
\end{bmatrix}
$$

**Step 4: ç¼©æ”¾ï¼ˆé™¤ä»¥ $\sqrt{d_k} = 2$ï¼‰**

$$
\frac{\mathbf{S}}{\sqrt{4}} = \begin{bmatrix}
1.0 & 0.25 & 0.5 \\
0.25 & 0.75 & 0.5 \\
0.5 & 0.5 & 1.0
\end{bmatrix}
$$

**Step 5: Softmaxï¼ˆæŒ‰è¡Œï¼‰**

ç¬¬ä¸€è¡Œï¼š$[e^{1.0}, e^{0.25}, e^{0.5}] = [2.72, 1.28, 1.65]$ï¼Œå’Œä¸º $5.65$

$$
\alpha_{I} = [2.72/5.65, 1.28/5.65, 1.65/5.65] = [0.48, 0.23, 0.29]
$$

ç±»ä¼¼è®¡ç®—å…¶ä»–è¡Œï¼š

$$
\mathbf{A} = \begin{bmatrix}
0.48 & 0.23 & 0.29 \\
0.26 & 0.43 & 0.31 \\
0.31 & 0.31 & 0.38
\end{bmatrix}
$$

**Step 6: åŠ æƒèšåˆ $\mathbf{Z} = \mathbf{A}\mathbf{V}$**

$$
\mathbf{z}_I = 0.48 \times \mathbf{x}_I + 0.23 \times \mathbf{x}_{love} + 0.29 \times \mathbf{x}_{NLP}
$$

$$
= 0.48 \times [1, 0, 1, 0] + 0.23 \times [0, 1, 0.5, 0.5] + 0.29 \times [1, 1, 0, 0]
$$

$$
= [0.48 + 0 + 0.29, 0 + 0.23 + 0.29, 0.48 + 0.115 + 0, 0 + 0.115 + 0]
$$

$$
= [0.77, 0.52, 0.60, 0.12]
$$

**è§£è¯»**ï¼š

"I" çš„Self-Attentionè¾“å‡ºä¸å†åªæ˜¯è‡ªå·±çš„è¡¨ç¤ºï¼Œè€Œæ˜¯èåˆäº†æ•´ä¸ªå¥å­çš„ä¿¡æ¯ã€‚æ³¨æ„åŠ›åˆ†å¸ƒ $[0.48, 0.23, 0.29]$ æ˜¾ç¤ºï¼š
- "I" æœ€å…³æ³¨è‡ªå·±ï¼ˆ0.48ï¼‰â€”â€”è¿™æ˜¯åˆç†çš„ï¼Œè‡ªå·±çš„ä¿¡æ¯æœ€ç›¸å…³
- å¯¹ "NLP"ï¼ˆ0.29ï¼‰çš„å…³æ³¨ç•¥é«˜äº "love"ï¼ˆ0.23ï¼‰â€”â€”åœ¨è¿™ä¸ªç®€åŒ–ä¾‹å­ä¸­ï¼Œè¿™æ¥è‡ªäºå‘é‡ç›¸ä¼¼åº¦

åœ¨çœŸå®çš„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè¿™äº›æƒé‡ä¼šå­¦ä¹ åˆ°è¯­è¨€çš„ç»“æ„ï¼šä¸»è¯­ä¼šå…³æ³¨åŠ¨è¯ï¼ŒåŠ¨è¯ä¼šå…³æ³¨å®¾è¯­ï¼Œä»£è¯ä¼šå…³æ³¨å®ƒçš„æŒ‡ä»£å¯¹è±¡ã€‚

### Self-Attentionçš„å¤æ‚åº¦åˆ†æ

**æ—¶é—´å¤æ‚åº¦**ï¼š

- æŠ•å½±ï¼š$O(n \cdot d \cdot d_k)$ï¼Œä¸‰æ¬¡æŠ•å½±å…± $O(n \cdot d^2)$
- è®¡ç®— $\mathbf{Q}\mathbf{K}^\top$ï¼š$O(n^2 \cdot d_k)$
- Softmaxï¼š$O(n^2)$
- è®¡ç®— $\mathbf{A}\mathbf{V}$ï¼š$O(n^2 \cdot d_v)$

æ€»ä½“ï¼š$O(n^2 \cdot d + n \cdot d^2)$

å½“ $n$ å¾ˆå¤§æ—¶ï¼Œ$O(n^2)$ æ˜¯ä¸»å¯¼é¡¹â€”â€”è¿™æ˜¯Self-Attentionçš„ä¸»è¦ç“¶é¢ˆã€‚

**ç©ºé—´å¤æ‚åº¦**ï¼š

éœ€è¦å­˜å‚¨ $n \times n$ çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå› æ­¤ç©ºé—´å¤æ‚åº¦ä¸º $O(n^2)$ã€‚

**ä¸RNNçš„å¯¹æ¯”**ï¼š

| æŒ‡æ ‡ | RNN | Self-Attention |
|------|-----|----------------|
| æ—¶é—´å¤æ‚åº¦ | $O(n \cdot d^2)$ | $O(n^2 \cdot d)$ |
| å¹¶è¡Œåº¦ | $O(1)$ | $O(n)$ |
| æœ€é•¿è·¯å¾„ | $O(n)$ | $O(1)$ |
| ç©ºé—´å¤æ‚åº¦ | $O(d)$ | $O(n^2)$ |

Self-Attentionç”¨ $O(n^2)$ çš„å¤æ‚åº¦æ¢æ¥äº†ï¼š
1. å®Œå…¨å¹¶è¡ŒåŒ–ï¼ˆä» $O(1)$ åˆ° $O(n)$ï¼‰
2. ä»»æ„ä½ç½®ä¹‹é—´çš„ç›´æ¥è·¯å¾„ï¼ˆä» $O(n)$ åˆ° $O(1)$ï¼‰

è¿™ä¸ªæƒè¡¡åœ¨å¤§å¤šæ•°NLPä»»åŠ¡ä¸­æ˜¯å€¼å¾—çš„ï¼Œå› ä¸ºå¥å­é•¿åº¦é€šå¸¸ä¸è¶…è¿‡å‡ ç™¾ä¸ªè¯ã€‚ä½†å¯¹äºè¶…é•¿åºåˆ—ï¼ˆå¦‚æ–‡æ¡£ã€ä»£ç ï¼‰ï¼Œ$O(n^2)$ ä¼šæˆä¸ºä¸¥é‡ç“¶é¢ˆâ€”â€”è¿™å‚¬ç”Ÿäº†åæ¥çš„é«˜æ•ˆæ³¨æ„åŠ›å˜ä½“ï¼ˆSparse Attentionã€Linear Attentionç­‰ï¼‰ã€‚

---

## Self-Attentionçš„è‡´å‘½ç¼ºé™·ï¼šä½ç½®ä¿¡æ¯ä¸¢å¤±

### é—®é¢˜çš„æœ¬è´¨

ä»”ç»†è§‚å¯ŸSelf-Attentionçš„è®¡ç®—å…¬å¼ï¼š

$$
\mathbf{z}_i = \sum_{j=1}^{n} \text{softmax}\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}}\right) \mathbf{v}_j
$$

è¿™é‡Œæœ‰ä¸€ä¸ªè‡´å‘½çš„é—®é¢˜ï¼š**è®¡ç®—å®Œå…¨ä¸ä½ç½®æ— å…³**ã€‚

å¦‚æœæˆ‘ä»¬æ‰“ä¹±è¾“å…¥åºåˆ—çš„é¡ºåºï¼Œæ¯”å¦‚æŠŠ "I love NLP" å˜æˆ "NLP I love"ï¼Œæ¯ä¸ªè¯çš„æ³¨æ„åŠ›æƒé‡åªæ˜¯ç›¸åº”åœ°æ‰“ä¹±ï¼Œæœ€ç»ˆè¾“å‡ºåªæ˜¯é‡æ–°æ’åˆ—ï¼Œä¸åŸæ¥ä¸€ä¸€å¯¹åº”ã€‚

æ›´æ­£å¼åœ°è¯´ï¼ŒSelf-Attentionæ˜¯**ç½®æ¢ç­‰å˜çš„ï¼ˆpermutation equivariantï¼‰**ï¼š

$$
\text{SelfAttn}(\mathbf{P}\mathbf{X}) = \mathbf{P} \cdot \text{SelfAttn}(\mathbf{X})
$$

å…¶ä¸­ $\mathbf{P}$ æ˜¯ä»»æ„ç½®æ¢çŸ©é˜µã€‚

è¿™æ„å‘³ç€ï¼šSelf-Attentionå®Œå…¨ä¸çŸ¥é“"è°åœ¨è°å‰é¢"ã€‚

### ä¸ºä»€ä¹ˆè¿™æ˜¯è‡´å‘½çš„ï¼Ÿ

åœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œé¡ºåºæºå¸¦ç€å…³é”®ä¿¡æ¯ï¼š

1. **è¯­æ³•è§’è‰²**ï¼š"The dog bit the man" vs "The man bit the dog" æ„æ€å®Œå…¨ä¸åŒ

2. **æ—¶æ€**ï¼š"I will go" vs "I go" vs "I went" ä¾èµ–äºè¯çš„ç›¸å¯¹ä½ç½®

3. **æŒ‡ä»£æ¶ˆè§£**ï¼š"John told Bill that he..." ä¸­ "he" é€šå¸¸æŒ‡ä»£è¾ƒè¿‘çš„åè¯

4. **å¦å®šèŒƒå›´**ï¼š"I didn't go to school" vs "I went to not school" å¦å®šè¯çš„ä½ç½®å†³å®šå¦å®šçš„èŒƒå›´

å¦‚æœæ¨¡å‹ä¸çŸ¥é“è¯çš„é¡ºåºï¼Œå®ƒæ€ä¹ˆå¯èƒ½ç†è§£è¯­è¨€ï¼Ÿ

### ç›´è§‰æ¼”ç¤ºï¼šä½ç½®ä¿¡æ¯ä¸ºä»€ä¹ˆé‡è¦

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç›´è§‚æ„Ÿå—ä½ç½®ä¿¡æ¯çš„é‡è¦æ€§ã€‚

è€ƒè™‘ä¸¤ä¸ªå¥å­ï¼š
- A: "The cat chased the mouse"
- B: "The mouse chased the cat"

åœ¨çº¯Self-Attentionä¸­ï¼ˆæ²¡æœ‰ä½ç½®ç¼–ç ï¼‰ï¼Œä¸¤ä¸ªå¥å­çš„è¡¨ç¤ºæ˜¯**å®Œå…¨ç›¸åŒçš„**ï¼Œå› ä¸ºåŒ…å«çš„è¯é›†åˆç›¸åŒï¼Œåªæ˜¯é¡ºåºä¸åŒã€‚

ä½†è¿™ä¸¤ä¸ªå¥å­çš„æ„æ€å®Œå…¨ç›¸åï¼å¦‚æœæ¨¡å‹ä¸èƒ½åŒºåˆ†å®ƒä»¬ï¼Œå°±æ— æ³•æ­£ç¡®ç†è§£è¯­è¨€ã€‚

---

## ä½ç½®ç¼–ç ï¼šå¼¥è¡¥ç¼ºå¤±çš„é¡ºåºä¿¡æ¯

### è®¾è®¡ç›®æ ‡

æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•ï¼Œå°†ä½ç½®ä¿¡æ¯æ³¨å…¥åˆ°Self-Attentionä¸­ã€‚è¿™ç§æ–¹æ³•åº”è¯¥æ»¡è¶³ï¼š

1. **å”¯ä¸€æ€§**ï¼šä¸åŒä½ç½®çš„ç¼–ç åº”è¯¥ä¸åŒ
2. **å¯æ³›åŒ–**ï¼šæ¨¡å‹åº”è¯¥èƒ½å¤Ÿå¤„ç†è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åº¦
3. **ç›¸å¯¹å…³ç³»**ï¼šç¼–ç åº”è¯¥èƒ½è¡¨è¾¾ä½ç½®ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ï¼Œè€Œéä»…ä»…æ˜¯ç»å¯¹ä½ç½®
4. **æœ‰ç•Œæ€§**ï¼šç¼–ç å€¼ä¸åº”éšä½ç½®æ— é™å¢é•¿

### æ–¹æ¡ˆä¸€ï¼šå¯å­¦ä¹ çš„ä½ç½®åµŒå…¥

æœ€ç®€å•çš„æƒ³æ³•ï¼šä¸ºæ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªå‘é‡ã€‚

$$
\mathbf{x}_i' = \mathbf{x}_i + \mathbf{p}_i
$$

å…¶ä¸­ $\mathbf{p}_i \in \mathbb{R}^d$ æ˜¯ä½ç½® $i$ çš„å¯å­¦ä¹ åµŒå…¥ã€‚

**ä¼˜ç‚¹**ï¼š
- ç®€å•ç›´è§‚
- æ¨¡å‹å¯ä»¥è‡ªç”±å­¦ä¹ ä½ç½®è¡¨ç¤º

**ç¼ºç‚¹**ï¼š
- æ— æ³•æ³›åŒ–åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åº¦
- å¦‚æœæœ€é•¿è®­ç»ƒåºåˆ—æ˜¯512ï¼Œå°±æ— æ³•å¤„ç†513é•¿åº¦çš„è¾“å…¥
- ä¸èƒ½è¡¨è¾¾ç›¸å¯¹ä½ç½®å…³ç³»

è¿™ç§æ–¹å¼è¢«BERTç­‰æ¨¡å‹é‡‡ç”¨ï¼Œä½†éœ€è¦é…åˆå›ºå®šçš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚

### æ–¹æ¡ˆäºŒï¼šæ­£å¼¦ä½ç½®ç¼–ç 

Transformerè®ºæ–‡æå‡ºäº†ä¸€ç§ä¼˜é›…çš„æ–¹æ¡ˆï¼šä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ã€‚

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

å…¶ä¸­ $pos$ æ˜¯ä½ç½®ï¼Œ$i$ æ˜¯ç»´åº¦ç´¢å¼•ã€‚

è¿™ä¸ªçœ‹èµ·æ¥å¤æ‚çš„å…¬å¼å®é™…ä¸Šæœ‰ç€æ·±åˆ»çš„è®¾è®¡æ€æƒ³ï¼š

**1. å”¯ä¸€æ€§**ï¼šæ¯ä¸ªä½ç½®æœ‰å”¯ä¸€çš„ç¼–ç å‘é‡

**2. æœ‰ç•Œæ€§**ï¼šæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å€¼åŸŸæ˜¯ $[-1, 1]$ï¼Œä¸ä¼šéšä½ç½®çˆ†ç‚¸

**3. é•¿åº¦æ³›åŒ–**ï¼šå‡½æ•°æ˜¯è¿ç»­çš„ï¼Œä»»ä½•é•¿åº¦éƒ½å¯ä»¥è®¡ç®—

**4. ç›¸å¯¹ä½ç½®å¯è¡¨è¾¾**ï¼šè¿™æ˜¯æœ€ç²¾å¦™çš„éƒ¨åˆ†ã€‚å¯ä»¥è¯æ˜ï¼Œä½ç½® $pos + k$ çš„ç¼–ç å¯ä»¥è¡¨ç¤ºä¸ºä½ç½® $pos$ çš„ç¼–ç çš„çº¿æ€§å˜æ¢ï¼š

$$
PE_{pos+k} = T_k \cdot PE_{pos}
$$

å…¶ä¸­ $T_k$ æ˜¯åªä¾èµ–äº $k$ï¼ˆåç§»é‡ï¼‰çš„çŸ©é˜µã€‚è¿™æ„å‘³ç€æ¨¡å‹æœ‰å¯èƒ½å­¦ä¹ åˆ°ç›¸å¯¹ä½ç½®å…³ç³»ã€‚

### æ­£å¼¦ç¼–ç çš„å‡ ä½•ç›´è§‰

æƒ³è±¡ä¸€ä¸ªæ—¶é’Ÿï¼š

- ç§’é’ˆè½¬ä¸€åœˆæ˜¯60ç§’ï¼ˆé«˜é¢‘ï¼‰
- åˆ†é’ˆè½¬ä¸€åœˆæ˜¯60åˆ†é’Ÿï¼ˆä¸­é¢‘ï¼‰
- æ—¶é’ˆè½¬ä¸€åœˆæ˜¯12å°æ—¶ï¼ˆä½é¢‘ï¼‰

ä¸åŒçš„æŒ‡é’ˆåœ¨ä¸åŒçš„"é¢‘ç‡"ä¸Šè¿åŠ¨ï¼Œç»„åˆèµ·æ¥å¯ä»¥å”¯ä¸€åœ°è¡¨ç¤ºæ—¶é—´ã€‚

æ­£å¼¦ä½ç½®ç¼–ç çš„åŸç†ç±»ä¼¼ï¼š
- ä½ç»´åº¦ï¼ˆå° $i$ï¼‰çš„æ­£å¼¦æ³¢é¢‘ç‡é«˜ï¼Œå˜åŒ–å¿«ï¼Œç¼–ç **å±€éƒ¨ä½ç½®ä¿¡æ¯**
- é«˜ç»´åº¦ï¼ˆå¤§ $i$ï¼‰çš„æ­£å¼¦æ³¢é¢‘ç‡ä½ï¼Œå˜åŒ–æ…¢ï¼Œç¼–ç **å…¨å±€ä½ç½®ä¿¡æ¯**

ç»„åˆæ‰€æœ‰ç»´åº¦ï¼Œå°±èƒ½å”¯ä¸€æ ‡è¯†æ¯ä¸ªä½ç½®ã€‚

```{python}
#| code-fold: false
import numpy as np
import matplotlib.pyplot as plt

def sinusoidal_position_encoding(max_len, d_model):
    """è®¡ç®—æ­£å¼¦ä½ç½®ç¼–ç """
    pe = np.zeros((max_len, d_model))
    position = np.arange(max_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))

    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)

    return pe

# å¯è§†åŒ–
pe = sinusoidal_position_encoding(100, 64)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›¾ï¼šçƒ­åŠ›å›¾
im = axes[0].imshow(pe, cmap='RdBu', aspect='auto')
axes[0].set_xlabel('Dimension')
axes[0].set_ylabel('Position')
axes[0].set_title('Sinusoidal Position Encoding')
plt.colorbar(im, ax=axes[0])

# å³å›¾ï¼šä¸åŒç»´åº¦çš„æ³¢å½¢
for dim in [0, 10, 20, 30]:
    axes[1].plot(pe[:, dim], label=f'dim {dim}')
axes[1].set_xlabel('Position')
axes[1].set_ylabel('Value')
axes[1].set_title('Different Dimensions = Different Frequencies')
axes[1].legend()

plt.tight_layout()
plt.show()
```

### æ–¹æ¡ˆä¸‰ï¼šç›¸å¯¹ä½ç½®ç¼–ç 

æ­£å¼¦ç¼–ç è™½ç„¶ç²¾å·§ï¼Œä½†å®ƒç¼–ç çš„æ˜¯**ç»å¯¹ä½ç½®**ã€‚ä¸€äº›ç ”ç©¶è€…è®¤ä¸ºï¼Œå¯¹äºè¯­è¨€ç†è§£æ¥è¯´ï¼Œ**ç›¸å¯¹ä½ç½®**ï¼ˆä¸¤ä¸ªè¯ä¹‹é—´çš„è·ç¦»ï¼‰å¯èƒ½æ¯”ç»å¯¹ä½ç½®æ›´é‡è¦ã€‚

ç›¸å¯¹ä½ç½®ç¼–ç çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¿®æ”¹æ³¨æ„åŠ›è®¡ç®—ï¼Œè®©å®ƒç›´æ¥è€ƒè™‘ä½ç½®å·®ï¼š

$$
\alpha_{ij} \propto \exp\left(\frac{\mathbf{q}_i^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{r}_{j-i}}{\sqrt{d_k}}\right)
$$

å…¶ä¸­ $\mathbf{r}_{j-i}$ æ˜¯è¡¨ç¤ºç›¸å¯¹ä½ç½® $j-i$ çš„å‘é‡ã€‚

è¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æ˜¯ï¼š
- ç›´æ¥å»ºæ¨¡ç›¸å¯¹å…³ç³»
- ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„ç›¸å¯¹ä½ç½®åå¥½
- ç†è®ºä¸Šå¯ä»¥æ³›åŒ–åˆ°ä»»æ„é•¿åº¦

åæ¥çš„RoPEï¼ˆRotary Position Embeddingï¼‰è¿›ä¸€æ­¥å‘å±•äº†è¿™ä¸ªæ€æƒ³ï¼Œæˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ‡é…â€”â€”æˆ‘ä»¬å°†åœ¨ç¬¬26ç« è¯¦ç»†è®¨è®ºã€‚

### ä½ç½®ç¼–ç çš„æ³¨å…¥æ–¹å¼

æœ‰äº†ä½ç½®ç¼–ç  $\mathbf{PE}$ï¼Œå¦‚ä½•å°†å…¶ä¸è¾“å…¥ç»“åˆï¼Ÿä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼š

**åŠ æ³•æ³¨å…¥ï¼ˆTransformeré‡‡ç”¨ï¼‰**ï¼š

$$
\mathbf{X}' = \mathbf{X} + \mathbf{PE}
$$

å°†ä½ç½®ç¼–ç ç›´æ¥åŠ åˆ°è¯åµŒå…¥ä¸Šã€‚è¿™å‡è®¾ä½ç½®ä¿¡æ¯å’Œè¯­ä¹‰ä¿¡æ¯å¯ä»¥åœ¨åŒä¸€ç©ºé—´ä¸­è¡¨è¾¾å’Œæ··åˆã€‚

**æ‹¼æ¥æ³¨å…¥**ï¼š

$$
\mathbf{X}' = [\mathbf{X}; \mathbf{PE}]
$$

å°†ä½ç½®ç¼–ç ä¸è¯åµŒå…¥æ‹¼æ¥ï¼Œä¿æŒä¸¤è€…åˆ†ç¦»ã€‚è¿™éœ€è¦æ›´å¤šå‚æ•°ï¼Œä½†é¿å…äº†ä¿¡æ¯æ··æ·†ã€‚

å®è·µä¸­ï¼ŒåŠ æ³•æ³¨å…¥æ›´å¸¸ç”¨ï¼Œå› ä¸ºå®ƒæ›´ç®€æ´ä¸”æ•ˆæœè‰¯å¥½ã€‚

---

## å·¥ç¨‹å®è·µ

### å®ç°Self-Attention

```{python}
#| code-fold: false
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    """
    Self-Attention æ¨¡å—
    """
    def __init__(self, d_model, d_k=None, d_v=None):
        super().__init__()
        self.d_model = d_model
        self.d_k = d_k if d_k is not None else d_model
        self.d_v = d_v if d_v is not None else d_model

        # Q, K, V æŠ•å½±çŸ©é˜µ
        self.W_Q = nn.Linear(d_model, self.d_k, bias=False)
        self.W_K = nn.Linear(d_model, self.d_k, bias=False)
        self.W_V = nn.Linear(d_model, self.d_v, bias=False)

        # ç¼©æ”¾å› å­
        self.scale = math.sqrt(self.d_k)

    def forward(self, X, mask=None):
        """
        X: [batch, seq_len, d_model]
        mask: [batch, seq_len, seq_len], True è¡¨ç¤ºéœ€è¦ mask çš„ä½ç½®

        è¿”å›:
            output: [batch, seq_len, d_v]
            attention_weights: [batch, seq_len, seq_len]
        """
        # Step 1: çº¿æ€§æŠ•å½±
        Q = self.W_Q(X)  # [batch, seq_len, d_k]
        K = self.W_K(X)  # [batch, seq_len, d_k]
        V = self.W_V(X)  # [batch, seq_len, d_v]

        # Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale  # [batch, seq_len, seq_len]

        # Step 3: åº”ç”¨ mask
        if mask is not None:
            scores = scores.masked_fill(mask, -1e9)

        # Step 4: Softmax
        attention_weights = F.softmax(scores, dim=-1)

        # Step 5: åŠ æƒèšåˆ
        output = torch.bmm(attention_weights, V)

        return output, attention_weights
```

### å®ç°ä½ç½®ç¼–ç 

```{python}
#| code-fold: false
class PositionalEncoding(nn.Module):
    """
    æ­£å¼¦ä½ç½®ç¼–ç 
    """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: [batch, seq_len, d_model]
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class LearnablePositionalEmbedding(nn.Module):
    """
    å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥
    """
    def __init__(self, d_model, max_len=512, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.position_embedding = nn.Embedding(max_len, d_model)

    def forward(self, x):
        """
        x: [batch, seq_len, d_model]
        """
        seq_len = x.size(1)
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        x = x + self.position_embedding(positions)
        return self.dropout(x)
```

### å®Œæ•´æ¼”ç¤ºï¼šSelf-Attentionçš„æ•ˆæœ

```{python}
#| code-fold: false
# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
seq_len = 5
d_model = 64

# éšæœºè¾“å…¥
X = torch.randn(batch_size, seq_len, d_model)

# åˆ›å»ºæ¨¡å‹
self_attn = SelfAttention(d_model)
pos_enc = PositionalEncoding(d_model)

# æ·»åŠ ä½ç½®ç¼–ç 
X_with_pos = pos_enc(X)

# è®¡ç®— Self-Attention
output, attention_weights = self_attn(X_with_pos)

print(f"è¾“å…¥å½¢çŠ¶: {X.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›çŸ©é˜µ:\n{attention_weights[0].detach().numpy().round(2)}")
```

### å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼

```{python}
#| code-fold: false
import matplotlib.pyplot as plt

# åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¾‹å­
words = ["The", "cat", "sat", "on", "mat"]
seq_len = len(words)

# æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒå¥½çš„æ³¨æ„åŠ›çŸ©é˜µ
# è¿™ä¸ªçŸ©é˜µå±•ç¤ºäº†ä¸€äº›è¯­è¨€å­¦ä¸Šåˆç†çš„æ¨¡å¼
attention_pattern = torch.tensor([
    [0.5, 0.3, 0.1, 0.05, 0.05],  # "The" ä¸»è¦å…³æ³¨è‡ªå·±å’Œ "cat"
    [0.2, 0.4, 0.3, 0.05, 0.05],  # "cat" å…³æ³¨ "sat"ï¼ˆåŠ¨è¯ï¼‰
    [0.1, 0.4, 0.2, 0.2, 0.1],    # "sat" å…³æ³¨ "cat"ï¼ˆä¸»è¯­ï¼‰å’Œ "on"
    [0.05, 0.1, 0.3, 0.3, 0.25],  # "on" å…³æ³¨ "sat" å’Œ "mat"
    [0.05, 0.1, 0.1, 0.25, 0.5],  # "mat" å…³æ³¨ "on" å’Œè‡ªå·±
])

fig, ax = plt.subplots(figsize=(8, 6))
im = ax.imshow(attention_pattern, cmap='Blues')

ax.set_xticks(range(seq_len))
ax.set_yticks(range(seq_len))
ax.set_xticklabels(words)
ax.set_yticklabels(words)
ax.set_xlabel('Key (attended to)')
ax.set_ylabel('Query (attending from)')
ax.set_title('Self-Attention Weights: "The cat sat on mat"')

# æ·»åŠ æ•°å€¼æ ‡æ³¨
for i in range(seq_len):
    for j in range(seq_len):
        ax.text(j, i, f'{attention_pattern[i, j]:.2f}',
                ha='center', va='center', color='black' if attention_pattern[i, j] < 0.3 else 'white')

plt.colorbar(im)
plt.tight_layout()
plt.show()
```

---

## æ·±å…¥ç†è§£

### ä¸ºä»€ä¹ˆSelf-Attentionæœ‰æ•ˆï¼Ÿâ€”â€”ç†è®ºè§†è§’

**è¡¨è¾¾èƒ½åŠ›åˆ†æ**

Self-Attentionå¯ä»¥çœ‹ä½œä¸€ç§ç‰¹æ®Šçš„å›¾ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­å›¾æ˜¯å®Œå…¨è¿æ¥çš„ï¼Œè¾¹æƒé‡ç”±æ³¨æ„åŠ›å†³å®šã€‚Yun et al. (2020) è¯æ˜äº†ï¼šåœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼ŒSelf-Attentionæ˜¯**é€šç”¨å‡½æ•°é€¼è¿‘å™¨**â€”â€”å®ƒå¯ä»¥é€¼è¿‘ä»»ä½•è¿ç»­çš„ç½®æ¢ç­‰å˜å‡½æ•°ã€‚

**ä¸å·ç§¯çš„å¯¹æ¯”**

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹Ÿå¯ä»¥å¤„ç†åºåˆ—ï¼Œä½†å®ƒæœ‰å›ºå®šçš„æ„Ÿå—é‡ã€‚ä¸€ä¸ª3-gramå·ç§¯åªèƒ½çœ‹åˆ°ç›¸é‚»çš„3ä¸ªè¯ï¼›è¦çœ‹åˆ°æ›´è¿œçš„è¯ï¼Œéœ€è¦å †å å¤šå±‚ã€‚Self-Attentionåˆ™ä¸€æ­¥å°±èƒ½çœ‹åˆ°å…¨å±€ã€‚

| ç‰¹æ€§ | CNN | Self-Attention |
|------|-----|----------------|
| æ„Ÿå—é‡ | å±€éƒ¨ï¼ˆé€å±‚æ‰©å¤§ï¼‰ | å…¨å±€ï¼ˆä¸€å±‚åˆ°ä½ï¼‰ |
| è®¡ç®—å¤æ‚åº¦ | $O(n \cdot k^2 \cdot d)$ | $O(n^2 \cdot d)$ |
| ä½ç½®åç½® | å¼ºï¼ˆç›¸å¯¹ä½ç½®å…³ç³»å›ºå®šï¼‰ | å¼±ï¼ˆéœ€è¦ä½ç½®ç¼–ç ï¼‰ |
| å‚æ•°å…±äº« | è·¨ä½ç½®å…±äº« | Q/K/VæŠ•å½±å…±äº« |

**Memory Networkè§†è§’**

å¦‚å‰æ‰€è¿°ï¼ŒSelf-Attentionå¯ä»¥ç†è§£ä¸ºåºåˆ—å¯¹è‡ªå·±è¿›è¡ŒMemory Networkå¼çš„æŸ¥è¯¢ã€‚æ¯ä¸€å±‚Self-Attentionéƒ½æ˜¯ä¸€æ¬¡"å¤šè·³æ¨ç†"â€”â€”å‰ä¸€å±‚çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€å±‚çš„"è®°å¿†"ã€‚

### æ–¹æ³•çš„è¾¹ç•Œæ¡ä»¶

**Self-Attentionçš„éšå«å‡è®¾**ï¼š

1. **å…¨å±€ç›¸å…³æ€§å‡è®¾**ï¼šæ¯ä¸ªä½ç½®éƒ½å¯èƒ½ä¸ä»»ä½•å…¶ä»–ä½ç½®ç›¸å…³ã€‚ä½†å¯¹äºæŸäº›ä»»åŠ¡ï¼ˆå¦‚æ—¶é—´åºåˆ—é¢„æµ‹ï¼‰ï¼Œå±€éƒ¨ç›¸å…³æ€§å¯èƒ½æ›´é‡è¦ã€‚

2. **å‡åŒ€è®¡ç®—å‡è®¾**ï¼šæ‰€æœ‰ä½ç½®ä¹‹é—´çš„äº¤äº’ç”¨ç›¸åŒçš„è®¡ç®—é‡ã€‚ä½†ç›´è§‰ä¸Šï¼Œ"é‡è¦"çš„äº¤äº’å¯èƒ½éœ€è¦æ›´å¤šè®¡ç®—ã€‚

3. **çº¿æ€§å¯åˆ†å‡è®¾**ï¼šç›¸å…³æ€§å¯ä»¥é€šè¿‡å‘é‡ç‚¹ç§¯æ¥è¡¡é‡ã€‚ä½†æœ‰äº›å¤æ‚çš„å…³ç³»å¯èƒ½æ— æ³•ç”¨ç‚¹ç§¯æ•è·ã€‚

**å¤±æ•ˆæ¡ä»¶**ï¼š

1. **è¶…é•¿åºåˆ—**ï¼šå½“ $n > 10000$ æ—¶ï¼Œ$O(n^2)$ å¤æ‚åº¦å˜å¾—ä¸å¯æ¥å—ã€‚
2. **å¼ºä½ç½®ä¾èµ–ä»»åŠ¡**ï¼šå¯¹äºéœ€è¦ç²¾ç¡®ä½ç½®æ¨ç†çš„ä»»åŠ¡ï¼ˆå¦‚ç®—æœ¯ï¼‰ï¼ŒSelf-Attentionå¸¸å¸¸å¤±è´¥ã€‚
3. **æ•°æ®é‡ä¸è¶³**ï¼šSelf-Attentionçš„å¼±å½’çº³åç½®éœ€è¦å¤§é‡æ•°æ®æ¥è¡¥å¿ã€‚

### å¼€æ”¾ç ”ç©¶é—®é¢˜

1. **ä½ç½®ç¼–ç çš„æœ€ä¼˜è®¾è®¡**ï¼šæ­£å¼¦ç¼–ç ã€å¯å­¦ä¹ ç¼–ç ã€ç›¸å¯¹ä½ç½®ç¼–ç â€”â€”å“ªç§æœ€å¥½ï¼Ÿæ˜¯å¦å­˜åœ¨"æœ€ä¼˜"çš„ä½ç½®ç¼–ç ï¼Ÿ

2. **Self-Attentionçš„å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡çœŸçš„ä»£è¡¨äº†"é‡è¦æ€§"å—ï¼Ÿè¿˜æ˜¯åªæ˜¯è®¡ç®—çš„å‰¯äº§å“ï¼Ÿ

3. **æ•ˆç‡ä¸è¡¨è¾¾èƒ½åŠ›çš„æƒè¡¡**ï¼šèƒ½å¦è®¾è®¡å‡º $O(n)$ å¤æ‚åº¦ä½†ä¿æŒ $O(n^2)$ è¡¨è¾¾èƒ½åŠ›çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ

4. **å½’çº³åç½®çš„è®¾è®¡**ï¼šå¦‚ä½•åœ¨Self-Attentionä¸­å¼•å…¥åˆé€‚çš„å½’çº³åç½®ï¼Œå‡å°‘å¯¹æ•°æ®é‡çš„ä¾èµ–ï¼Ÿ

---

## å±€é™æ€§ä¸å±•æœ›

### Self-Attentionçš„æ ¸å¿ƒå±€é™

**1. ä½ç½®ä¿¡æ¯ä»æ˜¯"è¡¥ä¸"**

è™½ç„¶ä½ç½®ç¼–ç è§£å†³äº†é—®é¢˜ï¼Œä½†å®ƒæ˜¯ä¸€ç§äº‹åè¡¥æ•‘ã€‚Self-Attentionæœ¬èº«ä¸å…·å¤‡ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½ç½®ä¿¡æ¯æ˜¯å¤–éƒ¨æ³¨å…¥çš„ã€‚è¿™ç§è®¾è®¡æ˜¯å¦æœ€ä¼˜ï¼Ÿæ˜¯å¦æœ‰æ›´ä¼˜é›…çš„æ–¹å¼è®©æ³¨æ„åŠ›æœºåˆ¶å¤©ç”Ÿå…·æœ‰ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Ÿ

**2. $O(n^2)$ å¤æ‚åº¦**

å…¨å±€æ³¨æ„åŠ›çš„ä»£ä»·æ˜¯ $n \times n$ çš„æ³¨æ„åŠ›çŸ©é˜µã€‚å½“åºåˆ—é•¿åº¦è¾¾åˆ°æ•°ä¸‡ç”šè‡³æ•°åä¸‡æ—¶ï¼Œè¿™å˜å¾—ä¸å¯è¡Œã€‚å¦‚ä½•åœ¨ä¿æŒå…¨å±€è§†é‡çš„åŒæ—¶é™ä½å¤æ‚åº¦ï¼Ÿ

**3. å•ä¸€çš„æ³¨æ„åŠ›æ¨¡å¼**

åœ¨ä¸€å±‚Self-Attentionä¸­ï¼Œæ¯ä¸ªä½ç½®åªç”Ÿæˆä¸€ç»„Queryã€Keyã€Valueã€‚ä½†ç›´è§‰ä¸Šï¼Œä¸€ä¸ªä½ç½®å¯èƒ½éœ€è¦åŒæ—¶å…³æ³¨å¤šç§ä¸åŒç±»å‹çš„ä¿¡æ¯â€”â€”æ¯”å¦‚ä¸»è¯­éœ€è¦å…³æ³¨åŠ¨è¯ï¼ˆè¯­æ³•ï¼‰ï¼Œä¹Ÿéœ€è¦å…³æ³¨è¯­å¢ƒä¸­çš„ç›¸å…³å®ä½“ï¼ˆè¯­ä¹‰ï¼‰ã€‚èƒ½å¦è®©æ¨¡å‹åŒæ—¶æ•è·å¤šç§å…³æ³¨æ¨¡å¼ï¼Ÿ

### è¿™äº›å±€é™æŒ‡å‘ä»€ä¹ˆï¼Ÿ

Self-Attentionä¸ºåºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€ï¼Œä½†è¦æ„å»ºå®Œæ•´çš„æ¶æ„ï¼Œè¿˜éœ€è¦å›ç­”å‡ ä¸ªå…³é”®é—®é¢˜ï¼š

**å¦‚ä½•å †å å¤šå±‚ï¼Ÿ** ç®€å•åœ°å †å Self-Attentionå±‚ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿéœ€è¦ä»€ä¹ˆé¢å¤–çš„ç»„ä»¶æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼Ÿ

**å¦‚ä½•å¤„ç†ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Ÿ** æœºå™¨ç¿»è¯‘éœ€è¦ç¼–ç å™¨ç†è§£æºè¯­è¨€ï¼Œè§£ç å™¨ç”Ÿæˆç›®æ ‡è¯­è¨€ã€‚Self-Attentionå¦‚ä½•åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ä½¿ç”¨ï¼Ÿè§£ç å™¨å¦‚ä½•æ—¢å…³æ³¨è‡ªå·±å·²ç”Ÿæˆçš„å†…å®¹ï¼Œåˆå…³æ³¨ç¼–ç å™¨çš„è¾“å‡ºï¼Ÿ

**å¦‚ä½•å®ç°"å¤šå¤´"æ³¨æ„åŠ›ï¼Ÿ** èƒ½å¦è®©å¤šä¸ªæ³¨æ„åŠ›"å¤´"åŒæ—¶å·¥ä½œï¼Œæ¯ä¸ªå¤´å…³æ³¨ä¸åŒçš„å­ç©ºé—´ï¼Œæœ€åç»¼åˆå®ƒä»¬çš„è¾“å‡ºï¼Ÿ

è¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œå°†åœ¨ä¸‹ä¸€ç« æ­æ™“ã€‚2017å¹´ï¼ŒGoogleçš„ç ”ç©¶å›¢é˜Ÿåœ¨è®ºæ–‡"Attention Is All You Need"ä¸­æå‡ºäº†**Transformer**æ¶æ„â€”â€”ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚å®ƒä¸ä»…å›ç­”äº†ä¸Šè¿°æ‰€æœ‰é—®é¢˜ï¼Œè¿˜ä»¥æƒŠäººçš„æ•ˆæœè¯æ˜äº†ä¸€ä¸ªå¤§èƒ†çš„è®ºæ–­ï¼š**æ³¨æ„åŠ›ï¼Œå°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡**ã€‚

> Self-Attentionè®©åºåˆ—å­¦ä¼šäº†"å®¡è§†è‡ªå·±"ï¼Œæ‰“ç ´äº†RNNçš„é¡ºåºæ·é”ã€‚ä½†å®ƒåªæ˜¯é©å‘½çš„åºæ›²ã€‚å½“ç ”ç©¶è€…æ„è¯†åˆ°å¯ä»¥å®Œå…¨æŠ›å¼ƒå¾ªç¯ç»“æ„ï¼Œåªç”¨æ³¨æ„åŠ›æ¥æ„å»ºæ•´ä¸ªæ¨¡å‹æ—¶ï¼Œæ·±åº¦å­¦ä¹ çš„å†å²ç¿»å¼€äº†æ–°çš„ä¸€é¡µã€‚

---

## æœ¬ç« å°ç»“

::: {.callout-important}
## æ ¸å¿ƒè¦ç‚¹

- **é—®é¢˜**ï¼šå¦‚ä½•è®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®è·å¾—å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶é¿å…RNNçš„é¡ºåºè®¡ç®—ç“¶é¢ˆï¼Ÿ
- **æ´å¯Ÿ**ï¼šè®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ç›´æ¥å…³æ³¨æ‰€æœ‰å…¶ä»–ä½ç½®ï¼ˆSelf-Attentionï¼‰ï¼Œå°† $O(n)$ çš„é—´æ¥è·¯å¾„å˜æˆ $O(1)$ çš„ç›´æ¥è·¯å¾„
- **æ–¹æ³•**ï¼šQuery-Key-Valueä¸‰å…ƒç»„ç»“æ„ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º
- **ä»£ä»·**ï¼šSelf-Attentionä¸¢å¤±ä½ç½®ä¿¡æ¯ï¼Œéœ€è¦ä½ç½®ç¼–ç æ¥å¼¥è¡¥ï¼›å¤æ‚åº¦ä¸º $O(n^2)$
- **æ„ä¹‰**ï¼šSelf-Attentionä¸ºå®Œå…¨æŠ›å¼ƒRNNã€æ„å»ºçº¯æ³¨æ„åŠ›æ¶æ„å¥ å®šäº†åŸºç¡€
:::

### å…³é”®å…¬å¼é€ŸæŸ¥

**Self-Attention**ï¼š

$$
\text{SelfAttn}(\mathbf{X}) = \text{softmax}\left(\frac{\mathbf{X}\mathbf{W}_Q (\mathbf{X}\mathbf{W}_K)^\top}{\sqrt{d_k}}\right) \mathbf{X}\mathbf{W}_V
$$

**æ­£å¼¦ä½ç½®ç¼–ç **ï¼š

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

**ä¸Cross-Attentionçš„å¯¹æ¯”**ï¼š

| ç±»å‹ | Queryæ¥æº | Key/Valueæ¥æº | ç”¨é€” |
|------|-----------|---------------|------|
| Cross-Attention | è§£ç å™¨ | ç¼–ç å™¨ | è·¨åºåˆ—å…³æ³¨ |
| Self-Attention | åŒä¸€åºåˆ— | åŒä¸€åºåˆ— | åºåˆ—å†…éƒ¨å»ºæ¨¡ |

---

## æ€è€ƒé¢˜

1. **[æ¦‚å¿µç†è§£]** Self-Attentionä¸ºä»€ä¹ˆæ˜¯"ç½®æ¢ç­‰å˜"çš„ï¼Ÿè®¾è®¡ä¸€ä¸ªç®€å•çš„å®éªŒæ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®©æ¨¡å‹åŒºåˆ† "dog bites man" å’Œ "man bites dog"ï¼Œä»…é Self-Attentionï¼ˆä¸åŠ ä½ç½®ç¼–ç ï¼‰èƒ½åšåˆ°å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

2. **[æ•°å­¦æ¨å¯¼]** è¯æ˜æ­£å¼¦ä½ç½®ç¼–ç çš„ç›¸å¯¹ä½ç½®æ€§è´¨ï¼š$PE_{pos+k}$ å¯ä»¥è¡¨ç¤ºä¸º $PE_{pos}$ çš„çº¿æ€§å˜æ¢ã€‚å…·ä½“å†™å‡ºå˜æ¢çŸ©é˜µçš„å½¢å¼ã€‚ï¼ˆæç¤ºï¼šåˆ©ç”¨ä¸‰è§’å‡½æ•°çš„å’Œå·®å…¬å¼ï¼‰

3. **[å·¥ç¨‹å®è·µ]** å®ç°ä¸€ä¸ªç®€å•çš„Self-Attentionæ–‡æœ¬åˆ†ç±»å™¨ï¼ˆä¸ä½¿ç”¨RNNï¼‰ï¼š
   - è¾“å…¥ï¼šIMDBç”µå½±è¯„è®º
   - æ¶æ„ï¼šEmbedding â†’ Self-Attention â†’ å¹³å‡æ± åŒ– â†’ åˆ†ç±»å¤´
   - å¯¹æ¯”æœ‰æ— ä½ç½®ç¼–ç çš„æ•ˆæœå·®å¼‚

4. **[æ‰¹åˆ¤æ€è€ƒ]** Memory Networkså’ŒSelf-Attentionåœ¨å½¢å¼ä¸Šéå¸¸ç›¸ä¼¼ã€‚å®ƒä»¬çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼ŸMemory Networksçš„"å¤–éƒ¨è®°å¿†"å’ŒSelf-Attentionçš„"åºåˆ—ä½œä¸ºè®°å¿†"æœ‰ä»€ä¹ˆä¸åŒçš„è®¾è®¡è€ƒé‡ï¼Ÿ

5. **[å¼€æ”¾é—®é¢˜]** Self-Attentionçš„ $O(n^2)$ å¤æ‚åº¦æ˜¯ä¸€ä¸ªæ ¹æœ¬é™åˆ¶å—ï¼Ÿæœ‰æ²¡æœ‰å¯èƒ½è®¾è®¡å‡º $O(n)$ å¤æ‚åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶ä¿æŒ $O(1)$ çš„æœ€é•¿è·¯å¾„ï¼Ÿç°æœ‰çš„çº¿æ€§æ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰ä¸ºä»€ä¹ˆä¼šæœ‰æ€§èƒ½æŸå¤±ï¼Ÿ

---

## å»¶ä¼¸é˜…è¯»

### æ ¸å¿ƒè®ºæ–‡ï¼ˆå¿…è¯»ï¼‰

- **[Vaswani et al., 2017] Attention Is All You Need**
  - æå‡ºTransformerå’ŒScaled Dot-Product Attention
  - é‡ç‚¹è¯»ï¼šSection 3.2ï¼ˆAttentionï¼‰ã€Section 3.5ï¼ˆä½ç½®ç¼–ç ï¼‰
  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)

- **[Sukhbaatar et al., 2015] End-To-End Memory Networks**
  - Self-Attentionçš„æ€æƒ³å…ˆé©±
  - é‡ç‚¹è¯»ï¼šSection 2ï¼ˆæ¨¡å‹æ¶æ„ï¼‰
  - arXiv: [1503.08895](https://arxiv.org/abs/1503.08895)

### ç†è®ºåŸºç¡€

- **[Yun et al., 2020] Are Transformers universal approximators of sequence-to-sequence functions?**
  - è¯æ˜Transformerçš„é€šç”¨é€¼è¿‘æ€§è´¨
  - arXiv: [1912.10077](https://arxiv.org/abs/1912.10077)

- **[Ramsauer et al., 2020] Hopfield Networks is All You Need**
  - å°†Attentionè§£é‡Šä¸ºç°ä»£Hopfieldç½‘ç»œ
  - arXiv: [2008.02217](https://arxiv.org/abs/2008.02217)

### åç»­å‘å±•

- **[Shaw et al., 2018] Self-Attention with Relative Position Representations**
  - ç›¸å¯¹ä½ç½®ç¼–ç çš„æ—©æœŸå·¥ä½œ
  - arXiv: [1803.02155](https://arxiv.org/abs/1803.02155)

- **[Su et al., 2021] RoFormer: Enhanced Transformer with Rotary Position Embedding**
  - æå‡ºRoPEï¼Œæˆä¸ºç°ä»£LLMçš„æ ‡é…
  - arXiv: [2104.09864](https://arxiv.org/abs/2104.09864)

### å¯è§†åŒ–èµ„æº

- **[Jay Alammar] The Illustrated Transformer**
  - æœ€ä½³å¯è§†åŒ–æ•™ç¨‹
  - ç½‘å€: [jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)

---

## å†å²æ³¨è„š

Self-Attentionçš„æƒ³æ³•å¹¶éå‡­ç©ºå‡ºç°ã€‚åœ¨2017å¹´Transformerè®ºæ–‡ä¹‹å‰ï¼Œå·²ç»æœ‰å¤šæ¡ç ”ç©¶è„‰ç»œåœ¨æ¢ç´¢ç±»ä¼¼çš„æ€æƒ³ï¼š

**Memory Networks (2014-2015)**ï¼šFacebook AI Researchçš„å›¢é˜Ÿæå‡ºäº†ç”¨æ³¨æ„åŠ›æœºåˆ¶ä»å¤–éƒ¨è®°å¿†ä¸­æ£€ç´¢ä¿¡æ¯çš„æƒ³æ³•ã€‚è™½ç„¶ä»–ä»¬çš„ç›®æ ‡æ˜¯é—®ç­”ç³»ç»Ÿè€Œéåºåˆ—å»ºæ¨¡ï¼Œä½†Key-Valueåˆ†ç¦»ã€è½¯æ³¨æ„åŠ›æ£€ç´¢ç­‰è®¾è®¡æ·±åˆ»å½±å“äº†åæ¥çš„å‘å±•ã€‚

**Neural Turing Machines (2014)**ï¼šDeepMindæå‡ºçš„ç¥ç»å›¾çµæœºä¹Ÿä½¿ç”¨äº†ç±»ä¼¼æ³¨æ„åŠ›çš„æœºåˆ¶æ¥è¯»å†™å¤–éƒ¨è®°å¿†ã€‚è™½ç„¶æ›´åŠ å¤æ‚ï¼Œä½†å®ƒå±•ç¤ºäº†ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ ç±»ä¼¼"æŸ¥æ‰¾è¡¨"çš„æ“ä½œã€‚

**Decomposable Attention (2016)**ï¼šParikhç­‰äººæå‡ºäº†ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›çš„è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œä¸ä½¿ç”¨ä»»ä½•RNNã€‚è¿™æ˜¯"çº¯æ³¨æ„åŠ›"æ¨¡å‹çš„æ—©æœŸæˆåŠŸæ¡ˆä¾‹ã€‚

**Transformerçš„è´¡çŒ®**ï¼šä¸æ˜¯å‘æ˜Self-Attentionï¼Œè€Œæ˜¯å°†å…¶ç³»ç»ŸåŒ–å¹¶è¯æ˜å®ƒå¯ä»¥å®Œå…¨æ›¿ä»£RNNã€‚"Attention Is All You Need"è¿™ä¸ªå¤§èƒ†çš„æ ‡é¢˜ï¼Œæ—¢æ˜¯æŠ€æœ¯å£°æ˜ï¼Œä¹Ÿæ˜¯ç ”ç©¶å®£è¨€ã€‚

æœ‰è¶£çš„æ˜¯ï¼ŒTransformerè®ºæ–‡çš„ä½œè€…ä¹‹ä¸€Ashish Vaswaniåæ¥å›å¿†è¯´ï¼Œä»–ä»¬æœ€åˆå¹¶ä¸ç¡®å®šå®Œå…¨æŠ›å¼ƒRNNæ˜¯å¦å¯è¡Œã€‚å®éªŒç»“æœçš„æƒŠäººæ•ˆæœè¶…å‡ºäº†æ‰€æœ‰äººçš„é¢„æœŸâ€”â€”ä¸ä»…æ•ˆæœæ›´å¥½ï¼Œè®­ç»ƒè¿˜å¿«äº†ä¸€ä¸ªæ•°é‡çº§ã€‚è¿™ä¸ªç»“æœæ”¹å˜äº†æ•´ä¸ªé¢†åŸŸçš„æ–¹å‘ã€‚

ä»Memory Networksåˆ°Transformerï¼Œä»è¾…åŠ©æœºåˆ¶åˆ°æ ¸å¿ƒæ¶æ„ï¼ŒSelf-Attentionå®Œæˆäº†ä»"é…è§’"åˆ°"ä¸»è§’"çš„åä¸½è½¬èº«ã€‚ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†è§è¯è¿™åœºé©å‘½çš„é«˜æ½®ï¼šTransformerâ€”â€”"Attention Is All You Need"ã€‚
