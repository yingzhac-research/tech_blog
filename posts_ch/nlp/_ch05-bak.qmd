---
title: "ç¬¬5ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„è¯ç”Ÿ"
subtitle: "æ‰“ç ´ä¿¡æ¯ç“¶é¢ˆï¼šè®©è§£ç å™¨å­¦ä¼š'å›å¤´çœ‹'"
author: "Ying Zha"
date: "2026-01-25"
categories: [NLP, Attention, Seq2Seq, æœºå™¨ç¿»è¯‘, Bahdanau]
tags: [æ³¨æ„åŠ›æœºåˆ¶, åŠ æ€§æ³¨æ„åŠ›, å¯¹é½æ¨¡å‹, ä¸Šä¸‹æ–‡å‘é‡, ç¥ç»æœºå™¨ç¿»è¯‘]
description: "Attentionæœºåˆ¶çš„è¯ç”Ÿï¼šBahdanauå¦‚ä½•è®©è§£ç å™¨å­¦ä¼š'å›å¤´çœ‹'ï¼Œæ‰“ç ´Seq2Seqçš„ä¿¡æ¯ç“¶é¢ˆï¼Œä»¥åŠæ³¨æ„åŠ›æƒé‡çš„å¯è§£é‡Šæ€§ã€‚"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    css: styles.css
    fig-cap-location: bottom
---

> **æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•è®©è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œèƒ½å¤Ÿè®¿é—®è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯åªä¾èµ–ä¸€ä¸ªå‹ç¼©åçš„å‘é‡ï¼Ÿ
>
> **å†å²åæ ‡**ï¼š2014-2015 | Bahdanau, Cho, Bengio | ç¥ç»æœºå™¨ç¿»è¯‘çš„çªç ´

---

## ä»ä¸Šä¸€ç« è¯´èµ·

ä¸Šä¸€ç« æˆ‘ä»¬è§è¯äº†RNNçš„è¾‰ç…Œä¸å›°å¢ƒã€‚LSTMå’ŒGRUé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒSeq2Seqæ¶æ„è®©ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤„ç†ç¿»è¯‘ã€æ‘˜è¦ç­‰åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ã€‚

ä½†Seq2Seqæœ‰ä¸€ä¸ªè‡´å‘½çš„è®¾è®¡ç¼ºé™·ï¼š**ä¿¡æ¯ç“¶é¢ˆ**ã€‚

å›é¡¾Seq2Seqçš„å·¥ä½œæ–¹å¼ï¼šç¼–ç å™¨è¯»å–æ•´ä¸ªè¾“å…¥åºåˆ—ï¼Œå°†æ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ ä¸­ï¼›è§£ç å™¨ä»…å‡­è¿™ä¸ªå‘é‡ï¼Œé€è¯ç”Ÿæˆè¾“å‡ºã€‚è¿™æ„å‘³ç€ï¼Œæ— è®ºè¾“å…¥æ˜¯5ä¸ªè¯è¿˜æ˜¯50ä¸ªè¯ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½è¦å¡è¿›åŒä¸€ä¸ªç»´åº¦çš„å‘é‡ã€‚

Sutskeverç­‰äºº(2014)çš„å®éªŒæ¸…æ¥šåœ°å±•ç¤ºäº†è¿™ä¸ªé—®é¢˜ï¼šå½“è¾“å…¥å¥å­è¶…è¿‡20ä¸ªè¯æ—¶ï¼Œç¿»è¯‘è´¨é‡æ€¥å‰§ä¸‹é™ã€‚æ›´é•¿çš„å¥å­åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œè€Œå›ºå®šå¤§å°çš„å‘é‡æ— æ³•æ‰¿è½½ã€‚

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ„Ÿå—è¿™ä¸ªé—®é¢˜ã€‚è€ƒè™‘ç¿»è¯‘ä»»åŠ¡ï¼š

> **è‹±è¯­**ï¼šThe agreement on the European Economic Area was signed in August 1992.
>
> **æ³•è¯­**ï¼šL'accord sur la zone Ã©conomique europÃ©enne a Ã©tÃ© signÃ© en aoÃ»t 1992.

å½“è§£ç å™¨ç”Ÿæˆ"aoÃ»t"ï¼ˆå…«æœˆï¼‰æ—¶ï¼Œå®ƒéœ€è¦çŸ¥é“åŸæ–‡ä¸­çš„"August"ã€‚ä½†åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œ"August"è¿™ä¸ªè¯é¦–å…ˆè¢«ç¼–ç è¿›éšè—çŠ¶æ€ï¼Œç„¶åä¸å…¶ä»–æ‰€æœ‰è¯çš„ä¿¡æ¯æ··åˆåœ¨ä¸€èµ·ï¼Œæœ€ç»ˆå‹ç¼©æˆä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ã€‚è§£ç å™¨è¦ä»è¿™ä¸ªå‹ç¼©åçš„å‘é‡ä¸­"æŒ–å‡º"Augustçš„ä¿¡æ¯â€”â€”è¿™å°±åƒä»ä¸€é”…æ±¤é‡Œæ‰¾å›åŸæ¥çš„é£Ÿæã€‚

æ›´ç³Ÿç³•çš„æ˜¯ï¼Œå¥å­ä¸­çš„æŸäº›è¯å¯¹å½“å‰ç”Ÿæˆçš„è¯æ›´é‡è¦ã€‚ç¿»è¯‘"August"æ—¶ï¼Œæ¨¡å‹æœ€éœ€è¦å…³æ³¨çš„æ˜¯åŸæ–‡ä¸­çš„"August"ï¼Œè€Œä¸æ˜¯"The"æˆ–"was"ã€‚ä½†æ ‡å‡†Seq2Seqå¯¹æ‰€æœ‰è¾“å…¥ä½ç½®ä¸€è§†åŒä»â€”â€”å®ƒä»¬éƒ½è¢«åŒç­‰åœ°å‹ç¼©è¿›äº† $\mathbf{c}$ã€‚

> ğŸ’¡ **æœ¬ç« æ ¸å¿ƒæ´å¯Ÿ**ï¼šè§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œåº”è¯¥èƒ½å¤Ÿ**æœ‰é€‰æ‹©åœ°å…³æ³¨**è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®ã€‚ä¸åŒçš„è¾“å‡ºè¯éœ€è¦å…³æ³¨ä¸åŒçš„è¾“å…¥è¯â€”â€”è¿™å°±æ˜¯"æ³¨æ„åŠ›"çš„æœ¬è´¨ã€‚Attentionæœºåˆ¶è®©è§£ç å™¨åœ¨æ¯ä¸€æ­¥éƒ½èƒ½è®¿é—®ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼Œå¹¶æ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€è®¡ç®—å®ƒä»¬çš„é‡è¦æ€§æƒé‡ã€‚

---

## é—®é¢˜çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ

### é—®é¢˜çš„ç²¾ç¡®å®šä¹‰

è®©æˆ‘ä»¬å½¢å¼åŒ–åœ°æè¿°Seq2Seqçš„ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ã€‚

åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œç¼–ç å™¨äº§ç”Ÿä¸€ç³»åˆ—éšè—çŠ¶æ€ $\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, \ldots, \mathbf{h}_T^{enc}$ï¼Œä½†åªæœ‰æœ€åä¸€ä¸ªçŠ¶æ€ $\mathbf{h}_T^{enc}$ è¢«ä¼ é€’ç»™è§£ç å™¨ä½œä¸ºä¸Šä¸‹æ–‡å‘é‡ï¼š

$$
\mathbf{c} = \mathbf{h}_T^{enc}
$$

è§£ç å™¨çš„æ¯ä¸€æ­¥éƒ½ä½¿ç”¨è¿™åŒä¸€ä¸ª $\mathbf{c}$ï¼š

$$
\mathbf{h}_t^{dec} = f(\mathbf{h}_{t-1}^{dec}, y_{t-1}, \mathbf{c})
$$

é—®é¢˜åœ¨äºï¼š$\mathbf{c}$ æ˜¯ä¸€ä¸ª**é™æ€**çš„ã€**å…¨å±€**çš„è¡¨ç¤ºã€‚å®ƒåœ¨è§£ç çš„æ¯ä¸€æ­¥éƒ½ä¿æŒä¸å˜ï¼Œæ— æ³•æ ¹æ®å½“å‰ç”Ÿæˆçš„è¯åŠ¨æ€è°ƒæ•´ã€‚

ä»ä¿¡æ¯è®ºçš„è§’åº¦çœ‹ï¼Œå¦‚æœè¾“å…¥åºåˆ— $\mathbf{x}$ çš„ä¿¡æ¯ç†µæ˜¯ $H(\mathbf{x})$ï¼Œè€Œ $\mathbf{c}$ çš„ç»´åº¦æ˜¯ $d$ï¼Œé‚£ä¹ˆ $\mathbf{c}$ æœ€å¤šèƒ½æºå¸¦ $O(d)$ çš„ä¿¡æ¯é‡ã€‚å½“ $H(\mathbf{x}) > O(d)$ æ—¶ï¼Œä¿¡æ¯ä¸¢å¤±æ˜¯ä¸å¯é¿å…çš„ã€‚

### ä¹‹å‰çš„å°è¯•ä¸ºä½•å¤±è´¥ï¼Ÿ

åœ¨Attentionå‡ºç°ä¹‹å‰ï¼Œç ”ç©¶è€…å°è¯•è¿‡ä¸€äº›ç¼“è§£ä¿¡æ¯ç“¶é¢ˆçš„æ–¹æ³•ï¼š

**å¢åŠ ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦**ï¼šç›´è§‰ä¸Šï¼Œæ›´å¤§çš„ $\mathbf{c}$ å¯ä»¥æºå¸¦æ›´å¤šä¿¡æ¯ã€‚ä½†è¿™åªæ˜¯å»¶ç¼“é—®é¢˜ï¼Œè€Œéè§£å†³é—®é¢˜ã€‚è€Œä¸”æ›´å¤§çš„å‘é‡æ„å‘³ç€æ›´å¤šå‚æ•°ï¼Œæ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

**ä½¿ç”¨åŒå‘RNN**ï¼šè®©ç¼–ç å™¨åŒæ—¶ä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦è¯»å–è¾“å…¥ï¼Œç„¶åæ‹¼æ¥ä¸¤ä¸ªæ–¹å‘çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚è¿™ç¡®å®èƒ½æ•è·æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†ä»ç„¶æ˜¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå‘é‡â€”â€”åªæ˜¯è¿™ä¸ªå‘é‡ç¨å¾®å¤§äº†ä¸€ç‚¹ã€‚

**è¾“å…¥åè½¬**ï¼šSutskeverç­‰äººå‘ç°ï¼Œå°†è¾“å…¥åºåˆ—åè½¬åå†è¾“å…¥ç¼–ç å™¨ï¼Œç¿»è¯‘æ•ˆæœæ›´å¥½ã€‚è¿™æ˜¯å› ä¸ºè¾“å…¥çš„æœ€åå‡ ä¸ªè¯ï¼ˆåè½¬åå˜æˆæœ€å…ˆè¾“å…¥çš„è¯ï¼‰ä¸è¾“å‡ºçš„æœ€å…ˆå‡ ä¸ªè¯å¾€å¾€æœ‰æ›´å¼ºçš„å¯¹åº”å…³ç³»ã€‚ä½†è¿™åªæ˜¯ä¸€ä¸ªå¯å‘å¼æŠ€å·§ï¼Œä¸èƒ½ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜ã€‚

è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è§¦åŠé—®é¢˜çš„æ ¸å¿ƒï¼š**è§£ç å™¨åªèƒ½çœ‹åˆ°ä¸€ä¸ªå›ºå®šçš„ã€å…¨å±€çš„è¡¨ç¤ºï¼Œæ— æ³•åŠ¨æ€åœ°è®¿é—®è¾“å…¥çš„ä¸åŒéƒ¨åˆ†**ã€‚

### æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„è§£å†³æ–¹æ¡ˆï¼Ÿ

ç†æƒ³çš„è§£å†³æ–¹æ¡ˆåº”è¯¥å…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

1. **åŠ¨æ€æ€§**ï¼šè§£ç å™¨åœ¨ç”Ÿæˆä¸åŒè¯æ—¶ï¼Œåº”è¯¥èƒ½å¤Ÿå…³æ³¨è¾“å…¥çš„ä¸åŒä½ç½®
2. **è½¯é€‰æ‹©**ï¼šä¸æ˜¯ç¡¬æ€§åœ°é€‰æ‹©æŸä¸€ä¸ªä½ç½®ï¼Œè€Œæ˜¯å¯¹æ‰€æœ‰ä½ç½®è®¡ç®—ä¸€ä¸ªé‡è¦æ€§åˆ†å¸ƒ
3. **ç«¯åˆ°ç«¯å¯è®­ç»ƒ**ï¼šæ•´ä¸ªæœºåˆ¶åº”è¯¥å¯ä»¥é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–
4. **å¯è§£é‡Šæ€§**ï¼šæ¨¡å‹å…³æ³¨å“ªäº›ä½ç½®åº”è¯¥æ˜¯å¯ä»¥è§‚å¯Ÿå’Œç†è§£çš„

è¿™äº›ç‰¹æ€§æ­£æ˜¯Attentionæœºåˆ¶æ‰€æä¾›çš„ã€‚

---

## æ ¸å¿ƒæ€æƒ³ä¸ç›´è§‰

### å…³é”®æ´å¯Ÿï¼šåŠ¨æ€çš„ã€åŸºäºå†…å®¹çš„å¯»å€

Attentionçš„æ ¸å¿ƒæ´å¯Ÿå¯ä»¥ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼š

> **è®©è§£ç å™¨åœ¨æ¯ä¸€æ­¥éƒ½èƒ½"å›å¤´çœ‹"ç¼–ç å™¨çš„æ‰€æœ‰ä½ç½®ï¼Œå¹¶æ ¹æ®å½“å‰éœ€è¦åŠ¨æ€å†³å®šå…³æ³¨å“ªäº›ä½ç½®ã€‚**

è¿™ä¸ªæƒ³æ³•å¬èµ·æ¥ç®€å•ï¼Œä½†å®ƒå½»åº•æ”¹å˜äº†åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„èŒƒå¼ã€‚

### ç›´è§‰è§£é‡Šï¼šèšå…‰ç¯ä¸å›¾ä¹¦é¦†

æƒ³è±¡ä½ åœ¨ä¸€ä¸ªé»‘æš—çš„å›¾ä¹¦é¦†é‡Œæ‰¾ä¹¦ã€‚ä¼ ç»ŸSeq2Seqå°±åƒæ˜¯ï¼šä½ å…ˆç”¨æ‰‹ç”µç­’å¿«é€Ÿæ‰«è¿‡æ‰€æœ‰ä¹¦æ¶ï¼Œç„¶åå…³æ‰æ‰‹ç”µç­’ï¼Œä»…å‡­è®°å¿†å»å–ä¹¦ã€‚ä½ å¯¹æ•´ä¸ªå›¾ä¹¦é¦†æœ‰ä¸€ä¸ªæ¨¡ç³Šçš„æ•´ä½“å°è±¡ï¼Œä½†ç»†èŠ‚å¾ˆå®¹æ˜“é—å¿˜ã€‚

Attentionæœºåˆ¶åˆ™åƒæ˜¯ï¼šä½ æ‰‹é‡Œæœ‰ä¸€ä¸ª**å¯è°ƒèŠ‚çš„èšå…‰ç¯**ã€‚å½“ä½ éœ€è¦æ‰¾æŸæœ¬ä¹¦æ—¶ï¼Œä½ å¯ä»¥æŠŠèšå…‰ç¯ç…§å‘ç›¸å…³çš„ä¹¦æ¶ï¼Œä»”ç»†æŸ¥çœ‹é‚£é‡Œçš„ä¹¦åã€‚ä¸åŒçš„æŸ¥è¯¢éœ€æ±‚ä¼šè®©ä½ æŠŠå…‰ç…§å‘ä¸åŒçš„ä½ç½®ã€‚

æ›´å…·ä½“åœ°è¯´ï¼Œå½“è§£ç å™¨ç”Ÿæˆ"aoÃ»t"ï¼ˆå…«æœˆï¼‰è¿™ä¸ªè¯æ—¶ï¼ŒAttentionæœºåˆ¶ä¼šï¼š

1. æŸ¥çœ‹ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼ˆå›¾ä¹¦é¦†çš„æ‰€æœ‰ä¹¦æ¶ï¼‰
2. è®¡ç®—æ¯ä¸ªä½ç½®ä¸å½“å‰ä»»åŠ¡çš„ç›¸å…³æ€§ï¼ˆåˆ¤æ–­æ¯ä¸ªä¹¦æ¶æ˜¯å¦å¯èƒ½æœ‰ä½ è¦çš„ä¹¦ï¼‰
3. æŠŠ"èšå…‰ç¯"ä¸»è¦ç…§å‘ç›¸å…³çš„ä½ç½®ï¼ˆ"August"å¯¹åº”çš„ç¼–ç å™¨çŠ¶æ€ï¼‰
4. ä»è¿™äº›ä½ç½®æ±‡æ€»ä¿¡æ¯ï¼Œè¾…åŠ©ç”Ÿæˆå½“å‰è¯

### å¦ä¸€ä¸ªç±»æ¯”ï¼šåŠ æƒæŠ•ç¥¨

ä½ ä¹Ÿå¯ä»¥æŠŠAttentionç†è§£ä¸ºä¸€ç§åŠ æƒæŠ•ç¥¨æœºåˆ¶ã€‚

æƒ³è±¡è§£ç å™¨æ˜¯ä¸€ä¸ªé¢†å¯¼ï¼Œéœ€è¦åšä¸€ä¸ªå†³å®šï¼ˆç”Ÿæˆä¸‹ä¸€ä¸ªè¯ï¼‰ã€‚å®ƒæœ‰ä¸€ä¸ªé¡¾é—®å›¢é˜Ÿï¼ˆç¼–ç å™¨çš„å„ä¸ªéšè—çŠ¶æ€ï¼‰ï¼Œæ¯ä¸ªé¡¾é—®æŒæ¡è¾“å…¥åºåˆ—ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯ã€‚

ä¼ ç»ŸSeq2Seqï¼šåªå¬ä¸€ä¸ª"æ€»é¡¾é—®"çš„æ„è§ï¼ˆä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ï¼‰ï¼Œè¿™ä¸ªæ€»é¡¾é—®è¦ç»¼åˆæ‰€æœ‰äººçš„ä¿¡æ¯ã€‚

Attentionæœºåˆ¶ï¼šç›´æ¥å¾è¯¢æ¯ä¸ªé¡¾é—®çš„æ„è§ï¼Œç„¶åæ ¹æ®è®®é¢˜ç›¸å…³æ€§ç»™ä¸åŒé¡¾é—®çš„æ„è§èµ‹äºˆä¸åŒæƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—å‡ºæœ€ç»ˆå†³å®šã€‚

### è®¾è®¡åŠ¨æœºï¼šä¸ºä»€ä¹ˆé€‰æ‹©è½¯æ³¨æ„åŠ›ï¼Ÿ

Attentionæœºåˆ¶æœ‰ä¸¤ç§å˜ä½“ï¼š

- **è½¯æ³¨æ„åŠ›ï¼ˆSoft Attentionï¼‰**ï¼šå¯¹æ‰€æœ‰ä½ç½®è®¡ç®—æ¦‚ç‡åˆ†å¸ƒï¼ŒåŠ æƒæ±‚å’Œ
- **ç¡¬æ³¨æ„åŠ›ï¼ˆHard Attentionï¼‰**ï¼šé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼Œåªçœ‹é‚£é‡Œçš„ä¿¡æ¯

Bahdanauç­‰äººé€‰æ‹©äº†è½¯æ³¨æ„åŠ›ï¼ŒåŸå› æ˜¯ï¼š

1. **å¯å¾®åˆ†**ï¼šè½¯æ³¨æ„åŠ›çš„åŠ æƒæ±‚å’Œæ˜¯å¯å¾®çš„ï¼Œå¯ä»¥ç”¨æ ‡å‡†çš„åå‘ä¼ æ’­è®­ç»ƒ
2. **ç¨³å®š**ï¼šç¡¬æ³¨æ„åŠ›éœ€è¦é‡‡æ ·æˆ–å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®­ç»ƒï¼Œæ–¹å·®å¤§ï¼Œä¸ç¨³å®š
3. **ä¿¡æ¯æ›´ä¸°å¯Œ**ï¼šè½¯æ³¨æ„åŠ›å¯ä»¥åŒæ—¶åˆ©ç”¨å¤šä¸ªä½ç½®çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯éæ­¤å³å½¼

ç¡¬æ³¨æ„åŠ›ä¹Ÿæœ‰å…¶ä¼˜åŠ¿ï¼ˆè®¡ç®—æ›´é«˜æ•ˆï¼Œæ›´ç¨€ç–ï¼‰ï¼Œä½†åœ¨å®è·µä¸­ï¼Œè½¯æ³¨æ„åŠ›å› å…¶ç®€å•å’Œæœ‰æ•ˆæˆä¸ºäº†ä¸»æµã€‚

---

## æŠ€æœ¯ç»†èŠ‚

### Bahdanau Attentionï¼šåŠ æ€§æ³¨æ„åŠ›

2014å¹´ï¼ŒBahdanauã€Choå’ŒBengioæå‡ºäº†ç¬¬ä¸€ä¸ªæˆåŠŸçš„æ³¨æ„åŠ›æœºåˆ¶ç”¨äºæœºå™¨ç¿»è¯‘ã€‚è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

é¦–å…ˆï¼Œç¼–ç å™¨ä½¿ç”¨**åŒå‘RNN**ï¼Œåœ¨æ¯ä¸ªä½ç½® $j$ äº§ç”Ÿä¸€ä¸ªéšè—çŠ¶æ€ï¼š

$$
\mathbf{h}_j = [\overrightarrow{\mathbf{h}}_j; \overleftarrow{\mathbf{h}}_j]
$$

å…¶ä¸­ $\overrightarrow{\mathbf{h}}_j$ æ˜¯å‰å‘RNNçš„éšè—çŠ¶æ€ï¼Œ$\overleftarrow{\mathbf{h}}_j$ æ˜¯åå‘RNNçš„éšè—çŠ¶æ€ã€‚æ‹¼æ¥åï¼Œ$\mathbf{h}_j$ åŒæ—¶åŒ…å«äº†ä½ç½® $j$ çš„å·¦ä¾§å’Œå³ä¾§ä¸Šä¸‹æ–‡ã€‚

åœ¨è§£ç çš„ç¬¬ $i$ æ­¥ï¼Œæˆ‘ä»¬è®¡ç®—ä¸€ä¸ª**åŠ¨æ€çš„ä¸Šä¸‹æ–‡å‘é‡** $\mathbf{c}_i$ï¼ˆæ³¨æ„ï¼šä¸å†æ˜¯å›ºå®šçš„ $\mathbf{c}$ï¼Œè€Œæ˜¯æ¯ä¸€æ­¥éƒ½ä¸åŒçš„ $\mathbf{c}_i$ï¼‰ï¼š

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

å…¶ä¸­ $\alpha_{ij}$ æ˜¯ç¬¬ $i$ æ­¥è§£ç æ—¶ï¼Œå¯¹è¾“å…¥ä½ç½® $j$ çš„æ³¨æ„åŠ›æƒé‡ã€‚

### æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—

é‚£ä¹ˆ $\alpha_{ij}$ æ˜¯æ€ä¹ˆè®¡ç®—çš„å‘¢ï¼Ÿè¿™æ˜¯Attentionæœºåˆ¶çš„æ ¸å¿ƒã€‚

é¦–å…ˆï¼Œè®¡ç®—ä¸€ä¸ª**å¯¹é½åˆ†æ•°ï¼ˆalignment scoreï¼‰** $e_{ij}$ï¼Œè¡¡é‡è§£ç å™¨å½“å‰çŠ¶æ€ä¸ç¼–ç å™¨ä½ç½® $j$ çš„ç›¸å…³æ€§ï¼š

$$
e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)
$$

å…¶ä¸­ $\mathbf{s}_{i-1}$ æ˜¯è§£ç å™¨åœ¨ç¬¬ $i-1$ æ­¥çš„éšè—çŠ¶æ€ï¼Œ$a$ æ˜¯ä¸€ä¸ª**å¯¹é½æ¨¡å‹ï¼ˆalignment modelï¼‰**ã€‚

Bahdanauä½¿ç”¨äº†ä¸€ä¸ªå•å±‚å‰é¦ˆç½‘ç»œä½œä¸ºå¯¹é½æ¨¡å‹ï¼š

$$
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
$$

è¿™è¢«ç§°ä¸º**åŠ æ€§æ³¨æ„åŠ›ï¼ˆadditive attentionï¼‰**ï¼Œå› ä¸º $\mathbf{s}_{i-1}$ å’Œ $\mathbf{h}_j$ æ˜¯é€šè¿‡åŠ æ³•ç»“åˆçš„ã€‚

ç„¶åï¼Œå¯¹æ‰€æœ‰ä½ç½®çš„åˆ†æ•°åšsoftmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼š

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$

softmaxç¡®ä¿äº†ï¼š

- æ‰€æœ‰æƒé‡éƒ½æ˜¯æ­£æ•°ï¼š$\alpha_{ij} > 0$
- æƒé‡ä¹‹å’Œä¸º1ï¼š$\sum_j \alpha_{ij} = 1$

è¿™æ ·ï¼Œ$\alpha_{ij}$ å¯ä»¥è§£é‡Šä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒâ€”â€”è§£ç å™¨åœ¨ç¬¬ $i$ æ­¥"å…³æ³¨"è¾“å…¥ä½ç½® $j$ çš„æ¦‚ç‡ã€‚

::: {.callout-note}
## Algorithm: Bahdanau Attention (Bahdanau et al., 2015)

```python
def bahdanau_attention(s_prev, encoder_outputs, W_a, U_a, v_a):
    """
    Bahdanau (åŠ æ€§) æ³¨æ„åŠ›æœºåˆ¶

    å‚æ•°:
        s_prev: è§£ç å™¨ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ [batch, dec_hidden]
        encoder_outputs: ç¼–ç å™¨æ‰€æœ‰éšè—çŠ¶æ€ [batch, src_len, enc_hidden]
        W_a, U_a, v_a: å¯å­¦ä¹ å‚æ•°

    è¿”å›:
        context: ä¸Šä¸‹æ–‡å‘é‡ [batch, enc_hidden]
        attention_weights: æ³¨æ„åŠ›æƒé‡ [batch, src_len]
    """
    # Step 1: è®¡ç®—å¯¹é½åˆ†æ•°
    # s_prev å¹¿æ’­åˆ°æ‰€æœ‰æºä½ç½®
    scores = v_a @ tanh(W_a @ s_prev + U_a @ encoder_outputs)  # [batch, src_len]

    # Step 2: Softmax å½’ä¸€åŒ–
    attention_weights = softmax(scores, dim=-1)  # [batch, src_len]

    # Step 3: åŠ æƒæ±‚å’Œ
    context = attention_weights @ encoder_outputs  # [batch, enc_hidden]

    return context, attention_weights
```

*Source: Bahdanau, Cho, & Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", ICLR 2015. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*
:::

ä¸‹å›¾å±•ç¤ºäº†å¸¦Bahdanau Attentionçš„RNN Encoder-Decoderæ¶æ„ï¼š

![å¸¦Bahdanau Attentionçš„RNN Encoder-Decoderæ¶æ„ã€‚ç¼–ç å™¨ï¼ˆåº•éƒ¨ï¼‰ä½¿ç”¨åŒå‘RNNå¤„ç†è¾“å…¥åºåˆ—ï¼Œäº§ç”Ÿéšè—çŠ¶æ€åºåˆ—ã€‚è§£ç å™¨ï¼ˆé¡¶éƒ¨ï¼‰åœ¨æ¯ä¸€æ­¥é€šè¿‡Attentionæœºåˆ¶åŠ¨æ€è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼šå°†å½“å‰è§£ç å™¨çŠ¶æ€ä¸æ‰€æœ‰ç¼–ç å™¨çŠ¶æ€æ¯”è¾ƒï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡$c_t$ï¼Œè¾…åŠ©ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ã€‚](figures/chapter-5/original/fig-bahdanau-attention-d2l.svg){#fig-attention-mechanism width=70%}

::: {.figure-caption}
*Source: Dive into Deep Learning, Figure 11.4.2. [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)*
:::

### å®Œæ•´æ•°å€¼ç¤ºä¾‹ï¼šAttentionè®¡ç®—

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå°ä¾‹å­èµ°ä¸€éå®Œæ•´çš„Attentionè®¡ç®—è¿‡ç¨‹ã€‚

**è®¾å®š**ï¼š

- è¾“å…¥åºåˆ—ï¼š3ä¸ªè¯ï¼ˆ"I love NLP"ï¼‰ï¼Œç¼–ç åå¾—åˆ°3ä¸ªéšè—çŠ¶æ€
- è§£ç å™¨éšè—çŠ¶æ€ç»´åº¦ï¼š$d_s = 4$
- ç¼–ç å™¨éšè—çŠ¶æ€ç»´åº¦ï¼š$d_h = 4$
- æ³¨æ„åŠ›ä¸­é—´ç»´åº¦ï¼š$d_a = 3$

**ç¼–ç å™¨è¾“å‡º**ï¼ˆå‡è®¾å·²ç»è®¡ç®—å¥½ï¼‰ï¼š

$$
\mathbf{h}_1 = [0.2, 0.5, -0.3, 0.8]^\top \quad \text{("I")}
$$

$$
\mathbf{h}_2 = [0.7, -0.2, 0.4, 0.1]^\top \quad \text{("love")}
$$

$$
\mathbf{h}_3 = [-0.1, 0.6, 0.5, -0.4]^\top \quad \text{("NLP")}
$$

**è§£ç å™¨å½“å‰çŠ¶æ€**ï¼ˆæ­£åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªç›®æ ‡è¯ï¼‰ï¼š

$$
\mathbf{s}_0 = [0.1, -0.3, 0.4, 0.2]^\top
$$

**å‚æ•°**ï¼ˆç®€åŒ–çš„éšæœºå€¼ï¼‰ï¼š

$$
\mathbf{W}_a = \begin{bmatrix} 0.1 & -0.2 & 0.3 & 0.1 \\ 0.2 & 0.1 & -0.1 & 0.2 \\ -0.1 & 0.3 & 0.2 & -0.2 \end{bmatrix}, \quad
\mathbf{U}_a = \begin{bmatrix} 0.2 & 0.1 & -0.2 & 0.3 \\ -0.1 & 0.2 & 0.1 & 0.1 \\ 0.3 & -0.1 & 0.2 & -0.1 \end{bmatrix}
$$

$$
\mathbf{v}_a = [0.5, -0.3, 0.4]^\top
$$

**Step 1ï¼šè®¡ç®— $\mathbf{W}_a \mathbf{s}_0$**

$$
\mathbf{W}_a \mathbf{s}_0 = \begin{bmatrix} 0.1 \cdot 0.1 + (-0.2) \cdot (-0.3) + 0.3 \cdot 0.4 + 0.1 \cdot 0.2 \\ \vdots \end{bmatrix} = \begin{bmatrix} 0.21 \\ 0.03 \\ 0.04 \end{bmatrix}
$$

**Step 2ï¼šå¯¹æ¯ä¸ªç¼–ç å™¨çŠ¶æ€è®¡ç®— $\mathbf{U}_a \mathbf{h}_j$**

$$
\mathbf{U}_a \mathbf{h}_1 = [0.33, 0.14, -0.05]^\top
$$

$$
\mathbf{U}_a \mathbf{h}_2 = [0.13, 0.06, 0.29]^\top
$$

$$
\mathbf{U}_a \mathbf{h}_3 = [-0.15, 0.17, 0.05]^\top
$$

**Step 3ï¼šè®¡ç®—å¯¹é½åˆ†æ•° $e_{1j}$**

$$
e_{11} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_0 + \mathbf{U}_a \mathbf{h}_1) = \mathbf{v}_a^\top \tanh([0.54, 0.17, -0.01]^\top)
$$

$$
= [0.5, -0.3, 0.4] \cdot [\tanh(0.54), \tanh(0.17), \tanh(-0.01)]^\top
$$

$$
= [0.5, -0.3, 0.4] \cdot [0.49, 0.17, -0.01]^\top = 0.24 - 0.05 - 0.004 \approx 0.19
$$

ç±»ä¼¼åœ°è®¡ç®— $e_{12}$ å’Œ $e_{13}$ï¼š

$$
e_{12} \approx 0.25, \quad e_{13} \approx 0.08
$$

**Step 4ï¼šSoftmaxå½’ä¸€åŒ–**

$$
\alpha_{11} = \frac{\exp(0.19)}{\exp(0.19) + \exp(0.25) + \exp(0.08)} = \frac{1.21}{1.21 + 1.28 + 1.08} = \frac{1.21}{3.57} \approx 0.34
$$

$$
\alpha_{12} = \frac{1.28}{3.57} \approx 0.36, \quad \alpha_{13} = \frac{1.08}{3.57} \approx 0.30
$$

**Step 5ï¼šè®¡ç®—ä¸Šä¸‹æ–‡å‘é‡**

$$
\mathbf{c}_1 = \alpha_{11} \mathbf{h}_1 + \alpha_{12} \mathbf{h}_2 + \alpha_{13} \mathbf{h}_3
$$

$$
= 0.34 \cdot [0.2, 0.5, -0.3, 0.8]^\top + 0.36 \cdot [0.7, -0.2, 0.4, 0.1]^\top + 0.30 \cdot [-0.1, 0.6, 0.5, -0.4]^\top
$$

$$
\approx [0.29, 0.18, 0.09, 0.19]^\top
$$

**è§£è¯»**ï¼šåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¨¡å‹å¯¹"love"çš„å…³æ³¨æœ€å¤šï¼ˆ0.36ï¼‰ï¼Œå…¶æ¬¡æ˜¯"I"ï¼ˆ0.34ï¼‰å’Œ"NLP"ï¼ˆ0.30ï¼‰ã€‚æ³¨æ„åŠ›æƒé‡ç›¸å¯¹å‡åŒ€ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯éšæœºå‚æ•°ã€‚åœ¨è®­ç»ƒåçš„çœŸå®æ¨¡å‹ä¸­ï¼Œæƒé‡åˆ†å¸ƒä¼šæ›´åŠ å°–é”â€”â€”æ¨¡å‹ä¼šå­¦ä¼šåœ¨éœ€è¦æ—¶èšç„¦äºç‰¹å®šä½ç½®ã€‚

### è§£ç å™¨çš„å®Œæ•´æµç¨‹

æœ‰äº†Attentionæœºåˆ¶ï¼Œè§£ç å™¨çš„æ¯ä¸€æ­¥å·¥ä½œæµç¨‹å˜ä¸ºï¼š

1. **è®¡ç®—æ³¨æ„åŠ›æƒé‡** $\alpha_{ij}$ï¼šåŸºäºå½“å‰è§£ç å™¨çŠ¶æ€å’Œæ‰€æœ‰ç¼–ç å™¨çŠ¶æ€
2. **è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡** $\mathbf{c}_i$ï¼šå¯¹ç¼–ç å™¨çŠ¶æ€åŠ æƒæ±‚å’Œ
3. **æ›´æ–°è§£ç å™¨çŠ¶æ€**ï¼šç»“åˆä¸Šä¸‹æ–‡å‘é‡ã€å‰ä¸€æ­¥è¾“å‡ºã€å‰ä¸€æ­¥çŠ¶æ€

$$
\mathbf{s}_i = f(\mathbf{s}_{i-1}, y_{i-1}, \mathbf{c}_i)
$$

4. **ç”Ÿæˆè¾“å‡º**ï¼šåŸºäºæ–°çš„è§£ç å™¨çŠ¶æ€

$$
P(y_i | y_{<i}, \mathbf{x}) = g(\mathbf{s}_i, y_{i-1}, \mathbf{c}_i)
$$

å…³é”®åŒºåˆ«æ˜¯ï¼š**æ¯ä¸€æ­¥éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„ä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}_i$**ï¼Œå®ƒæ˜¯æ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€è®¡ç®—çš„ã€‚

### å¤æ‚åº¦åˆ†æ

**æ—¶é—´å¤æ‚åº¦**ï¼š

- è®¡ç®—æ‰€æœ‰å¯¹é½åˆ†æ•°ï¼š$O(T_x \cdot T_y \cdot d)$
- å…¶ä¸­ $T_x$ æ˜¯æºåºåˆ—é•¿åº¦ï¼Œ$T_y$ æ˜¯ç›®æ ‡åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯éšè—ç»´åº¦

ä¸æ ‡å‡†Seq2Seqç›¸æ¯”ï¼ŒAttentionå¢åŠ äº† $O(T_x \cdot T_y)$ çš„è®¡ç®—é‡ã€‚å¯¹äºé•¿åºåˆ—ï¼Œè¿™ä¸ªå¼€é”€æ˜¯æ˜¾è‘—çš„ã€‚

**ç©ºé—´å¤æ‚åº¦**ï¼š

- éœ€è¦å­˜å‚¨æ‰€æœ‰ç¼–ç å™¨éšè—çŠ¶æ€ï¼š$O(T_x \cdot d)$
- æ ‡å‡†Seq2Seqåªéœ€è¦å­˜å‚¨æœ€ç»ˆçŠ¶æ€ï¼š$O(d)$

è¿™æ˜¯ç”¨ç©ºé—´æ¢å–æ€§èƒ½çš„å…¸å‹ä¾‹å­ã€‚

---

## æ³¨æ„åŠ›å¯è§†åŒ–ï¼šæ¨¡å‹åœ¨"çœ‹"ä»€ä¹ˆï¼Ÿ

### å¯¹é½çŸ©é˜µ

Attentionæœºåˆ¶çš„ä¸€ä¸ªç¾å¦™ç‰¹æ€§æ˜¯**å¯è§£é‡Šæ€§**ã€‚æ³¨æ„åŠ›æƒé‡ $\alpha_{ij}$ ç›´æ¥å‘Šè¯‰æˆ‘ä»¬ï¼šåœ¨ç”Ÿæˆç¬¬ $i$ ä¸ªç›®æ ‡è¯æ—¶ï¼Œæ¨¡å‹å…³æ³¨äº†å“ªäº›æºè¯ã€‚

æˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰çš„æ³¨æ„åŠ›æƒé‡æ’åˆ—æˆä¸€ä¸ªçŸ©é˜µï¼Œæ¨ªè½´æ˜¯æºåºåˆ—ï¼Œçºµè½´æ˜¯ç›®æ ‡åºåˆ—ã€‚è¿™ä¸ªçŸ©é˜µè¢«ç§°ä¸º**å¯¹é½çŸ©é˜µï¼ˆalignment matrixï¼‰**ã€‚

![å¯¹é½å¯è§†åŒ–ï¼šå››ä¸ªè‹±æ³•ç¿»è¯‘ä¾‹å­çš„æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ã€‚æ¨ªè½´æ˜¯è‹±è¯­æºå¥å­ï¼Œçºµè½´æ˜¯æ³•è¯­ç›®æ ‡å¥å­ã€‚ç™½è‰²è¡¨ç¤ºé«˜æ³¨æ„åŠ›æƒé‡ï¼Œé»‘è‰²è¡¨ç¤ºä½æƒé‡ã€‚æ³¨æ„å¯¹è§’çº¿æ¨¡å¼ï¼ˆå•è¯ä¸€ä¸€å¯¹åº”ï¼‰å’Œåç¦»å¯¹è§’çº¿çš„åŒºåŸŸï¼ˆè¯­åºè°ƒæ•´ï¼‰ã€‚ä¾‹å¦‚(a)ä¸­"August"å¯¹åº”"aoÃ»t"ï¼Œ"European Economic Area"å¯¹åº”"zone Ã©conomique europÃ©enne"ã€‚](figures/chapter-5/original/fig3-alignment-visualization.png){#fig-alignment-visualization width=95%}

::: {.figure-caption}
*Source: Bahdanau, Cho, & Bengio (2015) "Neural Machine Translation by Jointly Learning to Align and Translate", Figure 3. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)*
:::

### å¯¹é½æ¨¡å¼çš„è¯­è¨€å­¦æ„ä¹‰

é€šè¿‡è§‚å¯Ÿå¯¹é½çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸€äº›æœ‰è¶£çš„è¯­è¨€å­¦æ¨¡å¼ï¼š

**1. å•è°ƒå¯¹é½**

å¯¹äºè¯­åºç›¸ä¼¼çš„è¯­è¨€å¯¹ï¼ˆå¦‚è‹±è¯­åˆ°å¾·è¯­çš„æŸäº›ç»“æ„ï¼‰ï¼Œå¯¹é½çŸ©é˜µæ¥è¿‘å¯¹è§’çº¿â€”â€”ç¬¬1ä¸ªæºè¯å¯¹åº”ç¬¬1ä¸ªç›®æ ‡è¯ï¼Œç¬¬2ä¸ªå¯¹åº”ç¬¬2ä¸ªï¼Œä¾æ­¤ç±»æ¨ã€‚

**2. è¯­åºè°ƒæ•´**

å½“æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„è¯åºä¸åŒæ—¶ï¼Œå¯¹é½çŸ©é˜µä¼šåç¦»å¯¹è§’çº¿ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„"red car"ç¿»è¯‘æˆæ³•è¯­æ˜¯"voiture rouge"ï¼ˆè½¦ çº¢ï¼‰ï¼Œå¯¹é½çŸ©é˜µä¼šæ˜¾ç¤ºäº¤å‰æ¨¡å¼ã€‚

**3. ä¸€å¯¹å¤šå’Œå¤šå¯¹ä¸€**

æŸäº›è¯æ²¡æœ‰ç›´æ¥å¯¹åº”ï¼Œæˆ–ä¸€ä¸ªè¯å¯¹åº”å¤šä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„"going to"å¯èƒ½å¯¹åº”æ³•è¯­çš„å•ä¸ªè¯"va"ã€‚

**4. ç©ºå¯¹é½**

æŸäº›ç›®æ ‡è¯ï¼ˆå¦‚å† è¯ï¼‰å¯èƒ½æ²¡æœ‰æ˜ç¡®çš„æºè¯å¯¹åº”ï¼Œå®ƒä»¬çš„æ³¨æ„åŠ›æƒé‡ä¼šåˆ†æ•£åœ¨å¤šä¸ªä½ç½®ã€‚

### å¯è§†åŒ–çš„å±€é™æ€§

è™½ç„¶æ³¨æ„åŠ›å¯è§†åŒ–å¾ˆå¸å¼•äººï¼Œä½†æˆ‘ä»¬è¦è°¨æ…è§£è¯»ï¼š

1. **æ³¨æ„åŠ›ä¸ç­‰äºè§£é‡Š**ï¼šé«˜æ³¨æ„åŠ›æƒé‡ä¸ä¸€å®šæ„å‘³ç€æ¨¡å‹"ç†è§£"äº†é‚£ä¸ªä½ç½®çš„å†…å®¹
2. **å¯èƒ½æœ‰å¤šé‡å› ç´ **ï¼šæ¨¡å‹å¯èƒ½é€šè¿‡å…¶ä»–æœºåˆ¶ï¼ˆå¦‚ä½ç½®ä¿¡æ¯ï¼‰åšå‡ºå†³å®š
3. **è®­ç»ƒç›®æ ‡çš„å½±å“**ï¼šæ³¨æ„åŠ›æƒé‡æ˜¯ä¸ºäº†æœ€å°åŒ–ç¿»è¯‘æŸå¤±è€Œå­¦ä¹ çš„ï¼Œä¸ä¸€å®šåæ˜ äººç±»çš„å¯¹é½ç›´è§‰

åæ¥çš„ç ”ç©¶ï¼ˆå¦‚Jain & Wallace, 2019ï¼‰å¯¹æ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æå‡ºäº†è´¨ç–‘ã€‚ä½†ä½œä¸ºä¸€ä¸ªè¯Šæ–­å·¥å…·ï¼Œæ³¨æ„åŠ›å¯è§†åŒ–ä»ç„¶éå¸¸æœ‰ä»·å€¼ã€‚

---

## å·¥ç¨‹å®è·µï¼šå¸¦Attentionçš„Seq2Seq

è®©æˆ‘ä»¬ç”¨PyTorchå®ç°ä¸€ä¸ªå¸¦Attentionçš„Seq2Seqæ¨¡å‹ã€‚

### ç¼–ç å™¨

```{python}
#| code-fold: false
import torch
import torch.nn as nn
import torch.nn.functional as F

class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.rnn = nn.GRU(
            embed_dim, hidden_dim,
            num_layers=num_layers,
            bidirectional=True,  # åŒå‘
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        # å°†åŒå‘çš„éšè—çŠ¶æ€å‹ç¼©åˆ°å•å‘ç»´åº¦
        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_lengths=None):
        # src: [batch_size, src_len]
        embedded = self.dropout(self.embedding(src))  # [batch, src_len, embed_dim]

        if src_lengths is not None:
            packed = nn.utils.rnn.pack_padded_sequence(
                embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            packed_outputs, hidden = self.rnn(packed)
            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)
        else:
            outputs, hidden = self.rnn(embedded)

        # outputs: [batch, src_len, hidden_dim * 2] (åŒå‘æ‹¼æ¥)
        # hidden: [num_layers * 2, batch, hidden_dim]

        # åˆå¹¶å‰å‘å’Œåå‘çš„æœ€ç»ˆéšè—çŠ¶æ€
        # hidden[-2] æ˜¯æœ€åä¸€å±‚å‰å‘ï¼Œhidden[-1] æ˜¯æœ€åä¸€å±‚åå‘
        hidden = torch.tanh(self.fc(torch.cat([hidden[-2], hidden[-1]], dim=1)))
        # hidden: [batch, hidden_dim]

        return outputs, hidden
```

### Attentionå±‚

```{python}
#| code-fold: false
class BahdanauAttention(nn.Module):
    def __init__(self, enc_hidden_dim, dec_hidden_dim, attention_dim):
        super().__init__()
        # åŠ æ€§æ³¨æ„åŠ›çš„å‚æ•°
        self.W_a = nn.Linear(dec_hidden_dim, attention_dim, bias=False)
        self.U_a = nn.Linear(enc_hidden_dim * 2, attention_dim, bias=False)  # åŒå‘ç¼–ç å™¨
        self.v_a = nn.Linear(attention_dim, 1, bias=False)

    def forward(self, decoder_hidden, encoder_outputs, mask=None):
        """
        decoder_hidden: [batch, dec_hidden]
        encoder_outputs: [batch, src_len, enc_hidden * 2]
        mask: [batch, src_len], Trueè¡¨ç¤ºéœ€è¦maskçš„ä½ç½®ï¼ˆpaddingï¼‰
        """
        batch_size, src_len, _ = encoder_outputs.shape

        # decoder_hidden æ‰©å±•åˆ°æ‰€æœ‰æºä½ç½®
        # [batch, dec_hidden] -> [batch, src_len, dec_hidden]
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)

        # è®¡ç®—å¯¹é½åˆ†æ•°
        # [batch, src_len, attention_dim]
        energy = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))
        # [batch, src_len, 1] -> [batch, src_len]
        attention_scores = self.v_a(energy).squeeze(-1)

        # åº”ç”¨maskï¼ˆå°†paddingä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå°çš„è´Ÿæ•°ï¼‰
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask, -1e10)

        # Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(attention_scores, dim=1)  # [batch, src_len]

        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        # [batch, 1, src_len] @ [batch, src_len, enc_hidden*2] -> [batch, 1, enc_hidden*2]
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context = context.squeeze(1)  # [batch, enc_hidden * 2]

        return context, attention_weights
```

### è§£ç å™¨

```{python}
#| code-fold: false
class AttentionDecoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim,
                 attention_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim, attention_dim)

        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # GRUè¾“å…¥æ˜¯ï¼šembedded + context
        self.rnn = nn.GRU(
            embed_dim + enc_hidden_dim * 2,  # åŒå‘ç¼–ç å™¨
            dec_hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(dec_hidden_dim + enc_hidden_dim * 2 + embed_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_token, hidden, encoder_outputs, mask=None):
        """
        å•æ­¥è§£ç 
        input_token: [batch] - ä¸Šä¸€æ­¥çš„è¾“å‡ºtoken
        hidden: [1, batch, dec_hidden] - ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€
        encoder_outputs: [batch, src_len, enc_hidden * 2]
        """
        # Embedding
        embedded = self.dropout(self.embedding(input_token))  # [batch, embed_dim]

        # Attention
        # hidden[-1] å–æœ€åä¸€å±‚ï¼Œ[batch, dec_hidden]
        context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)

        # æ‹¼æ¥embeddedå’Œcontextä½œä¸ºRNNè¾“å…¥
        rnn_input = torch.cat([embedded, context], dim=1).unsqueeze(1)  # [batch, 1, embed+ctx]

        # RNN
        output, hidden = self.rnn(rnn_input, hidden)
        output = output.squeeze(1)  # [batch, dec_hidden]

        # è¾“å‡ºå±‚
        prediction = self.fc(torch.cat([output, context, embedded], dim=1))

        return prediction, hidden, attention_weights
```

### å®Œæ•´çš„Seq2Seqæ¨¡å‹

```{python}
#| code-fold: false
class Seq2SeqAttention(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        src: [batch, src_len]
        trg: [batch, trg_len]
        """
        batch_size = src.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.vocab_size

        # å­˜å‚¨è¾“å‡º
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        attentions = []

        # ç¼–ç 
        encoder_outputs, hidden = self.encoder(src)
        # hidden: [batch, dec_hidden] -> [1, batch, dec_hidden]
        hidden = hidden.unsqueeze(0)

        # ç¬¬ä¸€ä¸ªè§£ç è¾“å…¥æ˜¯ <sos> token
        input_token = trg[:, 0]

        for t in range(1, trg_len):
            prediction, hidden, attention = self.decoder(
                input_token, hidden, encoder_outputs
            )
            outputs[:, t] = prediction
            attentions.append(attention)

            # Teacher forcing
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = prediction.argmax(1)
            input_token = trg[:, t] if teacher_force else top1

        return outputs, torch.stack(attentions, dim=1)

# åˆ›å»ºæ¨¡å‹ç¤ºä¾‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

encoder = Encoder(vocab_size=10000, embed_dim=256, hidden_dim=512)
decoder = AttentionDecoder(
    vocab_size=10000, embed_dim=256,
    enc_hidden_dim=512, dec_hidden_dim=512, attention_dim=256
)
model = Seq2SeqAttention(encoder, decoder, device).to(device)

print(f"ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in encoder.parameters()):,}")
print(f"è§£ç å™¨å‚æ•°: {sum(p.numel() for p in decoder.parameters()):,}")
print(f"æ€»å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
```

### å…³é”®å®ç°ç»†èŠ‚

**1. Maskå¤„ç†**

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œbatchä¸­çš„åºåˆ—é•¿åº¦ä¸åŒï¼Œéœ€è¦paddingã€‚è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œpaddingä½ç½®ä¸åº”è¯¥è·å¾—ä»»ä½•æƒé‡ã€‚æˆ‘ä»¬é€šè¿‡maskå°†è¿™äº›ä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå¤§çš„è´Ÿæ•°ï¼Œsoftmaxåå®ƒä»¬çš„æƒé‡è¶‹è¿‘äº0ã€‚

**2. Teacher Forcing**

è®­ç»ƒæ—¶ï¼Œè§£ç å™¨çš„è¾“å…¥å¯ä»¥æ˜¯çœŸå®çš„ä¸Šä¸€ä¸ªè¯ï¼ˆteacher forcingï¼‰æˆ–æ¨¡å‹é¢„æµ‹çš„è¯ã€‚`teacher_forcing_ratio` æ§åˆ¶ä¸¤è€…çš„æ··åˆæ¯”ä¾‹ã€‚è¾ƒé«˜çš„æ¯”ä¾‹åŠ é€Ÿè®­ç»ƒï¼Œä½†å¯èƒ½å¯¼è‡´exposure biasã€‚

**3. åŒå‘ç¼–ç å™¨**

æˆ‘ä»¬ä½¿ç”¨åŒå‘GRUï¼Œç¼–ç å™¨è¾“å‡ºçš„ç»´åº¦æ˜¯ `hidden_dim * 2`ã€‚è¿™è®©æ¯ä¸ªä½ç½®éƒ½åŒ…å«å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

---

## æ·±å…¥ç†è§£

### ä¸ºä»€ä¹ˆAttentionæœ‰æ•ˆï¼Ÿâ€”â€”ç†è®ºè§†è§’

**1. ä¿¡æ¯è®ºè§†è§’**

æ ‡å‡†Seq2Seqçš„ä¸Šä¸‹æ–‡å‘é‡ $\mathbf{c}$ æ˜¯è¾“å…¥ $\mathbf{x}$ çš„ä¸€ä¸ª**å……åˆ†ç»Ÿè®¡é‡ï¼ˆsufficient statisticï¼‰**â€”â€”å¦‚æœ $\mathbf{c}$ å®Œç¾ï¼Œå®ƒåº”è¯¥åŒ…å«å…³äº $\mathbf{y}$ çš„æ‰€æœ‰å¿…è¦ä¿¡æ¯ã€‚ä½†åœ¨å®è·µä¸­ï¼Œæœ‰é™ç»´åº¦çš„ $\mathbf{c}$ æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚

Attentioné€šè¿‡è®©è§£ç å™¨è®¿é—®æ‰€æœ‰çš„ $\mathbf{h}_j$ï¼Œå®é™…ä¸Šæ˜¯åœ¨è¯´ï¼š**ä¸è¦æ±‚ä¸€ä¸ªå……åˆ†ç»Ÿè®¡é‡ï¼Œè€Œæ˜¯è®©æ¨¡å‹åœ¨éœ€è¦æ—¶ç›´æ¥æŸ¥è¯¢åŸå§‹ä¿¡æ¯**ã€‚è¿™ç»•è¿‡äº†ä¿¡æ¯ç“¶é¢ˆã€‚

**2. è®°å¿†å¯»å€è§†è§’**

å¯ä»¥æŠŠç¼–ç å™¨çš„éšè—çŠ¶æ€çœ‹ä½œä¸€ä¸ª**å¤–éƒ¨è®°å¿†ï¼ˆexternal memoryï¼‰**ï¼Œæ¯ä¸ª $\mathbf{h}_j$ æ˜¯ä¸€ä¸ªè®°å¿†æ§½ã€‚Attentionæœºåˆ¶å®ç°äº†**åŸºäºå†…å®¹çš„è½¯å¯»å€ï¼ˆcontent-based soft addressingï¼‰**â€”â€”æ ¹æ®å½“å‰æŸ¥è¯¢ï¼ˆè§£ç å™¨çŠ¶æ€ï¼‰æ£€ç´¢ç›¸å…³çš„è®°å¿†ã€‚

è¿™ä¸ªè§†è§’åæ¥è¢«æ˜¾å¼åŒ–ä¸ºMemory Networkså’ŒNeural Turing Machineã€‚

**3. æ¢¯åº¦æµè§†è§’**

ä»ä¼˜åŒ–è§’åº¦ï¼ŒAttentionæä¾›äº†ä¸€æ¡ä»è§£ç å™¨åˆ°ç¼–ç å™¨ç‰¹å®šä½ç½®çš„**ç›´æ¥è·¯å¾„**ã€‚åœ¨æ ‡å‡†Seq2Seqä¸­ï¼Œæ¢¯åº¦è¦ä»è§£ç å™¨æµå›ç¼–ç å™¨ï¼Œå¿…é¡»ç»è¿‡ $\mathbf{c}$ï¼Œå†ç»è¿‡æ•´ä¸ªç¼–ç è¿‡ç¨‹ã€‚Attentionåˆ›é€ äº†"æ·å¾„"â€”â€”æ¢¯åº¦å¯ä»¥é€šè¿‡æ³¨æ„åŠ›æƒé‡ç›´æ¥ä¼ åˆ°ç›¸å…³çš„ç¼–ç å™¨ä½ç½®ã€‚

### è¾¹ç•Œæ¡ä»¶ä¸å¤±æ•ˆæ¨¡å¼

**1. å•è°ƒå¯¹é½å‡è®¾**

Bahdanau Attentionéšå«å‡è®¾æºå’Œç›®æ ‡ä¹‹é—´å­˜åœ¨æŸç§å¯¹é½å…³ç³»ã€‚å¯¹äºç¿»è¯‘ä»»åŠ¡è¿™é€šå¸¸æˆç«‹ï¼Œä½†å¯¹äºæŸäº›ä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ï¼‰ï¼Œè¿™ä¸ªå‡è®¾å¯èƒ½ä¸æˆç«‹â€”â€”æ‘˜è¦å¯èƒ½éœ€è¦æ•´åˆåˆ†æ•£åœ¨å„å¤„çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯"å¯¹é½"åˆ°ç‰¹å®šä½ç½®ã€‚

**2. å¤æ‚åº¦é™åˆ¶**

å½“æºåºåˆ—å¾ˆé•¿æ—¶ï¼ˆå¦‚æ–‡æ¡£çº§ç¿»è¯‘ï¼‰ï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å˜å¾—æ˜‚è´µã€‚$O(T_x \cdot T_y)$ çš„å¤æ‚åº¦åœ¨ $T_x = 10000$ æ—¶æ˜¯ä¸å¯æ¥å—çš„ã€‚

**3. åˆ†å¸ƒåç§»**

è®­ç»ƒæ—¶ï¼Œè§£ç å™¨çœ‹åˆ°çš„ä¸Šä¸‹æ–‡å‘é‡åˆ†å¸ƒä¸æ¨ç†æ—¶å¯èƒ½ä¸åŒï¼ˆå› ä¸ºteacher forcingï¼‰ã€‚è¿™å¯èƒ½å¯¼è‡´æ³¨æ„åŠ›æƒé‡åœ¨æ¨ç†æ—¶ä¸å¤Ÿå‡†ç¡®ã€‚

### å¼€æ”¾ç ”ç©¶é—®é¢˜

1. **æœ€ä¼˜å¯¹é½**ï¼šAttentionå­¦åˆ°çš„å¯¹é½ä¸è¯­è¨€å­¦å®¶æ ‡æ³¨çš„å¯¹é½æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿæ˜¯å¦å¯ä»¥ç”¨è¯­è¨€å­¦çŸ¥è¯†æ”¹è¿›Attentionï¼Ÿ

2. **ç¨€ç–æ³¨æ„åŠ›**ï¼šèƒ½å¦å­¦ä¹ æ›´ç¨€ç–çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œåªå…³æ³¨å°‘æ•°å…³é”®ä½ç½®ï¼Œè€Œä¸æ˜¯softåœ°åˆ†å¸ƒåˆ°æ‰€æœ‰ä½ç½®ï¼Ÿ

3. **å±‚æ¬¡åŒ–æ³¨æ„åŠ›**ï¼šå¯¹äºé•¿æ–‡æ¡£ï¼Œèƒ½å¦è®¾è®¡å±‚æ¬¡åŒ–çš„Attentionâ€”â€”å…ˆå…³æ³¨æ®µè½ï¼Œå†å…³æ³¨å¥å­ï¼Œæœ€åå…³æ³¨è¯ï¼Ÿ

---

## å±€é™æ€§ä¸å±•æœ›

### æœ¬ç« æ–¹æ³•çš„æ ¸å¿ƒå±€é™

**1. ä»ç„¶ä¾èµ–RNN**

Bahdanau Attentionæ˜¯Seq2Seqçš„"è¡¥ä¸"â€”â€”å®ƒå¢å¼ºäº†è§£ç å™¨è®¿é—®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä½†åº•å±‚ä»ç„¶æ˜¯RNNã€‚è¿™æ„å‘³ç€ï¼š

- ä»ç„¶æ˜¯é¡ºåºè®¡ç®—ï¼Œæ— æ³•å¹¶è¡Œ
- ä»ç„¶å—é™äºRNNçš„é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼ˆè™½ç„¶å› ä¸ºAttentionæœ‰æ‰€ç¼“è§£ï¼‰

**2. æ³¨æ„åŠ›è®¡ç®—ä¸åºåˆ—é•¿åº¦å¹³æ–¹ç›¸å…³**

æ¯ä¸ªè§£ç æ­¥éœ€è¦è®¡ç®—å¯¹æ‰€æœ‰ç¼–ç ä½ç½®çš„æ³¨æ„åŠ›ï¼Œæ€»å¤æ‚åº¦æ˜¯ $O(T_x \cdot T_y)$ã€‚å¯¹äºé•¿åºåˆ—ï¼Œè¿™æ˜¯æ˜¾è‘—çš„å¼€é”€ã€‚

**3. æ²¡æœ‰ä½ç½®æ„ŸçŸ¥**

Attentionæ˜¯åŸºäºå†…å®¹çš„ï¼Œå®ƒä¸ç›´æ¥è€ƒè™‘ä½ç½®ä¿¡æ¯ã€‚è™½ç„¶åŒå‘RNNéšå¼ç¼–ç äº†ä½ç½®ï¼Œä½†Attentionæœ¬èº«å¯¹ä½ç½®æ˜¯"ç›²ç›®"çš„ã€‚

### è¿™äº›å±€é™æŒ‡å‘ä»€ä¹ˆï¼Ÿ

Attentionçš„æˆåŠŸå¼•å‘äº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼š**å¦‚æœAttentionå¦‚æ­¤å¼ºå¤§ï¼Œæˆ‘ä»¬è¿˜éœ€è¦RNNå—ï¼Ÿ**

ä¸‹ä¸€ç« å°†æ¢è®¨Attentionçš„å„ç§å˜ä½“ï¼ŒåŒ…æ‹¬Luongæå‡ºçš„ä¹˜æ€§æ³¨æ„åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ¢ç´¢æœ€ç»ˆå¯¼å‘äº†ä¸€ä¸ªé©å‘½æ€§çš„ç»“è®ºï¼š**æˆ‘ä»¬å¯ä»¥å®Œå…¨ç”¨Attentionå–ä»£RNN**ã€‚

è¿™å°±æ˜¯ç¬¬8ç« Transformerçš„æ ¸å¿ƒæ€æƒ³â€”â€”"Attention Is All You Need"ã€‚åœ¨é‚£é‡Œï¼ŒSelf-Attentionè®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥ä¸å…¶ä»–ä½ç½®äº¤äº’ï¼Œå®Œå…¨æŠ›å¼ƒäº†å¾ªç¯ç»“æ„ï¼Œå®ç°äº†çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ã€‚

> ä»Bahdanau Attentionåˆ°Transformerï¼ŒAttentionä»ä¸€ä¸ª"è¾…åŠ©æœºåˆ¶"æ¼”å˜ä¸º"æ ¸å¿ƒæ¶æ„"ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ å†å²ä¸Šæœ€é‡è¦çš„èŒƒå¼è½¬å˜ä¹‹ä¸€ã€‚

---

## æœ¬ç« å°ç»“

::: {.callout-important}
## æ ¸å¿ƒè¦ç‚¹

- **é—®é¢˜**ï¼šSeq2Seqçš„ä¿¡æ¯ç“¶é¢ˆâ€”â€”æ‰€æœ‰è¾“å…¥ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå‘é‡ï¼Œå¯¼è‡´é•¿åºåˆ—ä¿¡æ¯ä¸¢å¤±
- **æ´å¯Ÿ**ï¼šè§£ç å™¨åº”è¯¥èƒ½å¤ŸåŠ¨æ€åœ°ã€æœ‰é€‰æ‹©åœ°å…³æ³¨è¾“å…¥çš„ä¸åŒä½ç½®
- **æ–¹æ³•**ï¼šAttentionæœºåˆ¶è®¡ç®—è§£ç å™¨çŠ¶æ€ä¸æ¯ä¸ªç¼–ç å™¨çŠ¶æ€çš„ç›¸å…³æ€§ï¼Œç”Ÿæˆæ³¨æ„åŠ›æƒé‡ï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°åŠ¨æ€ä¸Šä¸‹æ–‡å‘é‡
- **æ„ä¹‰**ï¼šæ‰“ç ´äº†å›ºå®šå‘é‡çš„é™åˆ¶ï¼Œå¤§å¹…æå‡äº†é•¿åºåˆ—ç¿»è¯‘è´¨é‡ï¼Œä¸ºåç»­çš„Transformerå¥ å®šäº†åŸºç¡€
:::

### å…³é”®å…¬å¼é€ŸæŸ¥

**å¯¹é½åˆ†æ•°ï¼ˆBahdanauåŠ æ€§æ³¨æ„åŠ›ï¼‰**ï¼š

$$
e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j)
$$

**æ³¨æ„åŠ›æƒé‡**ï¼š

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$

**ä¸Šä¸‹æ–‡å‘é‡**ï¼š

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

---

## æ€è€ƒé¢˜

1. **[æ¦‚å¿µç†è§£]** ä¸ºä»€ä¹ˆè¯´Attentionå®ç°äº†"è½¯å¯»å€"ï¼Ÿå®ƒä¸è®¡ç®—æœºå†…å­˜çš„ç¡¬å¯»å€æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿè¿™ç§è½¯å¯»å€çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

2. **[æ•°å­¦æ¨å¯¼]** è¯æ˜ï¼šå½“æ³¨æ„åŠ›æƒé‡é›†ä¸­åœ¨å•ä¸€ä½ç½®æ—¶ï¼ˆå³ $\alpha_{ij} \to 1$ å¯¹æŸä¸ª $j$ï¼Œå…¶ä»–ä¸º0ï¼‰ï¼Œä¸Šä¸‹æ–‡å‘é‡å°±é€€åŒ–ä¸ºé‚£ä¸ªä½ç½®çš„ç¼–ç å™¨çŠ¶æ€ã€‚è¿™ä¸ç¡¬æ³¨æ„åŠ›æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

3. **[å·¥ç¨‹å®è·µ]** åœ¨å®ç°Attentionæ—¶ï¼Œä¸ºä»€ä¹ˆè¦å¯¹paddingä½ç½®åº”ç”¨maskï¼Ÿå¦‚æœä¸åšmaskä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿå¦‚ä½•æ­£ç¡®å®ç°maskï¼ˆè€ƒè™‘æ•°å€¼ç¨³å®šæ€§ï¼‰ï¼Ÿ

4. **[æ‰¹åˆ¤æ€è€ƒ]** Attentionçš„å¯è§†åŒ–ç»å¸¸è¢«ç”¨æ¥"è§£é‡Š"æ¨¡å‹çš„å†³ç­–ã€‚ä½†è¿™ç§è§£é‡Šæ˜¯å¦å¯é ï¼Ÿè®¾è®¡ä¸€ä¸ªå®éªŒæ¥æ£€éªŒï¼šæ³¨æ„åŠ›æƒé‡é«˜çš„ä½ç½®æ˜¯å¦çœŸçš„å¯¹æ¨¡å‹è¾“å‡ºæœ‰é‡è¦å½±å“ã€‚

5. **[å¼€æ”¾é—®é¢˜]** Bahdanau Attentionéœ€è¦ä¸ºæ¯ä¸ªè§£ç æ­¥è®¡ç®—å¯¹æ‰€æœ‰ç¼–ç ä½ç½®çš„æ³¨æ„åŠ›ï¼Œå¤æ‚åº¦æ˜¯ $O(T_x \cdot T_y)$ã€‚æœ‰å“ªäº›æ–¹æ³•å¯ä»¥é™ä½è¿™ä¸ªå¤æ‚åº¦ï¼Ÿï¼ˆæç¤ºï¼šè€ƒè™‘ç¨€ç–åŒ–ã€å±€éƒ¨åŒ–ã€æˆ–è¿‘ä¼¼æ–¹æ³•ï¼‰

---

## å»¶ä¼¸é˜…è¯»

### æ ¸å¿ƒè®ºæ–‡ï¼ˆå¿…è¯»ï¼‰

- **[Bahdanau et al., 2015] Neural Machine Translation by Jointly Learning to Align and Translate**
  - Attentionæœºåˆ¶åœ¨NMTä¸­çš„å¼€åˆ›æ€§å·¥ä½œ
  - é‡ç‚¹è¯»ï¼šSection 3ï¼ˆæ¨¡å‹æ¶æ„ï¼‰ã€Section 5ï¼ˆå¯è§†åŒ–åˆ†æï¼‰
  - arXiv: [1409.0473](https://arxiv.org/abs/1409.0473)

### ç†è®ºåŸºç¡€

- **[Graves et al., 2014] Neural Turing Machines**
  - æå‡ºäº†åŸºäºå†…å®¹çš„è½¯å¯»å€ï¼Œæ˜¯Attentionçš„ç†è®ºå…ˆé©±
  - é‡ç‚¹è¯»ï¼šSection 3.1ï¼ˆAttentionæœºåˆ¶ï¼‰

### åç»­å‘å±•

- **[Luong et al., 2015] Effective Approaches to Attention-based Neural Machine Translation**
  - æå‡ºä¹˜æ€§æ³¨æ„åŠ›ï¼Œå¯¹æ¯”ä¸åŒæ³¨æ„åŠ›å˜ä½“
  - è¿™æ˜¯ä¸‹ä¸€ç« çš„æ ¸å¿ƒå†…å®¹
  - arXiv: [1508.04025](https://arxiv.org/abs/1508.04025)

- **[Vaswani et al., 2017] Attention Is All You Need**
  - Transformerï¼šå®Œå…¨ç”¨Attentionå–ä»£RNN
  - è¿™æ˜¯ç¬¬8ç« çš„æ ¸å¿ƒå†…å®¹
  - arXiv: [1706.03762](https://arxiv.org/abs/1706.03762)

### å¯¹Attentionå¯è§£é‡Šæ€§çš„è®¨è®º

- **[Jain & Wallace, 2019] Attention is not Explanation**
  - è´¨ç–‘Attentionæƒé‡ä½œä¸ºè§£é‡Šçš„å¯é æ€§
  - arXiv: [1902.10186](https://arxiv.org/abs/1902.10186)

- **[Wiegreffe & Pinter, 2019] Attention is not not Explanation**
  - å¯¹ä¸Šè¿°è®ºæ–‡çš„å›åº”ï¼Œæ›´ç»†è‡´åœ°è®¨è®ºAttentionçš„è§£é‡Šæ€§
  - arXiv: [1908.04626](https://arxiv.org/abs/1908.04626)

---

## å†å²æ³¨è„š

Attentionæœºåˆ¶çš„çµæ„Ÿéƒ¨åˆ†æ¥è‡ªäººç±»è§†è§‰ç³»ç»Ÿã€‚å½“æˆ‘ä»¬çœ‹ä¸€å¹…å¤æ‚çš„å›¾åƒæ—¶ï¼Œæˆ‘ä»¬ä¸ä¼šåŒæ—¶å¤„ç†æ‰€æœ‰åƒç´ ï¼Œè€Œæ˜¯ä¼š"èšç„¦"åœ¨æ„Ÿå…´è¶£çš„åŒºåŸŸã€‚è¿™ç§é€‰æ‹©æ€§æ³¨æ„ï¼ˆselective attentionï¼‰æ˜¯è®¤çŸ¥ç§‘å­¦ç ”ç©¶çš„ç»å…¸è¯¾é¢˜ã€‚

Bahdanauåœ¨2014å¹´å°†è¿™ä¸ªæ€æƒ³å¼•å…¥ç¥ç»æœºå™¨ç¿»è¯‘æ—¶ï¼Œå¹¶æ²¡æœ‰é¢„æ–™åˆ°å®ƒä¼šæˆä¸ºæ·±åº¦å­¦ä¹ æœ€æ ¸å¿ƒçš„ç»„ä»¶ä¹‹ä¸€ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä»–ä»¬è°¦è™šåœ°ç§°ä¹‹ä¸º"å¯¹é½æ¨¡å‹"ï¼ˆalignment modelï¼‰ï¼Œè€Œä¸æ˜¯"æ³¨æ„åŠ›"ã€‚"Attention"è¿™ä¸ªæœ¯è¯­æ˜¯åæ¥è¢«ç¤¾åŒºå¹¿æ³›é‡‡ç”¨çš„ã€‚

æœ‰è¶£çš„æ˜¯ï¼ŒBahdanau Attentionçš„æˆåŠŸè®©ç ”ç©¶è€…å¼€å§‹æ€è€ƒï¼šå¦‚æœAttentionè¿™ä¹ˆæœ‰æ•ˆï¼Œæˆ‘ä»¬æ˜¯å¦éœ€è¦RNNï¼Ÿä¸¤å¹´åï¼ŒVaswaniç­‰äººç»™å‡ºäº†ç­”æ¡ˆâ€”â€”"Attention Is All You Need"ã€‚è¿™ç¯‡è®ºæ–‡ä¸ä»…åœ¨æŠ€æœ¯ä¸Šé©æ–°äº†åºåˆ—å»ºæ¨¡ï¼Œå…¶æ ‡é¢˜æœ¬èº«ä¹Ÿæˆä¸ºäº†æ·±åº¦å­¦ä¹ å†å²ä¸Šæœ€å…·å½±å“åŠ›çš„é‡‘å¥ä¹‹ä¸€ã€‚
