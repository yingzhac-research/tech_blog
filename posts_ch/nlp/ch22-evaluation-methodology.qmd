---
title: "第22章：评测方法论——如何判断模型变好了？"
subtitle: "Evaluation Methodology: Benchmarks, Metrics, and the Science of Measuring AI"
author: "Ying Zha"
date: "2026-01-28"
categories: [NLP, Deep Learning, LLM, Evaluation, Benchmarks, Metrics]
tags: [GLUE, SuperGLUE, MMLU, HELM, BigBench, Chatbot Arena, LLM-as-Judge, BLEU, ROUGE, Data Contamination, Goodhart, Evaluation Methodology]
description: "上一章关于涌现能力的讨论揭示了一个令人不安的事实：度量方式本身可能扭曲我们对模型能力的认知。如果连'涌现'这个核心概念的真实性都取决于评测度量的选择，那么我们对LLM能力的所有判断都需要更加审慎。本章系统讲述NLP评测方法论的演进：从BLEU/ROUGE等自动指标的局限，到GLUE/SuperGLUE/MMLU等静态benchmark的兴衰，再到LLM-as-Judge和Chatbot Arena等生成式评测的新范式。我们将讨论数据污染、Goodhart定律、评测者偏差等核心问题，试图回答一个根本性的问题：什么才是好的评测？"
image: "figures/chapter-22/original/fig1-evaluation-timeline.png"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    fig-cap-location: bottom
bibliography: references.bib
---

> **核心问题**：Benchmark 越刷越高，模型真的越来越强吗？我们如何设计不被"博弈"的评测体系？
>
> **历史坐标**：2018–2024 | GLUE → SuperGLUE → MMLU → HELM → Chatbot Arena | 从静态 benchmark 到动态人类偏好评测

::: {.callout-tip collapse="true"}
## 本章参考来源

### 论文
- **Wang et al. (2018)** "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding" (arXiv:1804.07461) — 参考了 Section 2-3（任务设计与基线）、Table 1（9个任务定义）
- **Wang et al. (2019)** "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems" (arXiv:1905.00537) — 参考了 Section 2（任务选择标准）、Table 1（8个任务）
- **Hendrycks et al. (2020)** "Measuring Massive Multitask Language Understanding" (arXiv:2009.03300) — 参考了 Section 2-3（57学科设计）、Figure 1（学科分布）
- **Liang et al. (2022)** "Holistic Evaluation of Language Models" (arXiv:2211.09110) — 参考了 Section 2（评测分类学：7个场景×7个度量维度）、Figure 1（HELM框架图）
- **Srivastava et al. (2022)** "Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models" (arXiv:2206.04615) — 参考了 Section 2（BIG-bench 200+任务设计）
- **Zheng et al. (2023)** "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena" (arXiv:2306.05685) — 参考了 Section 2-4（MT-Bench设计、Chatbot Arena Elo系统、LLM-as-Judge偏差分析）、Figure 1（评测框架图）；提取了论文原图
- **Lin et al. (2021)** "TruthfulQA: Measuring How Models Mimic Human Falsehoods" (arXiv:2109.07958) — 参考了 Section 2-3（事实性评测设计）
- **Schaeffer et al. (2023)** "Are Emergent Abilities of Large Language Models a Mirage?" (arXiv:2304.15004) — 参考了 Section 2-3（度量选择对涌现判断的影响），延续Ch21讨论；提取了 Figure 2（涌现消失的多面板对比图）
- **Papineni et al. (2002)** "BLEU: a Method for Automatic Evaluation of Machine Translation" (ACL 2002) — 参考了 Section 2（BLEU公式定义）
- **Lin (2004)** "ROUGE: A Package for Automatic Evaluation of Summaries" (ACL 2004 Workshop) — 参考了 ROUGE-N/L 定义
- **Oren et al. (2023)** "Proving Test Set Contamination in Black Box Language Models" (arXiv:2310.17623) — 参考了分区对比数据污染检测方法
- **Deng et al. (2023)** "Investigating Data Contamination in Modern Benchmarks for Large Language Models" (arXiv:2311.09783) — 参考了 TS-Guessing 方法（通过部分输入检测数据污染）
- **Golchin & Surdeanu (2023)** "Time Travel in LLMs: Tracing Data Contamination in Large Language Models" (arXiv:2308.08493) — 参考了 Time Travel 方法（通过逐字复现检测数据污染）

### 教材
- SLP3 Chapter 4 (Logistic Regression and Text Classification, Jan 2026 draft) — 参考了 precision/recall/F1/confusion matrix 的经典教学（Fig. 4.4）
- SLP3 Chapter 7 (Large Language Models) — 参考了 MMLU benchmark 的教学呈现（Fig. 7.16：2-shot prompting 示例）、Rawlsian 公平性评测
- SLP3 Chapter 12 (Machine Translation) — 参考了 BLEU、chrF、BERTScore、COMET/BLEURT 的完整评测指标演进，以及 paired bootstrap 统计显著性检验
- D2L Section 10.7 (Sequence-to-Sequence Learning) — 参考了 BLEU 的数值演算示例（p₁=4/5, p₂=3/4, p₃=1/3）和可运行 Python 实现

### 课程
- Stanford CS224N Lecture 11 (Spring 2024, Yann Dubois guest lecture) — 专题 "Benchmarking and Evaluation"，系统讲解了从 GLUE/MMLU 到 LLM-as-Judge 的评测演进、MMLU 错误率问题、评测不一致性（如 Gemini 非标准 MMLU prompting）
- CMU 11-711 ANLP Lecture 17 (Fall 2024, Graham Neubig) — "Evaluation and Multimodal"，覆盖 GLUE、MMLU、HELM、Chatbot Arena、MT-Bench 等评测体系
- Princeton COS 597R Lecture 5 (Fall 2024, Sanjeev Arora) — "Emergent Behavior"，讨论 Schaeffer et al. 的涌现幻觉论文及评测度量选择问题
:::

---

## 从上一章说起

上一章我们讲述了 Chain-of-Thought 推理和涌现能力的故事。CoT 通过在 few-shot 示例中展示推理过程，让大语言模型在数学推理上获得了飞跃式提升，而这种能力呈现出令人惊叹的"涌现"特征——小模型完全不受益，大模型突然爆发。

然而，上一章结尾埋下的伏笔令人不安。Schaeffer et al.（2023）的 NeurIPS 最佳论文表明，所谓的"涌现"可能很大程度上是**度量方式制造的假象**：同一份实验数据，使用精确匹配（Exact Match）度量时呈现出震撼的"阶跃"曲线，换成连续度量后却变成了平滑增长。一个被整个领域热烈讨论的"相变现象"，居然可能只是我们选择了错误的温度计。

![Schaeffer et al. (2023) 的核心发现：同一组模型在同一批任务上的表现，左侧使用非线性度量（如 Exact Match）时呈现"涌现"式阶跃，右侧换成连续度量（如 Token Edit Distance、Brier Score）后变为平滑增长。"涌现"消失了。](figures/chapter-22/original/fig4-emergence-mirage-schaeffer2023.png){#fig-emergence-mirage width=85%}

::: {.figure-caption}
*Source: Schaeffer, R., Miranda, B., & Koyejo, S. (2023) "Are Emergent Abilities of Large Language Models a Mirage?", Figure 2. NeurIPS 2023 Outstanding Paper Award. [arXiv:2304.15004](https://arxiv.org/abs/2304.15004)*
:::

这个发现的冲击远超涌现本身。它触及了一个更深层的问题：**如果评测方式可以根本性地改变我们对模型能力的判断，那么我们对 LLM 的所有评估——从 GLUE 排行榜到 ChatGPT "超越人类"的宣传——有多少是真实的，有多少是度量假象？**

这个问题不是学术象牙塔里的自娱自乐。当企业根据 benchmark 分数决定部署哪个模型，当政府根据评测结果制定 AI 监管政策，当研究者根据排行榜排名规划研究方向——评测的科学性直接关系到数十亿美元的决策和整个领域的研究方向。

> 💡 **本章核心洞察**：评测不是"选个 benchmark 跑个分"那么简单。它是一门方法论，受 Goodhart 定律笼罩——"当一个度量成为目标时，它就不再是好的度量"。从 BLEU 到 GLUE 到 Chatbot Arena，NLP 评测的每一次演进都是对前一代评测失效模式的回应。理解这段演进史，是成为严谨研究者的必修课。

---

## 问题的本质是什么？

### 为什么评测如此困难？

评测看似是一个简单的工程问题：设计一组测试题，让模型回答，计算分数。但实际上，NLP 评测面临一系列根本性的困难，每一个都足以让整个评测体系崩塌。

第一个困难是**能力的多维性**。一个语言模型需要同时具备语法理解、常识推理、事实知识、数学计算、代码生成、创意写作等数十种能力。任何单一 benchmark 都只能覆盖其中一小部分。用 MMLU 分数代表"模型智能"，就像用数学考试成绩代表"学生能力"一样片面。

第二个困难是**Goodhart 定律**（Goodhart's Law）。英国经济学家 Charles Goodhart 在 1975 年观察到："当一个度量成为目标时，它就不再是好的度量。"这在 NLP 领域体现得淋漓尽致：一旦某个 benchmark 成为社区追逐的目标，模型就会被专门优化来"刷分"，而这种优化往往不反映真正的能力提升。GLUE 排行榜在两年内被"刷满"（超过人类基线），但没有人真的相信 2020 年的模型已经"理解了语言"。

第三个困难是**评测对象的根本变化**。传统 NLP 评测假设模型是某个特定任务的求解器（分类器、翻译器、问答系统）。但 GPT-3 以来的 LLM 是通用的语言生成器——它们的输出是开放式的自然语言文本。如何评估一段自由文本的"质量"？这几乎是一个哲学问题。两个人类专家对同一段文本的评价都可能大相径庭。

第四个困难是**数据污染**（Data Contamination）。现代 LLM 的训练数据规模达到数万亿 token，覆盖了互联网上几乎所有公开文本。这意味着 benchmark 的测试题很可能已经出现在训练数据中——模型可能是"背出了答案"而非"理解了问题"。更棘手的是，对于闭源模型，外部研究者甚至无法验证是否存在数据污染。

### Goodhart 定律：评测领域的永恒诅咒

让我们更深入地理解 Goodhart 定律如何具体影响 NLP 评测。这个定律有一个更精确的表述，由人类学家 Marilyn Strathern 给出："当一个度量成为目标时，它就不再是好的度量。"在 NLP 领域，这个定律以惊人的规律性反复上演：

$$
\text{Benchmark 发布} \xrightarrow{\text{社区竞争}} \text{针对性优化} \xrightarrow{\text{分数饱和}} \text{Benchmark 失效} \xrightarrow{\text{需要新 Benchmark}}
$$

![Goodhart 循环：NLP benchmark 从发布到失效的必经之路。GLUE 在不到两年内走完了全部四个阶段。](figures/chapter-22/original/fig2-goodhart-cycle.png){#fig-goodhart-cycle width=70%}

::: {.figure-caption}
*作者绘制。概念基于 Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."*
:::

GLUE 的故事就是一个教科书级的案例。2018 年 GLUE 发布时，最好的系统在综合分上只有约 69 分（人类基线 87 分）。BERT 在几个月内将分数提升到 80 以上。到 2019 年底，多个模型超过了人类基线。到 2020 年，排行榜前列的模型之间差距微乎其微——benchmark 丧失了区分能力。

SuperGLUE 于 2019 年发布，专门针对 GLUE 的不足设计了更难的任务。但它的寿命同样不长——T5 等模型在 2020 年就逼近了人类基线。MMLU（2020）试图通过覆盖 57 个学科来延长 benchmark 的寿命。BIG-bench（2022）干脆众包了 200+ 个任务。但每一个 benchmark 都面临同样的命运：**发布、追逐、饱和、失效**。

一个自然的问题是：我们能否设计一个"不会饱和"的 benchmark？答案很可能是否定的，至少对于静态 benchmark 而言。只要测试题是固定的，只要训练数据足够大，模型终将覆盖所有题目。这就是为什么评测方法论需要持续演进——从静态 benchmark 到动态评测，从自动指标到人类判断，从固定题目到对抗性构造。

---

## 核心思想与直觉

### 评测方法论的三次范式转变

如果我们将 NLP 评测的历史画成一张演进图（见 @fig-eval-timeline），会清晰地看到三次范式转变，每一次都是对前一代评测失效模式的回应。

![NLP 评测方法论的演进：从 BLEU（2002）到 Chatbot Arena（2023），评测经历了自动指标、静态 benchmark、生成式评测三个时代。](figures/chapter-22/original/fig1-evaluation-timeline.png){#fig-eval-timeline width=95%}

::: {.figure-caption}
*作者绘制。时间线数据来源：各论文原始发表年份（BLEU: Papineni et al. 2002; GLUE: Wang et al. 2018; MMLU: Hendrycks et al. 2020; Chatbot Arena: Zheng et al. 2023 等）。*
:::

**第一代：自动指标时代（2002–2017）**。以 BLEU（机器翻译）和 ROUGE（文本摘要）为代表。核心思想是将文本质量量化为 n-gram 匹配的分数——一个可以全自动计算、不需要人类参与的数字。这让大规模评测成为可能，但代价是指标与真实文本质量之间的相关性有限。一个模型可以获得高 BLEU 分数，输出却读起来像机器生成的拼凑文本。

**第二代：理解能力 benchmark 时代（2018–2022）**。以 GLUE、SuperGLUE、MMLU、HELM 为代表。核心思想是设计一组精心挑选的任务，每个任务测试特定的语言理解能力（推理、蕴含、常识等），用准确率等清晰的度量衡量模型表现。这种方式比 BLEU 精确得多，但面临静态 benchmark 的固有困境——数据泄漏、过拟合、饱和。

**第三代：生成式评测时代（2023–）**。以 LLM-as-Judge（MT-Bench）和 Chatbot Arena 为代表。核心思想是用另一个 LLM（或众包人类）来评判模型的开放式输出。这解决了"如何评估自由文本"的难题，但引入了新的问题——评判者自身的偏差（LLM 评判者倾向于偏好更长、更华丽的回答）。

每一代评测方法都修复了上一代的某些痛点，但也引入了新的局限性。理解这种"修复→新问题→再修复"的循环，是理解评测方法论的关键。

### 一个类比：评测就像考试制度

把 NLP 评测比作教育中的考试制度，可以帮助建立直觉。

BLEU 就像用"学生作文中有多少个正确单词"来评分——简单粗暴，但完全忽略了语义和连贯性。GLUE 像是一场标准化考试（如 SAT）——精心设计、有明确的评分标准，但"应试教育"的模型可以通过刷题获得高分而不真正理解知识。Chatbot Arena 则像是让两个学生当面辩论，由观众投票决定谁更厉害——更接近"真实能力"的评估，但观众可能被口才而非逻辑说服。

没有完美的考试制度，也没有完美的评测方法。好的评测方法论不是追求"终极 benchmark"，而是理解每种评测方式的优势、局限和适用场景。

---

## 技术细节

### 第一代：自动指标

#### BLEU：n-gram 精度与短句惩罚

BLEU（Bilingual Evaluation Understudy，Papineni et al., 2002）是机器翻译领域最经典的自动评测指标。它的核心思想出奇地简单：比较机器翻译输出和参考译文之间的 n-gram 重合度。

BLEU 的计算分为两部分。第一部分是**修正的 n-gram 精度**（Modified n-gram Precision）。对于每个 n-gram，统计它在候选翻译中出现的次数，但用参考译文中出现的最大次数进行裁剪（clipping），避免一个词被重复计数：

$$
p_n = \frac{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}_{\text{clip}}(\text{n-gram})}{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}(\text{n-gram})}
$$

第二部分是**短句惩罚**（Brevity Penalty, BP）。如果没有这个惩罚，模型可以通过只输出最有把握的几个词来获得很高的精度。BP 的设计是：当候选翻译的长度 $c$ 短于参考翻译的有效长度 $r$ 时，施加指数惩罚：

$$
\text{BP} = \begin{cases} 1 & \text{if } c > r \\ e^{1 - r/c} & \text{if } c \leq r \end{cases}
$$

最终的 BLEU 分数将两者结合：

$$
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
$$

其中 $w_n$ 通常取均匀权重 $1/N$，$N$ 通常取 4（即 BLEU-4，考虑 1-gram 到 4-gram）。

#### 完整数值示例：BLEU 计算

**设定**：候选翻译（Candidate）和参考翻译（Reference）如下：

- **Reference**: "the cat sat on the mat"
- **Candidate**: "the the the the"

**Step 1: 1-gram 精度（不裁剪）**

Candidate 中 "the" 出现 4 次，Reference 中也有 "the"。如果不裁剪，$p_1 = 4/4 = 1.0$——满分！但这显然是荒谬的。

**Step 2: 1-gram 精度（裁剪后）**

"the" 在 Reference 中最多出现 2 次，所以裁剪后计数为 $\min(4, 2) = 2$：

$$
p_1 = \frac{2}{4} = 0.5
$$

**Step 3: 更正常的例子**

换一个更合理的候选翻译：

- **Reference**: "the cat sat on the mat"（6 个词）
- **Candidate**: "the cat on the mat"（5 个词）

1-gram 精度：5 个词中，"the"（clip 到 2）、"cat"（1）、"on"（1）、"mat"（1），共 5 个匹配。$p_1 = 5/5 = 1.0$。

2-gram 精度：Candidate 的 2-gram 有 "the cat"、"cat on"、"on the"、"the mat"。Reference 的 2-gram 有 "the cat"、"cat sat"、"sat on"、"on the"、"the mat"。匹配 3 个（"the cat"、"on the"、"the mat"），共 4 个。$p_2 = 3/4 = 0.75$。

**Step 4: 短句惩罚**

$c = 5$，$r = 6$，$c < r$，所以：

$$
\text{BP} = e^{1 - 6/5} = e^{-0.2} \approx 0.819
$$

**Step 5: 最终 BLEU-2 分数**（只用 1-gram 和 2-gram）

$$
\text{BLEU-2} = 0.819 \times \exp\left(\frac{1}{2} \ln 1.0 + \frac{1}{2} \ln 0.75\right) = 0.819 \times \exp(-0.144) = 0.819 \times 0.866 \approx 0.709
$$

**解读**：候选翻译遗漏了 "sat"，2-gram 中 "cat on" 不匹配，短句惩罚进一步扣分，最终得到 0.709。这个分数合理地反映了翻译的质量。

#### BLEU 的局限性

BLEU 的问题在本质上是 n-gram 匹配的局限。它无法识别同义词替换（"purchase" 和 "buy" 被视为完全不同），对语序的敏感性有限（只在高阶 n-gram 中间接体现），完全忽略了语义连贯性和流畅度。更深层的问题是，BLEU 假设存在"标准答案"——但翻译是一个一对多的问题，一个源句可能有多种同样好的译法。

Callahan et al. (2006) 的研究表明，BLEU 与人类翻译质量评判的相关性在系统级别（比较不同翻译系统）尚可接受，但在句子级别极不可靠。这意味着你可以用 BLEU 大致比较两个翻译系统的整体水平，但不能用它判断某个具体句子翻译得好不好。

#### ROUGE：召回导向的摘要评测

ROUGE（Recall-Oriented Understudy for Gisting Evaluation，Lin, 2004）是文本摘要领域的标准指标。与 BLEU 侧重精度（候选文本中有多少 n-gram 出现在参考中）不同，ROUGE 侧重**召回率**（参考文本中有多少 n-gram 被候选文本覆盖）。这反映了摘要任务的特点：好的摘要应该覆盖原文的关键信息。

ROUGE 有多个变体。ROUGE-N 计算 n-gram 的召回率：

$$
\text{ROUGE-N} = \frac{\sum_{S \in \text{Ref}} \sum_{\text{n-gram} \in S} \text{Count}_{\text{match}}(\text{n-gram})}{\sum_{S \in \text{Ref}} \sum_{\text{n-gram} \in S} \text{Count}(\text{n-gram})}
$$

ROUGE-L 使用**最长公共子序列**（Longest Common Subsequence, LCS）来衡量句子级的结构相似性，不要求 n-gram 连续出现：

$$
\text{ROUGE-L} = F_{\text{LCS}} = \frac{(1 + \beta^2) \cdot R_{\text{LCS}} \cdot P_{\text{LCS}}}{R_{\text{LCS}} + \beta^2 \cdot P_{\text{LCS}}}
$$

其中 $R_{\text{LCS}} = \text{LCS}(X, Y) / m$，$P_{\text{LCS}} = \text{LCS}(X, Y) / n$，$m$ 和 $n$ 分别是参考和候选的长度。

ROUGE 面临与 BLEU 类似的根本局限：n-gram 匹配无法捕捉语义。一个改写了措辞但保留了核心意思的摘要，可能获得很低的 ROUGE 分数。

#### 超越 n-gram：语义评测指标

BLEU 和 ROUGE 的局限催生了一系列基于语义的评测指标。**BERTScore**（Zhang et al., 2020）使用 BERT 的上下文嵌入来计算候选文本和参考文本之间的相似度，对同义词和释义更加鲁棒。**BLEURT**（Sellam et al., 2020）在 BERT 基础上进一步微调，学习预测人类质量评分。**COMET**（Rei et al., 2020）在机器翻译评测中引入源句信息，训练一个跨语言模型来预测翻译质量。

这些指标在与人类判断的相关性上显著优于 BLEU/ROUGE，但它们也引入了新的依赖：评测指标本身依赖于一个预训练模型。当被评测的模型和评测指标使用相同的预训练基础时，可能产生系统性偏差。

### 第二代：理解能力 benchmark

#### GLUE 与 SuperGLUE：多任务理解评测

GLUE（General Language Understanding Evaluation，Wang et al., 2018）的设计理念是：一个真正"理解语言"的模型应该能够同时处理多种自然语言理解任务。GLUE 包含 9 个任务，涵盖了语言理解的多个维度：

| 任务 | 类型 | 描述 | 度量 | 示例 |
|------|------|------|------|------|
| **MNLI** | 蕴含 | 三分类（蕴含/矛盾/中性） | Accuracy | "猫在垫子上" ⇒ "有动物在家具上"? |
| **QQP** | 释义 | 两个问题是否语义相同 | F1/Acc | "How old are you" ≈ "What is your age"? |
| **QNLI** | 问答/蕴含 | 句子是否包含问题的答案 | Accuracy | Q: "When did X happen?" + 候选句 |
| **SST-2** | 情感 | 电影评论正/负情感 | Accuracy | "This movie was wonderful" → 正面 |
| **CoLA** | 语法 | 句子是否语法正确 | Matthews Corr | "The boy the cat bit ran" → ? |
| **STS-B** | 相似度 | 两句话的语义相似度（1-5分） | Pearson/Spearman | 回归任务 |
| **MRPC** | 释义 | 两个句子是否语义等价 | F1/Acc | 新闻句子对 |
| **RTE** | 蕴含 | 二分类蕴含 | Accuracy | 前提→假设 |
| **WNLI** | 共指 | Winograd 共指消解 | Accuracy | 代词指代谁？ |

GLUE 的关键设计决策值得深入分析。首先，**任务多样性**：9 个任务覆盖了从句法（CoLA）到语义（STS-B）到推理（MNLI）的多个层次。其次，**单一分数**：所有任务的分数被平均为一个综合分数，方便排行榜比较。第三，**人类基线**：GLUE 公布了人类在每个任务上的表现，作为"天花板"参考。

然而，GLUE 的"寿命"出乎意料地短。BERT（2018年10月）发布后仅几个月，GLUE 排行榜就被"攻陷"——多个模型在综合分上接近甚至超过人类基线。这迫使设计者在 2019 年推出了 SuperGLUE，包含 8 个更具挑战性的任务：BoolQ（布尔问答）、CB（文本蕴含）、COPA（因果推理）、MultiRC（多句阅读理解）、ReCoRD（常识阅读理解）、RTE（增强版）、WiC（上下文词义）、WSC（Winograd 模式）。

SuperGLUE 的命运与 GLUE 类似——它在 2020 年也基本被"刷满"。这个"两年寿命"的模式揭示了静态 benchmark 的一个根本困境：**benchmark 的难度是固定的，但模型的能力在持续增长**。

#### MMLU：大规模多任务理解

MMLU（Massive Multitask Language Understanding，Hendrycks et al., 2020）试图通过**规模**来延长 benchmark 的寿命。它包含 57 个学科的 15,908 道选择题，覆盖从高中数学到专业法律、从天文学到计算机科学的广泛领域。每道题是四选一的选择题，如：

> **Abstract Algebra**: Let p = (1, 2, 5, 4)(2, 3) in S_5. Find the index of <p> in S_5.
> (A) 8  (B) 2  (C) 24  (D) 120

MMLU 的设计有几个值得注意的特点。首先，题目来源于真实的考试和教材，而非人工构造——这提高了"生态有效性"。其次，57 个学科的细粒度分类允许分析模型在不同知识领域的强弱。第三，四选一的格式使得评测完全自动化（只需检查模型选择了哪个选项）。

MMLU 很快成为 LLM 评测的"黄金标准"。GPT-4 的技术报告以 MMLU 分数作为核心能力指标之一（GPT-4: 86.4% vs GPT-3.5: 70.0%）。然而，MMLU 也面临自己的问题。选择题格式限制了评测的深度——模型可能通过排除法而非真正理解来答题。随着 LLM 训练数据的膨胀，MMLU 题目被"记住"的风险越来越高。

更令人不安的是，2024 年的一项质量审计揭示了 MMLU 本身的严重质量问题：**整体错误率约为 6.5%**，包括错误的标注答案、模糊的题目表述和过时的知识。某些学科的错误率远高于平均值——Virology（病毒学）子集的错误率高达 **57%**。这意味着当我们用 MMLU 分数来比较模型时，部分"错误"可能只是模型给出了比 benchmark 答案更正确的回答。这个发现从另一个角度动摇了 benchmark 评测的可信度：不仅模型可能"作弊"（数据污染），benchmark 本身也可能出错。

#### HELM：多维度整体评测

HELM（Holistic Evaluation of Language Models，Liang et al., 2022）是评测方法论上的一次重要尝试。它不再追求"更难的题目"，而是追求**更全面的评测维度**。HELM 定义了 7 个核心评测场景（Core Scenarios）和 7 个度量维度（Metrics）：

**评测场景**：问答、信息检索、文本摘要、情感分析、毒性检测、版权与公平性、对话。

**度量维度**：准确性（Accuracy）、校准性（Calibration）、鲁棒性（Robustness）、公平性（Fairness）、偏差（Bias）、毒性（Toxicity）、效率（Efficiency）。

HELM 的关键洞察是：一个模型不仅要"准确"，还要"校准良好"（知道自己什么时候不确定）、"鲁棒"（对输入扰动不敏感）、"公平"（对不同人群同等对待）。一个 MMLU 分数 90% 但输出大量有毒内容的模型，显然不应该被视为"好模型"。

这种多维度评测理念影响深远，后续的评测框架（如 OpenCompass、LM Evaluation Harness）都借鉴了 HELM 的设计哲学。

#### BIG-bench：众包任务的极端尝试

BIG-bench（Beyond the Imitation Game benchmark，Srivastava et al., 2022）走了另一条路：通过**众包**来构建超大规模 benchmark。444 位研究者贡献了 204 个任务，涵盖语言学、数学、常识推理、社会偏见等领域。BIG-bench Hard（BBH）是其中精选的 23 个特别困难的任务——这些任务上 Chain-of-Thought 才有显著效果。

BIG-bench 的贡献不仅是任务数量，更是评测方法论：它系统地分析了任务难度与模型规模的关系，为"涌现能力"的讨论提供了大量实证数据（也为 Schaeffer et al. 的反驳提供了分析素材）。

### 第三代：生成式评测的新范式

#### 为什么需要新范式？

第二代 benchmark 的核心假设是：模型的能力可以通过一组**有标准答案**的任务来衡量。但 ChatGPT 之后的 LLM 主要被用于**开放式生成**——写邮件、改代码、头脑风暴、角色扮演——这些任务没有唯一的"正确答案"。

想象你让两个 LLM 分别写一封求职信。模型 A 写了一封简洁有力的短信，模型 B 写了一封详尽周到的长信。哪个更好？这取决于求职者的风格偏好、目标公司的文化、甚至招聘经理的个人品味。传统 benchmark 无法处理这种主观性。

#### LLM-as-Judge：用模型评模型

Zheng et al.（2023）提出了一个大胆的方案：用一个强大的 LLM（如 GPT-4）作为评判者来评估其他模型的输出。@fig-llm-as-judge 展示了一个具体的评判过程：GPT-4 作为裁判，比较两个助手对同一个多轮问题的回答，并给出详细的评判理由。

![LLM-as-Judge 的工作示例：GPT-4 评判两个 AI 助手对"联储在二级市场买债"问题的多轮回答。注意评判理由中对"重复性"和"具体性"的分析——这种细粒度反馈是传统 benchmark 无法提供的。](figures/chapter-22/original/fig3-llm-as-judge-zheng2023.png){#fig-llm-as-judge width=80%}

::: {.figure-caption}
*Source: Zheng, L. et al. (2023) "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", Figure 1. [arXiv:2306.05685](https://arxiv.org/abs/2306.05685)*
:::

Zheng et al. 设计了两个互补的评测系统。

**MT-Bench** 是一个包含 80 个多轮对话问题的测试集，覆盖写作、推理、数学、编程、知识、角色扮演等 8 个类别。每个问题都设计为需要两轮对话——第二轮的问题依赖于第一轮的回答，测试模型的多轮对话能力。评判方式有两种：

**单一评分**（Single Rating）：GPT-4 对每个回答打 1-10 分，并给出理由。

**成对比较**（Pairwise Comparison）：GPT-4 看到两个模型对同一问题的回答，判断哪个更好。

::: {.callout-note}
## Algorithm 1: LLM-as-Judge Pairwise Comparison（改编自 Zheng et al., 2023）

```
Input: question Q, answer_A from Model A, answer_B from Model B, judge LLM J
Output: winner ∈ {A, B, Tie}

Step 1: Construct prompt for judge
    prompt = f"""
    [System] Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question.

    [Question] {Q}
    [Assistant A] {answer_A}
    [Assistant B] {answer_B}

    Please evaluate which assistant provides a better response.
    Output your verdict as: [[A]], [[B]], or [[Tie]].
    Provide your reasoning before the verdict.
    """

Step 2: Get judgment
    judgment = J(prompt)

Step 3: Swap positions and re-judge (to mitigate position bias)
    prompt_swapped = swap(answer_A, answer_B) in prompt
    judgment_swapped = J(prompt_swapped)

Step 4: Final verdict
    if both judgments agree: return agreed winner
    else: return Tie (inconsistent)
```

*改编自 Zheng et al. (2023) "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", Section 3. [arXiv:2306.05685](https://arxiv.org/abs/2306.05685)*
:::

注意 Step 3 的位置交换——这是一个关键设计。Zheng et al. 发现 GPT-4 作为评判者时存在显著的**位置偏差**：它倾向于偏好放在第一个位置的回答。通过交换位置并比较两次判断，可以部分缓解这一偏差。

#### LLM-as-Judge 的偏差分析

Zheng et al. 系统地研究了 LLM 评判者的多种偏差，这对理解生成式评测的局限性至关重要：

| 偏差类型 | 描述 | 严重程度 | 缓解方法 |
|----------|------|----------|----------|
| **位置偏差** | 倾向于偏好第一个/最后一个位置的回答 | 高 | 交换位置，取一致结果 |
| **冗长偏差** | 倾向于偏好更长的回答 | 中 | 在 prompt 中明确要求关注质量而非长度 |
| **自我偏好** | 模型倾向于偏好自己（或相似风格）的输出 | 中 | 使用与被评模型不同的 judge |
| **格式偏差** | 偏好带有列表、标题等格式化的回答 | 低-中 | 控制格式变量 |

尽管存在这些偏差，Zheng et al. 发现 GPT-4 作为评判者与人类评判者的一致率超过 80%——高于人类评判者之间的一致率（约 81%）。这个结果既令人鼓舞（LLM-as-Judge 可用），也令人不安（如果 GPT-4 是评判标准，那它自己的问题就永远无法被发现）。

#### Chatbot Arena：众包人类偏好

Chatbot Arena（LMSYS, 2023-）采用了完全不同的策略：不用 LLM 评判，而是**众包真实用户的偏好**。用户在网站上输入一个问题，两个匿名模型分别回答，用户选择哪个回答更好。模型的身份在用户投票后才揭示。

这种"盲品测试"的设计有几个关键优势。首先，用户可以自由提出任何问题——测试分布完全由真实使用场景决定，不受预设题目的限制。其次，匿名设计消除了品牌偏好（用户不知道自己在评价 GPT-4 还是 Claude）。第三，大规模众包（截至 2025 年已有数十万次投票）提供了统计上稳健的排名。

模型的排名使用 **Elo 评分系统**——与国际象棋的评分系统相同。每次对决后，赢家获得评分，输家失去评分，变化量取决于双方的评分差距：

$$
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}
$$

$$
R_A^{\text{new}} = R_A + K \cdot (S_A - E_A)
$$

其中 $E_A$ 是模型 A 的预期胜率，$R_A, R_B$ 是当前 Elo 评分，$S_A$ 是实际结果（赢=1，输=0，平=0.5），$K$ 是更新系数。

#### 完整数值示例：Elo 评分更新

**设定**：模型 A 当前 Elo = 1200，模型 B 当前 Elo = 1000，$K = 32$。

**Step 1: 计算预期胜率**

$$
E_A = \frac{1}{1 + 10^{(1000 - 1200)/400}} = \frac{1}{1 + 10^{-0.5}} = \frac{1}{1 + 0.316} = 0.760
$$

$$
E_B = 1 - E_A = 0.240
$$

**解读**：根据当前评分，系统预期模型 A 有 76% 的胜率。

**Step 2: 模型 A 获胜（符合预期）**

$$
R_A^{\text{new}} = 1200 + 32 \times (1 - 0.760) = 1200 + 7.7 = 1207.7
$$

$$
R_B^{\text{new}} = 1000 + 32 \times (0 - 0.240) = 1000 - 7.7 = 992.3
$$

**解读**：强者战胜弱者，评分变化不大（+7.7）。

**Step 3: 如果模型 B 爆冷获胜呢？**

$$
R_A^{\text{new}} = 1200 + 32 \times (0 - 0.760) = 1200 - 24.3 = 1175.7
$$

$$
R_B^{\text{new}} = 1000 + 32 \times (1 - 0.240) = 1000 + 24.3 = 1024.3
$$

**解读**：弱者战胜强者，评分变化大得多（±24.3）。这就是 Elo 系统的精妙之处——"爆冷"传递了更多信息，因此带来更大的评分更新。

#### Bradley-Terry 模型：成对比较的概率基础

Chatbot Arena 的排名本质上基于 **Bradley-Terry 模型**（Bradley & Terry, 1952），这是成对比较（paired comparison）领域最经典的概率模型。

给定 $N$ 个模型，每个模型 $i$ 有一个"能力参数" $\theta_i > 0$。模型 $i$ 在与模型 $j$ 的对决中获胜的概率为：

$$
P(i \succ j) = \frac{\theta_i}{\theta_i + \theta_j}
$$

取对数后，令 $\beta_i = \log \theta_i$：

$$
P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}} = \sigma(\beta_i - \beta_j)
$$

其中 $\sigma$ 是 sigmoid 函数。这正是一个逻辑回归模型！Elo 评分系统可以看作 Bradley-Terry 模型的在线更新版本（用 $R_i / 400$ 近似 $\beta_i$）。

给定一组成对比较数据 $\{(i_k, j_k, y_k)\}_{k=1}^{M}$（$y_k = 1$ 表示 $i_k$ 获胜），Bradley-Terry 模型的参数通过最大似然估计求解：

$$
\hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} \sum_{k=1}^{M} \left[ y_k \log \sigma(\beta_{i_k} - \beta_{j_k}) + (1 - y_k) \log \sigma(\beta_{j_k} - \beta_{i_k}) \right]
$$

这个优化问题是凸的，可以用标准的梯度方法高效求解。

### 数据污染与科学诚实

#### 数据污染的本质

数据污染（Data Contamination）是 LLM 评测面临的最严重威胁之一。当模型的训练数据包含了 benchmark 的测试题时，模型的高分可能反映的是"记忆"而非"能力"。这就像一个学生考前偷看了试卷——他的高分并不代表他真正掌握了知识。

数据污染的严重性在于其**隐蔽性**和**不可避免性**。现代 LLM 的训练数据通常来自互联网爬取，规模达到数万亿 token。MMLU 的题目来自公开的考试和教材，这些内容很可能出现在训练数据中。对于闭源模型（如 GPT-4、Claude），外部研究者甚至无法检查训练数据中是否包含特定 benchmark。

#### 污染检测方法

研究者开发了多种方法来检测数据污染。最直接的方法是 **n-gram 重叠检测**：检查 benchmark 测试题的 n-gram 是否出现在训练数据中。GPT-3 和 GPT-4 的技术报告都进行了这类分析。但这种方法有明显的局限——改写（paraphrase）后的题目不会被检测到，而且它要求能够访问训练数据，对闭源模型不适用。

面对黑盒模型，研究者发展出了更巧妙的检测策略。Oren et al.（2023）提出了**分区对比法**：通过分析模型在 benchmark 不同分割（partition）上的表现差异来推断污染——如果模型在某个子集上表现异常地好，这可能暗示该子集被"泄漏"到了训练数据中。Deng et al.（2023）提出了 **TS-Guessing** 方法：给模型展示 benchmark 题目的部分内容（如只给选项不给题干），检测模型是否仍能以远超随机的概率选出正确答案。如果模型"见过"这些题目，即使只看到部分信息也能猜出答案。Golchin & Surdeanu（2023）则提出了更直接的 **Time Travel** 方法：通过精心设计的 prompt，测试模型是否能逐字复现 benchmark 的原始文本——如果模型能几乎完美地重建某个 benchmark 实例的精确措辞，这就是数据污染的强证据。

另一种方法是**金丝雀检测**（Canary Detection）：在 benchmark 中故意嵌入一些独特的标记（canary tokens），如果模型能够复现这些标记，就证明存在污染。这种方法的优势在于它可以在 benchmark 设计阶段就内置防污染机制。

#### 著名的数据污染案例

2023 年，多项研究揭示了大规模数据污染的存在。研究者发现某些模型在 MMLU 特定学科上的表现远超预期，而这些学科的题目恰好在互联网上大量流传。更令人担忧的是，一些模型在 benchmark 发布前后的性能提升，与新增训练数据中包含 benchmark 相关内容的时间高度吻合。

这些案例提醒我们：**benchmark 分数的提升不一定意味着能力的提升**。在解读任何评测结果时，都需要考虑数据污染的可能性。

### 可靠性评测

#### 事实性：如何衡量幻觉？

LLM 的"幻觉"（Hallucination）——生成看似合理但实际错误的内容——是部署中最严重的问题之一。TruthfulQA（Lin et al., 2021）是最早系统评测 LLM 事实性的 benchmark。它包含 817 个问题，这些问题被设计为容易诱导模型生成错误答案——因为人类对这些问题也经常犯错（如常见的误解、都市传说等）。

TruthfulQA 的一个反直觉的发现是：**更大的模型并不总是更诚实**。在没有经过对齐训练的情况下，更大的模型由于更好地学习了人类文本中的常见错误，反而更容易生成"流畅但错误"的回答。这与模型在其他 benchmark 上"越大越好"的趋势形成了鲜明对比。

#### 一致性：同一问题不同问法

一个可靠的模型应该对同一问题的不同措辞给出一致的回答。但实践中，LLM 对 prompt 的措辞极其敏感——仅仅改变问题的语序或添加一个无关的前缀，都可能导致完全不同的回答。

这种不一致性对评测的影响是深远的：**一个模型在 MMLU 上 86% 的准确率，可能变成 82% 如果你稍微改写题目的措辞**。这意味着 benchmark 分数的精度远低于它表面上的精确度。

#### 拒答质量与越狱鲁棒性

对齐后的模型被训练在面对有害请求时拒绝回答。但"拒答"本身也是一个评测维度：模型是否能够在**应该拒绝时拒绝**（安全性），同时在**不应该拒绝时不拒绝**（可用性）？过度拒答（对无害问题也拒绝）和拒答不足（对有害问题回答）都是失败模式。

越狱（Jailbreak）测试评估模型的安全护栏是否能抵抗对抗性攻击。研究者不断发现新的方法绕过模型的安全限制——如角色扮演、编码变换、多步引导等。这是一个"攻防博弈"的领域，评测本身需要持续更新以跟上攻击技术的演进。

---

## 工程实践

### 搭建评测 Pipeline

一个完整的 LLM 评测 pipeline 通常包含以下组件：数据加载、推理执行、指标计算、结果聚合。下面是一个使用 Python 实现基础评测的示例：

```python
"""
最小化的 LLM 评测 Pipeline 示例
评测模型在 MMLU 风格选择题上的表现
"""
import json
import numpy as np
from collections import defaultdict

# ========== 数据加载 ==========
def load_mmlu_style_data(filepath):
    """加载 MMLU 风格的选择题数据
    格式: {"question": str, "choices": [str], "answer": int, "subject": str}
    """
    with open(filepath) as f:
        return [json.loads(line) for line in f]

# ========== 推理模板 ==========
def format_mmlu_prompt(question, choices, few_shot_examples=None):
    """构建 MMLU 评测 prompt
    关键细节：prompt 格式对结果有显著影响！
    """
    prompt = ""
    if few_shot_examples:
        for ex in few_shot_examples:
            prompt += f"Question: {ex['question']}\n"
            for i, c in enumerate(ex['choices']):
                prompt += f"({chr(65+i)}) {c}\n"
            prompt += f"Answer: ({chr(65+ex['answer'])})\n\n"

    prompt += f"Question: {question}\n"
    for i, c in enumerate(choices):
        prompt += f"({chr(65+i)}) {c}\n"
    prompt += "Answer: ("
    return prompt

# ========== 指标计算 ==========
def compute_metrics(predictions, references):
    """计算多维度评测指标"""
    correct = sum(p == r for p, r in zip(predictions, references))
    total = len(references)
    accuracy = correct / total

    # 按学科分组计算
    subject_scores = defaultdict(lambda: {"correct": 0, "total": 0})
    # ... (省略分组逻辑)

    return {
        "accuracy": accuracy,
        "correct": correct,
        "total": total,
        "95_ci": 1.96 * np.sqrt(accuracy * (1 - accuracy) / total)
    }

# ========== 结果报告 ==========
def report_results(metrics):
    """生成评测报告"""
    print(f"Overall Accuracy: {metrics['accuracy']:.1%} "
          f"(± {metrics['95_ci']:.1%})")
    print(f"Correct: {metrics['correct']}/{metrics['total']}")

# ========== 主流程 ==========
# data = load_mmlu_style_data("mmlu_test.jsonl")
# predictions = [call_model(format_mmlu_prompt(d)) for d in data]
# metrics = compute_metrics(predictions, [d["answer"] for d in data])
# report_results(metrics)
```

### 使用 lm-evaluation-harness

在实践中，评测通常使用成熟的框架。**lm-evaluation-harness**（EleutherAI）是最广泛使用的开源评测框架，支持数百个 benchmark：

```bash
# 安装
pip install lm-eval

# 在 MMLU 上评测 Hugging Face 模型
lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf \
    --tasks mmlu \
    --batch_size 8 \
    --output_path results/

# 评测多个 benchmark
lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf \
    --tasks mmlu,hellaswag,arc_challenge,truthfulqa_mc \
    --batch_size 8
```

### 实现 LLM-as-Judge

```python
"""
简化版 LLM-as-Judge 实现
使用成对比较 + 位置交换来缓解位置偏差
"""

JUDGE_PROMPT = """Please act as an impartial judge and evaluate the quality
of the responses provided by two AI assistants to the user question below.

[User Question]
{question}

[Assistant A's Answer]
{answer_a}

[Assistant B's Answer]
{answer_b}

Evaluate which assistant provides a better answer. Consider helpfulness,
relevance, accuracy, depth, and clarity. Avoid position bias.

Output your verdict EXACTLY as: [[A]], [[B]], or [[Tie]].
Provide brief reasoning before your verdict."""

import re

def judge_pair(question, answer_a, answer_b, judge_model):
    """
    对一对回答进行评判，带位置交换

    Returns: "A", "B", or "Tie"
    """
    # 第一次评判：A在前
    prompt_1 = JUDGE_PROMPT.format(
        question=question, answer_a=answer_a, answer_b=answer_b
    )
    result_1 = judge_model(prompt_1)
    verdict_1 = extract_verdict(result_1)

    # 第二次评判：B在前（交换位置）
    prompt_2 = JUDGE_PROMPT.format(
        question=question, answer_a=answer_b, answer_b=answer_a
    )
    result_2 = judge_model(prompt_2)
    verdict_2 = extract_verdict(result_2)
    # 注意：需要反转 verdict_2 的结果
    verdict_2_flipped = flip_verdict(verdict_2)

    # 两次一致才算数
    if verdict_1 == verdict_2_flipped:
        return verdict_1
    else:
        return "Tie"  # 不一致 → 视为平局

def extract_verdict(text):
    """从 judge 输出中提取判定"""
    match = re.search(r'\[\[(A|B|Tie)\]\]', text)
    return match.group(1) if match else "Tie"

def flip_verdict(verdict):
    """位置交换后，A和B的含义互换"""
    if verdict == "A": return "B"
    if verdict == "B": return "A"
    return "Tie"
```

### 数据污染快速检测

```python
"""
简易数据污染检测：检查模型是否能"背出" benchmark 题目
思路：给出题目的前半部分，看模型能否续写出后半部分
"""

def contamination_check(model, benchmark_items, n_samples=100):
    """
    通过 prefix completion 检测潜在的数据污染

    如果模型能高精度地续写 benchmark 题目，
    说明这些题目很可能出现在训练数据中。
    """
    import random
    samples = random.sample(benchmark_items, min(n_samples, len(benchmark_items)))

    exact_matches = 0
    high_overlap = 0

    for item in samples:
        text = item["question"]
        # 取前 50% 作为 prefix
        split_point = len(text) // 2
        prefix = text[:split_point]
        suffix = text[split_point:]

        # 让模型续写
        completion = model.generate(prefix, max_tokens=len(suffix.split()) * 2)

        # 计算重叠度
        overlap = compute_rouge_l(completion, suffix)

        if overlap > 0.9:
            exact_matches += 1
        if overlap > 0.7:
            high_overlap += 1

    return {
        "exact_match_rate": exact_matches / len(samples),
        "high_overlap_rate": high_overlap / len(samples),
        "verdict": "LIKELY CONTAMINATED" if exact_matches / len(samples) > 0.1
                   else "PROBABLY CLEAN"
    }
```

---

## 深入理解

> **研究者必读**：这一节探讨评测方法论的理论基础、边界条件和开放问题

### 为什么评测科学这么重要？——理论视角

从科学哲学的角度看，评测问题其实是**操作性定义**（operational definition）的问题。当我们说一个模型"理解语言"或"具备推理能力"时，这些概念需要通过具体的评测任务来操作化。正如智商测试（IQ test）不等于"智能"，MMLU 分数也不等于"理解"——但在没有更好替代的情况下，它是我们能得到的最好近似。

问题在于，不同的操作性定义可能给出截然不同的结论。Schaeffer et al.（2023）的工作正是这一点的极端体现：同一份数据、同一个能力，用精确匹配度量时"涌现"了，用连续度量时却"一直都在平滑增长"。这告诉我们：**任何评测结论都不能脱离其度量方式来理解**。

从信息论的角度，评测本质上是通过有限的观测（测试题目和模型回答）来推断模型的潜在能力（一个无限维的函数空间中的一个点）。这是一个严重**欠定**（underdetermined）的问题——有限的测试永远无法完全确定模型的能力边界。更糟糕的是，由于对抗性优化的存在，模型可能在测试分布上表现出色，但在测试分布之外完全崩溃。

### 评测的隐含假设

每种评测方式都隐含了一系列假设，理解这些假设是评判评测结论的前提。

**假设 1：独立同分布**。大多数 benchmark 假设测试样本是从某个目标分布中独立采样的。但实际上，benchmark 题目的选择往往有强烈的偏见——MMLU 偏向西方教育体系的知识，GLUE 偏向英语互联网文本的分布。模型在 benchmark 上的高分不意味着它在真实使用场景中同样表现良好。

**假设 2：任务可分解性**。GLUE 和 HELM 假设语言理解可以分解为若干独立的子任务（情感分析、蕴含判断等），且在这些子任务上的表现可以综合反映整体能力。但语言理解可能是一个整体性的能力，无法简单分解。

**假设 3：人类基线的可比性**。几乎所有 benchmark 都以人类表现作为参考。但人类和 LLM 的错误模式截然不同——人类可能因粗心犯简单错误，而 LLM 可能犯人类不可能犯的"荒谬"错误。直接比较人类和模型的分数可能具有误导性。

**假设 4：评测与应用的一致性**。benchmark 分数应该能够预测模型在实际应用中的表现。但由于分布偏移、prompt 敏感性和数据污染，这种预测关系往往很弱。许多用户反馈"benchmark 排名第一的模型在我的场景中表现不如排名第三的"。

### 方法的边界条件

**自动指标的边界**：BLEU/ROUGE 在系统级比较时尚可接受（特别是在差距较大时），但在句子级评估和相近系统的比较中几乎不可靠。语义指标（BERTScore、COMET）有所改善，但在跨领域、跨语言场景中可能退化。

**静态 benchmark 的边界**：GLUE/MMLU 等 benchmark 在模型能力远低于人类时是有效的区分工具，但一旦模型接近或超过人类基线，benchmark 就失去了区分能力（"天花板效应"）。此外，benchmark 的有效寿命受到数据污染和针对性优化的限制——通常只有 1-3 年。

**LLM-as-Judge 的边界**：当被评模型的能力远低于评判模型时，LLM-as-Judge 工作良好。但当被评模型接近或超过评判模型时，评判结果的可靠性急剧下降。这创造了一个"玻璃天花板"：无法评测比评判者更强的模型。

**Chatbot Arena 的边界**：需要大量真实用户参与，对小众模型或特定领域模型不友好。用户群体的偏见（如偏好英语、偏好编程任务）会影响排名的公平性。此外，Elo 评分假设模型能力是一维的——但现实中模型可能在不同维度（创意 vs 精度 vs 速度）上各有优劣。

### 开放研究问题

**问题 1：动态评测**。如何设计一个能够持续生成新题目、不会被"刷满"的评测系统？一种思路是**对抗性评测**（adversarial evaluation）——自动生成能够揭示模型弱点的测试用例。但如何保证生成的题目既有意义又有正确答案？

**问题 2：多维评测的聚合**。HELM 提出了 7 个评测维度，但如何将这 7 个维度聚合为一个可比较的分数？不同用户对不同维度的关注度不同（医疗领域更重视准确性，创意领域更重视多样性）。是否存在一种"帕累托最优"的聚合方式？

**问题 3：评测 Scalable Oversight**。当模型的能力超过人类评判者时，如何评测？这是 AI 安全领域的一个核心问题。如果连人类专家都无法判断模型的回答是否正确（如高级数学定理的证明），我们如何建立可靠的评测？

**问题 4：评测公平性**。现有 benchmark 严重偏向英语和西方知识体系。如何设计真正多语言、多文化公平的评测？MMLU 中"Abstract Algebra"的题目对一个从未接触过英语高等教育的人毫无意义，但这不代表他缺乏"智能"。

---

## 局限性与未解决的问题

### 当前评测方法论的核心局限

本章讨论的评测方法虽然代表了当前最先进的实践，但每一种都有根本性的局限。

静态 benchmark 受 Goodhart 定律支配，寿命有限。LLM-as-Judge 受限于评判模型自身的能力天花板和系统性偏差。Chatbot Arena 依赖大规模用户参与，且排名只反映"普通用户的平均偏好"而非特定任务的专业表现。所有方法都无法完全解决数据污染问题——特别是对闭源模型。

更根本的问题是，我们甚至缺乏对"模型能力"的清晰定义。当我们说一个模型"理解语言"或"具备推理能力"时，这些概念的精确含义是什么？没有清晰的定义，就不可能有完美的评测。

### 这些局限导向了什么？

评测方法论的核心洞察——**度量本身会影响我们对能力的判断**——直接引出了下一章的主题：**指令微调**。

如果我们连"模型好不好"都难以可靠地衡量，那么"训练模型变好"就更加困难。指令微调（Instruction Tuning）的目标是让模型"听话"——能够理解并遵循人类的指令。但"听话"本身就是一个评测问题：什么样的回答算"好的回答"？谁来判断？这些问题将在 Ch23（指令微调）和 Ch24（RLHF）中得到进一步讨论。

从更宏观的视角看，评测问题是对齐（Alignment）问题的缩影。对齐的核心困难——如何定义和衡量"符合人类意图"——本质上就是一个评测问题。本章讨论的方法论（Bradley-Terry 模型、人类偏好收集、LLM-as-Judge）将在对齐章节中反复出现。

> 下一章预告：第 23 章将讨论**指令微调**——如何通过在指令-回答数据上训练，让模型从"能力强但不好用"变成"既有能力又听话"。从 FLAN 到 Self-Instruct 到 Alpaca，指令数据的构建方法经历了怎样的演进？而评测这些"听话"模型的表现，正需要本章讨论的方法论。

---

## 本章小结

### 核心要点回顾

本章系统讲述了 NLP 评测方法论的演进，核心线索是**评测与模型之间的"军备竞赛"**——每一代评测方法都在修复上一代的缺陷，同时引入新的挑战。

**第一代自动指标**（BLEU、ROUGE）解决了"如何自动评测"的问题，通过 n-gram 匹配量化文本质量。BLEU 的核心公式是修正 n-gram 精度加短句惩罚，ROUGE 则侧重召回率。但 n-gram 匹配无法捕捉语义——同义替换、释义重写都会导致低分。

**第二代理解能力 benchmark**（GLUE、SuperGLUE、MMLU、HELM、BIG-bench）解决了"如何评测理解能力"的问题，通过精心设计的任务集测试模型的多种语言理解能力。但静态 benchmark 受 Goodhart 定律支配——发布、追逐、饱和、失效的循环通常只需 1-3 年。数据污染更是从根本上威胁了评测结果的可信度。

**第三代生成式评测**（LLM-as-Judge、Chatbot Arena）解决了"如何评测开放式生成"的问题。LLM-as-Judge 通过让 GPT-4 等强模型担任评判者，实现了大规模自动评测。Chatbot Arena 通过众包真实用户的盲品偏好，建立了 Elo 排名系统。但 LLM 评判者存在位置偏差、冗长偏差和自我偏好，而 Chatbot Arena 的用户群体偏见也不可忽视。

贯穿全章的核心教训是 **Goodhart 定律**："当一个度量成为目标时，它就不再是好的度量。"这不仅适用于 NLP 评测，也是理解 AI 对齐问题的关键视角。

### 关键对比速查

| 评测方法 | 时代 | 代表 | 优势 | 核心局限 |
|----------|------|------|------|----------|
| **自动指标** | 2002– | BLEU, ROUGE | 全自动、可复现 | 与人类判断相关性低 |
| **语义指标** | 2020– | BERTScore, COMET | 捕捉语义相似度 | 依赖预训练模型 |
| **静态 Benchmark** | 2018– | GLUE, MMLU | 标准化、可比较 | Goodhart 定律、数据污染 |
| **多维评测** | 2022– | HELM | 全面性 | 维度聚合困难 |
| **LLM-as-Judge** | 2023– | MT-Bench | 评测开放生成 | 评判者偏差、能力天花板 |
| **人类偏好** | 2023– | Chatbot Arena | 最接近真实使用 | 用户偏见、规模需求 |

### 关键公式速查

- **BLEU**：$\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$
- **短句惩罚**：$\text{BP} = e^{\min(0, 1 - r/c)}$
- **Elo 预期胜率**：$E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}$
- **Bradley-Terry**：$P(i \succ j) = \sigma(\beta_i - \beta_j)$

### 思考题

1. **[概念理解]** 解释 Goodhart 定律在 NLP 评测中的表现。为什么 GLUE 在发布仅两年后就"失效"了？如果你是 benchmark 设计者，你会采取什么策略来延长 benchmark 的有效寿命？

2. **[数学推导]** 在 Elo 评分系统中，假设模型 A 的 Elo 为 1400，模型 B 的 Elo 为 1000。(a) 计算 A 对 B 的预期胜率。(b) 如果 A 获胜，双方的 Elo 如何更新（$K=32$）？(c) 如果连续 5 场 B 都爆冷获胜，A 和 B 的 Elo 会变成多少？(d) 讨论 Elo 系统的一个局限：为什么一维评分可能不足以描述 LLM 的能力？

3. **[工程实践]** 使用 `lm-evaluation-harness` 框架，在 MMLU 的一个子集（如 abstract_algebra 或 high_school_mathematics）上评测一个开源模型。报告：(a) 5-shot 准确率及 95% 置信区间，(b) 将同样的题目改写（paraphrase），重新评测，观察分数变化，(c) 讨论这种不稳定性对评测结论的影响。

4. **[研究思考]** 当被评模型的能力超过评判模型时，LLM-as-Judge 会面临什么问题？设计一个实验来检验这种"评判天花板"效应。如果未来的模型在所有维度上都超过 GPT-4，我们应该如何评测它们？

5. **[开放思考]** 假设你要设计一个"不会过时"的 LLM 评测系统。你会采用什么架构？考虑：动态题目生成、多维能力评测、防数据污染机制、人机混合评判。讨论每个设计选择的 trade-off。

---

## 延伸阅读

### 核心论文（必读）

**Wang, A. et al. (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"**。理解能力评测的里程碑。重点阅读：Section 2（任务设计原则）。即使 GLUE 已"过时"，它的设计思想仍是后续 benchmark 的基础。[arXiv:1804.07461](https://arxiv.org/abs/1804.07461)

**Zheng, L. et al. (2023). "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"**。生成式评测新范式的代表作。重点阅读：Section 3（LLM-as-Judge 方法论）、Section 4（偏差分析）。[arXiv:2306.05685](https://arxiv.org/abs/2306.05685)

**Liang, P. et al. (2022). "Holistic Evaluation of Language Models"**。最全面的评测框架设计。重点阅读：Section 2（评测分类学）。可快速浏览：具体模型的结果（过时较快）。[arXiv:2211.09110](https://arxiv.org/abs/2211.09110)

### 经典指标

**Papineni, K. et al. (2002). "BLEU: a Method for Automatic Evaluation of Machine Translation"**。自动评测的开山之作。重点阅读：Section 2（BLEU 公式定义）。20 多年后仍被广泛使用。[ACL 2002](https://aclanthology.org/P02-1040/)

**Zhang, T. et al. (2020). "BERTScore: Evaluating Text Generation with BERT"**。语义评测指标的代表。[arXiv:1904.09675](https://arxiv.org/abs/1904.09675)

### 数据污染与科学诚实

**Oren, Y. et al. (2023). "Proving Test Set Contamination in Black Box Language Models"**。黑盒模型的污染检测——无需访问训练数据即可推断污染。[arXiv:2310.17623](https://arxiv.org/abs/2310.17623)

**Deng, C. et al. (2023). "Investigating Data Contamination in Modern Benchmarks for Large Language Models"**。TS-Guessing 方法——通过给模型部分输入来检测是否"见过"benchmark 题目。[arXiv:2311.09783](https://arxiv.org/abs/2311.09783)

**Golchin, S. & Surdeanu, M. (2023). "Time Travel in LLMs: Tracing Data Contamination in Large Language Models"**。Time Travel 方法——测试模型是否能逐字复现 benchmark 原文。[arXiv:2308.08493](https://arxiv.org/abs/2308.08493)

**Schaeffer, R. et al. (2023). "Are Emergent Abilities of Large Language Models a Mirage?"**。度量选择对能力判断的影响。NeurIPS 2023 最佳论文。[arXiv:2304.15004](https://arxiv.org/abs/2304.15004)

### 后续发展

**Chiang, W. et al. (2024). "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"**。Chatbot Arena 的完整技术报告。[arXiv:2403.04132](https://arxiv.org/abs/2403.04132)

**Hendrycks, D. et al. (2020). "Measuring Massive Multitask Language Understanding"**。MMLU benchmark。[arXiv:2009.03300](https://arxiv.org/abs/2009.03300)

### 综述与教程

**Chang, Y. et al. (2024). "A Survey on Evaluation of Large Language Models"**。LLM 评测方法的全面综述。[arXiv:2307.03109](https://arxiv.org/abs/2307.03109)

### 代码资源

- **lm-evaluation-harness**：[github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)（最广泛使用的评测框架）
- **OpenCompass**：[github.com/open-compass/opencompass](https://github.com/open-compass/opencompass)（支持中文评测）
- **Chatbot Arena**：[chat.lmsys.org](https://chat.lmsys.org/)（在线参与评测）
- **HELM**：[crfm.stanford.edu/helm](https://crfm.stanford.edu/helm/)（Stanford 评测平台）

---

## 历史注脚

NLP 评测的历史充满了意料之外的转折。

BLEU 的发明者 Kishore Papineni 最初是在 IBM 研究院工作时提出这个指标的。他的动机非常实际：机器翻译研究者需要一个自动化的评测方法来快速迭代系统，而人类评测太慢太贵。BLEU 论文发表于 2002 年 ACL，至今已被引用超过 20,000 次，是 NLP 领域被引用最多的论文之一。然而 Papineni 本人后来在多个场合表示，BLEU 被过度使用了——它从来不是为了替代人类判断，而只是一个快速的近似。

GLUE 的故事则带有一丝戏剧性。当 Sam Wang 等人在 2018 年设计 GLUE 时，他们认为这个 benchmark 至少能管用好几年。没想到仅仅几个月后，BERT 就让 GLUE 的排行榜变成了"卷王争霸赛"。Wang 等人不得不紧急设计 SuperGLUE——这可能是 NLP 历史上最快被自己的成功逼迫"升级"的 benchmark。

Chatbot Arena 的诞生同样充满偶然。LMSYS（Large Model Systems Organization）最初是 UC Berkeley 一群研究系统优化的学生组建的，他们的初衷是研究 LLM 推理效率。但当他们搭建了一个供公众体验不同模型的网站后，用户自发开始比较不同模型的回答质量。这个"副产品"——用户自发的模型比较——最终演变成了整个 LLM 领域最被信赖的排名系统。

评测方法论的演进还在继续。2024-2025 年，社区开始探讨"活性 benchmark"（living benchmarks）的概念——benchmark 不再是一次性发布的静态数据集，而是持续更新、动态生成、自动防污染的活系统。这或许是应对 Goodhart 定律的下一步尝试，但历史告诉我们，每一次尝试最终都会遇到新的挑战。正如一位评审人幽默地总结的："评测方法论的唯一不变定律是，没有评测方法是永恒的。"
