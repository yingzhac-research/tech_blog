---
title: "第17章：规模的力量——Scaling Laws"
subtitle: "When Bigger Is Predictably Better: The Science of Scaling Neural Language Models"
author: "Ying Zha"
date: "2026-01-27"
categories: [NLP, Deep Learning, LLM, Scaling Laws, GPT-2, Chinchilla]
tags: [Scaling Laws, GPT-2, Chinchilla, Kaplan, Hoffmann, 幂律, 计算最优, 涌现能力, compute-optimal]
description: "上一章确立了Decoder-only作为大语言模型时代的主流架构，但留下了一个根本性的问题：规模究竟如何影响模型的能力？2019年GPT-2用四个规模的模型展示了'越大越好'的初步证据；2020年Kaplan等人发现损失函数与参数量、数据量、计算量之间遵循优雅的幂律关系，且应优先扩大模型而非数据；2022年Hoffmann等人的Chinchilla工作颠覆了这一结论——数据与参数应等比例扩展，此前的大模型几乎都训练不足。这些发现将大模型训练从炼金术变成了可预测的工程科学，也深刻重塑了整个行业的训练策略。"
image: "figures/chapter-17/original/fig-kaplan-overview-loss-scaling.png"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    fig-cap-location: bottom
bibliography: references.bib
---

> **核心问题**：语言模型的性能与规模（参数量、数据量、计算量）之间遵循什么规律？给定有限的计算预算，如何最优地分配给模型大小和训练数据？
>
> **历史坐标**：2019–2022 | GPT-2 (Radford et al., 2019), Scaling Laws (Kaplan et al., 2020), Chinchilla (Hoffmann et al., 2022) | 从经验直觉到可预测的规模科学

::: {.callout-tip collapse="true"}
## 本章参考来源

### 论文
- **Radford et al. (2019)** "Language Models are Unsupervised Multitask Learners" (OpenAI, GPT-2 paper) — 参考了四个规模模型的零样本实验结果（Table 2-3）、模型规模与性能的关系、"too dangerous to release"的发布策略
- **Kaplan et al. (2020)** "Scaling Laws for Neural Language Models" (arXiv:2001.08361) — 参考了 Section 2-4 的三条幂律关系式、Figure 1-4 的幂律拟合图、Section 6 的最优计算资源分配；本章最核心的技术参考
- **Hoffmann et al. (2022)** "Training Compute-Optimal Large Language Models" (arXiv:2203.15556, "Chinchilla") — 参考了 Section 3-4 的三种估算方法、Figure 1（IsoFLOP曲线）、Figure 3（Chinchilla vs Gopher对比）、Table 3（最优参数-数据比）；本章最重要的修正性工作
- **Brown et al. (2020)** "Language Models are Few-Shot Learners" (arXiv:2005.14165, GPT-3) — 参考了 175B 参数规模下的涌现能力和 In-Context Learning 结果
- **Wei et al. (2022)** "Emergent Abilities of Large Language Models" (arXiv:2206.07682) — 参考了涌现能力的定义和相变现象
- **Schaeffer et al. (2023)** "Are Emergent Abilities of Large Language Models a Mirage?" (arXiv:2304.15004) — 参考了对涌现能力的质疑：度量方式的假象

### 教材
- **D2L** Section 9.5 — 参考了语言模型困惑度的定义和评估方法
- **SLP3** Chapter 11 — 参考了大语言模型的综述性讨论

### 课程
- **Stanford CS224N** Lecture 10-11 (Winter 2025) — 参考了 Scaling Laws 和 LLM 的教学组织方式
- **CMU 11-711 ANLP** Lecture 7 "Scaling" (Fall 2024) — 参考了 Kaplan vs Chinchilla 的对比讲解框架
:::

---

## 从上一章说起

上一章我们系统地解答了预训练时代最核心的架构问题：Encoder-only（BERT）、Decoder-only（GPT）、Encoder-Decoder（T5/BART）三种架构，谁更适合构建通用的语言智能？答案出人意料地指向了最朴素的选择——Decoder-only加上"预测下一个词"。不是因为它在受控实验中表现最优（T5的实验甚至表明Encoder-Decoder更强），而是因为自回归语言建模目标在**规模化**和**通用性**上展现出了压倒性的工程优势。

但上一章结尾也留下了一个意味深长的伏笔："规模是Decoder-only优势的前提"。GPT-2的15亿参数只展示了粗糙的零样本能力，GPT-3的1750亿参数才真正令人信服。这意味着，仅仅选对了架构是不够的——规模才是释放这种架构潜力的关键变量。

这就引出了一个更根本的问题：**规模究竟如何影响模型的能力？**"越大越好"是一条可以量化的规律，还是仅仅是一种模糊的经验直觉？如果我们有一笔固定的计算预算——比如1000块GPU跑30天——应该训练一个巨大的模型在少量数据上跑几遍，还是训练一个中等模型在海量数据上跑很多遍？在2019年之前，这些问题的回答基本靠直觉和试错。研究者们会训练几个不同大小的模型，看哪个效果最好，然后在论文中报告结果。但这种方法既昂贵又低效——训练一个千亿参数的模型可能花费数百万美元，你不可能"试几个看看"。

2020年，OpenAI的Jared Kaplan等人发表了一篇改变游戏规则的论文，首次揭示了语言模型性能与规模之间存在**优雅的幂律关系**——损失函数可以用简洁的数学公式精确预测。这意味着你可以通过小规模实验来推断大模型的表现，将大模型训练从"炼金术"变成了"可预测的工程科学"。

然而，这个故事还有一个戏剧性的转折。2022年，DeepMind的Jordan Hoffmann等人发表了Chinchilla论文，用一个70B参数的模型击败了四倍于它的280B参数Gopher——仅仅因为训练了更多的数据。Chinchilla的结论直接颠覆了Kaplan的资源分配建议，揭示了一个令整个行业尴尬的事实：**此前几乎所有的大模型都训练不足**。

> 💡 **本章核心洞察**：语言模型的损失函数与参数量、数据量、计算量之间遵循**幂律关系**（power law），这让模型性能变得可预测。但关于如何最优分配计算预算，Kaplan（2020）和Chinchilla（2022）给出了截然不同的答案——前者建议优先扩大模型，后者证明数据与参数应**等比例扩展**。这一修正深刻重塑了整个行业的训练策略。

---

## 问题的本质是什么？

### 一个昂贵的赌博

训练一个大语言模型本质上是一个资源分配问题。你拥有的核心资源是**计算量**（compute），通常用浮点运算次数（FLOPs）来衡量。在实践中，计算量受限于你的GPU数量、训练时间和电费预算。给定计算预算 $C$（以FLOPs计），你需要做出两个关键决策：

- **模型有多大？** 参数量 $N$ 决定了模型的容量——它能存储多少"知识"、捕获多复杂的模式。更多参数意味着更强的表达能力，但也意味着每一步训练需要更多计算。

- **训练多少数据？** 训练token数 $D$ 决定了模型看到多少"经验"。更多数据意味着更充分的学习，但也意味着需要更多计算步骤。

这两者之间存在一个根本性的权衡：在固定计算预算下，增大模型意味着每个token消耗更多FLOPs，能处理的数据量就会减少；反过来，增加数据则需要缩小模型来维持在预算内。计算量的大致关系是：

$$
C \approx 6ND
$$

其中 $C$ 是总FLOPs，$N$ 是参数量，$D$ 是训练token数，常数6来自前向传播（$2ND$）和反向传播（$4ND$）的浮点运算。这个公式告诉我们：**$N$ 和 $D$ 是一对被计算预算绑定的变量**，选择其中一个就隐含地决定了另一个。

### 之前的做法：直觉与试错

在Scaling Laws研究出现之前，研究者如何决定模型大小和训练数据量？答案是：**没有系统的方法**。

最常见的策略是"先决定模型大小，然后尽可能多训练"。BERT-base选择了1.1亿参数，因为这是当时GPU显存能装下的最大模型之一。GPT选择了1.2亿参数，大致与BERT对齐。这些选择更多是受硬件约束驱动，而非来自某个优化原则。

另一个流行的策略是"训练到收敛为止"——持续训练直到验证集上的损失不再下降。但对于大型语言模型，"收敛"是一个模糊的概念：loss可能在很长时间内缓慢下降，你永远不知道是否已经到了"值得停下来"的点。

这种靠直觉的方式有一个致命的缺陷：你无法在训练之前预测结果。要知道一个200亿参数的模型是否比一个100亿参数的模型更好，唯一的办法就是两个都训练——这意味着数百万美元的额外开销。这就好比你在建一座摩天大楼之前，不知道70层和100层哪个更合适，唯一的办法是把两座都建起来看看。显然，我们需要一种更科学的方法。

### 我们需要什么样的理论？

一个理想的规模理论应该提供三样东西：

第一，**可预测性**——给定模型大小 $N$ 和数据量 $D$，能够预测最终的损失（loss）或困惑度（perplexity）。这样你就可以通过小规模实验来推断大模型的表现，而不需要实际训练它。

第二，**最优分配**——给定计算预算 $C$，能够计算最优的 $(N^*, D^*)$ 组合，使得损失最小。这解决了"模型要多大、数据要多少"的核心决策问题。

第三，**泛化性**——这些规律应该在不同的架构、数据集、训练设置下保持成立，而不是只适用于特定的实验配置。

2019到2022年间，三项关键工作逐步构建了这样一个理论框架。

---

## 核心思想与直觉

### GPT-2：越大越好的初步证据

2019年2月，OpenAI发布了GPT-2——一个因"太危险而不能完全公开"引发广泛争议的语言模型。但从技术角度看，GPT-2最大的贡献不是零样本能力本身，而是它提供了**同一架构在四个不同规模下的系统性对比**。

GPT-2有四个版本：

| 模型 | 参数量 | 层数 | 隐藏维度 | $d_{model}$ |
|------|--------|------|----------|-------------|
| GPT-2 Small | 117M | 12 | 768 | 768 |
| GPT-2 Medium | 345M | 24 | 1024 | 1024 |
| GPT-2 Large | 762M | 36 | 1280 | 1280 |
| GPT-2 XL | 1.5B | 48 | 1600 | 1600 |

这四个模型使用完全相同的架构（Decoder-only Transformer）、相同的数据（WebText，约800万网页，40GB文本），唯一的区别是**规模**。结果呈现出一个清晰的趋势：在几乎所有任务上，**更大的模型表现更好**，而且这种改善是持续的、单调的。

在语言建模的核心指标——困惑度（perplexity）上，四个模型在WebText测试集上的表现为：

| 模型 | 参数量 | 困惑度（PPL）↓ |
|------|--------|----------------|
| Small | 117M | 29.41 |
| Medium | 345M | 22.76 |
| Large | 762M | 19.93 |
| XL | 1.5B | 18.34 |

参数量增长了约13倍（从117M到1.5B），困惑度下降了约38%。更引人注目的是零样本任务迁移的表现：GPT-2 XL在不做任何微调的情况下，在8个语言建模数据集中的7个上达到了当时的最优水平。这些结果强烈暗示了一个规律：**模型规模与性能之间存在某种系统性的关系**。但这种关系的精确形式是什么？GPT-2论文本身并没有给出数学上的回答。

### 幂律：自然界最常见的"量变到质变"

在揭示Scaling Laws的具体形式之前，让我们先建立一个关键的直觉：**幂律**（power law）。

幂律描述的是两个量之间的关系 $y = ax^b$，其中 $b$ 是一个常数指数。幂律在自然界和人类社会中无处不在：

- **齐普夫定律**（Zipf's Law）：一个词的出现频率与它的频率排名成反比。英语中排名第1的词（"the"）出现频率约为排名第10的词的10倍。
- **帕累托分布**：全球最富有的20%人口拥有约80%的财富。
- **城市规模**：人口超过100万的城市数量，与人口超过10万的城市数量之间，遵循幂律关系。

幂律的关键特征是**标度不变性**（scale invariance）：在对数坐标系中，幂律关系呈现为一条直线。这意味着无论你在什么尺度上观察，模式都是相同的——从千参数到百万参数再到十亿参数，改善的"速率"保持不变。

为什么这很重要？因为如果损失与规模之间是幂律关系，你就可以在小规模上拟合这条直线，然后**外推**到大规模——就像天文学家通过小范围的观测来推断遥远星系的性质。这正是Kaplan等人发现的。

### 从经验到科学：Scaling Laws的核心洞察

Kaplan等人（2020）训练了数百个不同大小的语言模型（从768个参数到15亿参数），系统性地改变参数量 $N$、数据量 $D$ 和计算量 $C$，然后观察交叉熵损失 $L$ 的变化。他们发现了一个惊人的规律：

**损失函数与 $N$、$D$、$C$ 中任何一个量之间，都呈现出干净的幂律关系。**

在双对数坐标系中，数据点几乎完美地落在一条直线上——不是"大致线性"，而是跨越多个数量级的精确线性。这意味着性能的改善是**平滑且可预测的**，没有突然的跳跃或饱和。

这个发现的意义是革命性的。它告诉我们，大模型训练不需要是"碰运气"的过程——你可以用数学公式来计划训练，就像工程师用力学公式来设计桥梁一样。

---

## 技术细节

### Kaplan Scaling Laws（2020）：三条幂律

Kaplan等人发现了三条独立的幂律关系，分别描述损失与参数量、数据量、计算量的关系。下面这张图——Kaplan论文中最核心的Figure 1——直观地展示了这三条幂律。在双对数坐标系中，数据点几乎完美地落在一条直线上，跨越了多个数量级。

![语言模型性能随计算量（左）、数据量（中）、参数量（右）的变化。三者都呈现出干净的幂律关系。](figures/chapter-17/original/fig-kaplan-overview-loss-scaling.png){#fig-kaplan-overview width=100%}

::: {.figure-caption}
*Source: Kaplan et al. (2020) "Scaling Laws for Neural Language Models", Figure 1. [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)*
:::

**幂律1：损失与参数量**

当数据量充分大（不是瓶颈）时，损失仅由模型参数量决定：

$$
L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}, \quad \alpha_N \approx 0.076, \quad N_c \approx 8.8 \times 10^{13}
$$

这里 $L$ 是交叉熵损失（减去不可约损失后的部分），$N$ 是非嵌入参数量（non-embedding parameters），$N_c$ 是一个常数。指数 $\alpha_N \approx 0.076$ 意味着**参数量每增加10倍，损失减少约为 $10^{-0.076} \approx 0.84$ 倍**——换句话说，每10倍参数带来约16%的损失降低。

让我们用具体数字建立直觉。假设一个100M参数模型的损失为3.0：

| 参数量 $N$ | 损失 $L$ | 变化 |
|-----------|----------|------|
| 100M (10⁸) | 3.00 | 基准 |
| 1B (10⁹) | 2.52 | −16% |
| 10B (10¹⁰) | 2.12 | −16% |
| 100B (10¹¹) | 1.78 | −16% |
| 1T (10¹²) | 1.50 | −16% |

注意每增加10倍参数，损失的**相对下降比例是恒定的**——这就是幂律的"标度不变性"。

**幂律2：损失与数据量**

当模型足够大（不是瓶颈）时，损失仅由训练数据量决定：

$$
L(D) = \left(\frac{D_c}{D}\right)^{\alpha_D}, \quad \alpha_D \approx 0.095, \quad D_c \approx 5.4 \times 10^{13}
$$

指数 $\alpha_D \approx 0.095$ 意味着**数据量每增加10倍，损失减少约为 $10^{-0.095} \approx 0.80$ 倍**——约20%的损失降低，比增加参数带来的改善更大（16%）。但要注意，这是在"模型足够大"的前提下——如果模型太小，再多的数据也无济于事。

**幂律3：损失与计算量**

最实用的幂律是损失与总计算量的关系。当计算预算被最优分配（后面会详细讨论）时：

$$
L(C) = \left(\frac{C_c}{C}\right)^{\alpha_C}, \quad \alpha_C \approx 0.050, \quad C_c \approx 3.1 \times 10^{8}
$$

指数 $\alpha_C \approx 0.050$ 意味着**计算量每增加10倍，损失减少约为 $10^{-0.050} \approx 0.89$ 倍**——约11%的损失降低。

你可能注意到了一个有趣的数学关系：$\alpha_C$ 比 $\alpha_N$ 和 $\alpha_D$ 都小。这是因为当你增加计算量时，你需要同时增加 $N$ 和 $D$，而增加 $N$ 和 $D$ 各自只带来"部分"的改善。粗略地说，$\alpha_C$ 是 $\alpha_N$ 和 $\alpha_D$ 的某种"调和"。

::: {.callout-note}
## 三条幂律的统一视角

Kaplan等人还提出了一个统一的参数化形式，同时描述损失对 $N$ 和 $D$ 的依赖：

$$
L(N, D) = \left[\left(\frac{N_c}{N}\right)^{\alpha_N / \alpha_D} + \frac{D_c}{D}\right]^{\alpha_D}
$$

这个公式的物理直觉是：损失由两个"瓶颈"中更严重的那个主导。当 $N$ 很小时，第一项主导（参数不足是瓶颈）；当 $D$ 很小时，第二项主导（数据不足是瓶颈）。只有当 $N$ 和 $D$ 都足够大时，两项才会同时变小。

*Source: Kaplan et al. (2020) "Scaling Laws for Neural Language Models", Equation 1.5*
:::

### Kaplan的最优分配：优先扩大模型

有了上述幂律关系，一个自然的问题是：**给定固定计算预算 $C$，如何分配给 $N$（模型大小）和 $D$（数据量）以最小化损失？**

这是一个约束优化问题。利用 $C \approx 6ND$ 的关系和损失公式 $L(N, D)$，Kaplan等人通过数学推导和实证拟合得出了最优分配方案：

$$
N^* \propto C^{0.73}, \quad D^* \propto C^{0.27}
$$

这个结果有一个非常明确的含义：**当计算预算增加时，应该优先扩大模型，而非增加数据**。具体来说，如果计算量增加10倍，最优策略是将模型增大约5.4倍（$10^{0.73} \approx 5.4$），而数据只增加约1.9倍（$10^{0.27} \approx 1.9$）。

在Kaplan的框架下，一个"计算最优"的训练配置看起来像这样：

| 计算量 (FLOPs) | 最优参数量 $N^*$ | 最优数据量 $D^*$ | $D^*/N^*$ 比例 |
|---------------|-----------------|-----------------|---------------|
| $10^{18}$ | ~10M | ~3B tokens | ~300 |
| $10^{20}$ | ~200M | ~10B tokens | ~50 |
| $10^{22}$ | ~4B | ~30B tokens | ~8 |
| $10^{24}$ | ~80B | ~100B tokens | ~1.3 |

注意 $D^*/N^*$ 的比例是**递减的**——随着规模增大，每个参数对应的训练token越来越少。按照Kaplan的建议，一个800亿参数的模型只需要约1000亿token的数据。这就是后来被Chinchilla推翻的结论。

### Chinchilla（2022）：数据比我们想的更重要

两年后，DeepMind的Jordan Hoffmann等人发表了一篇名为"Training Compute-Optimal Large Language Models"的论文——通常被称为"Chinchilla论文"——它彻底颠覆了Kaplan的资源分配建议。

Chinchilla团队采用了三种独立的方法来重新估算最优分配，而这三种方法得出了高度一致的结论。

![Chinchilla的三种估算方法。左图：IsoFLOP曲线——每条曲线对应一个固定计算预算，横轴是参数量，纵轴是损失。每条曲线都有一个最低点（最优模型大小）。中图和右图：从IsoFLOP谷值拟合出的最优参数量和token数，均与计算量呈幂律关系。](figures/chapter-17/original/fig-chinchilla-isoflop-curves.png){#fig-chinchilla-isoflop width=100%}

::: {.figure-caption}
*Source: Hoffmann et al. (2022) "Training Compute-Optimal Large Language Models", Figure 3. [arXiv:2203.15556](https://arxiv.org/abs/2203.15556)*
:::

**方法1：固定计算量，变换模型大小**

对于多个固定的计算预算（"IsoFLOP"曲线），训练不同大小的模型，找到每个预算下损失最低的模型大小（见 @fig-chinchilla-isoflop 左图）。这给出了一条 $N^*(C)$ 的曲线。

**方法2：固定模型大小，变换数据量**

对于多个固定的模型大小，改变训练数据量，拟合各自的损失曲线。通过参数化的损失函数拟合出最优的 $(N, D)$ 关系。

**方法3：直接拟合联合损失函数**

拟合一个形如下式的参数化损失函数：

$$
\hat{L}(N, D) = \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}} + E
$$

其中 $E$ 是不可约的"理想损失"（即使有无限数据和无限大模型，也无法消除的损失），$A/N^{\alpha}$ 是模型不够大导致的额外损失，$B/D^{\beta}$ 是数据不够多导致的额外损失。Chinchilla的拟合结果为：

$$
\alpha \approx 0.34, \quad \beta \approx 0.28, \quad A \approx 406.4, \quad B \approx 410.7, \quad E \approx 1.69
$$

**三种方法的一致结论：$N$ 和 $D$ 应等比例扩展**

$$
N^* \propto C^{0.50}, \quad D^* \propto C^{0.50}
$$

这与Kaplan的 $N^* \propto C^{0.73}$ 形成了鲜明对比。在Chinchilla的框架下，**计算量每增加10倍，模型和数据应该各增大约3.2倍**（$10^{0.50} \approx 3.16$）。

更实用的经验法则是：**最优训练的token数大约是参数量的20倍**。

$$
D^* \approx 20 \times N^*
$$

### Chinchilla的验证：以少胜多

Chinchilla论文最有说服力的证据是一个精心设计的实验。DeepMind此前训练了Gopher——一个280B参数的模型，在300B token上训练。按照Chinchilla的理论，这个配置是**严重失衡的**：280B参数理论上应该配约5600B token的数据，但实际只用了300B。换句话说，Gopher是一个"过大而欠训练"的模型。

于是Chinchilla团队在**相同的计算预算**下，训练了一个更小但数据更多的模型——70B参数，在1.4T token上训练。结果令人震惊：

| 模型 | 参数量 | 训练token | 计算量 | 平均性能 |
|------|--------|-----------|--------|---------|
| Gopher | 280B | 300B | $\sim 5.76 \times 10^{23}$ | 基准 |
| **Chinchilla** | **70B** | **1.4T** | $\sim 5.76 \times 10^{23}$ | **胜出** |

Chinchilla在大多数评测上超越了4倍于它的Gopher，同时因为模型更小，推理成本也大幅降低。这个结果不仅证明了Chinchilla scaling law的正确性，也揭示了一个令整个行业尴尬的事实：**此前的大模型——包括GPT-3（175B参数，300B token）、Gopher（280B参数，300B token）、PaLM（540B参数，780B token）——几乎全都训练不足**。

### Kaplan vs Chinchilla：为什么结论不同？

![Chinchilla三种方法的最优参数量预测（红/蓝/绿线）与Kaplan预测（虚线）的对比。横轴是计算预算（FLOPs），纵轴是最优参数量。星形标记为现有模型：Chinchilla（70B）恰好落在最优线上，而Gopher（280B）、GPT-3（175B）等明显偏高——它们的参数量远超最优值，即"过大而欠训练"。](figures/chapter-17/original/fig-chinchilla-compute-optimal-frontier.png){#fig-chinchilla-frontier width=85%}

::: {.figure-caption}
*Source: Hoffmann et al. (2022) "Training Compute-Optimal Large Language Models", Figure 1. [arXiv:2203.15556](https://arxiv.org/abs/2203.15556)*
:::

Kaplan和Chinchilla都是严谨的工作，为什么得出了如此不同的结论？这个分歧本身就值得深入理解。@fig-chinchilla-frontier 清楚地展示了两者的分歧：Kaplan的预测线（虚线）在高计算量下偏向更大的模型，而Chinchilla的三种方法（红/蓝/绿线）一致地建议更小但训练更充分的模型。

根本原因在于**学习率调度**。Kaplan的实验使用了固定的学习率调度方案——所有模型都用相同的cosine decay周期，而不是根据训练长度调整。这意味着训练更多数据的模型在后期可能使用了"过期的"学习率，导致后续的训练效率被低估。当Chinchilla团队修正了这个实验设计——让学习率调度与训练长度匹配——数据的价值被充分释放，$D$ 的最优指数从 $C^{0.27}$ 跳到了 $C^{0.50}$。

另一个差异在于模型架构和训练细节。Kaplan的实验主要使用较小的模型（最大15亿参数），而Chinchilla的实验覆盖了从70M到160亿参数的范围，且更接近当时最佳实践的训练配置。规模的差异可能导致两者拟合的指数有所不同。

::: {.callout-important}
## Kaplan vs Chinchilla：不同的配方，不同的配比

| 维度 | Kaplan (2020) | Chinchilla (2022) |
|------|--------------|------------------|
| **最优 $N$ 分配** | $N^* \propto C^{0.73}$ | $N^* \propto C^{0.50}$ |
| **最优 $D$ 分配** | $D^* \propto C^{0.27}$ | $D^* \propto C^{0.50}$ |
| **核心建议** | 优先扩大模型 | 模型与数据等比扩展 |
| **Token/参数比** | ~1.7（大模型） | ~20 |
| **实验范围** | ≤1.5B参数 | ≤16B参数（推断至更大） |
| **学习率** | 固定调度 | 匹配训练长度 |
| **行业影响** | GPT-3、Gopher采纳 | LLaMA、LLaMA-2采纳 |

Chinchilla的修正之所以影响深远，是因为它直接改变了行业的训练策略：从"堆参数"转向"数据与参数均衡"。Meta的LLaMA（2023）就是Chinchilla理论的直接践行者——7B参数在1T token上训练，远超Kaplan建议的数据量。
:::

### 完整数值示例：用Scaling Laws规划训练

让我们通过一个具体的例子来体会Scaling Laws的实用价值。假设你是一家初创公司的AI负责人，拥有一笔计算预算 $C = 6 \times 10^{21}$ FLOPs（大约相当于256块A100 GPU训练两周）。

**Step 1: 使用Chinchilla法则确定最优配置**

根据 $D^* \approx 20 \times N^*$ 和 $C \approx 6ND$：

$$
C = 6N \cdot (20N) = 120N^2
$$

$$
N^* = \sqrt{\frac{C}{120}} = \sqrt{\frac{6 \times 10^{21}}{120}} \approx 7.1 \times 10^{9} \approx 7B
$$

$$
D^* = 20 \times N^* \approx 1.4 \times 10^{11} = 140B \text{ tokens}
$$

**Step 2: 验证计算量**

$$
C = 6 \times 7 \times 10^{9} \times 1.4 \times 10^{11} = 5.88 \times 10^{21} \approx 6 \times 10^{21} \quad \checkmark
$$

**Step 3: 使用Chinchilla损失公式预测性能**

$$
\hat{L}(N, D) = \frac{406.4}{N^{0.34}} + \frac{410.7}{D^{0.28}} + 1.69
$$

$$
= \frac{406.4}{(7 \times 10^{9})^{0.34}} + \frac{410.7}{(1.4 \times 10^{11})^{0.28}} + 1.69
$$

$$
\approx \frac{406.4}{3670} + \frac{410.7}{2400} + 1.69 \approx 0.11 + 0.17 + 1.69 = 1.97
$$

**Step 4: 对比非最优配置**

如果按照Kaplan的建议训练一个更大但数据更少的模型会怎样？假设我们选择 $N = 30B$，那么 $D = C/(6N) \approx 33B$ tokens：

$$
\hat{L}(30B, 33B) = \frac{406.4}{(3 \times 10^{10})^{0.34}} + \frac{410.7}{(3.3 \times 10^{10})^{0.28}} + 1.69
$$

$$
\approx \frac{406.4}{5870} + \frac{410.7}{1480} + 1.69 \approx 0.07 + 0.28 + 1.69 = 2.04
$$

虽然30B模型的参数项损失更低（0.07 vs 0.11），但数据项损失大幅增加（0.28 vs 0.17），总损失反而更高。Chinchilla的7B+140B配置比Kaplan建议的30B+33B配置损失低了约3.5%——看似微小，但在大规模评测中这往往意味着许多任务上的显著差异。

---

## 工程实践

### 小规模实验预测大规模性能

Scaling Laws最大的实用价值在于：你可以用**小模型的实验结果来预测大模型的表现**。具体做法如下：

```{python}
#| label: scaling-law-fit
#| fig-cap: "幂律拟合示例：在小规模上拟合，外推到大规模"
#| code-fold: true

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

# =============================
# 模拟数据：不同参数量对应的损失
# =============================
# 小规模实验数据（实际训练得到）
N_small = np.array([1e7, 3e7, 1e8, 3e8, 1e9])  # 10M to 1B
L_small = np.array([3.50, 3.20, 2.90, 2.70, 2.52])

# 大规模真实数据（验证用，假设我们不知道）
N_large = np.array([3e9, 1e10, 3e10, 1e11])
L_large = np.array([2.35, 2.12, 1.95, 1.78])

# =============================
# 幂律拟合: L(N) = a * N^(-b) + c
# =============================
def power_law(N, a, b, c):
    return a * N**(-b) + c

# 仅用小规模数据拟合
popt, pcov = curve_fit(power_law, N_small, L_small, p0=[10, 0.07, 1.5], maxfev=10000)
a_fit, b_fit, c_fit = popt

# 外推到大规模
N_predict = np.logspace(7, 11.5, 100)
L_predict = power_law(N_predict, *popt)

# =============================
# 可视化
# =============================
fig, ax = plt.subplots(1, 1, figsize=(8, 5))

# 小规模数据（用于拟合）
ax.scatter(N_small, L_small, color='#6C9BC2', s=80, zorder=5,
           label='Small-scale experiments (used for fitting)', edgecolors='white', linewidth=1.5)

# 大规模数据（验证）
ax.scatter(N_large, L_large, color='#E8A87C', s=80, zorder=5, marker='D',
           label='Large-scale experiments (held out)', edgecolors='white', linewidth=1.5)

# 幂律拟合曲线
ax.plot(N_predict, L_predict, color='#4A4A4A', linewidth=2, linestyle='--',
        label=f'Power law fit: L = {a_fit:.1f} · N^(−{b_fit:.3f}) + {c_fit:.2f}')

# 外推区域标注
ax.axvspan(3e9, 2e11, alpha=0.08, color='orange', label='Extrapolation region')

ax.set_xscale('log')
ax.set_xlabel('Non-embedding parameters (N)', fontsize=12)
ax.set_ylabel('Cross-entropy loss (L)', fontsize=12)
ax.set_title('Scaling Law: Predicting Large-Scale Performance from Small Experiments', fontsize=13)
ax.legend(fontsize=9, loc='upper right')
ax.grid(True, alpha=0.3)
ax.set_xlim(5e6, 3e11)
ax.set_ylim(1.5, 3.8)

plt.tight_layout()
plt.savefig('figures/chapter-17/fig-scaling-law-fit-demo.png', dpi=200, bbox_inches='tight',
            facecolor='white')
plt.savefig('figures/chapter-17/fig-scaling-law-fit-demo.svg', bbox_inches='tight',
            facecolor='white')
plt.show()

print(f"\nFitted parameters: a={a_fit:.2f}, b={b_fit:.4f}, c={c_fit:.2f}")
print(f"Predicted loss at 10B params: {power_law(1e10, *popt):.2f}")
print(f"Predicted loss at 100B params: {power_law(1e11, *popt):.2f}")
```

这个例子展示了Scaling Laws在实践中的核心价值：用5个小规模实验（10M到1B参数）拟合幂律曲线，然后**外推**到100B参数级别。外推的预测与实际结果高度吻合——这就是为什么Scaling Laws被称为大模型训练的"指南针"。

### 使用Scaling Laws规划训练：实战清单

在实际工程中，使用Scaling Laws来规划大模型训练的典型流程如下：

**Phase 1：小规模探索**（占总预算的1-3%）

训练5-8个小模型（10M到1B参数），每个都训练到Chinchilla最优数据量。记录最终损失和训练曲线。所有模型使用**相同的数据分布**、**相同的tokenizer**和**相同的训练配方**（学习率、优化器等），唯一变量是规模。

**Phase 2：拟合幂律**

在双对数坐标系上拟合 $L = aN^{-b} + c$ 和 $L = aD^{-b} + c$。检查拟合质量（$R^2 > 0.99$），确认外推可靠。

**Phase 3：预测与决策**

将拟合曲线外推到目标规模，预测最终损失。用 Chinchilla 法则计算给定预算下的最优 $(N^*, D^*)$。评估预测损失是否满足业务需求。

**Phase 4：执行训练**

按预测的最优配置训练大模型。在训练过程中监控损失曲线，验证是否符合幂律预测。

```{python}
#| label: compute-optimal-calculator
#| fig-cap: "计算最优配置计算器"
#| code-fold: true

def chinchilla_optimal(C_flops):
    """根据Chinchilla法则计算最优训练配置

    Args:
        C_flops: 总计算预算（FLOPs）

    Returns:
        dict: 最优参数量、token数、预期损失
    """
    # Chinchilla: N* ≈ sqrt(C / 120)
    N_optimal = (C_flops / 120) ** 0.5
    D_optimal = 20 * N_optimal  # ~20 tokens per parameter

    # 验证: C ≈ 6ND
    C_check = 6 * N_optimal * D_optimal

    # 预测损失 (Chinchilla parametric fit)
    L_pred = 406.4 / (N_optimal ** 0.34) + 410.7 / (D_optimal ** 0.28) + 1.69

    return {
        'N_optimal': N_optimal,
        'D_optimal': D_optimal,
        'C_check': C_check,
        'L_predicted': L_pred,
        'tokens_per_param': D_optimal / N_optimal
    }


# =============================
# 不同计算预算下的最优配置
# =============================
print("=" * 75)
print(f"{'Compute (FLOPs)':>18} | {'Optimal N':>12} | {'Optimal D (tokens)':>18} | {'D/N':>6} | {'Loss':>6}")
print("=" * 75)

budgets = [1e18, 1e19, 1e20, 1e21, 1e22, 1e23, 1e24]
budget_labels = ['10^18', '10^19', '10^20', '10^21', '10^22', '10^23', '10^24']

for C, label in zip(budgets, budget_labels):
    result = chinchilla_optimal(C)
    N = result['N_optimal']
    D = result['D_optimal']
    L = result['L_predicted']
    ratio = D / N

    # 格式化显示
    if N >= 1e9:
        N_str = f"{N/1e9:.1f}B"
    elif N >= 1e6:
        N_str = f"{N/1e6:.0f}M"
    else:
        N_str = f"{N/1e3:.0f}K"

    if D >= 1e12:
        D_str = f"{D/1e12:.1f}T"
    elif D >= 1e9:
        D_str = f"{D/1e9:.0f}B"
    else:
        D_str = f"{D/1e6:.0f}M"

    print(f"{label:>18} | {N_str:>12} | {D_str:>18} | {ratio:>5.0f}x | {L:>5.2f}")

print("=" * 75)
print("\nNote: D/N ≈ 20 for all scales (Chinchilla optimal)")
```

---

## 深入理解

> **研究者必读**：这一节探讨Scaling Laws的理论基础、涌现能力的争议、以及这一研究方向的边界条件和开放问题

### 为什么是幂律？——理论视角

Scaling Laws中最令人困惑的问题之一是：**为什么损失与规模之间恰好是幂律关系，而不是指数关系或对数关系？**

一个被广泛讨论的理论解释来自统计力学。Bahri等人（2021）提出，神经网络的损失可以分解为两部分：一个与数据分布的"内在复杂度"有关的不可约部分（即使有无限资源也无法消除），以及一个与有限资源（参数、数据、计算）有关的可约部分。可约部分的幂律行为可以从数据分布的谱特性（spectral properties）推导出来。

直觉上，可以这样理解：自然语言的信息具有层次结构——最常见的模式（如语法规则）只需要少量参数就能学会，稍微罕见的模式（如语义关系）需要更多参数，极其罕见的模式（如特定领域的知识）需要大量参数。如果这些"信息层次"的数量按照幂律分布（类似于齐普夫定律），那么每增加一定比例的参数，模型就能捕获按幂律递减的新信息——这正是损失遵循幂律的原因。

另一个视角来自随机矩阵理论。Hutter（2021）认为，幂律指数反映了数据的"有效维度"——数据空间中有意义方向的数量。更高的有效维度意味着学习更困难（需要更多资源），对应更大的幂律指数。

值得注意的是，这些理论解释目前仍然是**不完整的**。它们能解释为什么幂律存在，但无法从第一原理准确预测指数的具体数值（如为什么 $\alpha_N \approx 0.076$ 而不是0.1）。幂律指数的精确值仍然需要通过实验拟合来确定。

### 涌现能力：量变何时引发质变？

Scaling Laws描绘的图景是**平滑、渐进的**——损失随规模单调下降，没有突然的跳跃或相变。但2022年，Wei等人报告了一个看似与此矛盾的现象：**涌现能力**（emergent abilities）。

涌现能力的定义是：**小模型上完全不存在、但在大模型上突然出现的能力**。典型的例子包括：

- **思维链推理**（Chain-of-Thought）：小于约100B参数的模型基本无法进行多步推理，但超过这个阈值后，推理能力突然涌现。
- **三步算术**：小模型在三位数加法上的准确率接近随机猜测，但在某个规模之后准确率突然从接近0%跳到接近100%。

这些现象似乎暗示了一种"相变"——就像水在100°C时突然沸腾一样，模型能力在某个临界规模上突然出现。

然而，2023年Schaeffer等人在一篇题为"Are Emergent Abilities of Large Language Models a Mirage?"的论文中提出了一个深刻的质疑：**涌现可能只是度量方式的假象**。

他们的核心论点是：所谓的"涌现"来自于使用**非线性或阈值化的评测指标**。以三位数加法为例：如果评测标准是"完全正确"（exact match），那么一个模型可能在每一位数字上的准确率随着规模平滑提升（从60%到70%到80%到90%），但三位全对的概率是各位准确率的乘积（$0.6^3 = 0.22$ → $0.9^3 = 0.73$），呈现出"突然跳跃"的假象。如果改用更平滑的指标（如Brier score或每位数字的准确率），涌现现象就消失了——性能改善是渐进的，与Scaling Laws一致。

这场争论至今没有定论。支持涌现的一方指出，某些能力（如In-Context Learning的突然出现）很难完全用指标解释。反对涌现的一方则认为，在正确的度量下，性能改善总是渐进的。

::: {.callout-warning}
## 涌现能力争论的启示

这场争论给研究者的核心教训是：**评测指标的选择可以显著影响你对模型能力的判断**。在设计评测时，要警惕使用阈值化的指标（如exact match），因为它们可能制造"虚假的相变"。更好的做法是同时报告连续指标和阈值指标，以区分"能力的渐进提升"和"阈值效应"。
:::

### 方法的边界条件

Scaling Laws并非在所有条件下都成立。以下是已知的边界条件和失效模式：

**数据质量 vs 数据数量**。Scaling Laws假设数据质量是恒定的，只考虑数据量。但实际上，当你将训练数据从1T token扩展到10T token时，新增数据的平均质量很可能下降——因为互联网上的高质量文本是有限的。在这种情况下，更多数据可能带来边际递减甚至负面效果。

**数据重复**。当训练数据被多次重复使用（epoch > 1）时，幂律行为会改变。Muennighoff等人（2023）发现，在4个epoch之后，数据重复带来的边际收益显著下降——每多训练一遍数据，等效的"新数据量"约为实际数据量的某个折减系数。这意味着 $D^* \approx 20N$ 的经验法则需要用**去重后的唯一token数**来计算。

**任务特定性**。Scaling Laws主要在语言建模损失（交叉熵）上建立和验证。但下游任务的性能不一定严格遵循同一幂律。某些任务（如简单的情感分类）可能很快"饱和"，而另一些任务（如复杂的逻辑推理）可能需要远超语言建模所需的规模才能有所改善。

**架构依赖性**。尽管幂律的形式在不同架构之间保持一致（都是 $L \propto N^{-\alpha}$），但具体的指数 $\alpha$ 和常数系数可能因架构而异。例如，Transformer的scaling exponent和LSTM的可能不同。这限制了跨架构外推的可靠性。

**Chinchilla参数的复现争议**。值得注意的是，Epoch AI（Besiroglu et al., 2024）在尝试复现Chinchilla的Approach 3时发现了一个微妙的问题：论文报告的参数化拟合系数（$A=406.4, B=410.7, \alpha=0.34, \beta=0.28, E=1.69$）代入最优化公式后，实际预测的最优比例约为70 tokens/parameter——而非论文声称的约20。经过重新拟合（$A=482.01, B=2085.43, \alpha=0.35, \beta=0.37, E=1.82$），修正后的参数确实给出了约20 tokens/parameter的比例，与Approach 1和Approach 2一致。这个插曲提醒我们：**即使是顶级机构的论文，其报告的具体数值也值得独立验证**。Chinchilla的核心结论（数据与参数等比扩展）是稳健的，但具体的参数化系数需要谨慎使用。

### 开放研究问题

如果你正在寻找研究方向，以下是这个领域中最重要的开放问题：

**Scaling Laws是否有上限？** 目前的幂律关系是在 $10^{18}$ 到 $10^{24}$ FLOPs范围内拟合的。但在 $10^{26}$ 或更高的计算量下，幂律是否仍然成立？是否存在某个"天花板"，超过之后继续扩大规模不再有收益？这个问题对于预测AGI的可行性有直接的影响。

**如何将Scaling Laws推广到下游任务？** 目前的Scaling Laws主要预测语言建模损失。但我们真正关心的是下游任务的表现——摘要质量、推理准确率、代码正确性。如何从语言建模损失推导出下游任务的性能？这需要建立"损失"和"能力"之间的映射。

**数据墙问题**。Chinchilla法则说数据应该与参数等比扩展。但互联网上的高质量文本估计只有几万亿token。当我们训练一个万亿参数的模型时，按 $D^* \approx 20N$ 需要20万亿token——这可能已经超过了可获取的高质量数据总量。如何突破这个"数据墙"？合成数据是否可以替代真实数据？数据重复时Scaling Laws如何修正？

**多模态Scaling Laws**。当训练跨模态模型（文本+图像+代码+音频）时，不同模态之间的数据量应该如何分配？是否存在跨模态的幂律关系？

---

## 局限性与未解决的问题

### Scaling Laws的局限

Scaling Laws提供了一个强大的框架，但它也有明确的局限性。

第一个局限是**只预测训练损失，不预测涌现行为**。幂律曲线是平滑的、渐进的，它告诉你"损失会持续下降"，但不告诉你"在什么规模上模型会突然学会多步推理"或"在什么规模上In-Context Learning会从不可用变成实用"。对于工程师来说，最关键的问题往往不是"损失是多少"，而是"模型能不能完成我需要的任务"——而Scaling Laws无法直接回答后者。

第二个局限是**假设训练配方不变**。Scaling Laws的预测基于"其他条件不变"——相同的架构、优化器、学习率调度、数据分布。但在实际中，当你从10B扩展到100B参数时，你几乎肯定会调整训练配方（更大的batch size、不同的学习率、可能的架构微调）。这些变化会使小规模拟合的幂律失准。

第三个局限是**对数据质量的盲区**。Scaling Laws将所有token视为等价的，只看"数量"不看"质量"。但实际上，精心筛选的10B高质量token可能比随机的100B token更有价值。近年来，Phi系列（Microsoft）、Qwen（Alibaba）等模型通过精选数据在远低于Chinchilla建议的规模上达到了令人印象深刻的性能，这对Scaling Laws的普适性提出了挑战。

### 这些局限导向了什么？

本章确立了一个核心认知：语言模型的性能是可预测的，计算预算的最优分配有科学依据。但这也立即引出了一个工程问题——**当你决定训练一个700亿参数的模型在1.4万亿token上跑时，你如何确保训练过程不会崩溃？**

百亿级别的训练面临严峻的工程挑战：数值精度的选择（FP32太慢、FP16会溢出）、优化器的稳定性（Adam的某些超参数在大规模下行为不同）、learning rate warmup的必要性、loss spike的诊断与处理……这些问题不在Scaling Laws的数学框架内，却是将理论转化为实践的关键环节。

> 下一章预告：第18章将聚焦**训练稳定性与数值工程**。优化器从SGD到Adam的演进为什么重要？为什么BF16成为大模型训练的标配？Loss spike是怎么回事、怎么处理？Warmup到底在做什么？——这些"让百亿参数的训练不崩溃"的工程细节，将是把Scaling Laws的理论预测变为现实的必修课。

---

## 本章小结

### 核心要点回顾

本章追溯了语言模型规模研究的三个里程碑，构建了从经验直觉到可预测科学的演进轨迹。

GPT-2（2019）用四个不同规模的模型首次提供了"越大越好"的系统性证据，揭示了模型规模与零样本能力之间的单调关系——但没有给出数学框架。Kaplan等人（2020）发现了损失与参数量、数据量、计算量之间的幂律关系，将大模型训练变成了可预测的科学。他们的资源分配建议是"优先扩大模型"（$N^* \propto C^{0.73}$），这深刻影响了GPT-3和Gopher的训练策略。Chinchilla（2022）修正了Kaplan的结论，证明数据与参数应等比例扩展（$N^* \propto C^{0.50}$），其经验法则"每个参数约20个token"成为后续大模型训练的标准配方——LLaMA、Falcon等模型都直接采纳了这一建议。

这些发现的核心意义在于：它们将大模型训练从**炼金术**变成了**工程科学**。你不再需要"训一个大的看看效果"，而是可以通过小规模实验和数学公式来规划训练。

### 关键公式速查

| 公式 | 含义 |
|------|------|
| $L(N) = (N_c/N)^{\alpha_N}, \; \alpha_N \approx 0.076$ | 损失与参数量的幂律（Kaplan） |
| $L(D) = (D_c/D)^{\alpha_D}, \; \alpha_D \approx 0.095$ | 损失与数据量的幂律（Kaplan） |
| $L(C) = (C_c/C)^{\alpha_C}, \; \alpha_C \approx 0.050$ | 损失与计算量的幂律（Kaplan） |
| $\hat{L}(N,D) = A/N^{\alpha} + B/D^{\beta} + E$ | Chinchilla参数化损失函数 |
| $C \approx 6ND$ | 计算量与参数、数据的关系 |
| $N^* \propto C^{0.73}$ (Kaplan) | Kaplan最优参数分配 |
| $N^* \propto C^{0.50}$ (Chinchilla) | Chinchilla最优参数分配（修正后） |
| $D^* \approx 20 \times N^*$ | Chinchilla经验法则 |

### 思考题

1. **[概念理解]** 为什么Scaling Laws是幂律（power law）而不是指数律（exponential law）？如果是指数律（$L = ae^{-bN}$），对大模型训练的实际意义会有什么不同？提示：考虑两种函数在大 $N$ 时的行为差异，以及对"继续扩大模型是否值得"的判断。

2. **[数学推导]** 利用 $C = 6ND$ 和 Chinchilla 的损失函数 $\hat{L}(N,D) = A/N^{\alpha} + B/D^{\beta} + E$，推导最优的 $N^*$ 和 $D^*$。提示：将 $D = C/(6N)$ 代入损失函数，对 $N$ 求导并令其为零。验证你的结果是否与 $N^* \propto C^{0.50}$ 一致。

3. **[工程实践]** 你有一个计算预算 $C = 10^{22}$ FLOPs。分别用 Kaplan 和 Chinchilla 的法则计算最优训练配置 $(N^*, D^*)$。在实际中你会选择哪个？为什么？如果你的训练数据只有100B token（无法获取更多），最优策略会如何变化？

4. **[研究思考]** Chinchilla法则的 $D^* \approx 20N$ 意味着训练一个万亿参数模型需要约20万亿token的数据。但估计高质量互联网文本总共只有几万亿token。这个"数据墙"问题有哪些可能的解决方案？考虑：合成数据、多模态数据、数据重复的Scaling Laws修正、以及"质量胜于数量"的可能性。

5. **[开放思考]** Scaling Laws的幂律指数（如 $\alpha_N \approx 0.076$）是语言的固有属性，还是Transformer架构的特性？如果我们发明了一种全新的架构，Scaling Laws会改变吗？如果指数变大（比如0.2），对大模型训练的意义是什么？

---

## 延伸阅读

### 核心论文（必读）

**Kaplan, J. et al. (2020). "Scaling Laws for Neural Language Models"**。本章最核心的技术参考。重点阅读：Section 2-4（三条幂律的实证发现和公式推导）、Figure 1-4（幂律拟合图——这些图本身就是Scaling Laws最好的"证明"）、Section 6（最优计算资源分配）。可快速浏览：Section 5（其他实验设置的影响）。[arXiv:2001.08361](https://arxiv.org/abs/2001.08361)

**Hoffmann, J. et al. (2022). "Training Compute-Optimal Large Language Models"**。Chinchilla论文，修正了Kaplan的资源分配结论。重点阅读：Section 3-4（三种独立估算方法及其一致结论）、Figure 1（IsoFLOP曲线——看到最优模型大小如何随计算量变化）、Table 3（Chinchilla vs Gopher vs GPT-3的对比）。可快速浏览：附录中的详细实验配置。[arXiv:2203.15556](https://arxiv.org/abs/2203.15556)

**Radford, A. et al. (2019). "Language Models are Unsupervised Multitask Learners"**。GPT-2论文，规模探索的起点。重点阅读：Table 2-3（不同规模模型的零样本表现）、Section 3（实验设置和结果分析）。注意这篇论文没有发表在学术会议上，只作为OpenAI技术报告发布。[openai.com/blog/better-language-models](https://openai.com/blog/better-language-models/)

### 理论基础

**Bahri, Y. et al. (2021). "Explaining Neural Scaling Laws"**。从统计力学角度解释为什么损失遵循幂律。重点阅读：Section 2（数据流形的维度与幂律指数的关系）。这是目前最被引用的理论解释之一。[arXiv:2102.06701](https://arxiv.org/abs/2102.06701)

**Hutter, M. (2021). "Learning Curve Theory"**。从学习理论角度分析Scaling Laws。[arXiv:2102.04074](https://arxiv.org/abs/2102.04074)

### 后续发展

**Muennighoff, N. et al. (2023). "Scaling Data-Constrained Language Models"**。研究数据重复对Scaling Laws的影响——当你不得不重复数据时，幂律如何变化？[arXiv:2305.16264](https://arxiv.org/abs/2305.16264)

**Wei, J. et al. (2022). "Emergent Abilities of Large Language Models"**。定义了"涌现能力"的概念，展示了多个只在大规模模型上出现的能力。[arXiv:2206.07682](https://arxiv.org/abs/2206.07682)

**Schaeffer, R. et al. (2023). "Are Emergent Abilities of Large Language Models a Mirage?"**。对涌现能力的系统性质疑——指出涌现可能只是度量方式的假象。[arXiv:2304.15004](https://arxiv.org/abs/2304.15004)

### 综述与教程

**Epoch AI. "Compute Trends Across Three Eras of Machine Learning"**。追踪机器学习计算量增长趋势的数据库，提供了大量模型的compute估算。[epochai.org/trends](https://epochai.org/trends)

### 代码资源

- **scaling-laws**: GitHub上有多个开源的Scaling Laws拟合和可视化工具
- **Chinchilla optimal calculator**: 在线计算器，输入计算预算即可得到最优配置

---

## 历史注脚

Scaling Laws的研究史充满了令人玩味的细节。

GPT-2的"分阶段发布"策略在当时引发了巨大的争议。2019年2月，OpenAI宣布GPT-2"太危险了，不能完全公开"，只发布了最小的117M版本。这个决定既被赞为"负责任的AI"实践，也被批评为"marketing噱头"——毕竟，当时Google已经公开了参数量相当的BERT。最终，OpenAI在接下来的9个月中逐步发布了更大的版本，到11月才完全公开1.5B版本。讽刺的是，GPT-2作为语言模型，其能力在今天看来相当平庸——它真正的历史价值不在于模型本身，而在于它开启了"规模探索"的范式。

Kaplan的Scaling Laws论文（2020年1月）发表时间恰好在GPT-3论文（2020年5月）之前——这不是巧合。Kaplan本人就是OpenAI的研究员，Scaling Laws的研究直接指导了GPT-3的训练决策。175B参数的选择正是基于Kaplan的资源分配建议：在当时可用的计算预算下，Kaplan法则建议训练一个非常大的模型、在相对少的数据上跑。这就是为什么GPT-3有175B参数但"只"训练了300B token——在Kaplan框架下这是"最优的"。

但两年后Chinchilla的出现让这个决策显得相当尴尬。按照Chinchilla法则，175B参数应该配约3.5T token，而GPT-3实际只用了300B——训练数据量不到"最优值"的十分之一。这意味着GPT-3是一个**严重欠训练的模型**。换句话说，如果OpenAI在2020年就知道Chinchilla法则，他们可能会训练一个更小但数据更充分的模型——或许40-50B参数在3T token上训练——获得相近甚至更好的性能，同时大幅节省推理成本。

Meta在2023年发布的LLaMA模型是Chinchilla理论最忠实的践行者。LLaMA-7B在1T token上训练（每参数约143个token，远超Chinchilla建议的20个），LLaMA-65B在1.4T token上训练（每参数约22个token，接近Chinchilla最优）。这种"过度训练"策略的逻辑是：虽然从训练效率看超过20个token/参数不是最优的，但更小的模型意味着更低的推理成本——对于需要大规模部署的场景，推理成本远比训练成本更重要。这揭示了Scaling Laws的另一个局限：**它优化的是训练效率，而非部署效率**。在实际中，你往往需要一个"训练时过度投入、但推理时足够小"的模型——这超出了经典Scaling Laws的考虑范围。
