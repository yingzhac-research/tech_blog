---
title: "第31章：推理优化"
subtitle: "From Training to Deployment: The Art of Making Giants Fast and Cheap"
author: "Ying Zha"
date: "2026-01-29"
categories: [NLP, Deep Learning, LLM, Inference, Quantization, Speculative Decoding]
tags: [推理优化, 量化, GPTQ, AWQ, 投机解码, vLLM, PagedAttention, 持续批处理, KV Cache]
description: "当大语言模型从训练走向部署，推理成本成为新的瓶颈——一个70B模型的FP16推理需要140GB显存，远超单卡容量；自回归生成的token-by-token方式让GPU利用率仅有10-20%。本章系统梳理推理优化的三条主线：量化技术（GPTQ、AWQ）用精度换内存，将70B模型压缩到单卡运行；投机解码用小模型猜测、大模型验证的方式打破自回归瓶颈，实现2-3倍无损加速；持续批处理和PagedAttention通过动态调度最大化硬件利用率。这些技术可以叠加使用，将推理成本降低10倍以上，让'只有大厂用得起'变成'人人可部署'。"
image: "figures/chapter-31/original/fig-awq-overview.png"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    fig-cap-location: bottom
bibliography: references.bib
---

> **核心问题**：当大语言模型从训练走向部署，如何在成本、延迟、质量三者之间取得最优平衡？
>
> **历史坐标**：2022–2024 | LLM.int8() (Dettmers, 2022), GPTQ (Frantar, 2022), SmoothQuant (Xiao, 2022), Speculative Decoding (Leviathan & Chen, 2023), AWQ (Lin, 2023), vLLM (Kwon, 2023) | 从算法到系统的推理优化全栈

::: {.callout-tip collapse="true"}
## 本章参考来源

### 论文
- **Dettmers et al. (2022)** "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale" ([arXiv:2208.07339](https://arxiv.org/abs/2208.07339)) — 参考了 outlier 特征的发现和混合精度分解方法
- **Frantar et al. (2022)** "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers" ([arXiv:2210.17323](https://arxiv.org/abs/2210.17323)) — 参考了基于 Hessian 的逐层量化方法（ICLR 2023）
- **Xiao et al. (2022)** "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models" ([arXiv:2211.10438](https://arxiv.org/abs/2211.10438)) — 参考了激活平滑技术
- **Lin et al. (2023)** "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration" ([arXiv:2306.00978](https://arxiv.org/abs/2306.00978)) — 参考了激活感知量化方法（MLSys 2024 最佳论文）；从论文提取了1张架构图
- **Leviathan et al. (2023)** "Fast Inference from Transformers via Speculative Decoding" ([arXiv:2211.17192](https://arxiv.org/abs/2211.17192)) — 参考了投机解码的基本框架和数学证明
- **Chen et al. (2023)** "Accelerating Large Language Model Decoding with Speculative Sampling" ([arXiv:2302.01318](https://arxiv.org/abs/2302.01318)) — 参考了 Google 的投机采样实现
- **Kwon et al. (2023)** "Efficient Memory Management for Large Language Model Serving with PagedAttention" ([arXiv:2309.06180](https://arxiv.org/abs/2309.06180)) — 参考了 PagedAttention 和 vLLM 系统设计（SOSP 2023）
- **Yu et al. (2022)** "Orca: A Distributed Serving System for Transformer-Based Generative Models" (OSDI 2022) — 参考了迭代级调度和持续批处理
- **Ainslie et al. (2023)** "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints" ([arXiv:2305.13245](https://arxiv.org/abs/2305.13245)) — 参考了 Grouped-Query Attention 的设计

### 课程
- **Stanford CS229S** Systems for Machine Learning (2023) — 参考了推理系统优化的框架
- **UC Berkeley CS294** Machine Learning Systems (2024) — 参考了量化技术的教学组织

### 代码资源
- [vllm-project/vllm](https://github.com/vllm-project/vllm) — vLLM 高性能推理引擎
- [huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference) — TGI 推理服务
- [IST-DASLab/gptq](https://github.com/IST-DASLab/gptq) — GPTQ 官方实现
- [mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq) — AWQ 官方实现
:::

---

## 从上一章说起

上一章我们见证了参数高效微调（PEFT）的革命——从 Adapter 的层间插入，到 LoRA 的低秩分解，再到 QLoRA 的 4-bit 量化微调。这些技术将大模型微调从"需要一个集群"变成"一张消费级 GPU"。70B 参数的 LLaMA 现在可以在单张 24GB 的 RTX 4090 上完成微调，成本从数万美元降到几百美元。

但微调只是模型生命周期的**一次性成本**。一旦模型上线，真正的成本来自**推理**——每一次用户请求都需要消耗计算资源。对于一个有百万日活用户的应用，推理成本可能是微调成本的数百倍。让我们做一个简单的算术：

假设你运营一个基于 LLaMA-70B 的 AI 助手：

- **微调成本**（一次性）：QLoRA 在单 GPU 上训练，约 $200（云 GPU 租用）
- **推理成本**（持续）：
  - 每次请求平均 500 token（输入 + 输出）
  - 每天 100 万次请求
  - 70B 模型 FP16 推理需要约 140GB 显存，需要 2 张 A100 80GB
  - A100 租用成本约 $2/小时 × 2 = $4/小时 = **$96/天**
  - 每月推理成本：**$2,880**

一年下来，推理成本接近 **$35,000**，是微调成本的 175 倍！

更糟糕的是，这还是在理想情况下的估算。实际部署中，由于自回归生成的顺序特性，GPU 的利用率通常只有 10–30%。大部分时间，昂贵的 A100 GPU 都在"等待"——等待上一个 token 生成完毕，才能开始下一个 token 的计算。这是一种巨大的浪费。

推理优化面临三大核心挑战：

**挑战一：显存瓶颈**

一个 70B 模型的 FP16 参数占用 140GB，这还没算 KV Cache。在长上下文场景下（比如 128K token），KV Cache 可能占用数百 GB。即使是 8 张 A100 组成的服务器，也可能被显存卡住。

**挑战二：延迟瓶颈**

自回归生成是天然顺序的——每个 token 都依赖前面所有 token。这意味着生成 100 个 token 需要进行 100 次完整的前向传播。用户看到的是"逐字蹦出"的体验，延迟感明显。

**挑战三：吞吐瓶颈**

传统的静态批处理要求所有请求同时开始、同时结束。但实际场景中，用户请求长度参差不齐——有的问"今天天气"，有的要求写一篇长文。短请求被迫等待长请求完成，系统吞吐量大打折扣。

> 💡 **本章核心洞察**：推理优化的本质是在**精度、速度、成本**三角形中寻找帕累托最优。量化技术通过降低数值精度换取内存和带宽——INT4 量化可以将 70B 模型压缩到 35GB，单卡可运行。投机解码通过"猜测-验证"的并行化打破自回归的顺序瓶颈——小模型一次猜 4 个 token，大模型一次验证全部，实现 2–3 倍无损加速。持续批处理通过迭代级调度最大化硬件利用率——新请求可以随时加入，完成的请求立即退出，不再"陪跑"。这些技术可以叠加使用：一个部署 INT4 量化 + 投机解码 + PagedAttention + 持续批处理的系统，相比 naive 实现可以将成本降低 **10 倍以上**。

---

## 问题的本质是什么？

### 推理的成本结构

要优化推理，首先要理解成本从何而来。大模型推理的资源消耗可以分解为三大部分：

**模型参数**

这是最直观的部分。70B 参数在 FP16 下占用 $70 \times 10^9 \times 2 = 140$ GB。参数需要从显存读取到计算单元（CUDA Core 或 Tensor Core），这是推理的第一个瓶颈。

**KV Cache**

这是容易被忽视但至关重要的部分。在自回归生成中，为了避免重复计算历史 token 的 Key 和 Value，模型会将它们缓存下来。KV Cache 的大小随序列长度线性增长：

$$
\text{KV Cache} = 2 \times L \times n \times d \times \text{sizeof(dtype)}
$$

其中 $L$ 是层数，$n$ 是序列长度，$d$ 是隐藏维度，因子 2 来自 K 和 V 两个矩阵。对于 LLaMA-70B（$L=80$，$d=8192$），128K 上下文的 KV Cache 占用：

$$
2 \times 80 \times 128000 \times 8192 \times 2 = 335 \text{ GB (FP16)}
$$

这比模型参数本身还大！在长上下文场景，KV Cache 是显存的主要消耗者。

**激活值**

前向传播过程中的中间结果。相比参数和 KV Cache，激活值占用较小（batch size 为 1 时通常只有几个 GB），但在大 batch 场景下也不可忽视。

### 自回归解码的瓶颈分析

为什么大模型推理这么慢？要理解这一点，我们需要深入分析自回归解码的计算特性。

在生成第 $t$ 个 token 时，模型需要：

1. 读取所有模型参数（140GB for 70B model）
2. 读取 KV Cache（随 $t$ 线性增长）
3. 计算注意力和 FFN
4. 输出一个 token 的 logits

关键观察是：**生成单个 token 只需要很少的计算（相对于参数量），但需要读取大量数据（所有参数）**。这让推理变成了一个 **内存带宽受限**（memory-bound）而非**计算受限**（compute-bound）的任务。

让我们算一下。A100 GPU 的规格是：

- 内存带宽：2 TB/s
- FP16 算力：312 TFLOPS

生成一个 token 需要读取 140GB 参数，按 2 TB/s 带宽计算：

$$
\text{读取时间} = \frac{140 \text{ GB}}{2000 \text{ GB/s}} = 70 \text{ ms}
$$

而 70B 模型的 FP16 计算量约为 $2 \times 70 \times 10^9 = 140$ GFLOPs（每个参数大约 2 次运算），按 312 TFLOPS 计算：

$$
\text{计算时间} = \frac{140 \text{ GFLOPs}}{312 \text{ TFLOPs}} = 0.45 \text{ ms}
$$

计算只需要 0.45ms，但读取参数需要 70ms——**实际利用率不到 1%**！GPU 的强大算力几乎完全浪费在等待数据传输上。

这就是为什么单纯增加 batch size 可以提升吞吐：多个请求共享同一份参数读取，摊薄了内存带宽的开销。但 batch size 受限于显存（更多请求 = 更多 KV Cache），不能无限增大。

### 我们需要什么样的解决方案？

理解了问题的本质，解决方案的方向就清晰了：

1. **减少需要读取的数据量** → **量化**：把 FP16 参数压缩成 INT8 或 INT4，读取量减半或减到四分之一
2. **打破顺序依赖** → **投机解码**：一次生成多个 token，减少前向传播次数
3. **最大化硬件利用率** → **持续批处理**：动态调度请求，让 GPU 时刻保持忙碌
4. **高效管理 KV Cache** → **PagedAttention**：像操作系统管理内存一样管理 KV Cache

这四类技术相互独立、可以叠加。一个现代推理系统（如 vLLM）会同时使用它们。接下来，我们逐一深入探讨每项技术的原理和实现。

---

## 核心思想与直觉

在进入技术细节之前，让我们先用直觉建立对各项技术的理解。

### 量化的直觉：有损压缩

想象你要通过一条窄带宽的网络传输一张高清照片。原图是 24-bit RGB，每个像素 3 个字节。如果带宽不够，你会怎么做？最直接的方法是**压缩**——JPEG 就是这样做的，它用 8-bit 甚至更低的精度存储像素，肉眼几乎看不出差别。

量化对神经网络做的事情完全类似。预训练的权重是 FP16（16-bit 浮点数），包含了大量"冗余"的精度。研究发现，大模型的权重分布高度集中——绝大多数权重的绝对值都很小，只有极少数"异常值"（outliers）比较大。这种分布意味着我们可以用更少的 bit 来表示大部分权重，而几乎不损失信息。

INT8 量化将 16-bit 压缩到 8-bit，参数量减半；INT4 量化更激进，压缩到四分之一。代价是轻微的精度损失——就像 JPEG 压缩会引入一些"噪点"，但通常在可接受范围内。

### 投机解码的直觉：草稿-审稿流程

想象你是一位繁忙的主编，需要审阅大量稿件。你可以亲自一字一句地写每篇文章，但这太慢了。一个更高效的流程是：让一位初级编辑先写草稿，然后你快速审阅——接受好的部分，修改不满意的部分。

投机解码（Speculative Decoding）正是这个思路。大模型（target model）太慢，每生成一个 token 都要 70ms。我们引入一个小模型（draft model），它虽然不如大模型准确，但生成速度快得多（比如只需 7ms）。流程变成：

1. 小模型快速生成 4 个候选 token：["The", "cat", "sat", "on"]
2. 大模型**一次性**验证这 4 个 token 是否可接受
3. 如果前 3 个被接受，第 4 个被拒绝，则保留前 3 个，从拒绝位置重新开始

关键洞察是：大模型验证 4 个 token 只需要**一次**前向传播（约 70ms），而如果大模型自己生成 4 个 token 需要**四次**前向传播（约 280ms）。只要小模型的预测足够准确（被接受的比例够高），整体速度就能提升。

更妙的是，通过精心设计的接受-拒绝机制，投机解码可以保证最终输出分布与直接使用大模型**完全相同**——这是数学上可证明的无损加速！

### 持续批处理的直觉：餐厅动态座位安排

想象一家繁忙的餐厅。传统的做法是"静态批处理"：服务员等凑齐 8 位客人，一起安排到一张大桌子；所有人必须等最慢的那位吃完，才能一起离席。如果有人只点了一杯咖啡，他也得陪着吃牛排的人坐到最后。

持续批处理（Continuous Batching）更像现代快餐店：客人随到随坐，吃完就走，座位立刻分配给下一位。这样，快速的请求不用等待慢速的请求，系统吞吐量大幅提升。

在 LLM 推理中，不同请求的输出长度差异巨大——有的回答只需 10 个 token，有的需要 1000 个。静态批处理让短请求"陪跑"长请求，GPU 利用率极低。持续批处理允许在每个 iteration（生成一个 token）的粒度上调度：完成的请求立即退出，新请求可以随时加入。

### PagedAttention的直觉：操作系统的内存分页

传统的 KV Cache 管理是"连续分配"的：为每个请求预留一大块连续显存，足以容纳其最大可能长度。这就像早期操作系统的内存管理——为每个程序分配一整块连续内存，导致严重的碎片化。

PagedAttention（分页注意力）借鉴了现代操作系统的**虚拟内存**技术：将 KV Cache 切分成固定大小的"页"（blocks），按需分配，不要求连续。这样，不同请求可以共享物理显存，碎片化问题迎刃而解。

一个额外的好处是：当多个请求共享相同的 prompt（比如系统提示），它们的 KV Cache 可以共享同一份物理内存，进一步节省空间。

---

## 技术细节

### 量化技术

#### 量化基础

量化的本质是用离散值近似连续值。给定一个浮点数权重 $w$，量化过程可以表示为：

$$
w_q = \text{round}\left(\frac{w - z}{s}\right), \quad w_{dq} = w_q \cdot s + z
$$

其中 $s$ 是**缩放因子**（scale），$z$ 是**零点**（zero point），$w_q$ 是量化后的整数，$w_{dq}$ 是反量化后的近似值。

量化有几个关键维度：

**对称 vs 非对称**

- **对称量化**：假设权重分布关于 0 对称，$z = 0$。只需要存储 $s$。
- **非对称量化**：允许任意分布，需要同时存储 $s$ 和 $z$。更灵活但开销稍大。

**量化粒度**

- **Per-tensor**：整个权重矩阵共享一组 $(s, z)$。最节省空间，但精度最低。
- **Per-channel**：每个输出通道有独立的 $(s, z)$。平衡了精度和开销。
- **Per-group**：每 $g$ 个元素共享 $(s, z)$，比如每 128 个元素一组。GPTQ 和 AWQ 常用。

#### 完整数值示例：FP16 → INT4 量化

让我们通过一个具体的例子，手把手走过量化的完整流程。

**设定**：一个 $4 \times 4$ 的权重矩阵，原始值为 FP16。

$$
W = \begin{bmatrix}
-2.1 & 0.3 & 1.5 & -0.8 \\
0.5 & 2.7 & -1.2 & 0.1 \\
1.8 & -0.4 & 0.9 & 2.3 \\
-1.5 & 1.1 & -0.2 & 0.7
\end{bmatrix}
$$

**Step 1: 计算量化参数**

首先找到权重的范围：
$$
w_{\min} = -2.1, \quad w_{\max} = 2.7
$$

INT4 可以表示 $2^4 = 16$ 个值，范围是 $[0, 15]$（无符号）或 $[-8, 7]$（有符号）。我们使用无符号 INT4，范围 $[0, 15]$。

计算缩放因子和零点：
$$
s = \frac{w_{\max} - w_{\min}}{2^4 - 1} = \frac{2.7 - (-2.1)}{15} = \frac{4.8}{15} = 0.32
$$

$$
z = \text{round}\left(-\frac{w_{\min}}{s}\right) = \text{round}\left(\frac{2.1}{0.32}\right) = \text{round}(6.56) = 7
$$

**Step 2: 量化**

对每个权重 $w$，计算量化值：
$$
w_q = \text{clamp}\left(\text{round}\left(\frac{w}{s}\right) + z, \, 0, \, 15\right)
$$

以 $w = 1.5$ 为例：
$$
w_q = \text{round}\left(\frac{1.5}{0.32}\right) + 7 = \text{round}(4.69) + 7 = 5 + 7 = 12
$$

完整的量化矩阵：
$$
W_q = \begin{bmatrix}
0 & 8 & 12 & 4 \\
9 & 15 & 3 & 7 \\
13 & 6 & 10 & 14 \\
2 & 10 & 6 & 9
\end{bmatrix}
$$

**Step 3: 反量化与误差分析**

反量化公式：$w_{dq} = (w_q - z) \cdot s$

以 $w_q = 12$（原始 $w = 1.5$）为例：
$$
w_{dq} = (12 - 7) \cdot 0.32 = 5 \cdot 0.32 = 1.60
$$

量化误差：$|1.5 - 1.60| = 0.10$

**误差分析**：INT4 量化的最大误差为 $s/2 = 0.16$。平均误差通常为最大误差的一半左右，即 $\sim 0.08$。相比原始权重的典型大小（$\sim 1$），相对误差约 8%。

**Step 4: 存储空间对比**

- **FP16**：$16 \times 2 = 32$ bytes
- **INT4**：$16 \times 0.5 + 2 \times 2 = 8 + 4 = 12$ bytes（权重 + scale & zero）
- **压缩比**：$32 / 12 \approx 2.7\times$

对于更大的矩阵，使用 per-group 量化（如每 128 个元素一组），压缩比可以接近理论值 $4\times$。

#### LLM.int8()：发现异常值

2022 年，Dettmers 等人在尝试直接对 LLM 进行 INT8 量化时，遇到了一个令人惊讶的现象：模型性能**断崖式下降**，远超预期。经过深入分析，他们发现了问题的根源：**异常值特征**（emergent outliers）。

在大模型的某些维度上，激活值会出现极端的异常值——比正常值大 10–100 倍。这些异常值只占全部特征的约 0.1%，但对模型输出至关重要。直接量化会严重损害这些异常值的精度，导致模型崩溃。

LLM.int8() 的解决方案是**混合精度分解**：

1. 检测异常值维度（绝对值超过阈值的维度）
2. 异常值维度保持 FP16 精度
3. 其他维度使用 INT8 量化

::: {.callout-note}
## Algorithm: LLM.int8() 混合精度矩阵乘法 (Dettmers et al., 2022)

**输入**：
- 激活矩阵 $X \in \mathbb{R}^{b \times d}$
- 权重矩阵 $W \in \mathbb{R}^{d \times k}$
- 异常值阈值 $\alpha$（通常为 6.0）

**输出**：$Y = XW$

```
1. 检测异常值维度:
   O ← {i : max|X[:, i]| > α}  // 异常值维度集合

2. 分解输入:
   X_outlier ← X[:, O]         // 异常值部分 (FP16)
   X_normal ← X[:, ~O]         // 正常部分

3. 分解权重:
   W_outlier ← W[O, :]         // 对应异常值的权重 (FP16)
   W_normal ← W[~O, :]         // 其他权重

4. 量化正常部分:
   X_q, s_x ← quantize_int8(X_normal)
   W_q, s_w ← quantize_int8(W_normal)

5. 混合精度计算:
   Y_outlier ← matmul_fp16(X_outlier, W_outlier)
   Y_normal ← matmul_int8(X_q, W_q) * s_x * s_w

6. return Y_outlier + Y_normal
```

**关键设计**：
- 约 0.1% 的维度是异常值，99.9% 的计算仍在 INT8
- 内存占用与纯 INT8 相近，推理速度略有下降（约 15–20%）

*Source: Dettmers, T. et al. (2022). "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale". [arXiv:2208.07339](https://arxiv.org/abs/2208.07339)*
:::

LLM.int8() 的重要贡献不仅是方法本身，更是对异常值现象的发现和分析。这一发现启发了后续的 SmoothQuant 和 AWQ 等工作。

#### GPTQ：基于 Hessian 的最优量化

GPTQ（Frantar et al., 2022）采用了完全不同的思路：与其简单地四舍五入，不如找到**最优**的量化方式，使量化误差对输出的影响最小化。

核心想法来自经典的**最优脑外科**（Optimal Brain Surgeon）方法：给定一个训练好的网络，如何"手术式"地移除一些参数（或降低精度），同时最小化对输出的影响？

数学上，我们希望最小化量化引入的重构误差：

$$
\arg\min_{W_q} \|WX - W_q X\|_2^2
$$

其中 $X$ 是校准数据的激活值。利用二阶 Taylor 展开，这个优化问题可以近似为：

$$
\arg\min_{\delta W} \delta W^\top H \delta W
$$

其中 $H = 2XX^\top$ 是 Hessian 矩阵（或 Fisher 信息矩阵）。直觉是：Hessian 的特征值反映了参数的"敏感度"——特征值大的方向，参数变化会显著影响输出；特征值小的方向，参数变化影响较小。

GPTQ 的关键创新是**逐列量化**：不是一次量化整个矩阵，而是一列一列地量化，每量化一列后，用 Hessian 信息更新剩余列，补偿已引入的误差。这种贪心策略在实践中效果极好。

::: {.callout-note}
## Algorithm: GPTQ 逐列量化 (Frantar et al., 2022)

**输入**：
- 权重矩阵 $W \in \mathbb{R}^{d_{row} \times d_{col}}$
- Hessian 矩阵 $H = 2XX^\top \in \mathbb{R}^{d_{col} \times d_{col}}$
- 目标量化位宽 $b$（如 4-bit）
- 分组大小 $B$（如 128）

**输出**：量化后的权重 $\hat{W}$

```
1. Cholesky 分解:
   H ← Cholesky(H)^{-1}     // 高效求逆

2. 初始化:
   E ← 0                     // 累计误差
   Q ← empty                 // 量化结果

3. for i = 0 to d_col - 1 in steps of B:
      // 当前块的列索引
      cols ← [i, i+1, ..., min(i+B-1, d_col-1)]

      // 量化当前块
      for j in cols:
         q_j ← quantize(W[:, j], b)   // 标准量化
         Q[:, j] ← q_j
         err ← (W[:, j] - q_j) / H[j, j]

         // 更新后续列（误差补偿）
         W[:, j+1:] ← W[:, j+1:] - err ⊗ H[j, j+1:]

4. return Q
```

**关键设计**：
- 逐列量化 + 误差补偿，利用 Hessian 信息指导更新
- 分块处理，平衡精度和效率
- 时间复杂度 $O(d^2 n)$，约 4 小时可量化 175B 模型

*Source: Frantar, E. et al. (2022). "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers". ICLR 2023. [arXiv:2210.17323](https://arxiv.org/abs/2210.17323)*
:::

GPTQ 的实验结果令人印象深刻：4-bit 量化后，175B 参数的 OPT-175B 模型的困惑度（perplexity）仅上升 0.5，几乎无损。这使得在单张 A100 上运行 175B 模型成为可能。

#### AWQ：激活感知的权重重要性

AWQ（Lin et al., 2023）提出了一个简洁而深刻的观察：**不是所有权重都同等重要**。

![AWQ的核心思想：保护重要的权重通道](figures/chapter-31/original/fig-awq-overview.png){#fig-awq-overview width=90%}

::: {.figure-caption}
*Source: Lin et al. (2023) "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration", Figure 1. MLSys 2024 Best Paper.*
:::

如上图所示，AWQ 的关键洞察是：权重的重要性应该由**激活值**来衡量，而非权重本身的大小。具体来说，如果某个输入通道的激活值很大，那么对应的权重列就很重要——即使这些权重本身数值不大，它们乘以大激活后对输出影响很大。

AWQ 的策略是：

1. 用校准数据统计每个输入通道的平均激活幅度
2. 对于重要通道（激活大的），放大权重后再量化
3. 放大系数存下来，推理时对输入做相应的缩小（数学等价变换）

数学上，原始计算是 $Y = XW$。AWQ 引入对角缩放矩阵 $S$：

$$
Y = XW = (XS^{-1})(SW)
$$

通过选择合适的 $S$，使 $SW$ 更容易量化。具体地，对于重要通道 $i$，设置 $S_{ii} > 1$，这会放大 $W$ 中对应的列，使其在量化时保留更高的精度。

AWQ 在 MLSys 2024 获得最佳论文奖，其简洁的设计和出色的效果代表了量化技术的前沿水平。

#### 量化技术对比

| 方法 | 位宽 | 核心思想 | 校准数据 | 速度 | 精度损失 |
|------|------|---------|---------|------|---------|
| LLM.int8() | W8A8 | 混合精度分解 | 无 | +50% | ~0 |
| SmoothQuant | W8A8 | 平滑激活分布 | ~1000 样本 | +80% | ~0 |
| GPTQ | W4A16 | Hessian 最优量化 | ~128 样本 | +200% | 0.1-0.5 |
| AWQ | W4A16 | 激活感知重要性 | ~128 样本 | +200% | 0.1-0.3 |

### 投机解码（Speculative Decoding）

#### 基本原理

投机解码的核心思想惊人地简单：用一个快速但不那么准确的小模型（draft model）生成候选 token 序列，然后用目标大模型（target model）**并行验证**这些候选。

传统自回归生成：
```
Token 1 → Token 2 → Token 3 → Token 4
   70ms      70ms      70ms      70ms
Total: 280ms
```

投机解码：
```
Draft: Token 1,2,3,4 (并行生成)  →  Target: 验证全部 (一次前向)
           28ms                          70ms
Total: ~100ms (假设全部接受)
```

关键问题是：如果 draft 的预测被拒绝怎么办？投机解码使用一种精心设计的**接受-拒绝采样**机制，保证最终输出分布与直接使用 target model 完全相同。

#### 数学原理：为什么能保证无损？

设 $p(x)$ 是 target model 在某位置的概率分布，$q(x)$ 是 draft model 的分布。投机解码的接受规则是：

$$
\text{Accept } x \text{ with probability } \min\left(1, \frac{p(x)}{q(x)}\right)
$$

如果 $x$ 被接受，输出就是 $x$。如果被拒绝，从修正分布 $p'(x)$ 中重新采样：

$$
p'(x) = \text{normalize}\left(\max(0, p(x) - q(x))\right)
$$

**定理**：在上述接受-拒绝规则下，最终输出的分布恰好是 $p(x)$。

**证明思路**：
输出 $x$ 的总概率 = (由 draft 生成 $x$ 的概率) × (接受 $x$ 的概率) + (从修正分布采样到 $x$ 的概率)

$$
P(\text{output}=x) = q(x) \cdot \min\left(1, \frac{p(x)}{q(x)}\right) + P(\text{reject}) \cdot p'(x)
$$

经过仔细计算，可以验证这确实等于 $p(x)$。关键洞察是：当 $q(x) > p(x)$ 时，我们"过度采样"了 $x$，需要通过拒绝来修正；当 $q(x) < p(x)$ 时，我们"欠采样"了 $x$，通过修正分布来补充。

这个结果非常优雅：它意味着投机解码在数学上是**无损**的，不会改变输出质量，只会提高速度。

#### 完整数值示例：Draft-Verify 流程

让我们通过一个具体例子，完整走过投机解码的一轮验证。

**设定**：
- Target model：70B 参数
- Draft model：7B 参数（同系列的小模型）
- 猜测长度 $K = 4$
- 当前已生成的前缀："The weather today is"

**Step 1: Draft 生成**

Draft model 并行生成 4 个候选 token 及其概率：

| 位置 | Token | Draft 概率 $q(x)$ |
|------|-------|-------------------|
| 1 | "very" | 0.60 |
| 2 | "nice" | 0.55 |
| 3 | "and" | 0.40 |
| 4 | "warm" | 0.35 |

**Step 2: Target 验证（一次前向传播）**

Target model 对序列 "The weather today is very nice and warm" 计算条件概率。这是**单次**前向传播，但输出每个位置的概率分布：

| 位置 | Token | Target 概率 $p(x)$ | Draft 概率 $q(x)$ |
|------|-------|-------------------|-------------------|
| 1 | "very" | 0.70 | 0.60 |
| 2 | "nice" | 0.45 | 0.55 |
| 3 | "and" | 0.50 | 0.40 |
| 4 | "warm" | 0.30 | 0.35 |

**Step 3: 逐位置接受/拒绝**

对每个位置，计算接受概率 $\min(1, p/q)$ 并掷骰子：

**位置 1 ("very")**：
$$
P(\text{accept}) = \min\left(1, \frac{0.70}{0.60}\right) = \min(1, 1.17) = 1.0
$$
因为 $p > q$，target 比 draft 更喜欢这个 token，一定接受。✅

**位置 2 ("nice")**：
$$
P(\text{accept}) = \min\left(1, \frac{0.45}{0.55}\right) = 0.82
$$
掷骰子得到 0.65 < 0.82，接受。✅

**位置 3 ("and")**：
$$
P(\text{accept}) = \min\left(1, \frac{0.50}{0.40}\right) = 1.0
$$
一定接受。✅

**位置 4 ("warm")**：
$$
P(\text{accept}) = \min\left(1, \frac{0.30}{0.35}\right) = 0.86
$$
掷骰子得到 0.91 > 0.86，拒绝。❌

**Step 4: 处理拒绝**

位置 4 被拒绝，需要从修正分布 $p'$ 中重新采样。假设 target 在位置 4 的完整分布是：

| Token | $p(x)$ | $q(x)$ | $\max(0, p-q)$ |
|-------|--------|--------|----------------|
| "warm" | 0.30 | 0.35 | 0.00 |
| "sunny" | 0.25 | 0.20 | 0.05 |
| "pleasant" | 0.20 | 0.15 | 0.05 |
| 其他 | 0.25 | 0.30 | 0.00 |

归一化后 $p'$：$P(\text{sunny}) = P(\text{pleasant}) = 0.5$

假设采样到 "sunny"，最终输出是：**"very nice and sunny"**

**Step 5: 效率分析**

- **无投机解码**：生成 4 个 token 需要 4 次 target 前向，约 $4 \times 70 = 280$ ms
- **有投机解码**：1 次 draft 前向 + 1 次 target 前向，约 $7 + 70 = 77$ ms
- **实际接受**：3 个 token + 1 个重采样 = 4 个 token
- **加速比**：$280 / 77 \approx 3.6\times$

实际场景中，由于 draft 和 target 的分布差异，接受率通常在 60–80%，整体加速比约 2–3 倍。

### 持续批处理（Continuous Batching）

#### 传统批处理的问题

传统的静态批处理（static batching）要求所有请求同时开始、同时结束：

```
请求 A: [生成 10 个 token]  ████░░░░░░░░░░░░░░░░░░░░░░░░░░░░
请求 B: [生成 50 个 token]  ████████████████████████████████
请求 C: [生成 20 个 token]  ██████████░░░░░░░░░░░░░░░░░░░░░░
                            ↑                              ↑
                          开始                            结束
```

请求 A 和 C 早早完成，却要等待请求 B，白白占用 GPU 资源。对于长度分布差异大的场景（这在实际应用中很常见），静态批处理的 GPU 利用率可能不到 30%。

#### Orca 的迭代级调度

Orca（Yu et al., OSDI 2022）提出了**迭代级调度**（iteration-level scheduling）：在每个 iteration（生成一个 token）的粒度上做调度决策。

```
请求 A: ████          （10 个 token 后退出，资源释放）
请求 B: ████████████████████████████████
请求 C: ██████████    （20 个 token 后退出）
请求 D:      ████████████████████████    （A 退出后加入）
请求 E:            ████████████████      （C 退出后加入）
```

这样，短请求不用"陪跑"长请求，系统吞吐量大幅提升。Orca 论文报告了 **36.9 倍**的吞吐提升（与 FasterTransformer 相比，在 GPT-3 175B 上）。

#### PagedAttention 与 vLLM

vLLM（Kwon et al., SOSP 2023）在 Orca 的基础上进一步优化了 KV Cache 管理，提出了 **PagedAttention**。

传统方法为每个请求预分配最大长度的 KV Cache，导致严重浪费：

```
请求 A (实际 100 token): [████████░░░░░░░░░░░░░░░░░░░░░░░░] 预留 1000 token
请求 B (实际 500 token): [████████████████████░░░░░░░░░░░░] 预留 1000 token
                          ↑ 实际使用        ↑ 浪费
```

PagedAttention 将 KV Cache 切分成固定大小的页（如 16 tokens 一页），按需分配：

```
请求 A: [Page 1][Page 2][Page 3][Page 4][Page 5][Page 6][Page 7]  (7 页, 实际 100 token)
请求 B: [Page 8][Page 9]...[Page 39]  (32 页, 实际 500 token)

物理内存: |Page 1|Page 8|Page 2|Page 9|Page 3|... (交错存储，无碎片)
```

额外好处：当多个请求共享相同的 prompt（如系统指令），它们的 KV Cache 可以共享：

```
请求 A: [系统提示(共享)][用户问题 A 的 KV]
请求 B: [系统提示(共享)][用户问题 B 的 KV]
                  ↑
            物理内存只存一份
```

vLLM 结合 PagedAttention 和持续批处理，相比 HuggingFace Transformers 和 FasterTransformer 实现了 **2–4 倍**的吞吐提升。

### 模型并行推理

当单卡显存不足以容纳模型时，需要将模型分布到多个 GPU 上。这与第19章讨论的分布式训练类似，但推理场景有其特殊性。

#### Tensor Parallelism（TP）

将单层的矩阵计算切分到多个 GPU：

```
GPU 0: W[:, :d/2]   →  计算前半部分输出
GPU 1: W[:, d/2:]   →  计算后半部分输出
        ↓ AllReduce ↓
      合并得到完整输出
```

优点：每层内并行，延迟低
缺点：每层都需要 AllReduce 通信

#### Pipeline Parallelism（PP）

将不同层放到不同 GPU：

```
GPU 0: Layer 0-9    →  处理前 10 层
GPU 1: Layer 10-19  →  处理中间 10 层
GPU 2: Layer 20-29  →  处理后 10 层
```

优点：通信量小（只在层边界传递激活）
缺点：存在流水线气泡，增加延迟

#### 推理场景的选择

| 场景 | 推荐策略 | 原因 |
|------|---------|------|
| 低延迟（实时对话） | 纯 TP | 延迟最低 |
| 高吞吐（批处理） | TP + PP | 最大化利用率 |
| 显存极度受限 | PP + 模型卸载 | 允许更大模型 |

---

## 工程实践

### 使用 GPTQ 量化模型

```{python}
#| code-fold: false
#| eval: false

from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 加载原始模型
model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 配置量化参数
quantize_config = BaseQuantizeConfig(
    bits=4,                    # 4-bit 量化
    group_size=128,            # 每 128 个权重共享量化参数
    desc_act=True,             # 激活感知排序（提升精度）
)

# 加载模型并量化
model = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config=quantize_config,
    device_map="auto"
)

# 准备校准数据
calibration_data = [
    tokenizer(text, return_tensors="pt")
    for text in ["校准文本1...", "校准文本2...", ...]
]

# 执行量化
model.quantize(calibration_data)

# 保存量化后的模型
model.save_quantized("llama-2-7b-gptq-4bit")
```

### 使用 AWQ 量化模型

```{python}
#| code-fold: false
#| eval: false

from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 加载模型
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoAWQForCausalLM.from_pretrained(
    model_name,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# AWQ 量化配置
quant_config = {
    "zero_point": True,        # 使用零点
    "q_group_size": 128,       # 分组大小
    "w_bit": 4,                # 4-bit 量化
    "version": "GEMM"          # 优化版本
}

# 执行量化
model.quantize(tokenizer, quant_config=quant_config)

# 保存
model.save_quantized("llama-2-7b-awq-4bit")
```

### vLLM 部署示例

```{python}
#| code-fold: false
#| eval: false

from vllm import LLM, SamplingParams

# 加载模型（自动启用 PagedAttention 和持续批处理）
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1,        # 单卡
    gpu_memory_utilization=0.90,   # GPU 内存利用率
    max_model_len=4096,            # 最大序列长度
)

# 配置采样参数
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=256,
)

# 批量推理
prompts = [
    "What is the capital of France?",
    "Explain quantum computing in simple terms.",
    "Write a haiku about programming.",
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
    print("-" * 50)
```

### TGI 部署（Docker）

```bash
# 拉取 TGI 镜像
docker pull ghcr.io/huggingface/text-generation-inference:latest

# 启动服务
docker run --gpus all --shm-size 1g -p 8080:80 \
    -v $HOME/.cache/huggingface:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --quantize gptq \
    --max-concurrent-requests 128 \
    --max-input-length 2048 \
    --max-total-tokens 4096

# 调用 API
curl http://localhost:8080/generate \
    -X POST \
    -H "Content-Type: application/json" \
    -d '{"inputs": "What is deep learning?", "parameters": {"max_new_tokens": 100}}'
```

---

## 深入理解

### 为什么有效？——理论视角

#### 量化的信息论基础

从信息论角度，量化的目标是找到一个编码，使重构误差最小化。对于服从某种分布的权重，最优量化（Lloyd-Max 量化）的误差与分布的方差和量化级数相关。

大模型权重的一个关键特性是其分布高度集中：绝大多数权重的绝对值小于某个阈值，呈近似正态分布。这意味着均匀量化（等间距划分）是次优的——大部分量化区间分配给了稀疏的尾部，而密集的中心区域反而精度不足。

GPTQ 和 AWQ 的优越性部分来自于对这种分布特性的利用：通过非均匀的缩放（GPTQ 的 Hessian 加权，AWQ 的激活感知）更好地适应权重分布。

#### 投机解码的概率论基础

投机解码的正确性基于**接受-拒绝采样**（acceptance-rejection sampling）的经典结果。这是蒙特卡洛方法的基础技术，数学上可以追溯到 von Neumann（1951）。

核心定理是：如果提议分布 $q$ 满足 $q(x) > 0$ 对所有 $p(x) > 0$ 的 $x$ 成立，那么接受-拒绝采样可以精确采样任何目标分布 $p$。投机解码的贡献是将这一经典技术巧妙地应用于 LLM 推理加速。

### 边界条件与失效模式

#### 量化的边界条件

**任务敏感性**：某些任务对精度特别敏感。研究发现，数学推理任务在 4-bit 量化后性能下降明显（5–10%），而常规问答和文本生成任务几乎不受影响。

**模型规模**：有趣的是，大模型通常比小模型更容易量化。70B 模型量化后的相对性能损失小于 7B 模型。一种解释是：大模型有更多的冗余，可以更好地"吸收"量化噪声。

**校准数据**：GPTQ 和 AWQ 都需要少量校准数据（通常 128–512 样本）。校准数据的领域应该与实际应用匹配——用英文数据校准后处理中文可能效果不佳。

#### 投机解码的边界条件

**Draft-Target 分布差异**：投机解码的加速比高度依赖于 draft 和 target 分布的相似度。当任务需要创意或探索性输出时（如创意写作），draft 的预测接受率会下降，加速比也随之降低。

**Token 长度分布**：对于非常短的输出（< 10 token），投机解码的额外开销可能抵消收益。最适合中等长度的生成任务。

### 开放研究问题

1. **超低 bit 量化**：能否可靠地实现 2-bit 甚至 1-bit 量化？目前的研究表明 3-bit 是一个关键阈值，低于此会显著损失性能。突破这一限制需要新的量化技术或训练方法。

2. **自适应投机解码**：当前的投机解码使用固定的猜测长度 $K$。能否根据上下文动态调整 $K$？在 draft 信心高的位置猜更多，信心低的位置猜更少？

3. **端到端协同优化**：量化、投机解码、批处理目前是独立设计的。它们之间是否存在协同效应或冲突？能否设计一个统一的优化框架？

4. **边缘设备推理**：手机、边缘设备的算力和内存远小于数据中心 GPU。如何将上述技术适配到极端资源受限的场景？

---

## 局限性与未解决的问题

### 本方法的局限

**局限 1：速度-质量权衡不可避免**

尽管量化技术不断进步，4-bit 量化仍然会在某些任务上造成可测量的性能损失。对于追求极致质量的场景（如医疗、法律领域的 AI 应用），需要仔细评估量化的影响。

**局限 2：技术栈复杂性**

完整的推理优化需要叠加多项技术：量化 + 投机解码 + 持续批处理 + KV Cache 优化。每项技术都有自己的超参数和适用条件，组合起来的调优空间巨大。对于资源有限的团队，部署和维护成本不可忽视。

**局限 3：硬件依赖性**

很多优化依赖特定硬件特性：INT4 需要 GPU 支持（如 A100、H100 的 INT4 Tensor Core），FlashAttention 需要足够的 shared memory。跨平台部署（如 CPU、移动端）时，部分优化可能不可用。

### 这些局限导向了什么？

推理优化解决了"如何高效地运行大模型"，但没有解决"大模型本身的局限"。即使推理成本降到足够低，模型仍然可能：

- 产生幻觉（hallucination），给出看似合理但实际错误的回答
- 缺乏实时知识，无法回答最新发生的事件
- 无法处理超长上下文，在需要综合大量信息的任务上表现不佳

这引出了下一章要讨论的**检索增强生成（RAG）**——一种通过外部知识库增强模型能力的范式。推理优化让模型跑得快，RAG 让模型知道更多。两者是互补的技术，共同构成现代 LLM 应用的基础设施。

---

## 本章小结

### 核心要点回顾

1. **问题**：推理成本（显存、延迟、费用）是大模型普及的主要障碍。70B 模型的 FP16 推理需要 140GB 显存，远超单卡容量；自回归生成的 token-by-token 方式让 GPU 利用率仅有 10–30%。

2. **洞察**：推理优化的本质是在精度、速度、成本三角形中寻找帕累托最优。关键在于识别和利用大模型推理的特殊结构——权重分布集中适合量化，自回归生成可以"猜测-验证"并行化，请求长度差异适合动态批处理。

3. **方法**：
   - **量化**（GPTQ、AWQ）：INT8/INT4 压缩，2–4 倍显存节省，几乎无损
   - **投机解码**：2–3 倍无损加速，数学上可证明正确
   - **持续批处理**（vLLM）：迭代级调度，2–5 倍吞吐提升
   - **PagedAttention**：消除 KV Cache 碎片，支持更大 batch

4. **意义**：这些技术可以叠加使用，将推理成本降低 10 倍以上，让"只有大厂用得起"变成"人人可部署"。

### 关键公式速查

| 公式 | 表达式 |
|------|--------|
| 线性量化 | $w_q = \text{round}((w - z) / s)$ |
| 投机解码接受概率 | $P_{\text{accept}} = \min(1, p(x) / q(x))$ |
| KV Cache 大小 | $2 \times L \times n \times d \times \text{sizeof(dtype)}$ |
| 内存带宽受限时间 | $T = \text{参数大小} / \text{带宽}$ |

### 方法对比速查

| 方法 | 显存影响 | 延迟影响 | 吞吐影响 | 质量影响 |
|------|---------|---------|---------|---------|
| INT8 量化 | -50% | -30% | +50% | ~0% |
| INT4 量化 | -75% | -50% | +100% | 1–3% |
| 投机解码 | +10%（draft） | -50–60% | 无 | 0%（数学无损） |
| 持续批处理 | 无 | +20%（首 token） | +200–500% | 0% |
| PagedAttention | -30–50% | 无 | +50–100% | 0% |

### 思考题

1. **[概念理解]** 解释为什么大模型推理是"内存带宽受限"而非"计算受限"的任务。这对优化策略有什么启示？

2. **[数学推导]** 证明投机解码的接受-拒绝采样保持目标分布不变。给出完整的概率计算。

3. **[工程实践]** 使用 vLLM 部署一个 INT4 量化的 LLaMA-7B 模型，测量不同 batch size 下的吞吐量（tokens/s）和首 token 延迟。观察并解释瓶颈点。

4. **[开放思考]** 量化、投机解码、持续批处理能否同时使用？设计一个完整的推理系统架构，考虑它们之间的协同与冲突。

---

## 延伸阅读

### 核心论文（必读）

- **[GPTQ](https://arxiv.org/abs/2210.17323)**：Frantar et al. (2022)，后训练量化的里程碑
  - 重点读：Section 3（方法）、Section 4（实验）
  - 可跳过：Appendix 的详细推导

- **[AWQ](https://arxiv.org/abs/2306.00978)**：Lin et al. (2023)，MLSys 2024 最佳论文
  - 重点读：Section 3（激活感知量化）、Figure 1-2

- **[vLLM/PagedAttention](https://arxiv.org/abs/2309.06180)**：Kwon et al. (2023)，SOSP 2023
  - 重点读：Section 3（PagedAttention）、Section 4（系统设计）

### 理论基础

- **[LLM.int8()](https://arxiv.org/abs/2208.07339)**：Dettmers et al. (2022)，发现异常值现象
- **[Speculative Decoding](https://arxiv.org/abs/2211.17192)**：Leviathan et al. (2023)，投机解码的理论框架

### 后续发展

- **[GGUF Format](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)**：llama.cpp 的量化格式，支持 CPU 推理
- **[GQA](https://arxiv.org/abs/2305.13245)**：Grouped-Query Attention，减少 KV Cache

### 代码资源

- **[vLLM](https://github.com/vllm-project/vllm)**：高性能推理引擎
- **[TGI](https://github.com/huggingface/text-generation-inference)**：Hugging Face 推理服务
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)**：CPU 推理，GGUF 格式

---

## 历史注脚

推理优化的需求伴随着 GPT-3 的发布而爆发。2020 年，175B 参数的 GPT-3 震惊了 AI 社区，但也让人们意识到：这样的模型几乎无法在任何单一设备上运行。最初，人们认为这只是"有钱人的游戏"——只有 OpenAI、Google 这样的公司才有资源部署如此规模的模型。

转折点出现在 2022 年。Dettmers 的 LLM.int8() 首次展示了大模型可以在"可承受"的硬件上运行。紧随其后，GPTQ 将 175B 模型压缩到单卡可运行的程度。2023 年，vLLM 和 TGI 的出现让高效推理成为"开箱即用"的能力。短短两年间，大模型推理从"天方夜谭"变成了"人人可及"。

这段历史告诉我们：AI 的民主化不仅需要开源模型，更需要开源的基础设施。量化、高效推理引擎这些"不那么性感"的技术，实际上是让 AI 惠及普通人的关键拼图。
