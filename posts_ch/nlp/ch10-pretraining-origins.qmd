---
title: "第10章：预训练思想的起源"
subtitle: "从词向量到迁移学习：NLP如何借鉴CV的成功经验"
author: "Ying Zha"
date: "2026-01-26"
categories: [NLP, Deep Learning, Pre-training, Transfer Learning]
tags: [Word2Vec, ImageNet, 迁移学习, ULMFiT, 预训练, Fine-tuning]
description: "预训练范式的思想起源：迁移学习的基本思想、Word2Vec作为预训练雏形、计算机视觉的ImageNet启示、以及从特征提取到模型微调的范式演进。"
image: "figures/pretraining-origins-banner.png"
toc: true
toc-depth: 3
number-sections: true
code-fold: true
code-tools: true
format:
  html:
    fig-cap-location: bottom
bibliography: references.bib
---

::: {.callout-tip collapse="true"}
## 本章参考来源

### 论文
- **Mikolov et al. (2013)** "Efficient Estimation of Word Representations in Vector Space" & "Distributed Representations of Words and Phrases" — 参考了 Word2Vec 的 Skip-gram/CBOW 架构及预训练思想
- **Howard & Ruder (2018)** "Universal Language Model Fine-tuning for Text Classification" (ULMFiT) — 参考了三阶段框架（Section 3）、判别式微调与逐步解冻（Section 3.2-3.3）、消融实验（Section 4）
- **Dai & Le (2015)** "Semi-supervised Sequence Learning" — 参考了序列模型预训练的早期验证
- **Collobert & Weston (2008/2011)** "Natural Language Processing (Almost) from Scratch" — 参考了多任务共享表示的先驱思想
- **Ben-David et al. (2010)** "A theory of learning from different domains" — 参考了域适应理论框架
- **Yosinski et al. (2014)** "How transferable are features in deep neural networks?" — 参考了CNN特征可迁移性的实证分析

### 教材
- **D2L** Section 15.1-15.6 (Word Embedding & Pretraining) — 参考了词向量预训练的教学组织方式
- **SLP3** Chapter 6 (Vector Semantics) & Chapter 10 (Transformers and Pre-training) — 参考了迁移学习和预训练范式的讲解框架

### 课程
- **Stanford CS224N** Lecture 9 (2025) "Pretraining" — 参考了从 Word2Vec 到 GPT/BERT 的演进脉络讲解
- **fast.ai** Practical Deep Learning Course (2018) — 参考了 ULMFiT 的工程实践经验和教学视角
:::

> **核心问题**：如何让模型从海量无标注数据中学到通用的语言知识，然后迁移到标注稀缺的具体任务？
>
> **历史坐标**：2013–2018 | Word2Vec → ImageNet预训练 → ULMFiT | 从"特征迁移"到"模型迁移"的范式演进

---

## 从上一章说起

::: {.callout-note}
## 第四部分开篇

本章是**第四部分：预训练范式的演进**的第一章。前面三个部分，我们沿着一条清晰的技术路线走来：从RNN的顺序建模（第5章），到注意力机制的诞生与演进（第6-7章），再到Transformer的横空出世（第8章）和高效注意力的优化尝试（第9章）。这条路线的主题是**架构设计**——如何设计更好的网络结构来处理序列数据。

从本章开始，我们将进入一个全新的主题：**训练范式**。不再只问"用什么架构"，而要问一个更根本的问题——"如何更聪明地利用数据来训练模型"。
:::

回顾前面的旅程，到第8章结束时我们已经拥有了Transformer——一个强大的序列建模架构。它用纯注意力机制替代了循环结构，实现了完全并行的计算和$O(1)$的最长路径。第9章进一步探讨了如何优化Transformer的$O(n^2)$计算瓶颈。从架构的角度看，我们似乎已经有了足够好的工具。

然而，一个尴尬的现实是：拥有好的架构，并不意味着拥有好的模型。

考虑一个典型的NLP任务——医学文本情感分析。你手上有500条标注好的医学评论，希望训练一个分类器。如果你从头训练一个Transformer，会发生什么？大概率是过拟合。512维的Transformer Base有6500万参数，而你只有500个训练样本——参数比数据多了5个数量级。这就像用一张百万像素的画布去画一条直线，绝大多数像素都是噪声。

这个困境在NLP中普遍存在。高质量的标注数据昂贵且稀缺：一个医学NER数据集可能需要领域专家逐字标注，每小时只能标注几十个句子；一个法律文本分类任务可能需要律师审核每条样本；即使是相对简单的情感分析，大规模标注也需要数百人时的众包工作。

与此同时，互联网上有几乎无限的无标注文本——Wikipedia有超过40亿词，Common Crawl包含PB级别的网页数据。这些文本蕴含着丰富的语言知识：语法结构、语义关系、世界知识、推理模式。问题是，我们能否从这些无标注数据中提取出有用的"语言理解能力"，然后迁移到标注稀缺的具体任务？

这正是**预训练（Pre-training）**要回答的问题。

> 💡 **本章核心洞察**：与其为每个任务从零学习语言知识，不如先在大规模无标注数据上学习通用的语言表示（预训练），然后在少量标注数据上适配具体任务（微调）。这个看似简单的想法，将彻底改变NLP的研究范式。

本章我们将追溯这个思想的起源：从迁移学习的基本框架出发，看Word2Vec如何成为预训练的雏形，再看计算机视觉领域的ImageNet预训练如何提供了关键启示，最后了解NLP社区如何在ULMFiT中找到了自己的预训练路径。这些探索为后面的ELMo（第11章）、GPT（第12章）和BERT（第13章）奠定了思想基础。

![NLP 预训练范式的演进：从2008年Collobert & Weston的多任务共享表示，到2013年Word2Vec的词级别迁移，再到2018年ULMFiT/ELMo/GPT/BERT的深层模型迁移。](figures/chapter-10/fig-pretraining-timeline.png){#fig-pretraining-timeline width=95%}

::: {.figure-caption}
*作者绘制。时间线基于各论文的发表日期整理。*
:::

---

## 问题的本质是什么？

### 标注数据的瓶颈

让我们用具体数字来感受这个问题的规模。

下表列出了几个经典NLP任务的标注数据量和训练成本：

| 任务 | 数据集 | 标注样本数 | 标注成本估算 |
|------|--------|-----------|-------------|
| 情感分析 | SST-2 | 67,349 | ~$5,000 |
| 自然语言推理 | MNLI | 392,702 | ~$50,000 |
| 命名实体识别 | CoNLL-2003 | 20,744句 | ~$15,000 |
| 机器翻译 | WMT14 En-De | 450万句对 | ~$500,000 |
| 阅读理解 | SQuAD 2.0 | 150,000 | ~$100,000 |

这些数据集的构建动辄数万美元，而且只覆盖特定的领域和语言。如果你想做一个新的任务——比如中文医学文献的关系抽取——你需要从头开始标注，而领域专家的时间极其宝贵。

更根本的问题在于：每个NLP任务都需要模型从零学习语言本身。训练情感分析模型时，模型需要学习什么是"好"什么是"坏"，但同时它也在学习英语的语法、词的语义、句子的结构——这些基础知识在所有NLP任务中都是共通的。当你训练命名实体识别模型时，它又要重新学习一遍这些基础知识。这种重复劳动既低效又浪费。

一个自然的问题是：能不能先让模型学会"语言是什么"，然后再教它"这个任务怎么做"？

### 从特征工程到表示学习

在深度学习之前，NLP研究者面临的是另一个版本的同一问题：**特征工程**。

传统机器学习方法需要人工设计特征。以情感分析为例，研究者需要手动定义哪些词是"积极的"，哪些搭配是"消极的"，是否出现了否定词，句子的语法结构是什么样的。这些特征的质量直接决定了模型的上限。一个经验丰富的NLP研究者可能会设计出几十种精心调校的特征，但这些特征很难迁移到其他任务——情感分析的特征对命名实体识别几乎没有用处。

第3章我们已经看到，Word2Vec等词向量方法迈出了关键的一步：让机器自动从数据中学习词的表示。"king"不再是一个任意的符号，而是一个300维的向量，其中编码了语义信息。这种表示可以在不同任务间共享——无论是做分类还是做翻译，"king"的语义都是相似的。

但词向量只是迁移了**词级别**的知识。一个自然的追问是：能否迁移**更高层次**的知识？比如句法结构、语义组合、篇章逻辑？

### 我们需要什么样的解决方案？

理想的解决方案应该满足几个关键条件。

首先，它必须能利用**无标注数据**。无标注文本几乎无限，而标注数据昂贵稀缺，解决方案必须能够从前者中提取有用的知识。其次，学到的知识应该是**通用的**——不局限于某个特定任务，而是捕获语言的基本结构和语义。第三，这些知识必须**可迁移**，能够轻松适配到各种下游任务。最后，迁移后的效果应该**优于从零训练**，尤其是在标注数据很少的情况下。

这套框架有一个名字：**迁移学习（Transfer Learning）**。

---

## 核心思想与直觉

### 迁移学习：站在巨人的肩膀上

迁移学习的核心思想可以用一个类比来理解。

想象你是一个精通中文的作家，现在要学习写日语散文。你不需要从零学起——中文和日语共享大量汉字，你已经理解了"什么是好的叙事结构"，你知道如何运用修辞手法，你甚至对东亚文化有共通的感知。你需要学习的只是日语特有的语法、假名系统和一些文化差异。你的中文写作能力为学习日语写作提供了巨大的"先验知识"。

迁移学习做的就是同样的事情，只不过"知识"被编码在神经网络的权重中。

更正式地说，迁移学习的框架包含两个阶段。第一个阶段是**预训练（Pre-training）**：在一个数据丰富的**源任务（source task）**上训练模型，让它学习通用的知识表示。第二个阶段是**适配（Adaptation）**：将预训练模型的知识迁移到一个数据稀缺的**目标任务（target task）**上。

适配的方式有两种主要路线。一种是**特征提取（Feature Extraction）**：冻结预训练模型的参数，用它的输出作为下游任务的输入特征。另一种是**微调（Fine-tuning）**：在下游任务的数据上继续训练整个模型（或部分参数），让模型适应新任务。

这两种路线的区别可以类比为：特征提取就像用一台固定的望远镜去看不同的风景——望远镜本身不变，只是对准的方向不同；微调则像在已有望远镜基础上进行调焦和校准——望远镜本身也会做出微调来适应新的观测条件。

![从零训练、浅层迁移（词向量预训练）和深层预训练+微调三种范式的对比。注意每种方式中哪些层是预训练的、哪些是随机初始化的。](figures/chapter-10/fig-transfer-comparison.png){#fig-transfer-comparison width=95%}

::: {.figure-caption}
*作者绘制。三种范式的对比基于 Ruder (2019) 博士论文中的迁移学习分类框架。*
:::

### 为什么迁移可以工作？

迁移学习能够成功的关键前提是：**不同任务之间存在共享的知识结构**。

在自然语言处理中，这个前提是高度成立的。所有的英语NLP任务都需要理解英语的语法（"the"后面通常接名词），都需要理解词的语义（"happy"是正面的），都需要理解一定的世界知识（"水在100°C时会沸腾"）。这些共享的知识构成了一个"通用语言理解能力"的基础层，不同的下游任务只是在这个基础层之上添加了各自的任务特定知识。

用神经网络的语言来说，低层的特征（如词的语义、常见短语）是通用的，可以跨任务共享；高层的特征（如情感极性、实体类型）是任务特定的，需要在目标任务上学习。迁移学习的效果取决于源任务和目标任务之间共享知识的比例——共享越多，迁移越有效。

### 预训练范式的演进脉络

预训练的思想并非一蹴而就，而是经历了几代演进。理解这个演进脉络对于把握后续章节（ELMo、GPT、BERT）的设计动机至关重要。

**第一代：词向量预训练（2013–2017）**。Word2Vec和GloVe在大规模无标注文本上训练词向量，然后用这些词向量初始化下游模型的Embedding层。这是最浅层的迁移——只迁移了词级别的语义知识，模型的其他层仍然从零训练。

**第二代：浅层模型预训练（2015–2017）**。Dai & Le (2015) 尝试用语言模型预训练LSTM，然后微调整个模型。这比词向量更进一步，但效果还不够惊艳，没有引起广泛关注。

**第三代：深层预训练 + 精细微调（2018）**。ULMFiT提出了一套完整的预训练-微调框架，包含判别式微调和逐步解冻等技巧。ELMo用双向LSTM生成上下文相关的词表示。GPT用Transformer Decoder进行自回归预训练。BERT用Transformer Encoder进行双向预训练。这一代标志着预训练范式的真正成熟。

本章聚焦第一代和第二代，为后续章节做铺垫。

---

## 技术细节

### Word2Vec：预训练的雏形

#### 回顾与重新审视

第3章我们介绍了Word2Vec的技术细节——Skip-gram和CBOW模型如何从大规模文本中学习词向量。在这里，我们要从一个新的角度重新审视Word2Vec：它作为**NLP预训练范式的原型**，包含了哪些后来被验证为正确的设计思想？

Word2Vec的训练过程可以看作一种无监督预训练：在大规模无标注文本上，通过预测上下文（Skip-gram）或由上下文预测中心词（CBOW）的方式，学习词的分布式表示。这个过程不需要任何人工标注，只需要原始文本本身。训练得到的词向量编码了丰富的语义和句法信息——著名的 $\vec{king} - \vec{man} + \vec{woman} \approx \vec{queen}$ 就是最好的例证。

关键的创新在于如何使用这些词向量。2014年前后，NLP社区逐渐形成了一套标准做法：用Word2Vec/GloVe预训练的词向量来初始化模型的Embedding层，而不是使用随机初始化。这看起来是一个简单的工程技巧，但其背后蕴含着深刻的思想——**从大规模无标注数据中学到的知识可以帮助小规模标注数据上的任务**。

#### 数值示例：预训练词向量的效果

让我们通过一个具体的数值例子来感受预训练词向量的威力。

假设我们要做情感分析，训练集中"fantastic"这个词只出现了2次（都是正面评论），而测试集中出现了"wonderful"。如果使用随机初始化的Embedding，模型对"wonderful"完全没有先验知识——它的向量是随机的，模型无法利用"wonderful"与"fantastic"的语义相似性。

但如果使用预训练的GloVe词向量：

$$
\cos(\vec{\text{wonderful}}, \vec{\text{fantastic}}) = 0.78
$$

$$
\cos(\vec{\text{wonderful}}, \vec{\text{terrible}}) = 0.23
$$

模型在看到"wonderful"时，即使从未在标注数据中见过这个词，也能通过它与"fantastic"的高相似度"推测"这可能是正面情感。预训练词向量充当了一个先验知识库，弥补了标注数据的不足。

这个简单的例子揭示了预训练的核心价值：**泛化**。预训练让模型能够处理训练集中未见过的词和表达，因为它已经在更大的数据上学习了词与词之间的关系。

#### Word2Vec预训练的具体用法

在实践中，使用预训练词向量有两种常见策略。

**策略一：冻结Embedding（Feature Extraction）**。将预训练词向量加载到Embedding层，然后冻结其权重不更新。下游模型只训练Embedding之上的层。这种方式保留了预训练知识的完整性，但缺乏针对目标任务的适应性。

**策略二：用预训练初始化 + 微调**。用预训练词向量初始化Embedding层，然后在下游任务训练时允许Embedding层的权重继续更新。这种方式在保留先验知识的同时，允许模型针对具体任务做出调整。

```{python}
#| code-fold: false
#| eval: false

import torch
import torch.nn as nn
import numpy as np

class SentimentClassifier(nn.Module):
    """使用预训练词向量的情感分类器"""
    def __init__(self, pretrained_embeddings, hidden_dim=128,
                 num_classes=2, freeze_embeddings=False):
        super().__init__()
        vocab_size, embed_dim = pretrained_embeddings.shape

        # 用预训练词向量初始化 Embedding 层
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.embedding.weight = nn.Parameter(
            torch.tensor(pretrained_embeddings, dtype=torch.float32)
        )

        # 是否冻结 Embedding 层
        if freeze_embeddings:
            self.embedding.weight.requires_grad = False

        # 下游分类头
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,
                            bidirectional=True)
        self.classifier = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, input_ids):
        embeds = self.embedding(input_ids)    # [batch, seq_len, embed_dim]
        lstm_out, (h_n, _) = self.lstm(embeds)
        # 拼接前向和后向最终隐状态
        hidden = torch.cat([h_n[0], h_n[1]], dim=-1)
        return self.classifier(hidden)

# 加载预训练 GloVe 词向量（示例）
# glove_embeddings = load_glove("glove.6B.300d.txt", vocab)
# model = SentimentClassifier(glove_embeddings, freeze_embeddings=False)
```

#### 效果有多大？

预训练词向量带来的提升是显著的。以文本分类任务为例，在小数据集（< 5000样本）上，使用预训练GloVe初始化相比随机初始化通常能提升2-5个百分点的准确率。在命名实体识别等序列标注任务上，提升更加明显，因为稀有实体名往往在标注数据中出现次数很少，预训练词向量提供的语义信息尤为关键。

但这种提升也有明显的上限。当标注数据足够多时（比如WMT翻译的450万句对），预训练词向量的边际收益会迅速递减——因为模型有足够的数据从零学习词的语义。这符合我们的直觉：先验知识在数据稀缺时最有价值。

### 计算机视觉的启示：ImageNet预训练

#### 一个价值百亿的发现

NLP的预训练革命在很大程度上受到了计算机视觉（CV）领域的启发。理解CV中的预训练历史，对于理解NLP的演进至关重要。

2012年，AlexNet在ImageNet图像分类竞赛中取得了突破性的成绩，将top-5错误率从26%降到了16%。这个事件本身已经载入史册。但真正改变CV研究范式的，是随后的一个发现：**在ImageNet上预训练的CNN，其学到的特征可以迁移到几乎所有视觉任务**。

2014年，Donahue等人发表了一篇影响深远的工作（DeCAF），他们做了一个简单但启发性的实验：将ImageNet上预训练的CNN的中间层特征提取出来，直接作为其他视觉任务（如物体检测、场景识别）的输入特征。结果令人惊讶——即使是这种最粗暴的迁移方式，效果也显著优于为每个任务单独训练的模型。

几乎同时，Razavian等人(2014)更进一步证明了"CNN features off-the-shelf"的有效性——预训练CNN特征几乎可以作为通用的视觉特征，应用于任何视觉识别任务。

#### 为什么CV的迁移如此成功？

CV中迁移学习成功的关键原因在于CNN学到的特征有清晰的**层级结构**。

Yosinski等人(2014)发表了一篇经典的实证研究"How transferable are features in deep neural networks?"，系统地分析了CNN每一层特征的可迁移性。他们发现了一个优美的规律：CNN的低层（靠近输入）学习通用的视觉特征，如边缘检测器、颜色斑块、纹理模式。这些特征在所有视觉任务中都有用。中间层学习更抽象的组合特征，如角、轮廓、简单形状。这些特征仍然有较好的通用性。高层（靠近输出）学习任务特定的特征，如"是否是狗的耳朵"。这些特征的通用性最差。

这个发现带来了一个实用的洞察：**迁移时，低层特征可以直接复用，高层特征需要重新学习**。这就是Fine-tuning的理论基础——冻结低层，微调高层。

::: {.callout-note collapse="true"}
## 📖 推荐阅读原图

Yosinski et al. (2014) 论文 "How transferable are features in deep neural networks?" 中的 Figure 2 展示了CNN每一层特征可迁移性的定量分析。图中清晰地显示了从低层（通用特征）到高层（任务特定特征）的梯度过渡，以及在不同层切断迁移时模型性能的变化曲线。建议读者参阅原论文以获得更直观的理解。
:::

#### 数值示例：ImageNet预训练的效果

下面的数据来自Yosinski et al. (2014)的实验，直观展示了预训练的巨大价值：

| 设置 | 准确率 (ImageNet子任务) |
|------|----------------------|
| 从零训练 (baseline) | 63.3% |
| 预训练前3层 + 微调高层 | 64.1% (+0.8%) |
| 预训练前6层 + 微调高层 | 67.7% (+4.4%) |
| 预训练全部层 + 微调全部 | **68.2% (+4.9%)** |

更令人印象深刻的是在**小数据集**上的表现：

| 目标数据量 | 从零训练 | ImageNet预训练+微调 | 提升 |
|-----------|---------|-------------------|------|
| 1000样本 | ~40% | ~70% | **+30%** |
| 5000样本 | ~55% | ~78% | **+23%** |
| 全部数据 | ~76% | ~82% | **+6%** |

规律非常清晰：**数据越少，预训练的价值越大**。这正是NLP最需要的——因为大多数NLP任务的标注数据都很有限。

#### 从CV到NLP的类比

CV的成功经验给NLP提供了一个清晰的蓝图：

| 维度 | 计算机视觉 | 自然语言处理（理想情况） |
|------|-----------|----------------------|
| 预训练数据 | ImageNet（120万张标注图片） | 大规模文本（Wikipedia等） |
| 预训练任务 | 图像分类（1000类） | 语言建模？完形填空？ |
| 预训练模型 | CNN（AlexNet/VGG/ResNet） | RNN/Transformer？ |
| 低层特征 | 边缘、纹理、颜色 | 词义、短语、语法 |
| 高层特征 | 物体部件、类别 | 任务特定（情感、实体等） |
| 迁移方式 | 冻结低层 + 微调高层 | ？ |

这张表中有很多问号——2015年前后的NLP社区确实不知道该如何回答这些问题。预训练任务应该是什么？语言模型吗？翻译？预训练模型应该用什么架构？LSTM？CNN？如何做微调？这些问题的答案将在后续章节逐步揭晓。

### NLP预训练的早期探索

#### Collobert & Weston (2008, 2011)：被低估的先驱

在Word2Vec之前，Ronan Collobert和Jason Weston就已经探索了NLP中的预训练思想。他们在2008年的工作中提出了一个"统一的NLP架构"：用一个共享的底层神经网络同时处理多个NLP任务（词性标注、命名实体识别、语义角色标注等），底层的词表示在所有任务间共享。

2011年，他们发表了更完整的版本"Natural Language Processing (Almost) from Scratch"。这篇论文的核心贡献是证明了：**用无监督方式预训练的词表示（通过语言模型目标），可以显著提升多个NLP任务的性能**，从而大幅减少对手工特征工程的依赖。

这个工作比Word2Vec早了两年，比ELMo早了七年，但当时并没有引起足够的重视。原因可能有几个：当时的硬件条件限制了模型规模，效果提升不够dramatic；深度学习在NLP中还不是主流；论文使用的CNN架构也不如后来的RNN/Transformer流行。但回顾来看，Collobert & Weston的思想是高度前瞻性的。

#### Dai & Le (2015)：序列模型的预训练

2015年，Andrew Dai和Quoc Le发表了一篇看似不起眼却意义深远的论文"Semi-supervised Sequence Learning"。他们做了一件简单的事：先用语言模型目标（或自编码器目标）在无标注数据上预训练一个LSTM，然后在标注数据上微调进行文本分类。

具体来说，他们尝试了两种预训练目标。第一种是**语言模型预训练（LM pre-training）**：训练LSTM预测下一个词，这与后来的GPT思想一脉相承。第二种是**自编码器预训练（SA pre-training）**：训练LSTM将输入序列编码再解码，类似于后来的降噪自编码器。

实验结果表明，预训练LSTM在文本分类任务上相比随机初始化能提升1-3个百分点。虽然提升幅度不算惊人，但这是NLP中第一次系统性地验证了"在无标注文本上预训练整个序列模型"的有效性。更重要的是，这个工作建立了一个关键的概念：**预训练不必局限于Embedding层，可以预训练整个模型**。

然而，这篇论文在当时并未引起广泛关注。可能的原因是提升幅度有限，而且当时NLP社区的注意力主要集中在注意力机制和Seq2Seq架构上（正是第三部分讨论的内容）。预训练的火种需要等到更强的架构（Transformer）和更大的规模才能真正燃烧起来。

#### ULMFiT (2018)：第一个完整的预训练-微调框架

预训练思想在NLP中的真正突破来自Jeremy Howard和Sebastian Ruder在2018年提出的ULMFiT（Universal Language Model Fine-tuning）。这个工作的重要性不在于某个单一的技术创新，而在于它提出了一套**完整的、可操作的预训练-微调流程**。

ULMFiT的框架分为三个阶段：

**第一阶段：通用语言模型预训练**。在大规模通用语料（如Wikitext-103）上训练一个语言模型。这一步学习通用的语言知识——语法、语义、常识。

**第二阶段：目标领域语言模型微调**。在目标任务的无标注数据上继续训练语言模型。例如，如果目标任务是IMDB影评分类，就在IMDB影评（包括未标注的部分）上继续训练语言模型。这一步让模型适应目标领域的词汇和表达习惯。

**第三阶段：目标任务分类器微调**。添加分类头，在标注数据上微调整个模型。

::: {.callout-note collapse="true"}
## 📖 推荐阅读原图

Howard & Ruder (2018) 论文 "Universal Language Model Fine-tuning for Text Classification" 的 Figure 1 完整展示了ULMFiT的三阶段流程图：通用LM预训练 → 领域LM微调 → 任务分类器微调。图中还标注了判别式微调和逐步解冻的具体操作方式。建议读者参阅原论文 (arXiv:1801.06146) 以获得完整的视觉理解。
:::

::: {.callout-note}
## 算法框：ULMFiT 三阶段预训练-微调流程

**输入**：通用语料 $\mathcal{D}_{\text{general}}$（如 Wikitext-103），目标领域无标注数据 $\mathcal{D}_{\text{domain}}$，目标任务标注数据 $\mathcal{D}_{\text{task}} = \{(x_i, y_i)\}$，$L$ 层 LSTM 语言模型

**Stage 1 — 通用语言模型预训练**：在 $\mathcal{D}_{\text{general}}$ 上训练语言模型，最小化
$$\mathcal{L}_{\text{LM}} = -\sum_{t=1}^{T} \log P(w_t \mid w_1, \ldots, w_{t-1}; \theta)$$
得到预训练参数 $\theta_{\text{pre}}$（学习通用语法、语义、常识）

**Stage 2 — 目标领域语言模型微调**：以 $\theta_{\text{pre}}$ 为初始化，在 $\mathcal{D}_{\text{domain}}$ 上继续训练语言模型，使用**判别式学习率**：
$$\theta_l^{(t+1)} = \theta_l^{(t)} - \eta_l \cdot \nabla_{\theta_l} \mathcal{L}_{\text{LM}}, \quad \eta_l = \frac{\eta_L}{2.6^{L-l}}$$
其中 $\eta_L$ 为最高层学习率，低层学习率逐层衰减（适应领域词汇和表达）

**Stage 3 — 目标任务分类器微调**：添加分类头 $f_{\text{cls}}$，使用**逐步解冻**策略：

- Epoch 1：仅训练分类头 $f_{\text{cls}}$（其余层冻结）
- Epoch 2：解冻第 $L$ 层，训练 $f_{\text{cls}}$ + 第 $L$ 层
- Epoch 3：解冻第 $L-1$ 层，训练 $f_{\text{cls}}$ + 第 $L$ 层 + 第 $L-1$ 层
- ……依次解冻直到所有层参与训练

**输出**：适配目标任务的完整模型 $\theta_{\text{task}}$

**数值示例**：假设 3 层 LSTM，$\eta_L = 0.01$。Stage 2 的各层学习率为：

- 第 3 层（最高层）：$\eta_3 = 0.01$
- 第 2 层：$\eta_2 = 0.01 / 2.6 \approx 0.00385$
- 第 1 层（最底层）：$\eta_1 = 0.01 / 2.6^2 \approx 0.00148$

最高层的学习率是最底层的 $2.6^2 \approx 6.76$ 倍——底层通用知识受到更强的保护。
:::

ULMFiT的关键技术贡献在于微调策略的设计。Howard和Ruder发现，粗暴的微调（所有层用相同学习率）效果并不好——底层学到的通用知识可能被破坏。他们提出了两个重要技巧。

第一个是**判别式微调（Discriminative Fine-tuning）**：不同层使用不同的学习率。底层（通用特征）使用较小的学习率以保护已学到的知识，高层（任务特定特征）使用较大的学习率以快速适应新任务。具体地，如果第$L$层的学习率是$\eta_L$，那么第$l$层的学习率是：

$$
\eta_l = \frac{\eta_L}{2.6^{L-l}}
$$

也就是说，每低一层，学习率缩小2.6倍。

第二个是**逐步解冻（Gradual Unfreezing）**：不是一开始就微调所有层，而是从最高层开始逐步解冻。第一个epoch只微调最高层，第二个epoch解冻并微调前两层，以此类推。这进一步保护了底层的通用知识。

ULMFiT的效果是显著的。在6个文本分类基准上，它达到了与从零训练的模型相当甚至更好的性能，但**只需要原来1/10到1/100的标注数据**。在仅有100个标注样本的极端情况下，ULMFiT的性能远超从零训练的基线。

这个工作的意义在于它证明了一个重要的命题：**NLP也可以像CV一样，通过预训练-微调的范式来大幅提升小数据场景下的性能**。它为同年出现的ELMo、GPT和BERT铺平了道路。

### 预训练的三个关键要素

回顾上述发展，我们可以提炼出预训练范式的三个关键要素：

**要素一：预训练任务的设计**。预训练任务决定了模型能学到什么样的知识。Word2Vec学的是词的共现关系，CV用的是图像分类，NLP语言模型学的是序列的概率分布。一个好的预训练任务应该迫使模型理解数据的深层结构，而不只是表面的统计模式。

**要素二：预训练数据的规模与质量**。更多的数据通常意味着更好的预训练效果。ImageNet的120万张标注图片在当时已经是"大规模"了，而NLP使用的Wikipedia、BookCorpus等无标注文本更是几个数量级的飞跃。但数据质量同样重要——噪声过大的数据可能导致预训练模型学到错误的模式。

**要素三：微调策略的设计**。如何在保留预训练知识的同时适应新任务是一个微妙的平衡。学习率太大会"遗忘"预训练知识（灾难性遗忘），学习率太小则适应太慢。ULMFiT的判别式微调和逐步解冻就是在这个平衡上做出的精细调整。

---

## 工程实践

### 使用预训练词向量

在2018年之前（BERT出现之前），使用预训练词向量是NLP中最普遍的迁移学习方式。以下是一个完整的工作流：

```{python}
#| code-fold: false
#| eval: false

import numpy as np
import torch
import torch.nn as nn

def load_glove_embeddings(glove_path, word2idx, embed_dim=300):
    """
    加载预训练 GloVe 词向量

    Args:
        glove_path: GloVe文件路径 (如 'glove.6B.300d.txt')
        word2idx: 词到索引的映射
        embed_dim: 词向量维度

    Returns:
        embedding_matrix: [vocab_size, embed_dim] 的numpy数组
    """
    vocab_size = len(word2idx)
    embedding_matrix = np.random.normal(0, 0.1, (vocab_size, embed_dim))

    found = 0
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            word = parts[0]
            if word in word2idx:
                idx = word2idx[word]
                vector = np.array(parts[1:], dtype=np.float32)
                embedding_matrix[idx] = vector
                found += 1

    coverage = found / vocab_size * 100
    print(f"词向量覆盖率: {found}/{vocab_size} ({coverage:.1f}%)")
    # 典型结果: 覆盖率约 85-95%（取决于词汇表大小和领域）

    return embedding_matrix


def compare_init_strategies(train_data, test_data, word2idx, glove_path):
    """对比随机初始化 vs 预训练初始化的效果"""

    # 策略1: 随机初始化
    model_random = SentimentClassifier(
        pretrained_embeddings=np.random.normal(0, 0.1, (len(word2idx), 300)),
        freeze_embeddings=False
    )

    # 策略2: GloVe初始化 + 冻结
    glove_embeds = load_glove_embeddings(glove_path, word2idx)
    model_frozen = SentimentClassifier(
        pretrained_embeddings=glove_embeds,
        freeze_embeddings=True  # 冻结 Embedding 层
    )

    # 策略3: GloVe初始化 + 微调 (通常效果最好)
    model_finetune = SentimentClassifier(
        pretrained_embeddings=glove_embeds,
        freeze_embeddings=False  # 允许 Embedding 层更新
    )

    # 训练并对比...
    # 典型结果 (SST-2, 5000样本):
    #   随机初始化:      ~78%
    #   GloVe + 冻结:    ~82%
    #   GloVe + 微调:    ~85%
```

### 实验验证：不同策略的效果对比

在典型的文本分类任务上，三种策略的表现如下（以SST-2情感分析为例）：

| 初始化策略 | 1000样本 | 5000样本 | 全部数据(67K) |
|-----------|---------|---------|-------------|
| 随机初始化 | 68.2% | 78.3% | 85.1% |
| GloVe + 冻结 | 74.5% | 82.1% | 85.8% |
| GloVe + 微调 | **76.8%** | **84.7%** | **86.3%** |

值得注意的规律：标注数据越少，预训练词向量的优势越明显。在1000样本时提升近9个百分点，在全部数据时仅提升约1个百分点。

### 实用建议

基于当时（2014-2017年）的最佳实践，以下几条经验法则在使用预训练词向量时非常有用：

**选择词向量**：GloVe(840B)通常优于Word2Vec，因为它在更大的语料上训练且同时考虑了全局和局部统计。维度选择300维是一个好的默认值。

**处理OOV词**：对于预训练词向量未覆盖的词（out-of-vocabulary），不要用零向量——用所有已知词向量的均值加上小噪声来初始化效果更好。

**冻结还是微调**：如果标注数据很少（< 1000样本），建议冻结Embedding层以避免过拟合；如果标注数据充足（> 10000样本），微调通常更好。

---

## 深入理解

> **研究者必读**：这一节探讨迁移学习和预训练的理论基础与开放问题

### 为什么迁移学习有效？——理论视角

迁移学习的理论基础可以从几个角度来理解。

#### 域适应理论

Ben-David等人(2010)提出了一个形式化的域适应理论框架。给定源域分布$\mathcal{D}_S$和目标域分布$\mathcal{D}_T$，在目标域上的误差$\epsilon_T(h)$可以被以下不等式约束：

$$
\epsilon_T(h) \leq \epsilon_S(h) + d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_S, \mathcal{D}_T) + \lambda
$$

其中$\epsilon_S(h)$是在源域上的误差，$d_{\mathcal{H}\Delta\mathcal{H}}$是两个域之间的"$\mathcal{H}$-散度"（衡量两个分布有多不同），$\lambda$是一个与最优联合假设相关的不可约误差。

这个不等式揭示了迁移学习成功的三个条件：源域表现好（$\epsilon_S(h)$小）、源域和目标域足够接近（$d$小）、存在一个在两个域上都表现好的假设（$\lambda$小）。

对于NLP预训练来说，源任务是语言建模（在大规模文本上），目标任务是具体的NLP任务（如情感分析）。两者共享同一种语言的基本结构，因此$d$相对较小；而$\lambda$取决于语言理解能力与具体任务之间的关联程度——对于大多数NLP任务，语言理解是基础，所以$\lambda$也较小。

#### 特征层级假说

Yosinski等人(2014)的实验为迁移学习提供了一个实证理论：神经网络的特征呈层级结构，从低层的通用特征逐渐过渡到高层的任务特定特征。这个"特征层级假说"在CV中已经得到充分验证，在NLP中也有类似的证据。

对于语言模型来说，低层可能学习词的语义和常见短语模式，中层可能学习句法结构和语义组合，高层可能学习更抽象的语篇结构和推理模式。预训练相当于为这些层级提供了一个好的初始化点，微调只需要在这个基础上做微调即可。

### 为什么NLP的预训练比CV更困难？

虽然CV的成功给NLP提供了巨大的启发，但NLP的预训练面临着几个独特的挑战。

**第一，缺少像ImageNet那样的"黄金标注数据集"**。ImageNet有120万张人工标注的图片，提供了强监督信号。NLP没有这样的资源——我们只有无标注的原始文本。这意味着NLP的预训练必须是**自监督的**（self-supervised），从文本本身构造训练信号。

**第二，语言的离散性增加了建模难度**。图像是连续的——像素值的微小变化不会改变语义。但语言是离散的——"cat"变成"bat"只差一个字母，语义却完全不同。这意味着语言的表示空间更加复杂，需要模型学习更精细的特征。

**第三，语言的组合性要求更深的理解**。图像中的物体相对独立——识别一只猫不需要理解图片的全局结构。但语言高度依赖上下文——"bank"可以是银行也可以是河岸，取决于上下文。这要求NLP的预训练模型必须捕获上下文依赖，而不仅仅是词级别的统计。

这些挑战解释了为什么NLP的预训练革命比CV晚了几年，也解释了为什么最终成功的方案（ELMo、GPT、BERT）都采用了上下文相关的模型，而非简单的词向量。

### 开放研究问题（2017年视角）

站在2017年的时间节点——Transformer刚刚发表，ELMo和BERT还未出现——NLP预训练面临着几个核心的开放问题。

**预训练任务的最优选择是什么？** 语言建模（预测下一个词）是一个自然的选择，但它是最好的吗？有没有其他自监督任务能让模型学到更好的语言知识？这个问题在BERT的MLM目标和后续的对比学习中得到了进一步探索。

**预训练模型的最优架构是什么？** LSTM？CNN？Transformer？2017年时还不清楚哪种架构最适合做NLP预训练的"骨架"。后来的事实证明，Transformer凭借其强大的表达能力和可扩展性成为了胜出者。

**微调策略如何优化？** ULMFiT的判别式微调和逐步解冻是一个好的开始，但还有没有更好的方法？如何避免灾难性遗忘？如何在保留通用知识和适应新任务之间取得最优平衡？

这些问题将在后续章节中逐步得到解答。

---

## 局限性与未解决的问题

### 静态词向量的根本缺陷

Word2Vec和GloVe预训练的词向量有一个致命的问题：**每个词只有一个固定的向量表示，无论它出现在什么上下文中**。

考虑以下两个句子：

- "I went to the **bank** to deposit money."（银行）
- "I sat on the **bank** of the river."（河岸）

在Word2Vec/GloVe中，这两个"bank"共享同一个向量。这意味着模型无法区分"bank"在不同语境中的不同含义。这个问题被称为**一词多义（polysemy）问题**。

这不是一个罕见的边缘情况。英语中最常用的1000个词中，绝大多数都有多个含义。"run"可以是跑步、运行、竞选、流淌……，"set"的含义更是多达数百个。静态词向量将所有含义压缩成一个向量，不可避免地损失了信息。

![静态词向量（左）vs 上下文词向量（右）："bank"一词在不同语境中的表示方式。静态词向量中，无论上下文如何，"bank"只有唯一的向量；上下文词向量则根据语境生成不同的表示。](figures/chapter-10/fig-static-vs-contextual.png){#fig-static-vs-contextual width=95%}

::: {.figure-caption}
*作者绘制。概念示意图，展示 Word2Vec/GloVe 的静态表示与 ELMo/BERT 的上下文表示之间的区别。*
:::

### 浅层迁移的局限

使用预训练词向量本质上是一种**浅层迁移**——只迁移了Embedding层（模型的第一层），而模型的其他层（LSTM、注意力层、分类头等）仍然从零训练。

这意味着预训练只提供了**词级别**的知识，而更高层次的知识——句法结构、语义组合、篇章逻辑——完全依赖于下游任务的标注数据来学习。在数据稀缺的场景下，这些高层知识往往学不好。

一个直观的类比是：使用预训练词向量就像给学生发了一本词典，然后让他直接参加阅读理解考试。词典确实有帮助——至少他认识了大部分词。但"认识词"和"理解文章"之间还有巨大的鸿沟。我们真正需要的不是一本更好的词典，而是一个"读过大量文章"的学生，他已经内化了阅读理解的各种技能。

### 预训练与下游任务的脱节

Word2Vec/GloVe的预训练目标（预测共现词）和下游任务的目标（如情感分类）之间存在明显的gap。预训练学到的是"哪些词经常一起出现"，但这不等于"哪些词表达了正面情感"。虽然两者有关联——"excellent"和"amazing"经常出现在相似的上下文中，所以它们的向量会相似——但这种关联是间接的、不完美的。

### 这些局限导向了什么？

上述三个局限指向了一个共同的方向：**我们需要能够生成上下文相关的、深层的语言表示的预训练方法**。

静态词向量的问题呼唤**上下文词向量**——同一个词在不同上下文中应该有不同的表示。这正是第11章ELMo要解决的问题。

浅层迁移的问题呼唤**整个模型的预训练**——不仅预训练Embedding层，还要预训练所有的隐藏层，让模型在预训练阶段就学习句法、语义、推理等高层知识。这是第12章GPT和第13章BERT的核心思想。

预训练-下游任务脱节的问题呼唤**更强的预训练目标**——预训练任务应该迫使模型理解语言的深层结构，而不仅仅是词的共现统计。掩码语言模型（BERT的MLM）和因果语言模型（GPT的CLM）就是在这个方向上的重大突破。

> 下一章预告：第11章将介绍ELMo——第一个生成上下文相关词向量的预训练模型。ELMo用双向LSTM在大规模文本上训练语言模型，为每个词生成依赖上下文的动态表示，标志着"深层预训练"时代的开端。

---

## 本章小结

### 核心要点回顾

这一章我们追溯了预训练思想的起源，建立了理解后续章节（ELMo、GPT、BERT）所需的概念基础。

核心问题是如何克服NLP中标注数据稀缺的瓶颈——每个任务都从零学习语言知识既低效又浪费。核心洞察是迁移学习：先在大规模无标注数据上学习通用的语言知识（预训练），然后在少量标注数据上适配具体任务（微调）。

我们看到了这个思想的三个演进阶段：Word2Vec的词向量预训练是最浅层的迁移，只提供词级别的语义知识；CV领域的ImageNet预训练证明了深层模型迁移的巨大潜力，为NLP提供了蓝图；ULMFiT提出了第一个完整的预训练-微调框架，证明NLP也可以像CV一样从预训练中获得巨大收益。

最终，Word2Vec预训练的三大局限——静态表示、浅层迁移、任务脱节——指向了一个共同的方向：我们需要上下文相关的、深层的、端到端的预训练方法。这正是后续章节的主题。

### 关键概念速查

| 概念 | 定义 |
|------|------|
| 迁移学习 | 在源任务上学到的知识应用到目标任务 |
| 预训练 | 在大规模数据上训练模型的第一阶段 |
| 微调 | 在目标任务数据上适配预训练模型的第二阶段 |
| 特征提取 | 冻结预训练模型，用其输出作为下游特征 |
| 判别式微调 | 不同层使用不同学习率（ULMFiT） |
| 逐步解冻 | 从高层到低层逐步开放训练（ULMFiT） |
| 灾难性遗忘 | 微调时丢失预训练学到的知识 |

### 思考题

1. **[概念理解]** 为什么说Word2Vec是"浅层"的预训练？如果模型有10层，Word2Vec只预训练了哪一层？其他层的知识来自哪里？

2. **[对比分析]** 比较NLP和CV中的预训练条件。为什么CV的ImageNet预训练使用的是有监督学习（图像分类），而NLP的预训练必须走自监督路线？如果NLP也有一个像ImageNet一样的大规模标注数据集，情况会不同吗？

3. **[数学推导]** ULMFiT的判别式微调中，如果最高层学习率为$\eta_L = 0.01$，衰减因子为2.6，模型有6层，计算每一层的学习率。最底层和最高层的学习率相差多少倍？

4. **[工程实践]** 在一个只有500条标注样本的文本分类任务上，分别使用(a)随机初始化、(b)GloVe冻结、(c)GloVe微调三种策略训练模型，对比效果。尝试改变标注样本数量（100, 500, 2000, 10000），观察预训练优势如何随数据量变化。

5. **[研究思考]** Dai & Le (2015) 的语言模型预训练在2015年效果平平，但同样的思想到了2018年（GPT）就大获成功。你认为这其中的关键差异是什么？是架构（LSTM vs Transformer）？是规模（数据和模型大小）？还是微调策略？

---

## 延伸阅读

### 核心论文（必读）

**Mikolov et al. (2013). "Efficient Estimation of Word Representations in Vector Space"** 和 **"Distributed Representations of Words and Phrases and their Compositionality"**。Word2Vec的两篇原始论文，虽然第3章已经介绍过技术细节，但从预训练的角度重新阅读会有新的收获。重点关注Skip-gram的训练目标如何从无标注数据中提取语义信息。

**Howard & Ruder (2018). "Universal Language Model Fine-tuning for Text Classification" (ULMFiT)**。第一个完整的NLP预训练-微调框架。重点阅读Section 3的判别式微调和逐步解冻技巧，以及Section 4的消融实验。arXiv:1801.06146。

### 理论基础

**Ben-David et al. (2010). "A theory of learning from different domains"**。域适应的理论框架，是理解迁移学习"为什么有效"的数学基础。重点阅读$\mathcal{H}$-散度的定义和上界定理。

**Yosinski et al. (2014). "How transferable are features in deep neural networks?"**。CV中特征可迁移性的经典实证研究。虽然是CV论文，但其发现的"低层通用、高层特定"的规律对NLP同样有启发。

### 先驱工作

**Collobert & Weston (2008/2011). "A Unified Architecture for Natural Language Processing" / "Natural Language Processing (Almost) from Scratch"**。NLP预训练的最早探索者之一。虽然效果不如后来的方法，但思想高度前瞻。

**Dai & Le (2015). "Semi-supervised Sequence Learning"**。第一篇系统性验证"预训练整个序列模型"的论文。arXiv:1511.01432。重点关注语言模型预训练 vs 自编码器预训练的对比。

### 后续发展

**Peters et al. (2018). "Deep contextualized word representations" (ELMo)**。解决静态词向量问题的第一个重要突破——下一章的主题。

**Radford et al. (2018). "Improving Language Understanding by Generative Pre-Training" (GPT)**。用Transformer Decoder进行自回归预训练——第12章的主题。

**Devlin et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers" (BERT)**。用Transformer Encoder进行双向预训练——第13章的主题。

### 综述与教程

**Ruder (2019). "Neural Transfer Learning for Natural Language Processing"**。Sebastian Ruder的博士论文，是NLP迁移学习最全面的综述，覆盖了从Word2Vec到BERT的完整演进。

---

## 历史注脚

NLP预训练的历史有一个有趣的"错过"。Dai & Le在2015年就已经验证了语言模型预训练的有效性，但当时的效果不够惊艳。如果他们使用Transformer（2017年才发明）而非LSTM，或者在更大的数据上训练，结果可能会大不相同。这提醒我们：好的想法需要在合适的时机、配合合适的技术条件才能发挥最大价值。

另一个值得思考的历史细节是：CV的迁移学习用的是有监督预训练（ImageNet的120万张标注图片），而NLP最终走向了自监督预训练（无标注文本）。这不是NLP社区主动的选择，而是被迫的——NLP没有ImageNet那样规模的标注数据集。但这个"被迫"的选择最终被证明是更好的路线：自监督预训练可以利用几乎无限的无标注数据，规模化的潜力远超有监督预训练。到了2020年代，CV社区反过来开始学习NLP的自监督预训练方式（如MAE、DINO等）。

ULMFiT的作者Jeremy Howard是一个非常有趣的人物。他不是传统的学术界人士，而是来自竞赛和应用背景（Kaggle冠军、fast.ai创始人）。他和Sebastian Ruder的这篇论文用相对简单的LSTM架构，通过精心设计的微调策略，取得了与同期的ELMo和OpenAI GPT相当的效果。这个故事告诉我们：有时候，工程上的精细调整和研究上的概念创新一样重要。
