---
title: "Bayesian Inference Primer — Beta–Binomial"
description: LaTeX demo with conjugacy, posterior, and predictive for Bernoulli data.
date: 2025-11-13
categories: [bayes, probability, math]
---

We briefly illustrate Bayesian inference for Bernoulli data using a Beta prior. Let observations be $y_1,\dots,y_n \in \{0,1\}$ with $y_i \sim \operatorname{Bernoulli}(p)$ and prior $p \sim \operatorname{Beta}(\alpha,\beta)$.

## Bayes’ Rule and Likelihood

Bayes’ rule states $p(p\mid y) \propto p(y\mid p)\,p(p)$. For $k = \sum_i y_i$, the likelihood is

$$
p(y\mid p) \;=\; p^{k}(1-p)^{n-k}.
$$

## Posterior and Moments

Using Beta–Binomial conjugacy, the posterior is

$$
p(p\mid y) \;=\; \operatorname{Beta}(\alpha + k,\; \beta + n - k),
$$

with posterior mean

$$
\mathbb{E}[p\mid y] \;=\; \frac{\alpha + k}{\alpha + \beta + n}.
$$

The posterior predictive for a new label $\tilde y$ has

$$
\Pr(\tilde y = 1 \mid y) \;=\; \mathbb{E}[p\mid y] 
\;=\; \frac{\alpha + k}{\alpha + \beta + n}.
$$

## Minimal Code Example

```python
def beta_binomial_posterior(alpha, beta, k, n):
    post_a = alpha + k
    post_b = beta + (n - k)
    mean = post_a / (post_a + post_b)
    return post_a, post_b, mean

# Example: prior Beta(1, 1), observations with k=7 successes out of n=10
pa, pb, pm = beta_binomial_posterior(1.0, 1.0, k=7, n=10)
print(f"Posterior: Beta({pa:.1f}, {pb:.1f})  mean={pm:.3f}")
```

## Summary

This compact primer uses inline math (e.g., $k, n, \alpha, \beta$) and block equations for conjugacy. The same `.qmd` can be rendered to HTML and exported to PDF.

